<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[maven之settings.xml]]></title>
    <url>%2Fmaven%E4%B9%8Bsettings-xml%2F</url>
    <content type="text"><![CDATA[[TOC] 0 参考 https://www.cnblogs.com/wdliu/p/8312543.html 1 settings.xml详解1.1 声明规范123&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; 1.2 localRepository12&lt;!-- 本地仓库的路径。默认值为 --&gt;&lt;localRepository&gt;/opt/repository&lt;/localRepository&gt; 1.3 interactiveMode12&lt;!--Maven是否需要和用户交互以获得输入。如果Maven需要和用户交互以获得输入，则设置成true，反之则应为false。默认为true。--&gt;&lt;interactiveMode&gt;true&lt;/interactiveMode&gt; 1.4 usePluginRegistry12&lt;!--Maven是否需要使用plugin-registry.xml文件来管理插件版本。如果需要让Maven使用文件来管理插件版本，则设为true。默认为false。--&gt;&lt;usePluginRegistry&gt;false&lt;/usePluginRegistry&gt; 1.5 offline12&lt;!--表示Maven是否需要在离线模式下运行。如果构建系统需要在离线模式下运行，则为true，默认为false。当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 --&gt;&lt;offline&gt;false&lt;/offline&gt; 1.6 pluginGroups12345&lt;!--当插件的组织Id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表。该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。默认情况下该列表包含了org.apache.maven.plugins和org.codehaus.mojo --&gt;&lt;pluginGroups&gt; &lt;!--plugin的组织Id（groupId） --&gt; &lt;pluginGroup&gt;org.codehaus.mojo&lt;/pluginGroup&gt;&lt;/pluginGroups&gt; 1.7 proxies12345678910111213141516171819202122&lt;!--用来配置不同的代理，多代理profiles 可以应对笔记本或移动设备的工作环境：通过简单的设置profile id就可以很容易的更换整个代理配置。 --&gt; &lt;proxies&gt; &lt;!--代理元素包含配置代理时需要的信息--&gt; &lt;proxy&gt; &lt;!--代理的唯一定义符，用来区分不同的代理元素。--&gt; &lt;id&gt;myproxy&lt;/id&gt; &lt;!--该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 --&gt; &lt;active&gt;true&lt;/active&gt; &lt;!--代理的协议。 协议://主机名:端口，分隔成离散的元素以方便配置。--&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;!--代理的主机名。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;host&gt;proxy.somewhere.com&lt;/host&gt; &lt;!--代理的端口。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;port&gt;8080&lt;/port&gt; &lt;!--代理的用户名，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;!--代理的密码，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;password&gt;somepassword&lt;/password&gt; &lt;!--不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。--&gt; &lt;nonProxyHosts&gt;*.google.com|ibiblio.org&lt;/nonProxyHosts&gt; &lt;/proxy&gt; &lt;/proxies&gt; 1.8 servers1234567891011121314151617181920&lt;!--配置服务端的一些设置。一些设置如安全证书不应该和pom.xml一起分发。这种类型的信息应该存在于构建服务器上的settings.xml文件中。--&gt; &lt;servers&gt; &lt;!--服务器元素包含配置服务器时需要的信息 --&gt; &lt;server&gt; &lt;!--这是server的id（注意不是用户登陆的id），该id与distributionManagement中repository元素的id相匹配。--&gt; &lt;id&gt;server001&lt;/id&gt; &lt;!--鉴权用户名。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。 --&gt; &lt;username&gt;my_login&lt;/username&gt; &lt;!--鉴权密码 。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。密码加密功能已被添加到2.1.0 +。详情请访问密码加密页面--&gt; &lt;password&gt;my_password&lt;/password&gt; &lt;!--鉴权时使用的私钥位置。和前两个元素类似，私钥位置和私钥密码指定了一个私钥的路径（默认是$&#123;user.home&#125;/.ssh/id_dsa）以及如果需要的话，一个密语。将来passphrase和password元素可能会被提取到外部，但目前它们必须在settings.xml文件以纯文本的形式声明。 --&gt; &lt;privateKey&gt;$&#123;usr.home&#125;/.ssh/id_dsa&lt;/privateKey&gt; &lt;!--鉴权时使用的私钥密码。--&gt; &lt;passphrase&gt;some_passphrase&lt;/passphrase&gt; &lt;!--文件被创建时的权限。如果在部署的时候会创建一个仓库文件或者目录，这时候就可以使用权限（permission）。这两个元素合法的值是一个三位数字，其对应了unix文件系统的权限，如664，或者775。 --&gt; &lt;filePermissions&gt;664&lt;/filePermissions&gt; &lt;!--目录被创建时的权限。 --&gt; &lt;directoryPermissions&gt;775&lt;/directoryPermissions&gt; &lt;/server&gt; &lt;/servers&gt; 1.9 mirrors1234567891011121314&lt;!--为仓库列表配置的下载镜像列表。高级设置请参阅镜像设置页面 --&gt; &lt;mirrors&gt; &lt;!--给定仓库的下载镜像。 --&gt; &lt;mirror&gt; &lt;!--该镜像的唯一标识符。id用来区分不同的mirror元素。 --&gt; &lt;id&gt;planetmirror.com&lt;/id&gt; &lt;!--镜像名称 --&gt; &lt;name&gt;PlanetMirror Australia&lt;/name&gt; &lt;!--该镜像的URL。构建系统会优先考虑使用该URL，而非使用默认的服务器URL。 --&gt; &lt;url&gt;http://downloads.planetmirror.com/pub/maven2&lt;/url&gt; &lt;!--被镜像的服务器的id。例如，如果我们要设置了一个Maven中央仓库（http://repo.maven.apache.org/maven2/）的镜像，就需要将该元素设置成central。这必须和中央仓库的id central完全一致。--&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; 1.10 profiles123456&lt;!--根据环境参数来调整构建配置的列表。settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。它包含了id，activation, repositories, pluginRepositories和 properties元素。这里的profile元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是settings.xml文件的角色定位），而非单独的项目对象模型设置。如果一个settings中的profile被激活，它的值会覆盖任何其它定义在POM中或者profile.xml中的带有相同id的profile。 --&gt; &lt;profiles&gt; &lt;!--根据环境参数来调整的构件的配置--&gt; &lt;profile&gt; &lt;!--该配置的唯一标识符。 --&gt; &lt;id&gt;test&lt;/id&gt; 1.11 Activation1234567891011121314151617181920212223242526272829303132&lt;!--自动触发profile的条件逻辑。Activation是profile的开启钥匙。如POM中的profile一样，profile的力量来自于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。activation元素并不是激活profile的唯一方式。settings.xml文件中的activeProfile元素可以包含profile的id。profile也可以通过在命令行，使用-P标记和逗号分隔的列表来显式的激活（如，-P test）。--&gt; &lt;activation&gt; &lt;!--profile默认是否激活的标识--&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK。--&gt; &lt;jdk&gt;1.5&lt;/jdk&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。--&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows') --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本--&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;name&#125;引用），其拥有对应的name = 值，Profile就会被激活。如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段--&gt; &lt;property&gt; &lt;!--激活profile的属性的名称--&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。--&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;$&#123;basedir&#125;/file2.properties&lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。--&gt; &lt;missing&gt;$&#123;basedir&#125;/file1.properties&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; 1.12 Repositories1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;local private nexus&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--远程仓库列表，它是Maven用来填充构建系统本地仓库所使用的一组远程项目。 --&gt; &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!--远程仓库唯一标识--&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;!--远程仓库名称 --&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;!--如何处理远程仓库里发布版本的下载--&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做-ignore（忽略），fail（失败），或者warn（警告）。--&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!--如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素--&gt; &lt;snapshots&gt; &lt;enabled/&gt;&lt;updatePolicy/&gt;&lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;!--用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表。仓库是两种主要构件的家。第一种构件被用作其它构件的依赖。这是中央仓库中存储的大部分构件类型。另外一种构件类型是插件。Maven插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories元素的结构和repositories元素的结构类似。每个pluginRepository元素指定一个Maven可以用来寻找新插件的远程地址。--&gt; &lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见profiles/profile/repositories/repository元素的说明--&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled/&gt;&lt;updatePolicy/&gt;&lt;checksumPolicy/&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled/&gt;&lt;updatePolicy/&gt;&lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;id/&gt;&lt;name/&gt;&lt;url/&gt;&lt;layout/&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; 1.13 activeProfiles123456789101112默认配置： &lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt; &lt;!-- &lt;activeProfile&gt;devitcast&lt;/activeProfile&gt;--&gt; &lt;/activeProfiles&gt; 配置解释： &lt;!--手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组activeProfile元素，每个activeProfile都含有一个profile id。任何在activeProfile中定义的profile id，不论环境设置如何，其对应的profile都会被激活。如果没有匹配的profile，则什么都不会发生。例如，env-test是一个activeProfile，则在pom.xml（或者profile.xml）中对应id的profile会被激活。如果运行过程中找不到这样一个profile，Maven则会像往常一样运行 --&gt; &lt;activeProfiles&gt; &lt;!-- --&gt; &lt;activeProfile&gt;env-test&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/settings&gt; 2 示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: $&#123;user.home&#125;/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;/opt/repository/maven&lt;/localRepository&gt; &lt;!-- interactiveMode | This will determine whether maven prompts you when it needs input. If set to false, | maven will use a sensible default value, perhaps based on some other setting, for | the parameter in question. | | Default: true &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; --&gt; &lt;!-- offline | Determines whether maven should attempt to connect to the network when executing a build. | This will have an effect on artifact downloads, artifact deployment, and others. | | Default: false &lt;offline&gt;false&lt;/offline&gt; --&gt; &lt;!-- pluginGroups | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e. | when invoking a command line like "mvn prefix:goal". Maven will automatically add the group identifiers | "org.apache.maven.plugins" and "org.codehaus.mojo" if these are not already contained in the list. |--&gt; &lt;pluginGroups&gt; &lt;!-- pluginGroup | Specifies a further group identifier to use for plugin lookup. &lt;pluginGroup&gt;com.your.plugins&lt;/pluginGroup&gt; --&gt; &lt;/pluginGroups&gt; &lt;!-- proxies | This is a list of proxies which can be used on this machine to connect to the network. | Unless otherwise specified (by system property or command-line switch), the first proxy | specification in this list marked as active will be used. |--&gt; &lt;proxies&gt; &lt;!-- proxy | Specification for one proxy, to be used in connecting to the network. | &lt;proxy&gt; &lt;id&gt;optional&lt;/id&gt; &lt;active&gt;true&lt;/active&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;password&gt;proxypass&lt;/password&gt; &lt;host&gt;proxy.host.net&lt;/host&gt; &lt;port&gt;80&lt;/port&gt; &lt;nonProxyHosts&gt;local.net|some.host.com&lt;/nonProxyHosts&gt; &lt;/proxy&gt; --&gt; &lt;/proxies&gt; &lt;!-- servers | This is a list of authentication profiles, keyed by the server-id used within the system. | Authentication profiles can be used whenever maven must make a connection to a remote server. |--&gt; &lt;servers&gt; &lt;!-- server | Specifies the authentication information to use when connecting to a particular server, identified by | a unique name within the system (referred to by the 'id' attribute below). | | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are | used together. | &lt;server&gt; &lt;id&gt;deploymentRepo&lt;/id&gt; &lt;username&gt;repouser&lt;/username&gt; &lt;password&gt;repopwd&lt;/password&gt; &lt;/server&gt; --&gt; &lt;!-- Another sample, using keys to authenticate. &lt;server&gt; &lt;id&gt;siteServer&lt;/id&gt; &lt;privateKey&gt;/path/to/private/key&lt;/privateKey&gt; &lt;passphrase&gt;optional; leave empty if not used.&lt;/passphrase&gt; &lt;/server&gt; --&gt; &lt;server&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;anonymous123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;anonymous123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;thirdparty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;anonymous123&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!-- mirrors | This is a list of mirrors to be used in downloading artifacts from remote repositories. | | It works like this: a POM may declare a repository to use in resolving certain artifacts. | However, this repository may have problems with heavy traffic at times, so people have mirrored | it to several places. | | That repository definition will have a unique id, so we can create a mirror reference for that | repository, to be used as an alternate download site. The mirror site will be the preferred | server for that repository. |--&gt; &lt;mirrors&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;internal nexus repository&lt;/name&gt; &lt;url&gt;http://10.0.0.241:9999/repository/maven-public/&lt;/url&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;!-- profiles | This is a list of profiles which can be activated in a variety of ways, and which can modify | the build process. Profiles provided in the settings.xml are intended to provide local machine- | specific paths and repository locations which allow the build to work in the local environment. | | For example, if you have an integration testing plugin - like cactus - that needs to know where | your Tomcat instance is installed, you can provide a variable here such that the variable is | dereferenced during the build process to configure the cactus plugin. | | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles | section of this document (settings.xml) - will be discussed later. Another way essentially | relies on the detection of a system property, either matching a particular value for the property, | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'. | Finally, the list of active profiles can be specified directly from the command line. | | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact | repositories, plugin repositories, and free-form properties to be used as configuration | variables for plugins in the POM. | |--&gt; &lt;profiles&gt; &lt;!-- profile | Specifies a set of introductions to the build process, to be activated using one or more of the | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/&gt; | or the command line, profiles have to have an ID that is unique. | | An encouraged best practice for profile identification is to use a consistent naming convention | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc. | This will make it more intuitive to understand what the set of introduced profiles is attempting | to accomplish, particularly when you only have a list of profile id's for debug. | | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo. &lt;profile&gt; &lt;id&gt;jdk-1.4&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.4&lt;/jdk&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jdk14&lt;/id&gt; &lt;name&gt;Repository for JDK 1.4 builds&lt;/name&gt; &lt;url&gt;http://www.myhost.com/maven/jdk14&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshotPolicy&gt;always&lt;/snapshotPolicy&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; --&gt; &lt;!-- | Here is another profile, activated by the system property 'target-env' with a value of 'dev', | which provides a specific path to the Tomcat instance. To use this, your plugin configuration | might hypothetically look like: | | ... | &lt;plugin&gt; | &lt;groupId&gt;org.myco.myplugins&lt;/groupId&gt; | &lt;artifactId&gt;myplugin&lt;/artifactId&gt; | | &lt;configuration&gt; | &lt;tomcatLocation&gt;$&#123;tomcatPath&#125;&lt;/tomcatLocation&gt; | &lt;/configuration&gt; | &lt;/plugin&gt; | ... | | NOTE: If you just wanted to inject this configuration whenever someone set 'target-env' to | anything, you could just leave off the &lt;value/&gt; inside the activation-property. | &lt;profile&gt; &lt;id&gt;env-dev&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;target-env&lt;/name&gt; &lt;value&gt;dev&lt;/value&gt; &lt;/property&gt; &lt;/activation&gt; &lt;properties&gt; &lt;tomcatPath&gt;/path/to/tomcat/instance&lt;/tomcatPath&gt; &lt;/properties&gt; &lt;/profile&gt; --&gt; &lt;profile&gt; &lt;id&gt;apatch&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;re-apatch&lt;/id&gt; &lt;name&gt;remote apatch&lt;/name&gt; &lt;url&gt;http://inno.starhubdev.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;ibiblio&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;re-ibiblio&lt;/id&gt; &lt;name&gt;remote ibiblio&lt;/name&gt; &lt;url&gt;https://nexus.sourcesense.com/nexus/content/repositories/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;redev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;re-nexus&lt;/id&gt; &lt;name&gt;remote nexus&lt;/name&gt; &lt;url&gt;http://repository.sonatype.org/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://10.0.0.241:9999/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://10.0.0.241:9999/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!-- activeProfiles | List of profiles that are active for all builds. | &lt;activeProfiles&gt; &lt;activeProfile&gt;alwaysActiveProfile&lt;/activeProfile&gt; &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;/activeProfile&gt; &lt;/activeProfiles&gt; --&gt; &lt;/settings&gt; 3 私有仓库配置浅析基本逻辑 pom&gt;启用的profile&gt;maven原有配置 mirror配置mirrorOf和id匹配优先 简单maven配置 一般大家的配置（略去无关私有仓库配置）都是这样的 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;mvn.xxx.com&lt;/name&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;url&gt;http://mvn.xxx.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://mvn.xxx.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;alibaba&lt;/id&gt; &lt;url&gt;http://code.alibabatech.com/mvn/releases/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://mvn.xxx.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt;&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 3.1 mirrors这个标签重要的属性包括id、mirrorOf。id用来唯一区分。mirrorOf用来关联repository。url用来表示私服地址。mirrorOf常见大家配置成*、central、repo啥的 id 唯一标识 mirrorOf 指定镜像的规则。就是什么情况会从镜像仓库拉取，而不是从原本的仓库拉取可选项参考链接: * 匹配所有 external:* 除了本地缓存之后的所有仓库 repo,repo1 repo 或者 repo1。 这里repo指的是仓库的id，下文会提到 *,!repo1 除了repo1的所有仓库 name 名称描述 url 地址 3.2 profile这个就简单说下吧，就是算是个配置，可以配多个，具体哪个生效可以通过mvn命令指定，或者配置 3.3 repositories这里面算是配置的重点 12345678910&lt;repository&gt; &lt;id&gt;alibaba&lt;/id&gt; &lt;url&gt;http://code.alibabatech.com/mvn/releases/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt;&lt;/repository&gt; 几个重要的配置，一目了然吧，id标识，url地址，是否从该仓库下release，是否从该仓库下快照版本。 这里就有人会懵逼了，这里怎么又配了个地址，跟mirrors里面的地址哪个生效呢？ 好的，那咱们试试。先规定一下配置： 12345678910111213141516&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;mvn.ws.netease.com&lt;/name&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;url&gt;http://mvn.xxx.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://mvn.ccc.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 把地址区分下，mirror里配成xxx，repository配成ccc随便找一个项目，设定一个不存在的依赖，mvn -U compile下： 可以发现去ccc找了。说明repository里的生效了。 那么mirror里的地址什么时候生效呢？其实刚才说了，mirror里的是靠mirrorOf中的内容和repository中id关联的。比如我们把刚才配置改为 12345678910111213141516&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;mvn.ws.netease.com&lt;/name&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;url&gt;http://mvn.xxx.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://mvn.ccc.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 把repository中的id改成central 这样就行了。此外mirrorOf中可以配置通配符，例如*，表示任何repository都和这个关联。 其实简单来说就是如果repository的id能和mirrorOf关联上，那么url以mirror的为准，否则以repository中自己的url为准。 其他还有一些点，repositories中可以配置多个repository，配置多个话，一个找不到会找下一个，比如我们在刚才基础上加上阿里的配置 123456789101112131415161718&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://mvn.ccc.com/nexus/content/groups/t_repo_group/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;alibaba&lt;/id&gt; &lt;url&gt;http://code.alibabatech.com/mvn/releases/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; 在构建一次： 3.4 总结咱们在回顾下起初的配置，可以看到启用的profile是dev，dev中的repository的id是nexus，跟mirrorOf没有匹配，那么生效的配置就是repository中自己的url配置，所以这里完全可以省略掉mirror的配置。当然如果多个profile公用一个私服地址，也可以指定mirror地址，然后repository中的id指定成和mirrorOf相同，同时可以省略掉自己标签中url。 此外还有几个点要说，pluginRepositories，配置信息基本和repository一致，不过这个地址是用来下maven的插件的，就是pom中这样的配置 12345678910111213141516171819&lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;configuration&gt; &lt;filteringDeploymentDescriptors&gt;true&lt;/filteringDeploymentDescriptors&gt; &lt;webResources&gt; &lt;resource&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/webapp/WEB-INF&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;targetPath&gt;WEB-INF&lt;/targetPath&gt; &lt;includes&gt; &lt;include&gt;**&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/webResources&gt; &lt;/configuration&gt; &lt;/plugin&gt;&lt;/plugins&gt; 还有，pom也可以指定repository: 这样配置会和settings.xml中生效的配置合并，并优先从这个库找，找不到继续走settings.xml配置。 3.5 多仓库配置第三方包，自己公司的包等除了手动install:install-file导入之外，最好的办法就是搭建自己公司的私有仓库，这里推荐使用nexus, 这样除了中央仓库之外就需要设置自己的仓库了 设置多仓库有2个方法: pom设置(java的pom文件) 12345678910111213141516&lt;project&gt;... &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;my-repo1&lt;/id&gt; &lt;name&gt;your custom repo&lt;/name&gt; &lt;url&gt;http://jarsm2.dyndns.dk&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;my-repo2&lt;/id&gt; &lt;name&gt;your custom repo&lt;/name&gt; &lt;url&gt;http://jarsm2.dyndns.dk&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt;...&lt;/project&gt; setting设置(${user.home}/.m2/settings.xml) 1234567891011121314151617181920212223&lt;settings&gt; ... &lt;profiles&gt; ... &lt;profile&gt; &lt;id&gt;myprofile&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;my-repo2&lt;/id&gt; &lt;name&gt;your custom repo&lt;/name&gt; &lt;url&gt;http://jarsm2.dyndns.dk&lt;/url&gt; &lt;/repository&gt; ... &lt;/repositories&gt; &lt;/profile&gt; ... &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;myprofile&lt;/activeProfile&gt; &lt;/activeProfiles&gt; ...&lt;/settings&gt; 激活配置文件除了放在activeProfiles中之外，也可以使用mvn的参数 12&gt; mvn -Pmyprofile ...&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven之pom文件详解]]></title>
    <url>%2Fmaven%E4%B9%8Bpom%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[[toc] 0 参考 https://blog.csdn.net/weixin_38569499/article/details/91456988 1 maven项目目录结构 pom文件定于了一个maven项目的maven配置，一般pom文件的放在项目或者模块的根目录下。 maven的遵循约定大于配置，约定了如下的目录结构： 目录 目的 ${basedir} 存放pom.xml和所有的子目录 ${basedir}/src/main/java 项目的java源代码 ${basedir}/src/main/resources 项目的资源，比如说property文件，springmvc.xml ${basedir}/src/test/java 项目的测试类，比如说Junit代码 ${basedir}/src/test/resources 测试用的资源 ${basedir}/src/main/scripts 项目脚本源码的目录 ${basedir}/src/main/webapp/WEB-INF web应用文件目录，web项目的信息，比如存放web.xml、本地图片、jsp视图页面 ${basedir}/target 打包输出目录 ${basedir}/target/classes 编译输出目录 ${basedir}/target/site 生成文档的目录，可以通过index.html查看项目的文档 ${basedir}/target/test-classes 测试编译输出目录 Test.java Maven只会自动运行符合该命名规则的测试类 ~/.m2/repository Maven默认的本地仓库目录位置 2 配置详解2.1 根元素和必要配置1234567891011121314151617181920&lt;project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;!-- 模型版本 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 公司或者组织的唯一标志，也是打包成jar包路径的依据 --&gt; &lt;!-- 例如com.companyname.project-group，maven打包jar包的路径：/com/companyname/project-group --&gt; &lt;groupId&gt;com.companyname.project-group&lt;/groupId&gt; &lt;!-- 项目的唯一ID，一个groupId下面可能多个项目，就是靠artifactId来区分的 --&gt; &lt;artifactId&gt;project&lt;/artifactId&gt; &lt;!-- 项目当前版本，格式为:主版本.次版本.增量版本-限定版本号 --&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;!--项目产生的构件类型，包括jar、war、ear、pom等 --&gt; &lt;packaging&gt;jar&lt;/packaging&gt;&lt;/project&gt; project是pom文件的根元素，project下有modelVersion、groupId、artifactId、version、packaging等重要的元素。其中，groupId、artifactId、version三个元素用来定义一个项目的坐标，也就是说，一个maven仓库中，完全相同的一组groupId、artifactId、version，只能有一个项目。 project：整个pom配置文件的根元素，所有的配置都是写在project元素里面的； modelVersion：指定了当前POM模型的版本，对于Maven2及Maven 3来说，它只能是4.0.0； groupId：这是项目组的标识。它在一个组织或者项目中通常是唯一的。 artifactId：这是项目的标识，通常是工程的名称。它在一个项目组（group）下是唯一的。 version：这是项目的版本号，用来区分同一个artifact的不同版本。 packaging：这是项目产生的构件类型，即项目通过maven打包的输出文件的后缀名，包括jar、war、ear、pom等。 2.2 父项目和parent元素123456789101112131415&lt;!--父项目的坐标，坐标包括group ID，artifact ID和version。 --&gt;&lt;!--如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值 --&gt;&lt;parent&gt; &lt;!--被继承的父项目的构件标识符 --&gt; &lt;artifactId&gt;com.companyname.project-group&lt;/artifactId&gt; &lt;!--被继承的父项目的全球唯一标识符 --&gt; &lt;groupId&gt;base-project&lt;/groupId&gt; &lt;!--被继承的父项目的版本 --&gt; &lt;version&gt;1.0.1-RELEASE&lt;/version&gt; &lt;!-- 父项目的pom.xml文件的相对路径,默认值是../pom.xml。 --&gt; &lt;!-- 寻找父项目的pom：构建当前项目的地方--)relativePath指定的位置--)本地仓库--)远程仓库 --&gt; &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt;&lt;/parent&gt; 所有的pom都继承自一个父pom（Super POM）。父pom包含了一些可以被继承的默认设置，如果项目的pom中没有设置这些元素，就会使用父pom中设置。例如，Super POM中配置了默认仓库http://repo1.maven.org/maven2，这样哪怕项目的pom中没有配置仓库，也可以去这个默认仓库中去下载依赖。实际上，maven pom文件约定大于配置的原则，就是通过在Super POM中预定义了一些配置信息来实现的。 Maven使用effective pom（Super pom加上工程自己的配置）来执行相关的目标，它帮助开发者在pom.xml中做尽可能少的配置。当然，这些配置也可以被重写。 parent元素可以指定父pom。用户可以通过增加parent元素来自定义一个父pom，从而继承该pom的配置。parent元素中包含一些子元素，用来定位父项目和父项目的pom文件位置。 parent：用于指定父项目； groupId：parent的子元素，父项目的groupId，用于定位父项目； artifactId：parent的子元素，父项目的artifactId，用于定位父项目； version：parent的子元素，父项目的version，用于定位父项目； relativePath：parent的子元素，用于定位父项目pom文件的位置。 2.3 项目构建需要的信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142&lt;!--构建项目需要的信息 --&gt;&lt;build&gt; &lt;!--------------------- 路径管理（在遵循约定大于配置原则下，不需要配置） ---------------------&gt; &lt;!--项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;sourceDirectory /&gt; &lt;!--该元素设置了项目单元测试使用的源码目录。该路径是相对于pom.xml的相对路径 --&gt; &lt;testSourceDirectory /&gt; &lt;!--被编译过的应用程序class文件存放的目录。 --&gt; &lt;outputDirectory /&gt; &lt;!--被编译过的测试class文件存放的目录。 --&gt; &lt;testOutputDirectory /&gt; &lt;!--项目脚本源码目录，该目录下的内容，会直接被拷贝到输出目录，因为脚本是被解释的，而不是被编译的 --&gt; &lt;scriptSourceDirectory /&gt; &lt;!--------------------- 资源管理 ---------------------&gt; &lt;!--这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --&gt; &lt;resources&gt; &lt;!--这个元素描述了项目相关或测试相关的所有资源路径 --&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标输出路径。该路径是相对于target/classes的路径 --&gt; &lt;!-- 如果是想要把资源直接放在target/classes下，不需要配置该元素 --&gt; &lt;targetPath /&gt; &lt;!--是否使用参数值代替参数名。参数值取自文件里配置的属性，文件在filters元素里列出。 --&gt; &lt;filtering /&gt; &lt;!--描述存放资源的目录，该路径相对POM路径 --&gt; &lt;directory /&gt; &lt;!--包含的模式列表，例如**/*.xml，只有符合条件的资源文件才会在打包的时候被放入到输出路径中 --&gt; &lt;includes /&gt; &lt;!--排除的模式列表，例如**/*.xml，符合的资源文件不会在打包的时候会被过滤掉 --&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!--这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --&gt; &lt;testResources&gt; &lt;!--这个元素描述了测试相关的所有资源路径，子元素说明参考build/resources/resource元素的说明 --&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!--------------------- 插件管理 ---------------------&gt; &lt;!-- 子项目可以引用的默认插件信息。pluginManagement中的插件直到被引用时才会被解析或绑定到生命周期 --&gt; &lt;!-- 这里只是做了声明，并没有真正的引入。给定插件的任何本地配置都会覆盖这里的配置--&gt; &lt;pluginManagement&gt; &lt;!-- 可使用的插件列表 --&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述插件所需要的信息。 --&gt; &lt;plugin&gt; &lt;!--插件在仓库里的group ID --&gt; &lt;groupId /&gt; &lt;!--插件在仓库里的artifact ID --&gt; &lt;artifactId /&gt; &lt;!--被使用的插件的版本（或版本范围） --&gt; &lt;version /&gt; &lt;!-- 是否从该插件下载Maven扩展(例如打包和类型处理器) --&gt; &lt;!-- 由于性能原因，只有在真需要下载时，该元素才被设置成enabled --&gt; &lt;extensions /&gt; &lt;!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;!--execution元素包含了插件执行需要的信息 --&gt; &lt;execution&gt; &lt;!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id /&gt; &lt;!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase /&gt; &lt;!--配置的执行目标 --&gt; &lt;goals /&gt; &lt;!--配置是否被传播到子POM --&gt; &lt;inherited /&gt; &lt;!--作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--项目引入插件所需要的额外依赖 --&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!--作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!--使用的插件列表 --&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;!--------------------- 构建扩展 ---------------------&gt; &lt;!--使用来自其他项目的一系列构建扩展 --&gt; &lt;extensions&gt; &lt;!--每个extension描述一个会使用到其构建扩展的一个项目，extension的子元素是项目的坐标 --&gt; &lt;extension&gt; &lt;!--项目坐标三元素：groupId + artifactId + version --&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!--------------------- 其他配置 ---------------------&gt; &lt;!--当项目没有规定目标（Maven2 叫做阶段）时的默认值 --&gt; &lt;defaultGoal /&gt; &lt;!--构建产生的所有文件存放的目录 --&gt; &lt;directory /&gt; &lt;!--产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt; &lt;finalName /&gt; &lt;!--当filtering开关打开时，使用到的过滤器属性文件列表 --&gt; &lt;filters /&gt;&lt;/build&gt; build标签定义了构建项目需要的信息，这部分信息对于定制化项目构建是非常重要的。这里会根据build的子元素的特点，简单地分类讲解。 2.3.1 路径管理1234567891011&lt;!--------------------- 路径管理（在遵循约定大于配置原则下，不需要配置） ---------------------&gt; &lt;!--项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;sourceDirectory /&gt; &lt;!--该元素设置了项目单元测试使用的源码目录。该路径是相对于pom.xml的相对路径 --&gt; &lt;testSourceDirectory /&gt; &lt;!--被编译过的应用程序class文件存放的目录。 --&gt; &lt;outputDirectory /&gt; &lt;!--被编译过的测试class文件存放的目录。 --&gt; &lt;testOutputDirectory /&gt; &lt;!--项目脚本源码目录，该目录下的内容，会直接被拷贝到输出目录，因为脚本是被解释的，而不是被编译的 --&gt; &lt;scriptSourceDirectory /&gt; 路径管理定义了各种源码和编译结果的输出路径。如果遵循maven默认的路径约定，这里的几个元素是不需要配置的。这些元素包括： sourceDirectory：项目源码目录，定义的是相对于pom文件的相对路径； testSourceDirectory：项目单元测试源码目录，定义的也是是相对于pom文件的相对路径； outputDirectory：被编译过的应用程序class文件存放的目录，也是是相对于pom文件的相对路径； testOutoutDIrectory：被编译过的测试class文件存放的目录，也是是相对于pom文件的相对路径； scriptSourceDirectory：项目脚本源码目录，也是是相对于pom文件的相对路径。由于脚本是解释性的语言，所以该目录下的内容，会直接被拷贝到输出目录，而不需要编译。 2.3.2 资源管理1234567891011121314151617181920212223242526272829&lt;!--------------------- 资源管理 ---------------------&gt;&lt;!--这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --&gt;&lt;resources&gt; &lt;!--这个元素描述了项目相关或测试相关的所有资源路径 --&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标输出路径。该路径是相对于target/classes的路径 --&gt; &lt;!-- 如果是想要把资源直接放在target/classes下，不需要配置该元素 --&gt; &lt;targetPath /&gt; &lt;!--是否使用参数值代替参数名。参数值取自文件里配置的属性，文件在filters元素里列出 --&gt; &lt;filtering /&gt; &lt;!--描述打包前的资源存放的目录，该路径相对POM路径 --&gt; &lt;directory /&gt; &lt;!--包含的模式列表，例如**/*.xml，只有符合条件的资源文件才会在打包的时候被放入到输出路径中 --&gt; &lt;includes /&gt; &lt;!--排除的模式列表，例如**/*.xml，符合的资源文件不会在打包的时候会被过滤掉 --&gt; &lt;excludes /&gt; &lt;/resource&gt;&lt;/resources&gt;&lt;!--这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --&gt;&lt;testResources&gt; &lt;!--这个元素描述了测试相关的所有资源路径，子元素说明参考build/resources/resource元素的说明 --&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt;&lt;/testResources&gt; 这里的元素主要是对应用程序resource资源和单元测试部分resource资源的管理，分别通过resource标签和testResource标签管理两种资源。两个标签元素可选的子元素都是一样的。子元素包括： targetPath：描述了资源的目标输出路径，该路径是相对于target/classes的路径； filtering：对文件中的参数值进行过滤，需要被过滤的文件在filter中指定； directory：描述打包前的资源的存放路径，这个路径是相对于pom文件所在目录的相对路径； includes：包含的模式列表，例如**/*.xml。只有符合条件的资源文件才会在打包的时候被放入到输出路径中； excludes：排除的模式列表，例如**/*.xml，符合的资源文件不会在打包的时候会被过滤掉。 2.3.3 插件管理插件管理相关的元素有两个，包括pluginManagement和plugins。pluginManagement中有子元素plugins，它和project下的直接子元素plugins的区别是，pluginManagement主要是用来声明子项目可以引用的默认插件信息，这些插件如果只写在pluginManagement中是不会被引入的。project下的直接子元素plugins中定义的才是这个项目中真正需要被引入的插件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!--------------------- 插件管理 ---------------------&gt;&lt;!-- 子项目可以引用的默认插件信息。pluginManagement中的插件直到被引用时才会被解析或绑定到生命周期 --&gt;&lt;!-- 这里只是做了声明，并没有真正的引入。给定插件的任何本地配置都会覆盖这里的配置--&gt;&lt;pluginManagement&gt; &lt;!-- 可使用的插件列表 --&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述插件所需要的信息。 --&gt; &lt;plugin&gt; &lt;!--插件定位坐标三元素：groupId + artifactId + version --&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;!-- 是否使用这个插件的Maven扩展(extensions)，默认为false --&gt; &lt;!-- 由于性能原因，只有在真需要下载时，该元素才被设置成enabled --&gt; &lt;extensions /&gt; &lt;!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;!--execution元素包含了插件执行需要的信息 --&gt; &lt;execution&gt; &lt;!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id /&gt; &lt;!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase /&gt; &lt;!--配置的执行目标 --&gt; &lt;goals /&gt; &lt;!--配置是否被传播到子POM --&gt; &lt;inherited /&gt; &lt;!--作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--项目引入插件所需要的额外依赖，参见dependencies元素 --&gt; &lt;dependencies&gt; ...... &lt;/dependencies&gt; &lt;!--任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!--作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/pluginManagement&gt;&lt;!--使用的插件列表，这里是真正的引入插件。参见build/pluginManagement/plugins元素 --&gt;&lt;plugins&gt; ......&lt;/plugins&gt; 2.3.4 构建扩展extensions是在此构建中使用的项目的列表，它们将被包含在运行构建的classpath中。这些项目可以启用对构建过程的扩展，并使活动的插件能够对构建生命周期进行更改。简而言之，扩展是在构建期间激活的artifacts。扩展不需要实际执行任何操作，也不包含 Mojo。因此，扩展对于指定普通插件接口的多个实现中的一个是非常好的。 1234567891011&lt;!--------------------- 构建扩展 ---------------------&gt;&lt;!--使用来自其他项目的一系列构建扩展 --&gt;&lt;extensions&gt; &lt;!--每个extension描述一个会使用到其构建扩展的一个项目，extension的子元素是项目的坐标 --&gt; &lt;extension&gt; &lt;!--项目坐标三元素：groupId + artifactId + version --&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;/extension&gt;&lt;/extensions&gt; 2.3.5 其他配置 build中还有一些配置，如下： 123456789&lt;!--------------------- 其他配置 ---------------------&gt;&lt;!--当项目没有规定目标（Maven2 叫做阶段）时的默认值 --&gt;&lt;defaultGoal /&gt;&lt;!--构建产生的所有文件存放的目录 --&gt;&lt;directory /&gt;&lt;!--产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt;&lt;finalName /&gt;&lt;!--当filtering开关打开时，使用到的过滤器属性文件列表 --&gt;&lt;filters /&gt; 2.4 项目依赖相关信息pom文件中通过dependencyManagement来声明依赖，通过dependencies元素来管理依赖。dependencyManagement下的子元素只有一个直接的子元素dependencice，其配置和dependencies子元素是完全一致的；而dependencies下只有一类直接的子元素：dependency。一个dependency子元素表示一个依赖项目。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;!--该元素描述了项目相关的所有依赖。 这些依赖自动从项目定义的仓库中下载 --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;!------------------- 依赖坐标 -------------------&gt; &lt;!--依赖项目的坐标三元素：groupId + artifactId + version --&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;artifactId&gt;maven-artifact&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;!------------------- 依赖类型 -------------------&gt; &lt;!-- 依赖类型，默认是jar。通常表示依赖文件的扩展名，但有例外。一个类型可以被映射成另外一个扩展名或分类器 --&gt; &lt;!-- 类型经常和使用的打包方式对应，尽管这也有例外，一些类型的例子：jar，war，ejb-client和test-jar --&gt; &lt;!-- 如果设置extensions为true，就可以在plugin里定义新的类型 --&gt; &lt;type&gt;jar&lt;/type&gt; &lt;!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面 --&gt; &lt;!-- 如果想将项目构建成两个单独的JAR，分别使用Java 4和6编译器，就可以使用分类器来生成两个单独的JAR构件 --&gt; &lt;classifier&gt;&lt;/classifier&gt; &lt;!------------------- 依赖传递 -------------------&gt; &lt;!--依赖排除，即告诉maven只依赖指定的项目，不依赖该项目的这些依赖。此元素主要用于解决版本冲突问题 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;!-- 可选依赖，用于阻断依赖的传递性。如果在项目B中把C依赖声明为可选，那么依赖B的项目中无法使用C依赖 --&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;!------------------- 依赖范围 -------------------&gt; &lt;!--依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来 - compile：默认范围，用于编译; - provided：类似于编译，但支持jdk或者容器提供，类似于classpath - runtime: 在执行时需要使用; - systemPath: 仅用于范围为system。提供相应的路径 - test: 用于test任务时使用; - system: 需要外在提供相应的元素。通过systemPath来取得 - optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;!-- 该元素为依赖规定了文件系统上的路径。仅供scope设置system时使用。但是不推荐使用这个元素 --&gt; &lt;!-- 不推荐使用绝对路径，如果必须要用，推荐使用属性匹配绝对路径，例如$&#123;java.home&#125; --&gt; &lt;systemPath&gt;&lt;/systemPath&gt; &lt;/dependency&gt;&lt;/dependencies&gt; &lt;!-- 继承自该项目的所有子项目的默认依赖信息，这部分的依赖信息不会被立即解析。 --&gt;&lt;!-- 当子项目声明一个依赖，如果group ID和artifact ID以外的一些信息没有描述，则使用这里的依赖信息 --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 这里也是根据元素的作用，简单的对dependency的子元素做了一下分类。下面按分类来看一下dependency的子元素： 2.4.1 依赖坐标依然是通过groupId + artifactId + version来在仓库中定位一个项目： groupId：parent的子元素，父项目的groupId，用于定位父项目； artifactId：parent的子元素，父项目的artifactId，用于定位父项目； version：parent的子元素，父项目的version，用于定位父项目； 2.4.2 依赖类型这个分类主要包括两个元素，分别是依赖类型和依赖的分类器。同一个项目，即使打包成同一种类型，也可以有多个文件同时存在，因为它们的分类器可能是不同的。 type：依赖类型，默认是jar。通常表示依赖文件的扩展名，但也有例外。一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应，尽管这也有例外，一些类型的例子：jar，war，ejb-client和test-jar。如果设置extensions为true，就可以在plugin里定义新的类型。 classifier：依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面，如果想将项目构建成两个单独的JAR，分别使用Java 4和6编译器，就可以使用分类器来生成两个单独的JAR构件 2.4.3 依赖传递依赖传递相关的子元素主要有两个，用于依赖排除的exclusions和设置依赖是否可选的optional。 exclusions：排除该项目中的一些依赖，即本项目A依赖该dependency指示的项目B，但是不依赖项目B中依赖的这些依赖； optional：可选依赖，用于阻断依赖的传递性，即本项目不会依赖父项目中optional设置为true的依赖。 2.4.4 依赖范围还有一些其他元素： scope：依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来 compile：默认范围，用于编译; provided：类似于编译，但支持jdk或者容器提供，类似于classpath runtime: 在执行时需要使用; systemPath: 仅用于范围为system，提供相应的路径 test: 用于test任务时使用; system: 需要外在提供相应的元素，通过systemPath来取得 optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 systemPath：该元素为依赖规定了文件系统上的绝对路径。仅在scope设置成system时才会使用。不推荐使用这个元素。不推荐使用绝对路径，如果必须要用，推荐使用属性匹配绝对路径，例如${java.home} 2.5 生成文档相关的元素123456789&lt;!--项目的名称, Maven生成文档使用 --&gt;&lt;name&gt;project-maven&lt;/name&gt; &lt;!--项目主页的URL, Maven生成文档使用 --&gt;&lt;url&gt;http://123.a.b/nsnxs&lt;/url&gt; &lt;!-- 项目的详细描述, Maven生成文档使用。当这个元素能够用HTML格式描述时，不鼓励使用纯文本描述 --&gt;&lt;!--如果你需要修改生成的web站点的索引页面，你应该修改你自己的索引页文件，而不是调整这里的文档 --&gt;&lt;description&gt;Description of this maven project&lt;/description&gt; 备注：maven可以通过mvn site命令生成项目的相关文档。 和生成文档相关的元素，包括name，url，和description。 name：项目名称，maven生成文档会使用项目名； url：项目主页的地址，maven生成文档的时候使用。 description：项目描述。如果可以使用HTML格式进行描述的时候，不推荐使用纯文本的描述。 2.6 远程仓库列表远程仓库列表的配置，包括依赖和扩展的远程仓库配置，以及插件的远程仓库配置。在本地仓库找不到的情况下，maven下载依赖、扩展和插件就是从这里配置的远程仓库中进行下载。 需要注意的是release和snapshot两者的区别。release是稳定版本，一经发布不再修改，想发布修改后的项目，只能升级项目版本再进行发布；snapshot是不稳定的，一个snapshot的版本可以不断改变。项目在开发期间一般会使用snapshot，更方便进行频繁的代码更新；一旦发布到外部，或者开发基本完成，代码迭代不再频繁，则推荐使用release。 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!--发现依赖和扩展的远程仓库列表。 --&gt;&lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!--如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!--值为true或者false，表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled /&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳 --&gt; &lt;!--选项：always，daily（默认），interval：X（X单位为分钟），或者never。 --&gt; &lt;updatePolicy /&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做。选项：ignore，fail，或者warn --&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载 --&gt; &lt;!-- 有了releases和snapshots这两组配置，就可以在每个单独的仓库中，为每种类型的构件采取不同的策略 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;!--远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库 --&gt; &lt;id&gt;nanxs-repository-proxy&lt;/id&gt; &lt;!--远程仓库名称 --&gt; &lt;name&gt;nanxs-repository-proxy&lt;/name&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://192.168.1.169:9999/repository/&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型。可以是default或者legacy --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;!--发现插件的远程仓库列表，这些插件用于构建和报表 --&gt;&lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息。参见repositories/repository元素 --&gt; &lt;pluginRepository&gt; ...... &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 2.7 项目分发信息相关元素12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!--项目分发信息，在执行mvn deploy后表示要发布的位置。用于把网站部署到远程服务器或者把构件部署到远程仓库 --&gt;&lt;distributionManagement&gt; &lt;!--部署项目产生的构件到远程仓库需要的信息 --&gt; &lt;repository&gt; &lt;!-- 是分配给快照一个唯一的版本号 --&gt; &lt;uniqueVersion /&gt; &lt;!-- 其他配置参见repositories/repository元素 --&gt; &lt;id&gt;nanxs-maven2&lt;/id&gt; &lt;name&gt;nanxsmaven2&lt;/name&gt; &lt;url&gt;file://$&#123;basedir&#125;/target/deploy&lt;/url&gt; &lt;layout /&gt; &lt;/repository&gt; &lt;!--构件的快照部署的仓库。默认部署到distributionManagement/repository元素配置的仓库 --&gt; &lt;snapshotRepository&gt; &lt;uniqueVersion /&gt; &lt;id&gt;nanxs-maven2&lt;/id&gt; &lt;name&gt;Nanxs-maven2 Snapshot Repository&lt;/name&gt; &lt;url&gt;scp://svn.baidu.com/nanxs:/usr/local/maven-snapshot&lt;/url&gt; &lt;layout /&gt; &lt;/snapshotRepository&gt; &lt;!--部署项目的网站需要的信息 --&gt; &lt;site&gt; &lt;!--部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置 --&gt; &lt;id&gt;nanxs-site&lt;/id&gt; &lt;!--部署位置的名称 --&gt; &lt;name&gt;business api website&lt;/name&gt; &lt;!--部署位置的URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;scp://svn.baidu.com/nanxs:/var/www/localhost/nanxs-web&lt;/url&gt; &lt;/site&gt; &lt;!--项目下载页面的URL。如果没有该元素，用户应该参考主页 --&gt; &lt;!--本元素是为了帮助定位那些不在仓库里的构件(license限制) --&gt; &lt;downloadUrl /&gt; &lt;!--如果构件有了新的group ID和artifact ID（构件移到了新的位置），这里列出构件的重定位信息 --&gt; &lt;relocation&gt; &lt;!--构件新的group ID --&gt; &lt;groupId /&gt; &lt;!--构件新的artifact ID --&gt; &lt;artifactId /&gt; &lt;!--构件新的版本号 --&gt; &lt;version /&gt; &lt;!--显示给用户的，关于移动的额外信息，例如原因 --&gt; &lt;message /&gt; &lt;/relocation&gt; &lt;!-- 给出该构件在远程仓库的状态。本地项目中不能设置该元素，因为这是工具自动更新的 --&gt; &lt;!-- 有效的值有：none（默认），converted（仓库管理员从 Maven 1 POM转换过来）， partner（直接从伙伴Maven 2仓库同步过来），deployed（从Maven 2实例部署）， verified（被核实时正确的和最终的） --&gt; &lt;status /&gt;&lt;/distributionManagement&gt; 项目分发信息的相关配置，在distributionManagement中设置。设置的内容包括： repository和snapshotRepository：项目产生的构建/快照构建部署的远程仓库。如果不配置 snapshotRepository，快照也会部署到repository中； site：部署项目的网站需要的信息； downloadUrl：项目下载页面的URL，这是为不在仓库中的构建提供的； relocation：如果构件有了新的group ID和artifact ID（移到了新的位置），这里列出构件的新的信息； status：给出该构件在远程仓库的状态，本地项目中不能设置该元素，这是工具自动更新的。 2.8 报表规范报表规范描述的是使用mvn site命令时使用的一些配置。 1234567891011121314151617181920212223242526272829303132333435363738&lt;!-- 该元素描述使用报表插件产生报表的规范 --&gt;&lt;!-- 当用户执行"mvn site"，这些报表就会运行，在页面导航栏能看到所有报表的链接 --&gt;&lt;reporting&gt; &lt;!--网站是否排除默认的报表。这包括"项目信息"菜单中的报表。 --&gt; &lt;excludeDefaults /&gt; &lt;!--所有产生的报表存放到哪里。默认值是$&#123;project.build.directory&#125;/site。 --&gt; &lt;outputDirectory /&gt; &lt;!--使用的报表插件和他们的配置。 --&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述报表插件需要的信息 --&gt; &lt;plugin&gt; &lt;!--报表插件定位：groupId + artifactId + version --&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;!--任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!--报表插件的配置 --&gt; &lt;configuration /&gt; &lt;!-- 一组报表的多重规范，每个规范可能有不同的配置。一个规范（报表集）对应一个执行目标 --&gt; &lt;!-- 例如，有1~9这9个报表。1，2构成A报表集，对应一个执行目标；2，5构成B报表集，对应另一个执行目标 --&gt; &lt;reportSets&gt; &lt;!--表示报表的一个集合，以及产生该集合的配置 --&gt; &lt;reportSet&gt; &lt;!--报表集合的唯一标识符，POM继承时用到 --&gt; &lt;id /&gt; &lt;!--产生报表集合时，被使用的报表的配置 --&gt; &lt;configuration /&gt; &lt;!--配置是否被继承到子POMs --&gt; &lt;inherited /&gt; &lt;!--这个集合里使用到哪些报表 --&gt; &lt;reports /&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/reporting&gt; 2.9 profile配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;!--在列的项目构建profile，如果被激活，会修改构建处理 --&gt;&lt;profiles&gt; &lt;!--根据环境参数或命令行参数激活某个构建处理 --&gt; &lt;profile&gt; &lt;!--构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 --&gt; &lt;id /&gt; &lt;!--自动触发profile的条件逻辑。Activation是profile的开启钥匙，profile的力量来自于它 --&gt; &lt;!-- 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。activation元素并不是激活profile的唯一方式 --&gt; &lt;activation&gt; &lt;!--profile默认是否激活的标志 --&gt; &lt;activeByDefault /&gt; &lt;!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK --&gt; &lt;jdk /&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows') --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;名称&#125;引用），其拥有对应的名称和值，Profile就会被激活 --&gt; &lt;!--如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --&gt; &lt;property&gt; &lt;!--激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活profile --&gt; &lt;!--另一方面，exists则会检查文件是否存在，如果存在则激活profile --&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/usr/local/abcd/abcd-home/jobs/maven-guide-zh-to-production/workspace/ &lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/usr/local/abcd/abcd-home/jobs/maven-guide-zh-to-production/workspace/ &lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!--构建项目所需要的信息。参见build元素 --&gt; &lt;build /&gt; &lt;!--发现依赖和扩展的远程仓库列表。详情参见repositories元素 --&gt; &lt;repositories /&gt; &lt;!--发现插件的远程仓库列表，这些插件用于构建和报表。详情参见pluginRepositories元素 --&gt; &lt;pluginRepositories /&gt; &lt;!--该元素描述了项目相关的所有依赖。 详细配置参见dependencies --&gt; &lt;dependencies /&gt; &lt;!--该元素包括使用报表插件产生报表的规范。当用户执行"mvn site"，这些报表就会运行。在页面导航栏能看到所有报表的链接。参见reporting元素 --&gt; &lt;reporting /&gt; &lt;!--参见dependencyManagement元素 --&gt; &lt;dependencyManagement /&gt; &lt;!--参见distributionManagement元素 --&gt; &lt;distributionManagement /&gt; &lt;!--不赞成使用. 现在Maven忽略该元素. --&gt; &lt;reports /&gt; &lt;!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --&gt; &lt;modules /&gt; &lt;!--参见properties元素 --&gt; &lt;properties /&gt; &lt;/profile&gt;&lt;/profiles&gt; 2.10 邮件列表和持续集成配置1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!--项目持续集成信息 --&gt;&lt;ciManagement&gt; &lt;!--持续集成系统的名字，例如continuum --&gt; &lt;system /&gt; &lt;!--该项目使用的持续集成系统的URL（如果持续集成系统有web接口的话）。 --&gt; &lt;url /&gt; &lt;!--构建完成时，需要通知的开发者/用户的配置项。包括被通知者信息和通知条件（错误，失败，成功，警告） --&gt; &lt;notifiers&gt; &lt;!--配置一种方式，当构建中断时，以该方式通知用户/开发者 --&gt; &lt;notifier&gt; &lt;!--传送通知的途径 --&gt; &lt;type /&gt; &lt;!--发生错误时是否通知 --&gt; &lt;sendOnError /&gt; &lt;!--构建失败时是否通知 --&gt; &lt;sendOnFailure /&gt; &lt;!--构建成功时是否通知 --&gt; &lt;sendOnSuccess /&gt; &lt;!--发生警告时是否通知 --&gt; &lt;sendOnWarning /&gt; &lt;!--不赞成使用。通知发送到哪里 --&gt; &lt;address /&gt; &lt;!--扩展配置项 --&gt; &lt;configuration /&gt; &lt;/notifier&gt; &lt;/notifiers&gt;&lt;/ciManagement&gt; &lt;!--项目相关邮件列表信息 --&gt;&lt;mailingLists&gt; &lt;!--该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。 --&gt; &lt;mailingList&gt; &lt;!--邮件的名称 --&gt; &lt;name&gt;Demo&lt;/name&gt; &lt;!--发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --&gt; &lt;post&gt;nanxs@123.com&lt;/post&gt; &lt;!--订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --&gt; &lt;subscribe&gt;nanxs@123.com&lt;/subscribe&gt; &lt;!--取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --&gt; &lt;unsubscribe&gt;nanxs@123.com&lt;/unsubscribe&gt; &lt;!--你可以浏览邮件信息的URL --&gt; &lt;archive&gt;http:/a.b.c/nanxs/demo/dev/&lt;/archive&gt; &lt;/mailingList&gt;&lt;/mailingLists&gt; 2.11 项目的描述性信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;!--项目的问题管理系统(Bugzilla, Jira, Scarab,或任何你喜欢的问题管理系统)的名称和URL --&gt;&lt;issueManagement&gt; &lt;!--问题管理系统（例如jira）的名字， --&gt; &lt;system&gt;jira&lt;/system&gt; &lt;!--该项目使用的问题管理系统的URL --&gt; &lt;url&gt;http://jira.baidu.com/nanxs&lt;/url&gt;&lt;/issueManagement&gt; &lt;!--项目创建年份，4位数字。当产生版权信息时需要使用这个值。 --&gt;&lt;inceptionYear /&gt; &lt;!--项目开发者列表 --&gt;&lt;developers&gt; &lt;!--某个项目开发者的信息 --&gt; &lt;developer&gt; &lt;!--SCM里项目开发者的唯一标识符 --&gt; &lt;id&gt;HELLO WORLD&lt;/id&gt; &lt;!--项目开发者的全名 --&gt; &lt;name&gt;nanxs&lt;/name&gt; &lt;!--项目开发者的email --&gt; &lt;email&gt;123@abc.com&lt;/email&gt; &lt;!--项目开发者的主页的URL --&gt; &lt;url /&gt; &lt;!--项目开发者在项目中扮演的角色，角色元素描述了各种角色 --&gt; &lt;roles&gt; &lt;role&gt;Project Manager&lt;/role&gt; &lt;role&gt;Architect&lt;/role&gt; &lt;/roles&gt; &lt;!--项目开发者所属组织 --&gt; &lt;organization&gt;demo&lt;/organization&gt; &lt;!--项目开发者所属组织的URL --&gt; &lt;organizationUrl&gt;http://a.b.com/nanxs&lt;/organizationUrl&gt; &lt;!--项目开发者属性，如即时消息如何处理等 --&gt; &lt;properties&gt; &lt;dept&gt;No&lt;/dept&gt; &lt;/properties&gt; &lt;!--项目开发者所在时区， -11到12范围内的整数。 --&gt; &lt;timezone&gt;-5&lt;/timezone&gt; &lt;/developer&gt;&lt;/developers&gt; &lt;!--项目的其他贡献者列表 --&gt;&lt;contributors&gt; &lt;!--项目的其他贡献者。参见developers/developer元素 --&gt; &lt;contributor&gt; &lt;name /&gt; &lt;email /&gt; &lt;url /&gt; &lt;organization /&gt; &lt;organizationUrl /&gt; &lt;roles /&gt; &lt;timezone /&gt; &lt;properties /&gt; &lt;/contributor&gt;&lt;/contributors&gt; &lt;!--该元素描述了项目所有License列表。 应该只列出该项目的license列表，不要列出依赖项目的license列表 --&gt;&lt;!--如果列出多个license，用户可以选择它们中的一个而不是接受所有license --&gt;&lt;licenses&gt; &lt;!--描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。 --&gt; &lt;license&gt; &lt;!--license用于法律上的名称 --&gt; &lt;name&gt;Apache 2&lt;/name&gt; &lt;!--官方的license正文页面的URL --&gt; &lt;url&gt;http://a.b.com/nanxs/LICENSE-1.0.txt&lt;/url&gt; &lt;!--项目分发的主要方式： repo，可以从Maven库下载 manual， 用户必须手动下载和安装依赖 --&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;!--关于license的补充信息 --&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt;&lt;/licenses&gt; &lt;!--SCM(Source Control Management)标签允许你配置你的代码库，供Maven web站点和其它插件使用。 --&gt;&lt;scm&gt; &lt;!--SCM的URL,该URL描述了版本库和如何连接到版本库。该连接只读 --&gt; &lt;connection&gt;scm:svn:http://a.b.com/nanxs&lt;/connection&gt; &lt;!--给开发者使用的，类似connection元素。即该连接不仅仅只读 --&gt; &lt;developerConnection&gt;scm:svn:http://a.b.com/nanxs&lt;/developerConnection&gt; &lt;!--当前代码的标签，在开发阶段默认为HEAD --&gt; &lt;tag /&gt; &lt;!--指向项目的可浏览SCM库（例如ViewVC或者Fisheye）的URL。 --&gt; &lt;url&gt;http://a.b.com/nanxs&lt;/url&gt;&lt;/scm&gt; &lt;!--描述项目所属组织的各种属性。Maven产生的文档用 --&gt;&lt;organization&gt; &lt;!--组织的全名 --&gt; &lt;name&gt;demo&lt;/name&gt; &lt;!--组织主页的URL --&gt; &lt;url&gt;http://a.b.com/nanxs&lt;/url&gt;&lt;/organization&gt; 2.12 其他配置123456789101112131415&lt;!--描述了这个项目构建环境中的前提条件。 --&gt;&lt;prerequisites&gt; &lt;!--构建该项目或使用该插件所需要的Maven的最低版本 --&gt; &lt;maven /&gt;&lt;/prerequisites&gt; &lt;!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --&gt;&lt;modules /&gt; &lt;!--以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件（见settings.xml中activation元素的说明） --&gt;&lt;!-格式是&lt;name&gt;value&lt;/name&gt;。 --&gt;&lt;properties /&gt; &lt;!--不推荐使用，现在Maven忽略该元素. --&gt;&lt;reports&gt;&lt;/reports&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes之StorageClassss——nfs-client]]></title>
    <url>%2FKubernetes%E4%B9%8BStorageClassss%E2%80%94%E2%80%94nfs-client%2F</url>
    <content type="text"><![CDATA[[TOC] 0 参考 https://kubernetes.io/docs/concepts/storage/storage-classes/ http://tingcream.com/blogArticle/detail/28acd1174a49432ba733cf6bbfa077a5 1 简介 Kubernetes集群管理员通过提供不同的存储类，可以满足用户不同的服务质量级别、备份策略和任意策略要求的存储需求。动态存储卷供应使用StorageClass进行实现，其允许存储卷按需被创建。如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。 比如，我们可能会在kubernetes集群中安装一些带持久化数据功能的服务，例如redis、mysql、mq、es等。因此，当有这类需求时，我们必须给整个集群给定一个特定的存储方案。本章我们是NFS的网络存储方案 2 部署2.1 NFS服务器1234567yum install rpcbind nfs-utilsmkdir /nfs_datachmod +w /nfs_datachmod +x /nfs_data 编辑/etc/exports,内容如下： 1/nfs_data 192.168.11.0/24(rw,no_root_squash) 启动nfs服务 12service nfs start #启动nfs服务service rpcbind start #启动rpc服务 1showmount -e 192.168.11.24 #查看共享盘 ok 2.2 使用helm部署nfs-client12helm search nfs-clienthelm fetch nfs-client 编辑helm的values文件 123nfs.server=192.168.11.24nfs.path=/nfs_datastorageClass.defaultClass=true 安装 1helm install .... 3 测试创建一个pvc使用默认的storageclass，查看pv是否自动创建成功 如果没有将nfs-client设为默认的storageclass，可使用以下命令进行操作 12# 这里的 `&lt;your-class-name&gt;` 是您选择的 StorageClass 的名字。kubectl patch storageclass &lt;your-class-name&gt; -p '&#123;"metadata": &#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"true"&#125;&#125;&#125;']]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>storageclass</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跨域资源共享 CORS 详解]]></title>
    <url>%2F%E8%B7%A8%E5%9F%9F%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB-CORS-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[[TOC] 0 转载 http://www.ruanyifeng.com/blog/2016/04/cors.html 1 简介CORS需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE浏览器不能低于IE10。 整个CORS通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。 因此，实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。 2 两种请求浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。 只要同时满足以下两大条件，就属于简单请求。 （1) 请求方法是以下三种方法之一： HEAD GET POST （2）HTTP的头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 这是为了兼容表单（form），因为历史上表单一直可以发出跨域请求。AJAX 的跨域设计就是，只要表单可以发，AJAX 就可以直接发。 凡是不同时满足上面两个条件，就属于非简单请求。 浏览器对这两种请求的处理，是不一样的。 3 简单请求3.1 基本流程对于简单请求，浏览器直接发出CORS请求。具体来说，就是在头信息之中，增加一个Origin字段。 下面是一个例子，浏览器发现这次跨源AJAX请求是简单请求，就自动在头信息之中，添加一个Origin字段。 1234567&gt; GET /cors HTTP/1.1&gt; Origin: http://api.bob.com&gt; Host: api.alice.com&gt; Accept-Language: en-US&gt; Connection: keep-alive&gt; User-Agent: Mozilla/5.0...&gt; 上面的头信息中，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求。 如果Origin指定的源，不在许可范围内，服务器会返回一个正常的HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文），就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror回调函数捕获。注意，这种错误无法通过状态码识别，因为HTTP回应的状态码有可能是200。 如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。 12345&gt; Access-Control-Allow-Origin: http://api.bob.com&gt; Access-Control-Allow-Credentials: true&gt; Access-Control-Expose-Headers: FooBar&gt; Content-Type: text/html; charset=utf-8&gt; 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。 （1）Access-Control-Allow-Origin 该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。 （2）Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 （3）Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader(&#39;FooBar&#39;)可以返回FooBar字段的值。 3.2 withCredentials 属性上面说到，CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。 12&gt; Access-Control-Allow-Credentials: true&gt; 另一方面，开发者必须在AJAX请求中打开withCredentials属性。 123&gt; var xhr = new XMLHttpRequest();&gt; xhr.withCredentials = true;&gt; 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 但是，如果省略withCredentials设置，有的浏览器还是会一起发送Cookie。这时，可以显式关闭withCredentials。 12&gt; xhr.withCredentials = false;&gt; 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为星号，必须指定明确的、与请求网页一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨源）原网页代码中的document.cookie也无法读取服务器域名下的Cookie。 4 非简单请求4.1 预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为”预检”请求（preflight）。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。 123456&gt; var url = 'http://api.alice.com/cors';&gt; var xhr = new XMLHttpRequest();&gt; xhr.open('PUT', url, true);&gt; xhr.setRequestHeader('X-Custom-Header', 'value');&gt; xhr.send();&gt; 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个”预检”请求，要求服务器确认可以这样请求。下面是这个”预检”请求的HTTP头信息。 123456789&gt; OPTIONS /cors HTTP/1.1&gt; Origin: http://api.bob.com&gt; Access-Control-Request-Method: PUT&gt; Access-Control-Request-Headers: X-Custom-Header&gt; Host: api.alice.com&gt; Accept-Language: en-US&gt; Connection: keep-alive&gt; User-Agent: Mozilla/5.0...&gt; “预检”请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，”预检”请求的头信息包括两个特殊字段。 （1）Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 （2）Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 4.2 预检请求的回应服务器收到”预检”请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 12345678910111213&gt; HTTP/1.1 200 OK&gt; Date: Mon, 01 Dec 2008 01:15:39 GMT&gt; Server: Apache/2.0.61 (Unix)&gt; Access-Control-Allow-Origin: http://api.bob.com&gt; Access-Control-Allow-Methods: GET, POST, PUT&gt; Access-Control-Allow-Headers: X-Custom-Header&gt; Content-Type: text/html; charset=utf-8&gt; Content-Encoding: gzip&gt; Content-Length: 0&gt; Keep-Alive: timeout=2, max=100&gt; Connection: Keep-Alive&gt; Content-Type: text/plain&gt; 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。 12&gt; Access-Control-Allow-Origin: *&gt; 如果服务器否定了”预检”请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror回调函数捕获。控制台会打印出如下的报错信息。 123&gt; XMLHttpRequest cannot load http://api.alice.com.&gt; Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin.&gt; 服务器回应的其他CORS相关字段如下。 12345&gt; Access-Control-Allow-Methods: GET, POST, PUT&gt; Access-Control-Allow-Headers: X-Custom-Header&gt; Access-Control-Allow-Credentials: true&gt; Access-Control-Max-Age: 1728000&gt; （1）Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次”预检”请求。 （2）Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在”预检”中请求的字段。 （3）Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 （4）Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 4.3 浏览器的正常请求和回应一旦服务器通过了”预检”请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是”预检”请求之后，浏览器的正常CORS请求。 12345678&gt; PUT /cors HTTP/1.1&gt; Origin: http://api.bob.com&gt; Host: api.alice.com&gt; X-Custom-Header: value&gt; Accept-Language: en-US&gt; Connection: keep-alive&gt; User-Agent: Mozilla/5.0...&gt; 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。 123&gt; Access-Control-Allow-Origin: http://api.bob.com&gt; Content-Type: text/html; charset=utf-8&gt; 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 5 与JSONP的比较CORS与JSONP的使用目的相同，但是比JSONP更强大。 JSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。]]></content>
      <tags>
        <tag>cors</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAuth详解]]></title>
    <url>%2FOAuth%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[[TOC] 0 参考文章 https://www.jianshu.com/p/b06944c92228 http://www.ruanyifeng.com/blog/2014/05/oauth_2_0.html 1 什么是OAuth官网这样说… An open protocol to allow secure authorization in a simple and standard method from web, mobile and desktop applications. OAuth 即 Open standard for Authorization OAuth就是一个网络开放协议。为保证用户资源的安全授权提供了简易的标准 在知乎上看到了一个比较直观的比喻，有助于初学者理解。知乎话题链接 2 OAuth历史版本2007-12 OAuth 1.0发布并迅速成为工业标准。 2008-06 OAuth 1.0 Revision A发布，这是个稍作修改的修订版本，主要修正一个安全方面的漏洞。 2010-04，OAuth 1.0 协议发布为 RFC 5849 2011-05 OAuth 2.0 草案发布 2012-10 OAuth 2.0 协议发布为 RFC 6749 3 OAuth 1.0OAuth 1.0 协议过于复杂，易用性差，所以并没有得到普及，下文中给出了授权的流程图，可以简单了解了解，现在很少有用的了。 关于OAuth 1.0， 想了解的可以看看这些： RFC 5849 OAUTH协议简介 4 OAuth 2.0OAuth 2.0是目前的最新版本，OAuth 2.0不兼容OAuth 1.0。这篇文章主要讲讲OAuth 2.0，并以此展开。 先来说一个场景：比如你第一次打开简书官网，想要注册一个账号。你会看到简书允许你通过新浪微博账号登陆。当你点击之后，需要你登陆微博。之后会出现“是否同意简书获取你的个人信息”等等，如果你选择授权，之后会跳转回简书。你会发现你在简书的用户名就是你微博的用户名。然后，你会发出一条新的微博比如“我加入了简书，一个基于内容分享的社区！”(这只是举个例子，不知道简书有没有这样做)。 好了，这回开始进入正题 4.1 角色（roles）OAuth 2.0 定义了四个角色： 资源拥有者(Resource Owner) 资源拥有者其实就是用户(user)，用户将会授权一个第三方应用可以获取他们的账户资源。当然第三方应用程序对于用户账户的操作是有限制的(比如，read access, read and write access)！这个限制就是用户授权时给予的权限范围(scope) 上面场景中，微博账户就是资源拥有者。read access就比如读取微博用户名，write access就比如以你的名义发了一个微博。 客户端(Client) 客户端就是前面说的第三方应用程序，他们想要获取用户的账户资源，但在这么做之前必须经过授权 上面场景中，简书就是客户端 资源服务器(Resource Server) 资源服务器存放用户账户以及账户信息和资源 上面场景中，新浪微博就是资源服务器，同时也是授权服务器 授权服务器(Authorization Server) 授权服务器验证用户身份，并为第三方应用程序颁发授权令牌(access token) 资源服务器与授权服务器可以是同一台服务器，这里分开主要是便于解释清楚OAuth协议。从程序开发者的角度，这两个都是service’s API会执行的事情。 在了解完OAuth中的四个角色之后，我们看看这四个角色之间是如何互动的。下面是基本运行流程。 应用程序向用户请求给予授权，以便获取服务器资源 如果用户同意授权，应用程序将获得相应授权 应用程序向授权服务器提供自己的身份证明(app key和app secret)和已被授权的证明(authorization grant)，并请求访问令牌(access token) 如果应用程序的身份被核实，并且授权是有效地，那么授权服务器将会发放访问令牌给应用程序。此时，授权完成 应用程序向资源服务器出示访问令牌，并请求资源 如果访问令牌是有效的(比如：是否伪造，是否越权，是否过期)，资源服务器将会为应用程序提供资源 4.2 应用程序注册(Application Registration)对于一个应用程序来说，如果它想要使用OAuth，那么首先它要在服务提供商那里注册。一般出现在网站的“developer”或者“API”部分。 应用程序要提供： 应用程序名称(Application Name) 应用程序网站(Application Website) 回调URL(Redirect URI or Callback URL) 在用户同意授权(或者拒绝)之后，服务提供商会将用户重新导向这个Callback URL，这个Callback URL要来负责处理授权码或者访问令牌。 应用程序注册完成之后，服务提供商会颁发给应用程序一个“客户端认证信息(client credentials)”。Client Credential包括： Client Identifier (client ID/API key/consumer key) 提供给服务提供商，用于识别(identify)应用程序 用于构建提供给用户请求授权的URLs Client Secret (secret key/API secret/consumer secret) 提供给服务提供商，用于验证(authenticate)应用程序 只有应用程序和服务提供商两者可知 4.3 授权许可类型(Authorization grant types)OAuth 2.0 定义了四种授权模式： 授权码模式(Authorization Code) used with server-side Applications 隐式授权模式(Implicit) used with Mobile Apps or Web Applications (applications that run on the user’s device) 资源拥有者密码凭证模式(Resource Owner Password Credentials) used with trusted Applications, such as those owned by the service itself 客户端模式(Client Credentials) used with Applications API access 4.4 授权码模式(Authorization Code) 授权码模式是最常见的一种授权模式，最为安全和完善。 对于服务器端应用程序(server-side application)来说，可以保证Client Secret的安全性。应用程序必须能够和用户代理(user agent)进行交互，获取API授权码。 第一步：客户端把用户代理定向到授权终端(Authorizaiton Endpoint) 1https://www.example.com/v1/oauth/authorize?response_type=code&amp;client_id=CLIENT_ID&amp;redirect_uri=CALLBACK_URL&amp;scope=read www.example.com/v1/oauth/authorize: API授权的终端(Authorization Endpoint) client_id: 应用程序的client ID，用于API识别应用程序 redirect_uri：获得授权码之后，服务提供商重定向用户代理(比如浏览器)的地址 response_type：表明授权类型，默认值是code，即授权码模式(Authorization Code Grant) scope: 即应用程序可以获得的授权级别(access level)，默认值为read state: 表示客户端的当前状态，可以指定任意值，认证服务器会原封不动地返回这个值，用于抵制CSRF攻击。 第二步：用户授权应用程序 用户点击上述URI之后，用户首先要登陆，证明其身份。然后选择同意授权应用程序可以访问他们的账户或者拒绝。 第三步：应用程序获取授权码 如果用户同意授权，服务提供商会将用户代理重定向到第一步中的redirect_uri，并且会包含授权码。 1https://www.jianshu.com/callback?code=AUTHORIZATION_CODE 第四步：应用程序请求授权令牌 应用程序向API token终端发送刚刚获得的授权码，以及认证信息。 1https://www.example.com/v1/oauth/token?client_id=CLIENT_ID&amp;client_secret=CLIENT_SECRET&amp;grant_type=authorization_code&amp;code=AUTHORIZATION_CODE&amp;redirect_uri=CALLBACK_URL URI中包括： www.example.com/v1/oauth/token： API token的终端 client_id：即app key/consumer key，用于验证应用程序 client_secret： 即app secret/consumer secret，用于验证应用程序 grant_type：刚刚获得的授权码 redirect_uri: 重定向URI，与第一步一致 第五步：应用程序获得授权令牌 如果上一步验证有效，API将返回一个HTTP回复。 1&#123;"access_token":"ACCESS_TOKEN","token_type":"bearer","expires_in":2592000,"refresh_token":"REFRESH_TOKEN","scope":"read"&#125; HTTP回复中包含： access_token：访问令牌 token_type：令牌类型，bearer类型或mac类型。详细 expires_in：过期时间，单位为秒。 refresh_token：更新令牌，用来获取下一次的访问令牌，可选项。 scope：权限范围，如果与客户端申请的范围一致，此项可省略。 4.5 隐式授权模式(Implicit) 隐式授权模式主要用于客户端应用程序(client-side application)，比如手机应用、桌面客户端应用程序和运行于浏览器上的web应用程序。 因为没有server端，client secret的保密性不能得到保证。授权令牌给予用户代理(比如，浏览器)，再由用户代理交给应用程序。所以用户设备上的其他应用程序同样可以得到授权令牌。 隐式授权模式中，应用程序不需要认证。 隐式授权模式不支持refresh token。 第一步：客户端把用户代理定向到授权终端(Authorizaiton Endpoint) 授权终端是https://www.example.com/authorize 与授权码链接类似，只不过response_type=token而不是code 1https://www.example.com/authorize?response_type=token&amp;client_id=CLIENT_ID&amp;redirect_uri=CALLBACK_URL&amp;scope=read 第二步：用户授权应用程序 用户点击上述URI之后，用户首先要登陆，证明其身份。然后选择同意授权应用程序可访问他们的账户或者拒绝。 第三步：用户代理收到授权令牌 假设用户同意授权，授权服务器重定向用户代理到第一步提到的redirect_uri。并在URI fragment中包含授权令牌(但不能查看)。 1https://www.example.com/callback#token=ACCESS_TOKEN 第四步：用户代理向资源服务器发出请求 用户代理依照重定向的指令，向资源服务器发出请求，但并不包含上一步中得到的授权令牌(#后面的部分)。用户代理将完整的重定向URI保存在本地。 第五步：资源服务器返回一个网页 资源服务器会返回一个网页(通常是一个HTML文件内嵌一段脚本)。这段内嵌的脚本(script)可以访问第三步中用户代理保存在本地的完整的重定向URI，并从中提取授权令牌。 第六步：用户代理提取授权令牌 用户代理执行上面提到的脚本，提取出授权令牌。然后将授权令牌传递给应用程序。 4.6 资源拥有者密码凭证模式(Resource Owner Password Credentials) 在资源拥有者密码凭证模式中，用户直接向应用程序提供其认证信息(即用户名和密码)。应用程序依此向授权服务器获取授权令牌。 这种模式适用于用户对应用程序高度信任的情况。比如是用户操作系统的一部分。 认证服务器只有在其他授权模式无法执行的情况下，才能考虑使用这种模式。 第一步：用户传递认证信息 用户将用户名和密码交给应用程序。 第二步：应用程序请求授权令牌 应用程序得到用户认证信息后，向授权服务器请求授权令牌。 请求授权令牌终端https://www.example.com/token response_type=password，前面提到的两种分别是code和token。 1https://www.example.com/token?grant_type=password&amp;username=USERNAME&amp;password=PASSWORD&amp;client_id=CLIENT_ID 第三步：授权服务器返回授权令牌 如果用户的认证信息得到验证，授权服务器将向应用程序返回授权令牌。 4.7 客户端模式(Client Credentials) 客户端模式应用于应用程序想要以自己的名义与授权服务器以及资源服务器进行互动。 比如应用程序想要修改自身注册信息或者redirect URI。又或者想要获取资源服务器中不与具体用户相关的信息。比如一个应用程序想要获取新浪微博中含有#happy的微博。 严格意义上说，客户端模式并不是OAuth协议要解决的问题，因为并未涉及授权。 第一步：应用程序请求授权令牌 应用程序向授权服务器提供自身认证信息(client ID和client secret)，并请求授权令牌。 请求授权令牌终端https://www.example.com/token response_type=client_credentials，前面提到的两种分别是code、token和password。 1https://www.example.com/token?grant_type=client_credentials&amp;client_id=CLIENT_ID&amp;client_secret=CLIENT_SECRET 第二步：授权服务器返回授权令牌 授权服务器验证认证信息，向应用程序返回授权令牌。 4.8 更新令牌(Refresh Token) 如果授权令牌过期，在进行API请求时会报错Invalid Token Error 如果在获取授权令牌时，同时获得了refresh token，那么我们可以向授权服务器申请更新令牌。 grant_type = refresh_token scope：表示申请的授权范围，不可以超出上一次申请的范围，如果省略该参数，则表示与上一次一致。 1https://www.example.com/v1/oauth/token?grant_type=refresh_token&amp;client_id=CLIENT_ID&amp;client_secret=CLIENT_SECRET&amp;refresh_token=REFRESH_TOKEN]]></content>
      <tags>
        <tag>OAuth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes之Ingress]]></title>
    <url>%2FKubernetes%E4%B9%8BIngress%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://blog.csdn.net/qianghaohao/article/details/99354304 https://github.com/kubernetes/ingress-nginx https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md https://kubernetes.github.io/ingress-nginx/ 1 k8s 对外暴露服务的方法向 k8s 集群外部暴露服务的方式有三种： nodePort，LoadBalancer 和本文要介绍的 Ingress。每种方式都有各自的优缺点，nodePort 方式在服务变多的情况下会导致节点要开的端口越来越多，不好管理。而 LoadBalancer 更适合结合云提供商的 LB 来使用，但是在 LB 越来越多的情况下对成本的花费也是不可小觑。Ingress 是 k8s 官方提供的用于对外暴露服务的方式，也是在生产环境用的比较多的方式，一般在云环境下是 LB + Ingress Ctroller 方式对外提供服务，这样就可以在一个 LB 的情况下根据域名路由到对应后端的 Service，有点类似于 Nginx 反向代理，只不过在 k8s 集群中，这个反向代理是集群外部流量的统一入口。 2 Ingress 及 Ingress Controller 简介Ingress 是 k8s 资源对象，用于对外暴露服务，该资源对象定义了不同主机名（域名）及 URL 和对应后端 Service（k8s Service）的绑定，根据不同的路径路由 http 和 https 流量。而 Ingress Contoller 是一个 pod 服务，封装了一个 web 前端负载均衡器，同时在其基础上实现了动态感知 Ingress 并根据 Ingress 的定义动态生成 前端 web 负载均衡器的配置文件，比如 Nginx Ingress Controller 本质上就是一个 Nginx，只不过它能根据 Ingress 资源的定义动态生成 Nginx 的配置文件，然后动态 Reload。个人觉得 Ingress Controller 的重大作用是将前端负载均衡器和 Kubernetes 完美地结合了起来，一方面在云、容器平台下方便配置的管理，另一方面实现了集群统一的流量入口，而不是像 nodePort 那样给集群打多个孔。 所以，总的来说要使用 Ingress，得先部署 Ingress Controller 实体（相当于前端 Nginx），然后再创建 Ingress （相当于 Nginx 配置的 k8s 资源体现），Ingress Controller 部署好后会动态检测 Ingress 的创建情况生成相应配置。Ingress Controller 的实现有很多种：有基于 Nginx 的，也有基于 HAProxy的，还有基于 OpenResty 的 Kong Ingress Controller 等，更多 Controller 见：https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/，本文使用基于 Nginx 的 Ingress Controller：ingress-nginx。 3 helm 部署 Nginx Ingress Controller基于 Nginx 的 Ingress Controller 有两种，一种是 k8s 社区提供的 ingress-nginx，另一种是 Nginx 社区提供的 kubernetes-igress，关于两者的区别见 这里。 在这里我们部署 k8s 社区提供的 ingress-nginx，Ingress Controller 对外暴露方式采用 hostNetwork，在裸机环境下更多其他暴露方式见：https://kubernetes.github.io/ingress-nginx/deploy/baremetal/ 使用 Helm 官方提供的 Chart stable/nginx-ingress，修改 values 文件： 使用 DaemonSet 控制器，默认是 Deployment：controller.kind 设为 DaemonSet； pod 使用主机网络：controller.hostNetwork 设为 true； 在hostNetwork 下 pod 使用集群提供 dns 服务：controller.dnsPolicy 设为 ClusterFirstWithHostNet； Service 类型设为 ClusterIP，默认是 LoadBalancer：controller.service.type 设为 ClusterIP； 默认后端镜像使用 docker hub 提供的镜像，Google 国内无法访问； helm 部署(国内可以使用微软的helm源) 1helm install stable/nginx-ingress --name nginx-ingress -f myValues.yaml 部署完成后我们可以看到 Kubernetes 服务中增加了 nginx-ingress-controller 和 nginx-ingress-default-backend 两个服务。nginx-ingress-controller 为 Ingress Controller，主要做为一个七层的负载均衡器来提供 HTTP 路由、粘性会话、SSL 终止、SSL直通、TCP 和 UDP 负载平衡等功能。nginx-ingress-default-backend 为默认的后端，当集群外部的请求通过 Ingress 进入到集群内部时，如果无法负载到相应后端的 Service 上时，这种未知的请求将会被负载到这个默认的后端上。nginx-ingress-default-backend 默认提供了两个 URL 进行访问，其中的 /healthz 用作健康检查返回 200，而 / 返回 404 错误。 4 使用 Ingress 对外暴露服务-7层4.1 HTTP类型服务为了快速体验 Ingress，下面部署一个 nginx 服务，然后通过 Ingress 对外暴露 nginx service 进行访问。首先部署 nginx 服务：Deployment + Service 123456789101112131415161718192021222324252627apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80---kind: ServiceapiVersion: v1metadata: name: nginxspec: selector: app: nginx ports: - port: 80 targetPort: 80 接下来创建 Ingress 对外暴露 nginx service 80 端口：ingress.yml: 12345678910111213141516apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-nginx annotations: # use the shared ingress-nginx kubernetes.io/ingress.class: "nginx"spec: rules: - host: nginx.kube.com http: paths: - path: / backend: serviceName: nginx servicePort: 80 说明： kubernetes.io/ingress.class: “nginx”：Nginx Ingress Controller 根据该注解自动发现 Ingress； host: nginx.kube.com：对外访问的域名； serviceName: nginx：对外暴露的 Service 名称； servicePort: 80：nginx service 监听的端口； 创建的 Ingress 必须要和对外暴露的 Service 在同一命名空间下！ 4.2 HTTPS类型服务通过 Ingress 访问 kubernetes dashboard（支持 HTTPS 访问）首先，练习使用，先用自签名证书来代替吧： 1openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout kube-dashboard.key -out kube-dashboard.crt -subj "/CN=dashboard.kube.com/O=dashboard.kube.com" 使用生成的证书创建 k8s Secret 资源，下一步创建的 Ingress 会引用这个 Secret： 1kubectl create secret tls kube-dasboard-ssl --key kube-dashboard.key --cert kube-dashboard.crt -n kube-system 创建 Ingress 资源对象（支持 HTTPS 访问）： kube-dashboard-ingress.yml 123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-kube-dashboard annotations: # use the shared ingress-nginx kubernetes.io/ingress.class: "nginx" nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"spec: tls: - hosts: - dashboard.kube.com secretName: kube-dasboard-ssl rules: - host: dashboard.kube.com http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443 说明： kubernetes.io/ingress.class: “nginx”：Inginx Ingress Controller 根据该注解自动发现 Ingress； nginx.ingress.kubernetes.io/backend-protocol: Controller 向后端 Service 转发时使用 HTTPS 协议，这个注解必须添加，否则访问会报错 secretName: kube-dasboard-ssl：https 证书 Secret； host: dashboard.kube.com：对外访问的域名； serviceName: kubernetes-dashboard：集群对外暴露的 Service 名称； servicePort: 443：service 监听的端口； 注意：创建的 Ingress 必须要和对外暴露的 Service 在同一命名空间下！ 将域名 dashboard.kube.com绑定到 k8s 任意节点 ip 即可访问. 4.3 basic-auth认证4.3.1 创建用户名密码首先需要安装htpasswd二进制文件，通过htpasswd生成一个“auth”文件;用来存取我们创建的用户及加密之后的密码。 12345678910111213htpasswd -c auth user1New password: &lt;bar&gt;New password:Re-type new password:Adding password for user user1htpasswd auth user22nd user:htpasswd auth user2New password: &lt;bar&gt;New password:Re-type new password:Adding password for user user2 4.3.2 创建kubernetes secret来存储user/pass pairs12345678910111213kubectl -n &lt;namespace&gt; create secret generic basic-auth --from-file=authsecret "basic-auth" createdkubectl get secret basic-auth -o yamlapiVersion: v1data: auth: Zm9vOiRhcHIxJE9DRzZYeWJcJGNrKDBGSERBa29YWUlsSDkuY3lzVDAKkind: Secretmetadata: name: basic-auth namespace: defaulttype: Opaque 4.3.3 创建Ingress12345678910111213141516171819---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: prometheus namespace: monitoring annotations: nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: basic-auth #此secret需和ingress在用一namespace下 nginx.ingress.kubernetes.io/auth-realm: &quot;Authentication Required&quot; #提示符spec: rules: - host: prom.xxxxx.im http: paths: - path: / backend: serviceName: prometheus-svc servicePort: 9090 4.4 多路径及虚拟主机例子123456789101112131415161718192021222324252627apiVersion: extensions/v1beta1kind: Ingressmetadata: name: my-ingress annotations: nginx.ingress.kubernetes.io/use-regex: "true"spec: rules: - host: api.mydomain.com http: paths: - backend: serviceName: api servicePort: 80 - host: domain.com http: paths: - path: /web/* backend: serviceName: web servicePort: 8080 - host: backoffice.domain.com http: paths: - backend: serviceName: backoffice servicePort: 8080 5 使用 Ingress 对外暴露服务-4层我们可以看到ingress nginx的args里有这两行： 12- --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services- --udp-services-configmap=$(POD_NAMESPACE)/udp-services 从选项和值可以猜测出，要想代理四层（tcp/udp），得写同namespace里一个名为tcp-service和udp-service的两个configmap的数据 四层的话这边我们创建一个mysql的pod，来代理3306端口到集群外面，则需要写tcp-services这个configmap： 1234567kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginxdata: 3306: &quot;default/mysql:3306&quot; 四层写这两个ConfigMap的data即可，按照这样去写即可out_port: namespaces/svc_name:port，要给每个ingress加一些nginx里的配置可以查看官方的annotation字段以及值（traefik同理）。 如果我们用如上helm方式部署ingress-nginx的话 直接编辑它的vales.yaml文件，在TCP/UDP字段之后添加out_port: namespaces/svc_name:port即可，helm会自动帮你生成对应的configmap，并暴露对应的四层端口 其他例子 12345678910kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: ingress-nginxdata: 2200: "default/gitlab:22" 3306: "kube-public/mysql:3306" 2202: "kube-public/centos:22" 2203: "kube-public/mongodb:27017" 1234567kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: ingress-nginxdata: 53: "kube-system/kube-dns:53"]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>ingress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python模块之logging]]></title>
    <url>%2Fpython%E6%A8%A1%E5%9D%97%E4%B9%8Blogging%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://www.cnblogs.com/yyds/p/6901864.html https://www.cnblogs.com/yuanyongqiang/p/11913812.html https://www.cnblogs.com/nancyzhu/p/8551506.html 日志相关概念日志是一种可以追踪某些软件运行时所发生事件的方法。软件开发人员可以向他们的代码中调用日志记录相关的方法来表明发生了某些事情。一个事件可以用一个可包含可选变量数据的消息来描述。此外，事件也有重要性的概念，这个重要性也可以被称为严重性级别（level）。 日志的作用通过log的分析，可以方便用户了解系统或软件、应用的运行情况；如果你的应用log足够丰富，也可以分析以往用户的操作行为、类型喜好、地域分布或其他更多信息；如果一个应用的log同时也分了多个级别，那么可以很轻易地分析得到该应用的健康状况，及时发现问题并快速定位、解决问题，补救损失。简单来讲就是，我们通过记录和分析日志可以了解一个系统或软件程序运行情况是否正常，也可以在应用程序出现故障时快速定位问题。比如，做运维的同学，在接收到报警或各种问题反馈后，进行问题排查时通常都会先去看各种日志，大部分问题都可以在日志中找到答案。再比如，做开发的同学，可以通过IDE控制台上输出的各种日志进行程序调试。对于运维老司机或者有经验的开发人员，可以快速的通过日志定位到问题的根源。可见，日志的重要性不可小觑。日志的作用可以简单总结为以下3点： 程序调试 了解软件程序运行情况，是否正常 软件程序运行故障分析与问题定位 如果应用的日志信息足够详细和丰富，还可以用来做用户行为分析，如：分析用户的操作行为、类型洗好、地域分布以及其它更多的信息，由此可以实现改进业务、提高商业利益。 日志的等级我们先来思考下下面的两个问题： 作为开发人员，在开发一个应用程序时需要什么日志信息？在应用程序正式上线后需要什么日志信息？ 作为应用运维人员，在部署开发环境时需要什么日志信息？在部署生产环境时需要什么日志信息？ 在软件开发阶段或部署开发环境时，为了尽可能详细的查看应用程序的运行状态来保证上线后的稳定性，我们可能需要把该应用程序所有的运行日志全部记录下来进行分析，这是非常耗费机器性能的。当应用程序正式发布或在生产环境部署应用程序时，我们通常只需要记录应用程序的异常信息、错误信息等，这样既可以减小服务器的I/O压力，也可以避免我们在排查故障时被淹没在日志的海洋里。那么，怎样才能在不改动应用程序代码的情况下实现在不同的环境记录不同详细程度的日志呢？这就是日志等级的作用了，我们通过配置文件指定我们需要的日志等级就可以了。 不同的应用程序所定义的日志等级可能会有所差别，分的详细点的会包含以下几个等级： DEBUG INFO NOTICE WARNING ERROR CRITICAL ALERT EMERGENCY 日志字段信息与日志格式本节开始问题提到过，一条日志信息对应的是一个事件的发生，而一个事件通常需要包括以下几个内容： 事件发生时间 事件发生位置 事件的严重程度–日志级别 事件内容 上面这些都是一条日志记录中可能包含的字段信息，当然还可以包括一些其他信息，如进程ID、进程名称、线程ID、线程名称等。日志格式就是用来定义一条日志记录中包含那些字段的，且日志格式通常都是可以自定义的。 说明： 输出一条日志时，日志内容和日志级别是需要开发人员明确指定的。对于而其它字段信息，只需要是否显示在日志中就可以了。 日志功能的实现几乎所有开发语言都会内置日志相关功能，或者会有比较优秀的第三方库来提供日志操作功能，比如：log4j，log4php等。它们功能强大、使用简单。Python自身也提供了一个用于记录日志的标准库模块–logging。 logging模块简介logging模块定义的函数和类为应用程序和库的开发实现了一个灵活的事件日志系统。logging模块是Python的一个标准库模块，由标准库模块提供日志记录API的关键好处是所有Python模块都可以使用这个日志记录功能。所以，你的应用日志可以将你自己的日志信息与来自第三方模块的信息整合起来。 logging模块的日志级别logging模块默认定义了以下几个日志等级，它允许开发人员自定义其他日志级别，但是这是不被推荐的，尤其是在开发供别人使用的库时，因为这会导致日志级别的混乱。 日志等级（level） 描述 DEBUG 最详细的日志信息，典型应用场景是 问题诊断 INFO 信息详细程度仅次于DEBUG，通常只记录关键节点信息，用于确认一切都是按照我们预期的那样进行工作 WARNING 当某些不期望的事情发生时记录的信息（如，磁盘可用空间较低），但是此时应用程序还是正常运行的 ERROR 由于一个更严重的问题导致某些功能不能正常运行时记录的信息 CRITICAL 当发生严重错误，导致应用程序不能继续运行时记录的信息 开发应用程序或部署开发环境时，可以使用DEBUG或INFO级别的日志获取尽可能详细的日志信息来进行开发或部署调试；应用上线或部署生产环境时，应该使用WARNING或ERROR或CRITICAL级别的日志来降低机器的I/O压力和提高获取错误日志信息的效率。日志级别的指定通常都是在应用程序的配置文件中进行指定的。 说明： 上面列表中的日志等级是从上到下依次升高的，即：DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL，而日志的信息量是依次减少的； 当为某个应用程序指定一个日志级别后，应用程序会记录所有日志级别大于或等于指定日志级别的日志信息，而不是仅仅记录指定级别的日志信息，nginx、php等应用程序以及这里要提高的python的logging模块都是这样的。同样，logging模块也可以指定日志记录器的日志级别，只有级别大于或等于该指定日志级别的日志记录才会被输出，小于该等级的日志记录将会被丢弃。 logging模块的使用方式介绍logging模块提供了两种记录日志的方式： 第一种方式是使用logging提供的模块级别的函数 第二种方式是使用Logging日志系统的四大组件 其实，logging所提供的模块级别的日志记录函数也是对logging日志系统相关类的封装而已。 logging模块定义的模块级别的常用函数 函数 说明 logging.debug(msg, args, *kwargs) 创建一条严重级别为DEBUG的日志记录 logging.info(msg, args, *kwargs) 创建一条严重级别为INFO的日志记录 logging.warning(msg, args, *kwargs) 创建一条严重级别为WARNING的日志记录 logging.error(msg, args, *kwargs) 创建一条严重级别为ERROR的日志记录 logging.critical(msg, args, *kwargs) 创建一条严重级别为CRITICAL的日志记录 logging.log(level, args, *kwargs) 创建一条严重级别为level的日志记录 logging.basicConfig(**kwargs) 对root logger进行一次性配置 其中logging.basicConfig(**kwargs)函数用于指定“要记录的日志级别”、“日志格式”、“日志输出位置”、“日志文件的打开模式”等信息，其他几个都是用于记录各个级别日志的函数。 logging模块的四大组件 组件 说明 loggers 提供应用程序代码直接使用的接口 handlers 用于将日志记录发送到指定的目的位置 filters 提供更细粒度的日志过滤功能，用于决定哪些日志记录将会被输出（其它的日志记录将会被忽略） formatters 用于控制日志信息的最终输出格式 说明： logging模块提供的模块级别的那些函数实际上也是通过这几个组件的相关实现类来记录日志的，只是在创建这些类的实例时设置了一些默认值。 使用logging提供的模块级别的函数记录日志回顾下前面提到的几个重要信息： 可以通过logging模块定义的模块级别的方法去完成简单的日志记录 只有级别大于或等于日志记录器指定级别的日志记录才会被输出，小于该级别的日志记录将会被丢弃。 最简单的日志输出先来试着分别输出一条不同日志级别的日志记录： 1234567import logginglogging.debug(&quot;This is a debug log.&quot;)logging.info(&quot;This is a info log.&quot;)logging.warning(&quot;This is a warning log.&quot;)logging.error(&quot;This is a error log.&quot;)logging.critical(&quot;This is a critical log.&quot;) 也可以这样写： 12345logging.log(logging.DEBUG, &quot;This is a debug log.&quot;)logging.log(logging.INFO, &quot;This is a info log.&quot;)logging.log(logging.WARNING, &quot;This is a warning log.&quot;)logging.log(logging.ERROR, &quot;This is a error log.&quot;)logging.log(logging.CRITICAL, &quot;This is a critical log.&quot;) 输出结果： 123WARNING:root:This is a warning log.ERROR:root:This is a error log.CRITICAL:root:This is a critical log. 那么问题来了问题1：为什么前面两条日志没有被打印出来？ 这是因为logging模块提供的日志记录函数所使用的日志器设置的日志级别是WARNING，因此只有WARNING级别的日志记录以及大于它的ERROR和CRITICAL级别的日志记录被输出了，而小于它的DEBUG和INFO级别的日志记录被丢弃了。 问题2：打印出来的日志信息中各字段表示什么意思？为什么会这样输出？ 上面输出结果中每行日志记录的各个字段含义分别是： 1日志级别:日志器名称:日志内容 之所以会这样输出，是因为logging模块提供的日志记录函数所使用的日志器设置的日志格式默认是BASIC_FORMAT，其值为： 1&quot;%(levelname)s:%(name)s:%(message)s&quot; 问题3：如果将日志记录输出到文件中，而不是打印到控制台？ 因为在logging模块提供的日志记录函数所使用的日志器设置的处理器所指定的日志输出位置默认为:sys.stderr。 问题4：我是怎么知道这些的？ 查看这些日志记录函数的实现代码，可以发现：当我们没有提供任何配置信息的时候，这些函数都会去调用logging.basicConfig(**kwargs)方法，且不会向该方法传递任何参数。继续查看basicConfig()方法的代码就可以找到上面这些问题的答案了。 问题5：怎么修改这些默认设置呢？ 其实很简单，在我们调用上面这些日志记录函数之前，手动调用一下basicConfig()方法，把我们想设置的内容以参数的形式传递进去就可以了。 logging.basicConfig()函数说明该方法用于为logging日志系统做一些基本配置，方法定义如下： 1logging.basicConfig(**kwargs) 该函数可接收的关键字参数如下： 参数名称 描述 filename 指定日志输出目标文件的文件名，指定该设置项后日志信心就不会被输出到控制台了 filemode 指定日志文件的打开模式，默认为’a’。需要注意的是，该选项要在filename指定时才有效 format 指定日志格式字符串，即指定日志输出时所包含的字段信息以及它们的顺序。logging模块定义的格式字段下面会列出。 datefmt 指定日期/时间格式。需要注意的是，该选项要在format中包含时间字段%(asctime)s时才有效 level 指定日志器的日志级别 stream 指定日志输出目标stream，如sys.stdout、sys.stderr以及网络stream。需要说明的是，stream和filename不能同时提供，否则会引发 ValueError异常 style Python 3.2中新添加的配置项。指定format格式字符串的风格，可取值为’%’、’{‘和’$’，默认为’%’ handlers Python 3.3中新添加的配置项。该选项如果被指定，它应该是一个创建了多个Handler的可迭代对象，这些handler将会被添加到root logger。需要说明的是：filename、stream和handlers这三个配置项只能有一个存在，不能同时出现2个或3个，否则会引发ValueError异常。 logging模块定义的格式字符串字段我们来列举一下logging模块中定义好的可以用于format格式字符串中字段有哪些： 字段/属性名称 使用格式 描述 asctime %(asctime)s 日志事件发生的时间–人类可读时间，如：2003-07-08 16:49:45,896 created %(created)f 日志事件发生的时间–时间戳，就是当时调用time.time()函数返回的值 relativeCreated %(relativeCreated)d 日志事件发生的时间相对于logging模块加载时间的相对毫秒数（目前还不知道干嘛用的） msecs %(msecs)d 日志事件发生事件的毫秒部分 levelname %(levelname)s 该日志记录的文字形式的日志级别（’DEBUG’, ‘INFO’, ‘WARNING’, ‘ERROR’, ‘CRITICAL’） levelno %(levelno)s 该日志记录的数字形式的日志级别（10, 20, 30, 40, 50） name %(name)s 所使用的日志器名称，默认是’root’，因为默认使用的是 rootLogger message %(message)s 日志记录的文本内容，通过 msg % args计算得到的 pathname %(pathname)s 调用日志记录函数的源码文件的全路径 filename %(filename)s pathname的文件名部分，包含文件后缀 module %(module)s filename的名称部分，不包含后缀 lineno %(lineno)d 调用日志记录函数的源代码所在的行号 funcName %(funcName)s 调用日志记录函数的函数名 process %(process)d 进程ID processName %(processName)s 进程名称，Python 3.1新增 thread %(thread)d 线程ID threadName %(thread)s 线程名称 经过配置的日志输出(举例)先简单配置下日志器的日志级别 1234567logging.basicConfig(level=logging.DEBUG)logging.debug(&quot;This is a debug log.&quot;)logging.info(&quot;This is a info log.&quot;)logging.warning(&quot;This is a warning log.&quot;)logging.error(&quot;This is a error log.&quot;)logging.critical(&quot;This is a critical log.&quot;) 输出结果： 12345DEBUG:root:This is a debug log.INFO:root:This is a info log.WARNING:root:This is a warning log.ERROR:root:This is a error log.CRITICAL:root:This is a critical log. 所有等级的日志信息都被输出了，说明配置生效了。 在配置日志器日志级别的基础上，在配置下日志输出目标文件和日志格式 12345678LOG_FORMAT = &quot;%(asctime)s - %(levelname)s - %(message)s&quot;logging.basicConfig(filename=&apos;my.log&apos;, level=logging.DEBUG, format=LOG_FORMAT)logging.debug(&quot;This is a debug log.&quot;)logging.info(&quot;This is a info log.&quot;)logging.warning(&quot;This is a warning log.&quot;)logging.error(&quot;This is a error log.&quot;)logging.critical(&quot;This is a critical log.&quot;) 此时会发现控制台中已经没有输出日志内容了，但是在python代码文件的相同目录下会生成一个名为’my.log’的日志文件，该文件中的内容为： 123452017-05-08 14:29:53,783 - DEBUG - This is a debug log.2017-05-08 14:29:53,784 - INFO - This is a info log.2017-05-08 14:29:53,784 - WARNING - This is a warning log.2017-05-08 14:29:53,784 - ERROR - This is a error log.2017-05-08 14:29:53,784 - CRITICAL - This is a critical log. 在上面的基础上，我们再来设置下日期/时间格式 12345678910LOG_FORMAT = &quot;%(asctime)s - %(levelname)s - %(message)s&quot;DATE_FORMAT = &quot;%m/%d/%Y %H:%M:%S %p&quot;logging.basicConfig(filename=&apos;my.log&apos;, level=logging.DEBUG, format=LOG_FORMAT, datefmt=DATE_FORMAT)logging.debug(&quot;This is a debug log.&quot;)logging.info(&quot;This is a info log.&quot;)logging.warning(&quot;This is a warning log.&quot;)logging.error(&quot;This is a error log.&quot;)logging.critical(&quot;This is a critical log.&quot;) 此时会在my.log日志文件中看到如下输出内容： 1234505/08/2017 14:29:04 PM - DEBUG - This is a debug log.05/08/2017 14:29:04 PM - INFO - This is a info log.05/08/2017 14:29:04 PM - WARNING - This is a warning log.05/08/2017 14:29:04 PM - ERROR - This is a error log.05/08/2017 14:29:04 PM - CRITICAL - This is a critical log. 掌握了上面的内容之后，已经能够满足我们平时开发中需要的日志记录功能。 另外一个例子 12345678910import logginglogging.basicConfig(level=logging.DEBUG, format="%(asctime)s %(name)s %(levelname)s %(message)s", datefmt = '%Y-%m-%d %H:%M:%S %a' #注意月份和天数不要搞乱了，这里的格式化符与time模块相同 )logging.debug("msg1")logging.info("msg2")logging.warning("msg3")logging.error("msg4")logging.critical("msg5") 输出结果 123452018-05-09 23:37:49 Wed root DEBUG msg12018-05-09 23:37:49 Wed root INFO msg22018-05-09 23:37:49 Wed root WARNING msg32018-05-09 23:37:49 Wed root ERROR msg42018-05-09 23:37:49 Wed root CRITICAL msg5 输出到文件例子 12345678logging.basicConfig(level=logging.DEBUG,#控制台打印的日志级别 filename='new.log', filemode='a',##模式，有w和a，w就是写模式，每次都会重新写日志，覆盖之前的日志 #a是追加模式，默认如果不写的话，就是追加模式 format= '%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s' #日志格式 ) 12345678910111213import loggingLOG_FORMAT = "%(asctime)s %(name)s %(levelname)s %(pathname)s %(message)s "#配置输出日志格式DATE_FORMAT = '%Y-%m-%d %H:%M:%S %a ' #配置输出时间的格式，注意月份和天数不要搞乱了logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT, datefmt = DATE_FORMAT , filename=r"d:\test\test.log" #有了filename参数就不会直接输出显示到控制台，而是直接写入文件 )logging.debug("msg1")logging.info("msg2")logging.warning("msg3")logging.error("msg4")logging.critical("msg5") 日志在d:\test目录下，日志具体内容 123452018-05-10 02:13:46 Thu AM root DEBUG D:/06python/exercise/test_package/test.py msg12018-05-10 02:13:46 Thu AM root INFO D:/06python/exercise/test_package/test.py msg22018-05-10 02:13:46 Thu AM root WARNING D:/06python/exercise/test_package/test.py msg32018-05-10 02:13:46 Thu AM root ERROR D:/06python/exercise/test_package/test.py msg42018-05-10 02:13:46 Thu AM root CRITICAL D:/06python/exercise/test_package/test.py msg5 其他说明几个要说明的内容： logging.basicConfig()函数是一个一次性的简单配置工具使，也就是说只有在第一次调用该函数时会起作用，后续再次调用该函数时完全不会产生任何操作的，多次调用的设置并不是累加操作。 日志器（Logger）是有层级关系的，上面调用的logging模块级别的函数所使用的日志器是RootLogger类的实例，其名称为’root’，它是处于日志器层级关系最顶层的日志器，且该实例是以单例模式存在的。 如果要记录的日志中包含变量数据，可使用一个格式字符串作为这个事件的描述消息（logging.debug、logging.info等函数的第一个参数），然后将变量数据作为第二个参数*args的值进行传递，如:logging.warning(&#39;%s is %d years old.&#39;, &#39;Tom&#39;, 10)，输出内容为WARNING:root:Tom is 10 years old. logging.debug(), logging.info()等方法的定义中，除了msg和args参数外，还有一个**kwargs参数。它们支持3个关键字参数: exc_info, stack_info, extra，下面对这几个关键字参数作个说明。 关于exc_info, stack_info, extra关键词参数的说明: exc_info： 其值为布尔值，如果该参数的值设置为True，则会将异常异常信息添加到日志消息中。如果没有异常信息则添加None到日志信息中。 stack_info： 其值也为布尔值，默认值为False。如果该参数的值设置为True，栈信息将会被添加到日志信息中。 extra： 这是一个字典（dict）参数，它可以用来自定义消息格式中所包含的字段，但是它的key不能与logging模块定义的字段冲突。 一个例子： 在日志消息中添加exc_info和stack_info信息，并添加两个自定义的字端 ip和user 12345LOG_FORMAT = &quot;%(asctime)s - %(levelname)s - %(user)s[%(ip)s] - %(message)s&quot;DATE_FORMAT = &quot;%m/%d/%Y %H:%M:%S %p&quot;logging.basicConfig(format=LOG_FORMAT, datefmt=DATE_FORMAT)logging.warning(&quot;Some one delete the log file.&quot;, exc_info=True, stack_info=True, extra=&#123;&apos;user&apos;: &apos;Tom&apos;, &apos;ip&apos;:&apos;47.98.53.222&apos;&#125;) 输出结果： 1234505/08/2017 16:35:00 PM - WARNING - Tom[47.98.53.222] - Some one delete the log file.NoneTypeStack (most recent call last): File &quot;C:/Users/wader/PycharmProjects/LearnPython/day06/log.py&quot;, line 45, in &lt;module&gt; logging.warning(&quot;Some one delete the log file.&quot;, exc_info=True, stack_info=True, extra=&#123;&apos;user&apos;: &apos;Tom&apos;, &apos;ip&apos;:&apos;47.98.53.222&apos;&#125;) logging模块日志流处理流程在介绍logging模块的高级用法之前，很有必要对logging模块所包含的重要组件以及其工作流程做个全面、简要的介绍，这有助于我们更好的理解我们所写的代码（将会触发什么样的操作）。 logging日志模块四大组件在介绍logging模块的日志流处理流程之前，我们先来介绍下logging模块的四大组件： 组件名称 对应类名 功能描述 日志器 Logger 提供了应用程序可一直使用的接口 处理器 Handler 将logger创建的日志记录发送到合适的目的输出 过滤器 Filter 提供了更细粒度的控制工具来决定输出哪条日志记录，丢弃哪条日志记录 格式器 Formatter 决定日志记录的最终输出格式 logging模块就是通过这些组件来完成日志处理的，上面所使用的logging模块级别的函数也是通过这些组件对应的类来实现的。 这些组件之间的关系描述： 日志器（logger）需要通过处理器（handler）将日志信息输出到目标位置，如：文件、sys.stdout、网络等； 不同的处理器（handler）可以将日志输出到不同的位置； 日志器（logger）可以设置多个处理器（handler）将同一条日志记录输出到不同的位置； 每个处理器（handler）都可以设置自己的过滤器（filter）实现日志过滤，从而只保留感兴趣的日志； 每个处理器（handler）都可以设置自己的格式器（formatter）实现同一条日志以不同的格式输出到不同的地方。 简单点说就是：日志器（logger）是入口，真正干活儿的是处理器（handler），处理器（handler）还可以通过过滤器（filter）和格式器（formatter）对要输出的日志内容做过滤和格式化等处理操作。 logging日志模块相关类及其常用方法介绍下面介绍下与logging四大组件相关的类：Logger, Handler, Filter, Formatter。 Logger类Logger对象有3个任务要做： 1）向应用程序代码暴露几个方法，使应用程序可以在运行时记录日志消息； 2）基于日志严重等级（默认的过滤设施）或filter对象来决定要对哪些日志进行后续处理； 3）将日志消息传送给所有感兴趣的日志handlers。 Logger对象最常用的方法分为两类：配置方法 和 消息发送方法 最常用的配置方法如下： 方法 描述 Logger.setLevel() 设置日志器将会处理的日志消息的最低严重级别 Logger.addHandler() 和 Logger.removeHandler() 为该logger对象添加 和 移除一个handler对象 Logger.addFilter() 和 Logger.removeFilter() 为该logger对象添加 和 移除一个filter对象 关于Logger.setLevel()方法的说明： 内建等级中，级别最低的是DEBUG，级别最高的是CRITICAL。例如setLevel([logging.INFO])，此时函数参数为INFO，那么该logger将只会处理INFO、WARNING、ERROR和CRITICAL级别的日志，而DEBUG级别的消息将会被忽略/丢弃。 Logger对象配置完成后，可以使用下面的方法来创建日志记录： 方法 描述 Logger.debug(), Logger.info(), Logger.warning(), Logger.error(), Logger.critical() 创建一个与它们的方法名对应等级的日志记录 Logger.exception() 创建一个类似于Logger.error()的日志消息 Logger.log() 需要获取一个明确的日志level参数来创建一个日志记录 说明： Logger.exception()与Logger.error()的区别在于：Logger.exception()将会输出堆栈追踪信息，另外通常只是在一个exception handler中调用该方法。 Logger.log()与Logger.debug()、Logger.info()等方法相比，虽然需要多传一个level参数，显得不是那么方便，但是当需要记录自定义level的日志时还是需要该方法来完成。 那么，怎样得到一个Logger对象呢？一种方式是通过Logger类的实例化方法创建一个Logger类的实例，但是我们通常都是用第二种方式–logging.getLogger()方法。 logging.getLogger()方法有一个可选参数name，该参数表示将要返回的日志器的名称标识，如果不提供该参数，则其值为’root’。若以相同的name参数值多次调用getLogger()方法，将会返回指向同一个logger对象的引用。 关于logger的层级结构与有效等级的说明： logger的名称是一个以’.’分割的层级结构，每个’.’后面的logger都是’.’前面的logger的children，例如，有一个名称为 foo 的logger，其它名称分别为 foo.bar, foo.bar.baz 和 foo.bam都是 foo 的后代。 logger有一个”有效等级（effective level）”的概念。如果一个logger上没有被明确设置一个level，那么该logger就是使用它parent的level;如果它的parent也没有明确设置level则继续向上查找parent的parent的有效level，依次类推，直到找到个一个明确设置了level的祖先为止。需要说明的是，root logger总是会有一个明确的level设置（默认为 WARNING）。当决定是否去处理一个已发生的事件时，logger的有效等级将会被用来决定是否将该事件传递给该logger的handlers进行处理。 child loggers在完成对日志消息的处理后，默认会将日志消息传递给与它们的祖先loggers相关的handlers。因此，我们不必为一个应用程序中所使用的所有loggers定义和配置handlers，只需要为一个顶层的logger配置handlers，然后按照需要创建child loggers就可足够了。我们也可以通过将一个logger的propagate属性设置为False来关闭这种传递机制。 Handler类Handler对象的作用是（基于日志消息的level）将消息分发到handler指定的位置（文件、网络、邮件等）。Logger对象可以通过addHandler()方法为自己添加0个或者更多个handler对象。比如，一个应用程序可能想要实现以下几个日志需求： 1）把所有日志都发送到一个日志文件中； 2）把所有严重级别大于等于error的日志发送到stdout（标准输出）； 3）把所有严重级别为critical的日志发送到一个email邮件地址。这种场景就需要3个不同的handlers，每个handler复杂发送一个特定严重级别的日志到一个特定的位置。 一个handler中只有非常少数的方法是需要应用开发人员去关心的。对于使用内建handler对象的应用开发人员来说，似乎唯一相关的handler方法就是下面这几个配置方法： 方法 描述 Handler.setLevel() 设置handler将会处理的日志消息的最低严重级别 Handler.setFormatter() 为handler设置一个格式器对象 Handler.addFilter() 和 Handler.removeFilter() 为handler添加 和 删除一个过滤器对象 需要说明的是，应用程序代码不应该直接实例化和使用Handler实例。因为Handler是一个基类，它只定义了素有handlers都应该有的接口，同时提供了一些子类可以直接使用或覆盖的默认行为。下面是一些常用的Handler： Handler 描述 logging.StreamHandler 将日志消息发送到输出到Stream，如std.out, std.err或任何file-like对象。 logging.FileHandler 将日志消息发送到磁盘文件，默认情况下文件大小会无限增长 logging.handlers.RotatingFileHandler 将日志消息发送到磁盘文件，并支持日志文件按大小切割 logging.hanlders.TimedRotatingFileHandler 将日志消息发送到磁盘文件，并支持日志文件按时间切割 logging.handlers.HTTPHandler 将日志消息以GET或POST的方式发送给一个HTTP服务器 logging.handlers.SMTPHandler 将日志消息发送给一个指定的email地址 logging.NullHandler 该Handler实例会忽略error messages，通常被想使用logging的library开发者使用来避免’No handlers could be found for logger XXX’信息的出现。 Formater类Formater对象用于配置日志信息的最终顺序、结构和内容。与logging.Handler基类不同的是，应用代码可以直接实例化Formatter类。另外，如果你的应用程序需要一些特殊的处理行为，也可以实现一个Formatter的子类来完成。 Formatter类的构造方法定义如下： 1logging.Formatter.__init__(fmt=None, datefmt=None, style=&apos;%&apos;) 可见，该构造方法接收3个可选参数： fmt：指定消息格式化字符串，如果不指定该参数则默认使用message的原始值 datefmt：指定日期格式字符串，如果不指定该参数则默认使用”%Y-%m-%d %H:%M:%S” style：Python 3.2新增的参数，可取值为 ‘%’, ‘{‘和 ‘$’，如果不指定该参数则默认使用’%’ Filter类Filter可以被Handler和Logger用来做比level更细粒度的、更复杂的过滤功能。Filter是一个过滤器基类，它只允许某个logger层级下的日志事件通过过滤。该类定义如下： 12class logging.Filter(name=&apos;&apos;) filter(record) 比如，一个filter实例化时传递的name参数值为’A.B’，那么该filter实例将只允许名称为类似如下规则的loggers产生的日志记录通过过滤：’A.B’，’A.B,C’，’A.B.C.D’，’A.B.D’，而名称为’A.BB‘, ‘B.A.B’的loggers产生的日志则会被过滤掉。如果name的值为空字符串，则允许所有的日志事件通过过滤。 filter方法用于具体控制传递的record记录是否能通过过滤，如果该方法返回值为0表示不能通过过滤，返回值为非0表示可以通过过滤。 说明： 如果有需要，也可以在filter(record)方法内部改变该record，比如添加、删除或修改一些属性。 我们还可以通过filter做一些统计工作，比如可以计算下被一个特殊的logger或handler所处理的record数量等。 logging日志流处理流程 我们来描述下上面这个图的日志流处理流程： 1）（在用户代码中进行）日志记录函数调用，如：logger.info(…)，logger.debug(…)等； 2）判断要记录的日志级别是否满足日志器设置的级别要求（要记录的日志级别要大于或等于日志器设置的级别才算满足要求），如果不满足则该日志记录会被丢弃并终止后续的操作，如果满足则继续下一步操作； 3）根据日志记录函数调用时掺入的参数，创建一个日志记录（LogRecord类）对象； 4）判断日志记录器上设置的过滤器是否拒绝这条日志记录，如果日志记录器上的某个过滤器拒绝，则该日志记录会被丢弃并终止后续的操作，如果日志记录器上设置的过滤器不拒绝这条日志记录或者日志记录器上没有设置过滤器则继续下一步操作–将日志记录分别交给该日志器上添加的各个处理器； 5）判断要记录的日志级别是否满足处理器设置的级别要求（要记录的日志级别要大于或等于该处理器设置的日志级别才算满足要求），如果不满足记录将会被该处理器丢弃并终止后续的操作，如果满足则继续下一步操作； 6）判断该处理器上设置的过滤器是否拒绝这条日志记录，如果该处理器上的某个过滤器拒绝，则该日志记录会被当前处理器丢弃并终止后续的操作，如果当前处理器上设置的过滤器不拒绝这条日志记录或当前处理器上没有设置过滤器测继续下一步操作； 7）如果能到这一步，说明这条日志记录经过了层层关卡允许被输出了，此时当前处理器会根据自身被设置的格式器（如果没有设置则使用默认格式）将这条日志记录进行格式化，最后将格式化后的结果输出到指定位置（文件、网络、类文件的Stream等）； 8）如果日志器被设置了多个处理器的话，上面的第5-8步会执行多次； 9）这里才是完整流程的最后一步：判断该日志器输出的日志消息是否需要传递给上一级logger（之前提到过，日志器是有层级关系的）的处理器，如果propagate属性值为1则表示日志消息将会被输出到处理器指定的位置，同时还会被传递给parent日志器的handlers进行处理直到当前日志器的propagate属性为0停止，如果propagate值为0则表示不向parent日志器的handlers传递该消息，到此结束。 可见，一条日志信息要想被最终输出需要依次经过以下几次过滤： 日志器等级过滤； 日志器的过滤器过滤； 日志器的处理器等级过滤； 日志器的处理器的过滤器过滤； 需要说明的是： 关于上面第9个步骤，如果propagate值为1，那么日志消息会直接传递交给上一级logger的handlers进行处理，此时上一级logger的日志等级并不会对该日志消息进行等级过滤。 简化处理流程: 1、创建一个logger 2、设置下logger的日志的等级 3、创建合适的Handler(FileHandler要有路径) 4、设置下每个Handler的日志等级 5、创建下日志的格式 6、向Handler中添加上面创建的格式 7、将上面创建的Handler添加到logger中 8、打印输出logger.debug\logger.info\logger.warning\logger.error\logger.critical 使用logging四大组件记录日志现在，我们对logging模块的重要组件及整个日志流处理流程都应该有了一个比较全面的了解，下面我们来看一个例子。 1. 需求现在有以下几个日志记录的需求： 1）要求将所有级别的所有日志都写入磁盘文件中 2）all.log文件中记录所有的日志信息，日志格式为：日期和时间 - 日志级别 - 日志信息 3）error.log文件中单独记录error及以上级别的日志信息，日志格式为：日期和时间 - 日志级别 - 文件名[:行号] - 日志信息 4）要求all.log在每天凌晨进行日志切割 2. 分析 1）要记录所有级别的日志，因此日志器的有效level需要设置为最低级别–DEBUG; 2）日志需要被发送到两个不同的目的地，因此需要为日志器设置两个handler；另外，两个目的地都是磁盘文件，因此这两个handler都是与FileHandler相关的； 3）all.log要求按照时间进行日志切割，因此他需要用logging.handlers.TimedRotatingFileHandler; 而error.log没有要求日志切割，因此可以使用FileHandler; 4）两个日志文件的格式不同，因此需要对这两个handler分别设置格式器； 3. 代码实现12345678910111213141516171819202122import loggingimport logging.handlersimport datetimelogger = logging.getLogger(&apos;mylogger&apos;)logger.setLevel(logging.DEBUG)rf_handler = logging.handlers.TimedRotatingFileHandler(&apos;all.log&apos;, when=&apos;midnight&apos;, interval=1, backupCount=7, atTime=datetime.time(0, 0, 0, 0))rf_handler.setFormatter(logging.Formatter(&quot;%(asctime)s - %(levelname)s - %(message)s&quot;))f_handler = logging.FileHandler(&apos;error.log&apos;)f_handler.setLevel(logging.ERROR)f_handler.setFormatter(logging.Formatter(&quot;%(asctime)s - %(levelname)s - %(filename)s[:%(lineno)d] - %(message)s&quot;))logger.addHandler(rf_handler)logger.addHandler(f_handler)logger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warning(&apos;warning message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;) all.log文件输出 123452017-05-13 16:12:40,612 - DEBUG - debug message2017-05-13 16:12:40,612 - INFO - info message2017-05-13 16:12:40,612 - WARNING - warning message2017-05-13 16:12:40,612 - ERROR - error message2017-05-13 16:12:40,613 - CRITICAL - critical message error.log文件输出 122017-05-13 16:12:40,612 - ERROR - log.py[:81] - error message2017-05-13 16:12:40,613 - CRITICAL - log.py[:82] - critical message 4.其它例子123456789101112131415161718192021222324252627282930import logging#创建logger，如果参数为空则返回root loggerlogger = logging.getLogger("nick")logger.setLevel(logging.DEBUG) #设置logger日志等级#创建handlerfh = logging.FileHandler("test.log",encoding="utf-8")ch = logging.StreamHandler()#设置输出日志格式formatter = logging.Formatter( fmt="%(asctime)s %(name)s %(filename)s %(message)s", datefmt="%Y/%m/%d %X" )#注意 logging.Formatter的大小写 #为handler指定输出格式，注意大小写fh.setFormatter(formatter)ch.setFormatter(formatter)#为logger添加的日志处理器logger.addHandler(fh)logger.addHandler(ch)#输出不同级别的loglogger.warning("泰拳警告")logger.info("提示")logger.error("错误") python logging 重复写日志问题 用Python的logging模块记录日志时，可能会遇到重复记录日志的问题，第一条记录写一次，第二条记录写两次，第三条记录写三次 原因：没有移除handler 解决：在日志记录完之后removeHandler 例子 1234567891011121314151617181920212223242526272829def log(msg): #创建logger，如果参数为空则返回root logger logger = logging.getLogger("nick") logger.setLevel(logging.DEBUG) #设置logger日志等级 #创建handler fh = logging.FileHandler("test.log",encoding="utf-8") ch = logging.StreamHandler() #设置输出日志格式 formatter = logging.Formatter( fmt="%(asctime)s %(name)s %(filename)s %(message)s", datefmt="%Y/%m/%d %X" ) #为handler指定输出格式 fh.setFormatter(formatter) ch.setFormatter(formatter) #为logger添加的日志处理器 logger.addHandler(fh) logger.addHandler(ch) # 输出不同级别的log logger.info(msg)log("泰拳警告")log("提示")log("错误") 输出 1234562018/05/10 20:06:18 nick test.py 泰拳警告2018/05/10 20:06:18 nick test.py 提示2018/05/10 20:06:18 nick test.py 提示2018/05/10 20:06:18 nick test.py 错误2018/05/10 20:06:18 nick test.py 错误2018/05/10 20:06:18 nick test.py 错误 分析：可以看到输出结果有重复打印 原因：第二次调用log的时候，根据getLogger(name)里的name获取同一个logger，而这个logger里已经有了第一次你添加的handler，第二次调用又添加了一个handler，所以，这个logger里有了两个同样的handler，以此类推，调用几次就会有几个handler。 解决方案1 添加removeHandler语句 1234567891011121314151617181920212223242526272829303132333435import loggingdef log(msg): #创建logger，如果参数为空则返回root logger logger = logging.getLogger("nick") logger.setLevel(logging.DEBUG) #设置logger日志等级 #创建handler fh = logging.FileHandler("test.log",encoding="utf-8") ch = logging.StreamHandler() #设置输出日志格式 formatter = logging.Formatter( fmt="%(asctime)s %(name)s %(filename)s %(message)s", datefmt="%Y/%m/%d %X" ) #为handler指定输出格式 fh.setFormatter(formatter) ch.setFormatter(formatter) #为logger添加的日志处理器 logger.addHandler(fh) logger.addHandler(ch) # 输出不同级别的log logger.info(msg) #解决方案1，添加removeHandler语句，每次用完之后移除Handler logger.removeHandler(fh) logger.removeHandler(ch)log("泰拳警告")log("提示")log("错误") 解决方案2 在log方法里做判断，如果这个logger已有handler，则不再添加handler。 123456789101112131415161718192021222324252627282930313233import loggingdef log(msg): #创建logger，如果参数为空则返回root logger logger = logging.getLogger("nick") logger.setLevel(logging.DEBUG) #设置logger日志等级 #解决方案2：这里进行判断，如果logger.handlers列表为空，则添加，否则，直接去写日志 if not logger.handlers: #创建handler fh = logging.FileHandler("test.log",encoding="utf-8") ch = logging.StreamHandler() #设置输出日志格式 formatter = logging.Formatter( fmt="%(asctime)s %(name)s %(filename)s %(message)s", datefmt="%Y/%m/%d %X" ) #为handler指定输出格式 fh.setFormatter(formatter) ch.setFormatter(formatter) #为logger添加的日志处理器 logger.addHandler(fh) logger.addHandler(ch) # 输出不同级别的log logger.info(msg)log("泰拳警告")log("提示")log("错误") logger调用方式例子 123456789101112131415161718192021222324252627282930313233import loggingdef log(): #创建logger，如果参数为空则返回root logger logger = logging.getLogger("nick") logger.setLevel(logging.DEBUG) #设置logger日志等级 #这里进行判断，如果logger.handlers列表为空，则添加，否则，直接去写日志 if not logger.handlers: #创建handler fh = logging.FileHandler("test.log",encoding="utf-8") ch = logging.StreamHandler() #设置输出日志格式 formatter = logging.Formatter( fmt="%(asctime)s %(name)s %(filename)s %(message)s", datefmt="%Y/%m/%d %X" ) #为handler指定输出格式 fh.setFormatter(formatter) ch.setFormatter(formatter) #为logger添加的日志处理器 logger.addHandler(fh) logger.addHandler(ch) return logger #直接返回loggerlogger = log()logger.warning("泰拳警告")logger.info("提示")logger.error("错误")logger.debug("查错") 5.屏幕和文件同时输出logging库采取了模块化的设计，提供了许多组件：记录器、处理器、过滤器和格式化器。 Logger 暴露了应用程序代码能直接使用的接口。 Handler将（记录器产生的）日志记录发送至合适的目的地。 Filter提供了更好的粒度控制，它可以决定输出哪些日志记录。 Formatter 指明了最终输出中日志记录的布局。 Loggers:Logger 对象要做三件事情。首先，它们向应用代码暴露了许多方法，这样应用可以在运行时记录消息。其次，记录器对象通过严重程度（默认的过滤设施）或者过滤器对象来决定哪些日志消息需要记录下来。第三，记录器对象将相关的日志消息传递给所有感兴趣的日志处理器。 常用的记录器对象的方法分为两类：配置和发送消息。 这些是最常用的配置方法： Logger.setLevel()指定logger将会处理的最低的安全等级日志信息, debug是最低的内置安全等级，critical是最高的内建安全等级。例如，如果严重程度为INFO，记录器将只处理INFO，WARNING，ERROR和CRITICAL消息，DEBUG消息被忽略。Logger.addHandler()和Logger.removeHandler()从记录器对象中添加和删除处理程序对象。处理器详见Handlers。Logger.addFilter()和Logger.removeFilter()从记录器对象添加和删除过滤器对象。 Handlers处理程序对象负责将适当的日志消息（基于日志消息的严重性）分派到处理程序的指定目标。Logger 对象可以通过addHandler()方法增加零个或多个handler对象。举个例子，一个应用可以将所有的日志消息发送至日志文件，所有的错误级别（error）及以上的日志消息发送至标准输出，所有的严重级别（critical）日志消息发送至某个电子邮箱。在这个例子中需要三个独立的处理器，每一个负责将特定级别的消息发送至特定的位置。 常用的有4种： 1) logging.StreamHandler -&gt; 控制台输出 使用这个Handler可以向类似与sys.stdout或者sys.stderr的任何文件对象(file object)输出信息。 它的构造函数是：StreamHandler([strm])其中strm参数是一个文件对象。默认是sys.stderr 2) logging.FileHandler -&gt; 文件输出 和StreamHandler类似，用于向一个文件输出日志信息。不过FileHandler会帮你打开这个文件。它的构造函数是：FileHandler(filename[,mode])filename是文件名，必须指定一个文件名。mode是文件的打开方式。默认是’a’，即添加到文件末尾。 3) logging.handlers.RotatingFileHandler -&gt; 按照大小自动分割日志文件，一旦达到指定的大小重新生成文件 这个Handler类似于上面的FileHandler，但是它可以管理文件大小。当文件达到一定大小之后，它会自动将当前日志文件改名，然后创建 一个新的同名日志文件继续输出。比如日志文件是chat.log。当chat.log达到指定的大小之后，RotatingFileHandler自动把 文件改名为chat.log.1。不过，如果chat.log.1已经存在，会先把chat.log.1重命名为chat.log.2。最后重新创建 chat.log，继续输出日志信息。它的构造函数是：RotatingFileHandler( filename[, mode[, maxBytes[, backupCount]]])其中filename和mode两个参数和FileHandler一样。maxBytes用于指定日志文件的最大文件大小。如果maxBytes为0，意味着日志文件可以无限大，这时上面描述的重命名过程就不会发生。backupCount用于指定保留的备份文件的个数。比如，如果指定为2，当上面描述的重命名过程发生时，原有的chat.log.2并不会被更名，而是被删除。 4) logging.handlers.TimedRotatingFileHandler -&gt; 按照时间自动分割日志文件 这个Handler和RotatingFileHandler类似，不过，它没有通过判断文件大小来决定何时重新创建日志文件，而是间隔一定时间就 自动创建新的日志文件。重命名的过程与RotatingFileHandler类似，不过新的文件不是附加数字，而是当前时间。它的构造函数是：TimedRotatingFileHandler( filename [,when [,interval [,backupCount]]])其中filename参数和backupCount参数和RotatingFileHandler具有相同的意义。interval是时间间隔。when参数是一个字符串。表示时间间隔的单位，不区分大小写。它有以下取值：S 秒M 分H 小时D 天W 每星期（interval==0时代表星期一）midnight 每天凌晨 FormattersFormatter对象设置日志信息最后的规则、结构和内容，默认的时间格式为%Y-%m-%d %H:%M:%S，下面是Formatter常用的一些信息 %(name)s Logger的名字 %(levelno)s 数字形式的日志级别 %(levelname)s 文本形式的日志级别 %(pathname)s 调用日志输出函数的模块的完整路径名，可能没有 %(filename)s 调用日志输出函数的模块的文件名 %(module)s 调用日志输出函数的模块名 %(funcName)s 调用日志输出函数的函数名 %(lineno)d 调用日志输出函数的语句所在的代码行 %(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示 %(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数 %(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒 %(thread)d 线程ID。可能没有 %(threadName)s 线程名。可能没有 %(process)d 进程ID。可能没有 %(message)s 用户输出的消息 需求输出log到控制台以及将日志写入log文件。保存2种类型的log， all.log 保存debug, info, warning, critical 信息， error.log则只保存error信息，同时按照时间自动分割日志文件 1234567891011121314151617181920212223242526272829303132333435363738import loggingfrom logging import handlersclass Logger(object): level_relations = &#123; 'debug':logging.DEBUG, 'info':logging.INFO, 'warning':logging.WARNING, 'error':logging.ERROR, 'crit':logging.CRITICAL &#125;#日志级别关系映射 def __init__(self,filename,level='info',when='D',backCount=3,fmt='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'): self.logger = logging.getLogger(filename) format_str = logging.Formatter(fmt)#设置日志格式 self.logger.setLevel(self.level_relations.get(level))#设置日志级别 sh = logging.StreamHandler()#往屏幕上输出 sh.setFormatter(format_str) #设置屏幕上显示的格式 th = handlers.TimedRotatingFileHandler(filename=filename,when=when,backupCount=backCount,encoding='utf-8')#往文件里写入#指定间隔时间自动生成文件的处理器 #实例化TimedRotatingFileHandler #interval是时间间隔，backupCount是备份文件的个数，如果超过这个个数，就会自动删除，when是间隔的时间单位，单位有以下几种： # S 秒 # M 分 # H 小时、 # D 天、 # W 每星期（interval==0时代表星期一） # midnight 每天凌晨 th.setFormatter(format_str)#设置文件里写入的格式 self.logger.addHandler(sh) #把对象加到logger里 self.logger.addHandler(th)if __name__ == '__main__': log = Logger('all.log',level='debug') log.logger.debug('debug') log.logger.info('info') log.logger.warning('警告') log.logger.error('报错') log.logger.critical('严重') Logger('error.log', level='error').logger.error('error') 屏幕上结果如下 1234562018-03-13 21:06:46,092 - D:/write_to_log.py[line:25] - DEBUG: debug2018-03-13 21:06:46,092 - D:/write_to_log.py[line:26] - INFO: info2018-03-13 21:06:46,092 - D:/write_to_log.py[line:27] - WARNING: 警告2018-03-13 21:06:46,099 - D:/write_to_log.py[line:28] - ERROR: 报错2018-03-13 21:06:46,099 - D:/write_to_log.py[line:29] - CRITICAL: 严重2018-03-13 21:06:46,100 - D:/write_to_log.py[line:30] - ERROR: error 配置logging的几种方式 作为开发者，我们可以通过以下3中方式来配置logging: 1）使用Python代码显式的创建loggers, handlers和formatters并分别调用它们的配置函数； 2）创建一个日志配置文件，然后使用fileConfig()函数来读取该文件的内容； 3）创建一个包含配置信息的dict，然后把它传递个dictConfig()函数； 具体说明请参考另一篇博文 《python之配置日志的几种方式》 向日志输出中添加上下文信息 除了传递给日志记录函数的参数外，有时候我们还想在日志输出中包含一些额外的上下文信息。比如，在一个网络应用中，可能希望在日志中记录客户端的特定信息，如：远程客户端的IP地址和用户名。这里我们来介绍以下几种实现方式： 通过向日志记录函数传递一个extra参数引入上下文信息 使用LoggerAdapters引入上下文信息 使用Filters引入上下文信息 具体说明请参考另一篇博文 《Python之向日志输出中添加上下文信息》 关于Python logging的更多高级用法，请参考文档&lt;&lt; Logging CookBook &gt;&gt;。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python模块之时间模块]]></title>
    <url>%2Fpython%E6%A8%A1%E5%9D%97%E4%B9%8B%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://itopic.org/python-time-module.html https://www.liaoxuefeng.com/wiki/1016959663602400/1017648783851616 https://blog.csdn.net/watfe/article/details/84943732 time模块123456789101112131415&gt;&gt;&gt; print(time.__doc__)Functions:time() -- return current time in seconds since the Epoch as a floatclock() -- return CPU time since process start as a floatsleep() -- delay for a number of seconds given as a floatgmtime() -- convert seconds since Epoch to UTC tuplelocaltime() -- convert seconds since Epoch to local time tupleasctime() -- convert time tuple to stringctime() -- convert time in seconds to stringmktime() -- convert local time tuple to seconds since Epochstrftime() -- convert time tuple to string according to format specificationstrptime() -- parse string to time tuple according to format specificationtzset() -- change the local timezone 获取时间戳1234&gt;&gt;&gt; time.time()1552827527.447389&gt;&gt;&gt; int(time.time())1552827538 停留x秒12345&gt;&gt;&gt; print time.sleep.__doc__sleep(seconds)Delay execution for a given number of seconds. The argument may bea floating point number for subsecond precision. time与字符串互转123456&gt;&gt;&gt; print time.strftime.__doc__strftime(format[, tuple]) -&gt; stringConvert a time tuple to a string according to a format specification.See the library reference manual for formatting codes. When the time tupleis not present, current time as returned by localtime() is used. 12345&gt;&gt;&gt; print time.strptime.__doc__strptime(string, format) -&gt; struct_timeParse a string to a time tuple according to a format specification.See the library reference manual for formatting codes (same as strftime()). 12345678910111213#!/usr/bin/python3import time# 格式化成2016-03-20 11:45:39形式print (time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()))# 格式化成Sat Mar 28 22:24:24 2016形式print (time.strftime(&quot;%a %b %d %H:%M:%S %Y&quot;, time.localtime())) # 将格式字符串转换为时间戳a = &quot;Sat Mar 28 22:24:24 2016&quot;print (time.mktime(time.strptime(a,&quot;%a %b %d %H:%M:%S %Y&quot;))) datetime模块获取当前日期和时间123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now() # 获取当前datetime&gt;&gt;&gt; print(now)2015-05-18 16:28:07.198690&gt;&gt;&gt; print(type(now))&lt;class 'datetime.datetime'&gt; 注意到datetime是模块，datetime模块还包含一个datetime类，通过from datetime import datetime导入的才是datetime这个类。 如果仅导入import datetime，则必须引用全名datetime.datetime。 datetime.now()返回当前日期和时间，其类型是datetime。 获取指定日期和时间要指定某个日期和时间，我们直接用参数构造一个datetime： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; print(dt)2015-04-19 12:20:00 datetime转换为timestamp在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00时区的时刻称为epoch time，记为0（1970年以前的时间timestamp为负数），当前时间就是相对于epoch time的秒数，称为timestamp。 你可以认为： 1timestamp = 0 = 1970-1-1 00:00:00 UTC+0:00 对应的北京时间是： 1timestamp = 0 = 1970-1-1 08:00:00 UTC+8:00 可见timestamp的值与时区毫无关系，因为timestamp一旦确定，其UTC时间就确定了，转换到任意时区的时间也是完全确定的，这就是为什么计算机存储的当前时间是以timestamp表示的，因为全球各地的计算机在任意时刻的timestamp都是完全相同的（假定时间已校准）。 把一个datetime类型转换为timestamp只需要简单调用timestamp()方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; dt.timestamp() # 把datetime转换为timestamp1429417200.0 注意Python的timestamp是一个浮点数。如果有小数位，小数位表示毫秒数。 某些编程语言（如Java和JavaScript）的timestamp使用整数表示毫秒数，这种情况下只需要把timestamp除以1000就得到Python的浮点表示方法。 timestamp转换为datetime要把timestamp转换为datetime，使用datetime提供的fromtimestamp()方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t))2015-04-19 12:20:00 注意到timestamp是一个浮点数，它没有时区的概念，而datetime是有时区的。上述转换是在timestamp和本地时间做转换。 本地时间是指当前操作系统设定的时区。例如北京时区是东8区，则本地时间： 12015-04-19 12:20:00 实际上就是UTC+8:00时区的时间： 12015-04-19 12:20:00 UTC+8:00 而此刻的格林威治标准时间与北京时间差了8小时，也就是UTC+0:00时区的时间应该是： 12015-04-19 04:20:00 UTC+0:00 timestamp也可以直接被转换到UTC标准时区的时间： 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t)) # 本地时间2015-04-19 12:20:00&gt;&gt;&gt; print(datetime.utcfromtimestamp(t)) # UTC时间2015-04-19 04:20:00 str转换为datetime很多时候，用户输入的日期和时间是字符串，要处理日期和时间，首先必须把str转换为datetime。转换方法是通过datetime.strptime()实现，需要一个日期和时间的格式化字符串： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; cday = datetime.strptime(&apos;2015-6-1 18:19:59&apos;, &apos;%Y-%m-%d %H:%M:%S&apos;)&gt;&gt;&gt; print(cday)2015-06-01 18:19:59 字符串&#39;%Y-%m-%d %H:%M:%S&#39;规定了日期和时间部分的格式。详细的说明请参考Python文档。 注意转换后的datetime是没有时区信息的。 datetime转换为str如果已经有了datetime对象，要把它格式化为字符串显示给用户，就需要转换为str，转换方法是通过strftime()实现的，同样需要一个日期和时间的格式化字符串： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; print(now.strftime(&apos;%a, %b %d %H:%M&apos;))Mon, May 05 16:28 datetime加减对日期和时间进行加减实际上就是把datetime往后或往前计算，得到新的datetime。加减可以直接用+和-运算符，不过需要导入timedelta这个类： 12345678910&gt;&gt;&gt; from datetime import datetime, timedelta&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(hours=10)datetime.datetime(2015, 5, 19, 2, 57, 3, 540997)&gt;&gt;&gt; now - timedelta(days=1)datetime.datetime(2015, 5, 17, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(days=2, hours=12)datetime.datetime(2015, 5, 21, 4, 57, 3, 540997) 可见，使用timedelta你可以很容易地算出前几天和后几天的时刻。 本地时间转换为UTC时间本地时间是指系统设定时区的时间，例如北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间。 一个datetime类型有一个时区属性tzinfo，但是默认为None，所以无法区分这个datetime到底是哪个时区，除非强行给datetime设置一个时区： 12345678&gt;&gt;&gt; from datetime import datetime, timedelta, timezone&gt;&gt;&gt; tz_utc_8 = timezone(timedelta(hours=8)) # 创建时区UTC+8:00&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012)&gt;&gt;&gt; dt = now.replace(tzinfo=tz_utc_8) # 强制设置为UTC+8:00&gt;&gt;&gt; dtdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012, tzinfo=datetime.timezone(datetime.timedelta(0, 28800))) 如果系统时区恰好是UTC+8:00，那么上述代码就是正确的，否则，不能强制设置为UTC+8:00时区。 时区转换 我们可以先通过utcnow()拿到当前的UTC时间，再转换为任意时区的时间： 12345678910111213141516# 拿到UTC时间，并强制设置时区为UTC+0:00:&gt;&gt;&gt; utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)&gt;&gt;&gt; print(utc_dt)2015-05-18 09:05:12.377316+00:00# astimezone()将转换时区为北京时间:&gt;&gt;&gt; bj_dt = utc_dt.astimezone(timezone(timedelta(hours=8)))&gt;&gt;&gt; print(bj_dt)2015-05-18 17:05:12.377316+08:00# astimezone()将转换时区为东京时间:&gt;&gt;&gt; tokyo_dt = utc_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt)2015-05-18 18:05:12.377316+09:00# astimezone()将bj_dt转换时区为东京时间:&gt;&gt;&gt; tokyo_dt2 = bj_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt2)2015-05-18 18:05:12.377316+09:00 时区转换的关键在于，拿到一个datetime时，要获知其正确的时区，然后强制设置时区，作为基准时间。 利用带时区的datetime，通过astimezone()方法，可以转换到任意时区。 注：不是必须从UTC+0:00时区转换到其他时区，任何带时区的datetime都可以正确转换，例如上述bj_dt到tokyo_dt的转换。 小结datetime表示的时间需要时区信息才能确定一个特定的时间，否则只能视为本地时间。 如果要存储datetime，最佳方法是将其转换为timestamp再存储，因为timestamp的值与时区完全无关。 dateutil模块安装 1pip install python-dateutil parserparser是根据字符串解析成datetime,字符串可以很随意，可以用时间日期的英文单词，可以用横线、逗号、空格等做分隔符，可以包含时区。没指定时间默认是0点，没指定日期默认是今天，没指定年份默认是今年。 12345678910111213141516&gt;&gt;&gt; from dateutil.parser import parse&gt;&gt;&gt; parse("2018-10-21")datetime.datetime(2018, 10, 21, 0, 0)&gt;&gt;&gt; parse("20181021")datetime.datetime(2018, 10, 21, 0, 0)&gt;&gt;&gt; parse("2018/10/21")datetime.datetime(2018, 10, 21, 0, 0)&gt;&gt;&gt; parse("10-21")datetime.datetime(2018, 10, 21, 0, 0)&gt;&gt;&gt; parse("10/21")datetime.datetime(2018, 10, 21, 0, 0) rrulerrule(self, freq, dtstart=None, interval=1, wkst=None,count=None, until=None, bysetpos=None,bymonth=None, bymonthday=None, byyearday=None, byeaster=None,byweekno=None, byweekday=None, byhour=None, byminute=None, bysecond=None,cache=False) freq:可以理解为单位。可以是 YEARLY, MONTHLY, WEEKLY,DAILY, HOURLY, MINUTELY, SECONDLY。即年月日周时分秒 dtstart,until:是开始和结束时间 wkst:周开始时间 interval:间隔 count:指定生成多少个 byxxx:指定匹配的周期。比如byweekday=(MO,TU)则只有周一周二的匹配。byweekday可以指定MO,TU,WE,TH,FR,SA,SU。即周一到周日。 1234567891011121314151617181920212223242526272829303132333435363738&gt;&gt;&gt; from dateutil import rrule生成一个连续的日期列表&gt;&gt;&gt; list(rrule.rrule(rrule.DAILY,dtstart=parse('2018-11-1'),until=parse('2018-11-5')))[datetime.datetime(2018, 11, 1, 0, 0), datetime.datetime(2018, 11, 2, 0, 0), datetime.datetime(2018, 11, 3, 0, 0), datetime.datetime(2018, 11, 4, 0, 0), datetime.datetime(2018, 11, 5, 0, 0)]间隔一天&gt;&gt;&gt; list(rrule.rrule(rrule.DAILY,interval=2,dtstart=parse('2018-11-1'),until=parse('2018-11-5')))[datetime.datetime(2018, 11, 1, 0, 0), datetime.datetime(2018, 11, 3, 0, 0), datetime.datetime(2018, 11, 5, 0, 0)]只保留前3个元素&gt;&gt;&gt; list(rrule.rrule(rrule.DAILY,count=3,dtstart=parse('2018-11-1'),until=parse('2018-11-5')))[datetime.datetime(2018, 11, 1, 0, 0), datetime.datetime(2018, 11, 2, 0, 0), datetime.datetime(2018, 11, 3, 0, 0)]只要周一的&gt;&gt;&gt; list(rrule.rrule(rrule.DAILY,byweekday=rrule.MO,dtstart=parse('2018-11-1'),until=parse('2018-11-5')))[datetime.datetime(2018, 11, 5, 0, 0)]只要周六和周日的&gt;&gt;&gt; list(rrule.rrule(rrule.DAILY,byweekday=(rrule.SA,rrule.SU),dtstart=parse('2018-11-1'),until=parse('2018-11-5')))[datetime.datetime(2018, 11, 3, 0, 0), datetime.datetime(2018, 11, 4, 0, 0)]以月为间隔&gt;&gt;&gt; list(rrule.rrule(rrule.MONTHLY,dtstart=parse('2018-3-15'),until=parse('2018-7-30')))[datetime.datetime(2018, 3, 15, 0, 0), datetime.datetime(2018, 4, 15, 0, 0), datetime.datetime(2018, 5, 15, 0, 0), datetime.datetime(2018, 6, 15, 0, 0), datetime.datetime(2018, 7, 15, 0, 0)] 计算时间差 rrule可计算出两个datetime对象间相差的年月日等时间数量 1234567两个日期相差10天&gt;&gt;&gt; rrule.rrule(rrule.DAILY,dtstart=parse('2018-11-1'),until=parse('2018-11-10')).count()10某个日期到今天相差多少天&gt;&gt;&gt; rrule.rrule(rrule.DAILY,dtstart=parse('2018-11-1'),until=datetime.date.today()).count()10 两个日期相差几个月前一个月为m月，后一个月为n月，当日期不满整月时，差的月数按n-m算，当日期满整月后，差的月数按n-m+1算。差的年数同月数的情况一样。例子如下： 123456789&gt;&gt;&gt; rrule.rrule(rrule.MONTHLY,dtstart=parse('2018-3-15'),until=parse('2018-11-10')).count()8&gt;&gt;&gt; rrule.rrule(rrule.MONTHLY,dtstart=parse('2018-3-15'),until=parse('2018-11-20')).count()9&gt;&gt;&gt; rrule.rrule(rrule.YEARLY,dtstart=parse('2016-3-15'),until=parse('2018-2-10')).count()2&gt;&gt;&gt; rrule.rrule(rrule.YEARLY,dtstart=parse('2016-3-15'),until=parse('2018-3-15')).count()3 技巧当前时间转文本无论是time或datetime，哪个模块都可以，具体怎么输出，自行调整格式参数&#39;%Y-%m-%d %H:%M:%S&#39; %字符 表意 数值范围 %y 年(2位) 00, 01, …, 99 %Y 年(4位) 0001, 0002, …, 2013, 2014, …, 9998, 9999 %m 月 01, 02, …, 12 %d 日 01, 02, …, 31 %H 时(24小时制) 00, 01, …, 23 %M 分 00, 01, …, 59 %S 秒 00, 01, …, 59 %f 毫秒 000000, 000001, …, 999999 %z 时区 (empty), +0000, -0400, +1030, +063415, -030712.345216 这里只列一下我用到的，更多可以看官方文档：https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior 【注意】：time、datetime两种时间模块虽然都有strftime，但是(格式,时间)参数位置正好相反。 time模块（格式在前，时间在后） 1234567import timet = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime())print(type(t),t)# 输出结果&lt;class 'str'&gt; 2019-05-25 08:56:45123456 datetime模块（格式在后，时间在前） 1234567import datetimedt = datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')print(type(dt),dt)# 输出结果&lt;class 'str'&gt; 2019-05-25 08:56:45123456 文本转日期常规方法strptime(&quot;日期时间文本&quot;,&quot;文本格式&quot;) 制定时间格式，进行解析 123456import datetimedt = datetime.datetime.strptime('2019-05-25T07:46:45.743+0000','%Y-%m-%dT%H:%M:%S.%f%z')print(type(dt),dt)# 输出结果&lt;class 'datetime.datetime'&gt; 2019-05-25 07:46:45.743000+00:00 万能方法parse(&quot;日期时间文本&quot;) 自动解析 123456from dateutil.parser import parsedt = parse(&apos;2019-05-25 07:46:45&apos;)print(type(dt),dt)# 输出结果&lt;class &apos;datetime.datetime&apos;&gt; 2019-05-25 07:46:45 该方法适用于很多类型时间格式，建议使用前自行测试 123456789101112131415161718192021222324dt = [&quot;2001.07.04 AD at 12:08:56 PDT&quot;,&quot;Wed, 4 Jul 2001 12:08:56 -0700&quot;,&quot;190525&quot;,&quot;2018-08-06T10:00:00.000Z&quot;,&quot;Wed, Jul 4&quot;,&quot;12:08 PM&quot;,&quot;02001.July.04 AD 12:08 PM&quot;,&quot;20190525083855-0700&quot;,&quot;2001-07-04T12:08:56.235-0700&quot;,&quot;Thu Oct 16 07:13:48 GMT 2014&quot;]for i in dt: print(parse(i))# 输出结果2001-07-04 12:08:562001-07-04 12:08:56-07:002025-05-19 00:00:002018-08-06 10:00:00+00:002019-07-04 00:00:002019-05-25 12:08:002001-07-04 12:08:002019-05-25 08:38:55-07:002001-07-04 12:08:56.235000-07:002014-10-16 07:13:48+00:00 但是注意，国外日期时间的常用格式和国内不一样。单写19-05-25会被解析成2025年5月19日 12345print(parse("19-05-25"))# 输出结果2025-05-19 00:00:001234 如果是2019-05-25就不会错了 1234print(parse("2019-05-25"))# 输出结果2019-05-25 00:00:00 时间戳相关生成10或13位时间戳（做一些网页爬虫或构造提交时候可能用到） 1234567891011import timet = time.time()print(type(t),t)print('10位时间戳:',str(int(t)))print('13位时间戳:',str(int(t*1000)))# 输出结果&lt;class 'float'&gt; 1558750679.087248610位时间戳: 155875067913位时间戳: 155875067908712345678910 时间戳转日期时间（10位或13位通用） 1234567import datetimets = '1517302458364' # 该值为int或string类型均可dt1 = datetime.datetime.fromtimestamp(float(ts)/10**(len(str(ts))-10))print(type(dt1),dt1)# 输出结果&lt;class 'datetime.datetime'&gt; 2018-01-30 16:54:18.364000 时间差计算如果是两个datatime格式的日期，直接计算一下差值即可 12345678910111213141516import datetimedt1 = datetime.datetime.fromtimestamp(1517302458)dt2 = datetime.datetime.now()#print(type(dt1),dt1)#print(type(dt2),dt2)td = dt2-dt1#print(type(td),td)output = '相差%d天%.1f小时'%(td.days,td.seconds/60/60)print(output)# 输出结果&lt;class 'datetime.datetime'&gt; 2018-01-30 16:54:18&lt;class 'datetime.datetime'&gt; 2019-12-16 16:18:37.443000&lt;class 'datetime.timedelta'&gt; 684 days, 23:24:19.443000相差684天23.4小时123456789101112131415 如果是两个文本格式的日期，用parse()转换成datetime，同样计算差值即可 12345678import datetimefrom dateutil.parser import parsearr = ['2018-01-30 16:54:18','2019-12-16 16:18:37']td = parse(arr[1])-parse(arr[0])print('相差%d天%.1f小时'%(td.days,td.seconds/60/60))# 输出结果相差684天23.4小时 时区转换直接获取0时区的datetime，并转化为东8区datetime。 123456789from datetime import datetime,timezone,timedeltadt0 = datetime.utcnow().replace(tzinfo=timezone.utc)dt8 = dt0.astimezone(timezone(timedelta(hours=8))) # 转换时区到东八区print('UTC协调世界时 \t%s\nUTC+8北京时间\t%s'%(dt0,dt8))# 输出结果UTC协调世界时 2019-05-25 02:48:54.281741+00:00UTC+8北京时间 2019-05-25 10:48:54.281741+08:0012345678 如果是想将文本格式的2019-05-25T02:48:54.281741+00:00转化到东8区datetime 1234567891011from datetime import datetime,timezone,timedeltafrom dateutil.parser import parsedt0_str = '2019-05-25T02:48:54.281741+00:00'dt0 = parse(dt0_str)dt8 = dt0.astimezone(timezone(timedelta(hours=8)))print(type(dt0),dt0)print(type(dt8),dt8)# 输出结果&lt;class 'datetime.datetime'&gt; 2019-05-25 02:48:54.281741+00:00&lt;class 'datetime.datetime'&gt; 2019-05-25 10:48:54.281741+08:00 获取常用时间1234567891011from datetime import datetime, timedeltat1 = datetime.now()# 获取本月第一天t2 = datetime(t1.year, t1.month, 1)print(t2.strftime(&quot;%Y-%m-%d&quot;))# 本月最后一天t3 = datetime(year=t1.year, month=t1.month+1, day=1) - timedelta(days=1)print(t3.strftime(&quot;%Y-%m-%d&quot;))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之基础知识]]></title>
    <url>%2Fpython%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[python变量下划线小结下划线类型在python模块或类里面，一些变量的命名上有时会有下划线，表示了变量的特性和被访问限制 描述 例如 访问限制 前面单下划线 _var 变量所在模块/类以外的地方也能访问该变量，但最好不要 前面双下划线 __privateVar 变量所在模块/类以外的地方不能访问该变量，这是私有变量 前后双下划线 如__name__和__init__ Python内置特殊变量，哪儿都可以访问 什么是__name__一个python脚本，比如 hello.py，就是一个模块，这个模块的名字叫hello；一个模块既可以被其它模块导入（importable），也可以被直接执行（executable）. __name__是python的内置变量。如果一个模块是被直接执行的话，那么这个模块的__name__变量的值就是 __main__值；而如果这个模块是被其它模块导入的，那么这个模块的__name__变量的值就是模块的名字。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之小技巧]]></title>
    <url>%2Fpython%E4%B9%8B%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[[TOC] python字符串中包含大括号时怎么使用string的.format方法？123'hello &#123;name&#125;'.format(name='world')的时候大括号是特殊转义字符，如果需要原始的大括号，用&#123;&#123;代替&#123;, 用&#125;&#125;代替&#125;， 如下:&gt;&gt;&gt; 'hello &#123;&#123;worlds in braces!&#125;&#125;, &#123;name&#125;'.format(name='zhangsan')'hello &#123;worlds in braces!&#125;, zhangsan' python将一组数分成每3个一组的实例如下所示 1234a = [1,2,3,4,5,6,7,8,9,10,11]step = 3b = [a[i:i+step] for i in range(0,len(a),step)]print(b) 结果： 123[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11]]&gt;&gt;&gt; b[1][4, 5, 6] Python中字符串、列表、元组、字典之间的相互转换https://www.jb51.net/article/174331.htm 字符串字符串转换为列表 使用list()方法 123456789101112str_1 = "1235"str_2 = 'zhangsan'str_3 = '''lisi'''tuple_1 = list(str_1)tuple_2 = list(str_2)tuple_3 = list(str_3)print(type(tuple_1))print(type(tuple_2))print(type(tuple_3))print(tuple_1)print(tuple_2)print(tuple_3) 结果： 使用Python中字符串的内置方法split() Python split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串语法：str.split(str=””, num=string.count(str)).①str – 分隔符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。②num – 分割次数。默认为 -1, 即分隔所有。 12345678910111213141516str_1 = "12 35 213"str_2 = 'zhang san shi a 'str_3 = 'zhang san shi a 'str_4 = '''li si wang wu'''list_1 = str_1.split(" ")list_2 = str_2.split(" ",1)list_3 = str_3.split(" ")list_4 = str_4.split(" ",2)print(type(list_1))print(type(list_2))print(type(list_3))print(type(list_4))print(list_1)print(list_2)print(list_3)print(list_4) 结果： ​ 字符串 转换为 元组使用tuple()方法 123456789101112str_1 = "1235"str_2 = 'zhangsan'str_3 = '''lisi'''list_1 = tuple(str_1)list_2 = tuple(str_2)list_3 = tuple(str_3)print(type(list_1))print(type(list_2))print(type(list_3))print(list_1)print(list_2)print(list_3) 结果： 字符串 转换为 字典利用eval()方法，可以将字典格式的字符串转换为字典 eval() 函数用来执行一个字符串表达式，并返回表达式的值。语法：eval(expression[, globals[, locals]])①expression – 表达式。②globals – 变量作用域，全局命名空间，如果被提供，则必须是一个字典对象。③locals – 变量作用域，局部命名空间，如果被提供，可以是任何映射对象。 1234str_1 = "&#123;'name':'zhangsan','age':14,'gender':'girl'&#125;"dict_1 = eval(str_1)print(type(dict_1))print(dict_1) 结果： 利用json.loads()方法，可以将字典格式的字符串转换为字典 json.loads 用于解码 JSON 数据。该函数返回 Python 字段的数据类型。语法：json.loads(s[, encoding[, cls[, object_hook[, parse_float[, parse_int[, parse_constant[, object_pairs_hook[, **kw]]]]]]]]) 123456import jsonstr_1 = '&#123;"name":"xiaoming","age":18&#125;'dict_1 = json.loads(str_1)print(type(dict_1))print(dict_1) 结果： 列表列表转字符串利用‘’.join()将列表中的内容拼接程一个字符串 Python join() 方法用于将序列中的元素（必须是str） 以指定的字符(‘’中指定的) 连接生成一个新的字符串。 1234list_1 = ['a', 'b', 'c']str_1 = ''.join(list_1)print(type(str_1))print(str_1) 结果： 列表转字典利用for in rang将两个列表转换为字典 1234567list_1 = ['a', 'b', 'c']list_2 = [1, 2, 3]dict_1 = &#123;&#125;for i in range(len(list_1)): dict_1[list_1[i]] = list_2[i]print(type(dict_1))print(dict_1) 结果： 利用python内置方法dict()和zip()将两个列表转换为字典 dict() 函数用于创建一个字典。语法：class dict(kwarg)class dict(mapping, *kwarg)class dict(iterable, *kwarg)①kwargs – 关键字②mapping – 元素的容器。③iterable – 可迭代对象。 zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。语法：zip([iterable, …])iterabl – 一个或多个迭代器; 12345list_1 = ['name', 'age']list_2 = ['zhangsan',18]dict_1 = dict(zip(list_1, list_2))print(type(dict_1))print(dict_1) 结果： 元组（tuple）元组转换为字符串 使用方法__str__ 返回一个对象的描述信息 1234tuple_1 = (1, 2, 3)str_1 = tuple_1.__str__()print(type(str_1))print(str_1) 结果： 元组转换为列表使用方法list() list() 方法用于将元组转换为列表。语法：list( tup )tup – 要转换为列表的元组。 1234tuple_1 = (1, 2, 3)list_1 = list(tuple_1)print(type(list_1))print(list_1) 结果： 字典(dict)字典转换为字符串使用 json.dumps()方法 json.dumps 用于将 Python 对象编码成 JSON 字符串。json.dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding=“utf-8”, default=None, sort_keys=False, **kw) 字典转换为元组 使用方法 tuple() 字典在转换为元组之后，只会保存关键字 12345dict_1 = &#123;&quot;name&quot;:&quot;zhangsan&quot;, &quot;age&quot;:18&#125;tuple_1 = tuple(dict_1)print(type(tuple_1))print(tuple_1) 结果： 字典转换为列表 使用方法 list() 字典在转换为列表之后，只会保存关键字 12345dict_1 = &#123;"name":"zhangsan", "age":18&#125;list_1 = list(dict_1)print(type(list_1))print(list_1) 结果：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python模块之subprocess]]></title>
    <url>%2Fpython%E6%A8%A1%E5%9D%97%E4%B9%8Bsubprocess%2F</url>
    <content type="text"><![CDATA[[TOC] 简介subprocess最早在2.4版本引入。用来生成子进程，并可以通过管道连接他们的输入/输出/错误，以及获得他们的返回值。 subprocess用来替换多个旧模块和函数： os.system os.spawn* os.popen* popen2.* commands.* 运行python的时候，我们都是在创建并运行一个进程，linux中一个进程可以fork一个子进程，并让这个子进程exec另外一个程序。在python中，我们通过标准库中的subprocess包来fork一个子进程，并且运行一个外部的程序。subprocess包中定义有数个创建子进程的函数，这些函数分别以不同的方式创建子进程，所欲我们可以根据需要来从中选取一个使用。另外subprocess还提供了一些管理标准流(standard stream)和管道(pipe)的工具，从而在进程间使用文本通信。 subprocess模块常用函数 函数 描述 subprocess.run() Python 3.5中新增的函数。执行指定的命令，等待命令执行完成后返回一个包含执行结果的CompletedProcess类的实例。 subprocess.call() 执行指定的命令，返回命令执行状态，其功能类似于os.system(cmd)。 subprocess.check_call() Python 2.5中新增的函数。 执行指定的命令，如果执行成功则返回状态码，否则抛出异常。其功能等价于subprocess.run(…, check=True)。 subprocess.check_output() Python 2.7中新增的的函数。执行指定的命令，如果执行状态码为0则返回命令执行结果，否则抛出异常。 subprocess.getoutput(cmd) 接收字符串格式的命令，执行命令并返回执行结果，其功能类似于os.popen(cmd).read()和commands.getoutput(cmd)。 subprocess.getstatusoutput(cmd) 执行cmd命令，返回一个元组(命令执行状态, 命令执行结果输出)，其功能类似于commands.getstatusoutput()。 说明： 在Python 3.5之后的版本中，官方文档中提倡通过subprocess.run()函数替代其他函数来使用subproccess模块的功能； 在Python 3.5之前的版本中，我们可以通过subprocess.call()，subprocess.getoutput()等上面列出的其他函数来使用subprocess模块的功能； subprocess.run()、subprocess.call()、subprocess.check_call()和subprocess.check_output()都是通过对subprocess.Popen的封装来实现的高级函数，因此如果我们需要更复杂功能时，可以通过subprocess.Popen来完成。 subprocess.getoutput()和subprocess.getstatusoutput()函数是来自Python 2.x的commands模块的两个遗留函数。它们隐式的调用系统shell，并且不保证其他函数所具有的安全性和异常处理的一致性。另外，它们从Python 3.3.4开始才支持Windows平台。 subprocess.run()12345678910111213&gt;&gt;&gt; import subprocess# python 解析则传入命令的每个参数的列表&gt;&gt;&gt; subprocess.run(["df","-h"])Filesystem Size Used Avail Use% Mounted on/dev/mapper/VolGroup-LogVol00 289G 70G 204G 26% /tmpfs 64G 0 64G 0% /dev/shm/dev/sda1 283M 27M 241M 11% /bootCompletedProcess(args=['df', '-h'], returncode=0)# 需要交给Linux shell自己解析，则:传入命令字符串，shell=True&gt;&gt;&gt; subprocess.run("df -h|grep /dev/sda1",shell=True)/dev/sda1 283M 27M 241M 11% /bootCompletedProcess(args='df -h|grep /dev/sda1', returncode=0) subprocess.call()执行命令，返回命令的结果和执行状态，0或者非0 12345678910&gt;&gt;&gt; res = subprocess.call(["ls","-l"])总用量 28-rw-r--r-- 1 root root 0 6月 16 10:28 1drwxr-xr-x 2 root root 4096 6月 22 17:48 _1748-rw-------. 1 root root 1264 4月 28 20:51 anaconda-ks.cfgdrwxr-xr-x 2 root root 4096 5月 25 14:45 monitor-rw-r--r-- 1 root root 13160 5月 9 13:36 npm-debug.log# 命令执行状态&gt;&gt;&gt; res0 subprocess.check_call()执行命令，返回结果和状态，正常为0 ，执行错误则抛出异常 1234567891011121314151617181920&gt;&gt;&gt; subprocess.check_call(["ls","-l"])总用量 28-rw-r--r-- 1 root root 0 6月 16 10:28 1drwxr-xr-x 2 root root 4096 6月 22 17:48 _1748-rw-------. 1 root root 1264 4月 28 20:51 anaconda-ks.cfgdrwxr-xr-x 2 root root 4096 5月 25 14:45 monitor-rw-r--r-- 1 root root 13160 5月 9 13:36 npm-debug.log0&gt;&gt;&gt; subprocess.check_call(["lm","-l"])Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/usr/lib64/python2.7/subprocess.py", line 537, in check_call retcode = call(*popenargs, **kwargs) File "/usr/lib64/python2.7/subprocess.py", line 524, in call return Popen(*popenargs, **kwargs).wait() File "/usr/lib64/python2.7/subprocess.py", line 711, in __init__ errread, errwrite) File "/usr/lib64/python2.7/subprocess.py", line 1327, in _execute_child raise child_exceptionOSError: [Errno 2] No such file or directory subprocess.getoutput()接受字符串形式的命令，放回执行结果 12&gt;&gt;&gt; subprocess.getoutput('pwd')'/root' subprocess.getstatusoutput()接受字符串形式的命令，返回 一个元组形式的结果，第一个元素是命令执行状态，第二个为执行结果 123456#执行正确&gt;&gt;&gt; subprocess.getstatusoutput('pwd')(0, '/root')#执行错误&gt;&gt;&gt; subprocess.getstatusoutput('pd')(127, '/bin/sh: pd: command not found') subprocess.check_output()如果当返回值为0时，直接返回输出结果，如果返回值不为0，直接抛出异常 123&gt;&gt;&gt; res = subprocess.check_output("pwd")&gt;&gt;&gt; resb'/root\n' # 结果以字节形式返回 subprocess.Popen()其实以上subprocess使用的方法，都是对subprocess.Popen的封装，下面我们就来看看这个Popen方法。 subprocess.Popen的构造函数123class subprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=False, startup_info=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=()) 参数说明： args： 要执行的shell命令，可以是字符串，也可以是命令各个参数组成的序列。当该参数的值是一个字符串时，该命令的解释过程是与平台相关的，因此通常建议将args参数作为一个序列传递。 bufsize： 指定缓存策略，0表示不缓冲，1表示行缓冲，其他大于1的数字表示缓冲区大小，负数 表示使用系统默认缓冲策略。 stdin, stdout, stderr： 分别表示程序标准输入、输出、错误句柄。 preexec_fn： 用于指定一个将在子进程运行之前被调用的可执行对象，只在Unix平台下有效。 close_fds： 如果该参数的值为True，则除了0,1和2之外的所有文件描述符都将会在子进程执行之前被关闭。 shell： 该参数用于标识是否使用shell作为要执行的程序，如果shell值为True，则建议将args参数作为一个字符串传递而不要作为一个序列传递。 cwd： 如果该参数值不是None，则该函数将会在执行这个子进程之前改变当前工作目录。 env： 用于指定子进程的环境变量，如果env=None，那么子进程的环境变量将从父进程中继承。如果env!=None，它的值必须是一个映射对象。 universal_newlines： 如果该参数值为True，则该文件对象的stdin，stdout和stderr将会作为文本流被打开，否则他们将会被作为二进制流被打开。 startupinfo和creationflags： 这两个参数只在Windows下有效，它们将被传递给底层的CreateProcess()函数，用于设置子进程的一些属性，如主窗口的外观，进程优先级等。 stdout标准输出 1234&gt;&gt;&gt; res = subprocess.Popen("ls /tmp/yum.log", shell=True, stdout=subprocess.PIPE) # 使用管道&gt;&gt;&gt; res.stdout.read() # 标准输出b'/tmp/yum.log\n'res.stdout.close() # 关闭 stderr标准错误 12345678&gt;&gt;&gt; import subprocess&gt;&gt;&gt; res = subprocess.Popen("lm -l",shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)# 标准输出为空&gt;&gt;&gt; res.stdout.read()b''#标准错误中有错误信息&gt;&gt;&gt; res.stderr.read()b'/bin/sh: lm: command not found\n' poll()定时检查命令有没有执行完毕，执行完毕后返回执行结果的状态，没有执行完毕返回None 1234567&gt;&gt;&gt; res = subprocess.Popen("sleep 10;echo 'hello'",shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)&gt;&gt;&gt; print(res.poll())None&gt;&gt;&gt; print(res.poll())None&gt;&gt;&gt; print(res.poll())0 wait()等待命令执行完成，并且返回结果状态 1234&gt;&gt;&gt; obj = subprocess.Popen("sleep 10;echo 'hello'",shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)&gt;&gt;&gt; obj.wait()# 中间会一直等待0 terminate()结束进程 12345import subprocess&gt;&gt;&gt; res = subprocess.Popen("sleep 20;echo 'hello'",shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)&gt;&gt;&gt; res.terminate() # 结束进程&gt;&gt;&gt; res.stdout.read()b'' pid获取当前执行子shell的程序的进程号 1234import subprocess&gt;&gt;&gt; res = subprocess.Popen("sleep 5;echo 'hello'",shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)&gt;&gt;&gt; res.pid # 获取这个linux shell 的 进程号2778]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shadowsocks服务端搭建]]></title>
    <url>%2Fshadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[[TOC] 安装12yum install python-setuptools &amp;&amp; easy_install pip pip install shadowsocks 创建配置文件12sudo mkdir /etc/shadowsockssudo vi /etc/shadowsocks/shadowsocks.json 123456&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: 8838, &quot;method&quot;: &quot;rc4-md5&quot;, &quot;password&quot;: &quot;abcd1234&quot;&#125; 启动1ssserver -c /etc/shadowsocks/shadowsocks.json -d start 停止1ssserver -c /etc/shadowsocks/shadowsocks.json -d stop 端口服务器的TCP端口8838要放行 优化最大连接数提高最大连接数,添加如下参数 1234vim /etc/security/limits.conf* soft nofile 51200* hard nofile 51200 内核参数调整内核参数的目标是： 1.尽可能重用连接和端口号 2.尽可能增大队列和缓冲区 3.为高延迟和高流量选择合适的TCP拥塞算法 123456789101112131415161718192021vim /etc/sysctl.conffs.file-max = 51200net.core.rmem_max = 67108864net.core.wmem_max = 67108864net.core.netdev_max_backlog = 250000net.core.somaxconn = 4096net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 1200net.ipv4.ip_local_port_range = 10000 65000net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_fastopen = 3net.ipv4.tcp_mem = 25600 51200 102400net.ipv4.tcp_rmem = 4096 87380 67108864net.ipv4.tcp_wmem = 4096 65536 67108864net.ipv4.tcp_mtu_probing = 1net.ipv4.tcp_congestion_control = hybla 使生效 1sysctl -p]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金经理之陈洲]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E7%BB%8F%E7%90%86%E4%B9%8B%E9%99%88%E6%B4%B2%2F</url>
    <content type="text"><![CDATA[[TOC] 参考https://mp.weixin.qq.com/s/aqRaTx2FF-AzTH22UcaHyg 资产管理行业，是一个责任、经验和知识并重的行业。那些常年默默无闻，努力为投资者获取长期稳健收益的基金经理，会受到投资者的认可和尊重。在我的印象中，在2016年年底就有上海的持有人自发众筹请基金经理吃饭的事儿，在资管界传为美谈。这位基金经理，就是国泰基金的程洲。他管理的国泰聚信价值优势在牛市中能涨、股灾中抗跌，从基金成立日起持有五年平均收益翻倍，让部分持有人收获了满满的幸福感。 公募老将程洲自2000年就进入证券行业，从业经历长达19年。他在2004年加盟国泰基金，2008年开始担任基金经理，担任过11年的基金经理，是一位十足的公募老将。其管理的国泰聚信价值优势获得了2016年的金牛奖和三年持续回报明星基金奖，程洲本人也获得过3年期英华奖。 程洲管理过对“长期稳健收益”要求极高的社保基金。多年的投资实践中，他逐渐形成了对风险控制高度重视的“稳健收益”投资理念，为获取长期优秀的业绩奠定了基础。 过往业绩优秀目前程洲管理的基金资产约48亿，并且产品类型多样，有开放式也有定开式，有混合型也有股票型，有灵活配置型也有行业主题型，还管理社保基金，是一位资产管理的多面手。 我们从中选取两个代表性的产品进行重点分析，一个是灵活配置型的国泰聚信价值（000362）；另一个是行业主题基金国泰大农业（001579）。 国泰聚信价值成立于2013年12月，是灵活配置型基金（股票投资比例范围30%-95%）。程洲自基金成立任职至今（10月23日），任期已接近6年，任职回报157%，同类排名TOP5%(WIND，90/1939，一级分类，截止2019-10-23）。 国泰大农业成立于2017年6月，是行业主题型基金（股票投资比例不低于80%），规模2.4亿。程洲也是自基金成立开始任职至今，任期2.4年，任职回报55%，同类排名TOP2%（WIND，83/6023，一级分类，截止2019-10-23）。 这两只基金虽然投资范围、类型不一样，但业绩在同类中排名都非常靠前。可见程洲的投资管理能力比较全面，没有明显的短板，能驾驭各种类型的产品，应对各种市场行情。 风险控制 程洲在管理国泰聚信价值期间，任职回报相对于同期沪深300指数的超额收益达到94%。该基金年化收益率高达20%，远高于沪深300指数的9%。尤其是年化波动率和区间最大回撤也远低于沪深300指数，这一点难能可贵。凭借优异的业绩，该基金在2016年度获得一年期开放式混合型金牛基金、三年持续回报平衡混合型明星基金奖。 在管理国泰大农业期间，任职回报相对于中证大农业指数的超额收益达到41%。该基金的年化收益率高达22%，远高于中证大农业指数的6%。年化波动率和区间最大回撤也低于中证大农业指数。 从上述数据分析，程洲的投资风格是追求稳健收益，最大程度的控制回撤，减小净值波动。他管理的基金获取超额收益的能力较强，回撤控制的较好，风险收益比较高。 上马击狂胡：自上而下能择时程洲的投资框架可以概括为“取势、明道、优术”。取势，就是判断影响市场的主流发展趋势，进行大类资产配置，追求稳健组合回报。明道，就是善于极端行情下择时，在泡沫严重时及时离场。优术，即坚持简单可靠的基本面投资，选出优质的投资标的。 程洲具有宏观策略研究经验，具备自上而下的判断能力，在“取势”、“明道”方面相具有独特的优势。他是业内为数不多的具备择时能力、并成功进行择时操作的基金经理。在十多年的牛熊轮回中，择时次数不多，但是往往都在关键点上，有效控制了净值的波动和回撤。 以仓位灵活的国泰聚信价值为例，2015年上半年在牛市最疯狂的时候，程洲未雨绸缪，大幅将股票仓位从一季度末的87%降至三季度末的35%，并在2016年保持较低的仓位，不仅成功守住了2014-2015年那场牛市中的收益，还降低了2016年初“熔断”期间的损失。 2018年年初，白马股慢牛行情的末期，程洲也表现出敏锐的嗅觉，将国泰聚信价值的股票仓位从一季度末的91%大幅降至二季度末的59%。 程洲的择时周期相对较长，更多的是在极端情况下的择时操作，是一种资产泡沫严重时的避险行为。也从侧面反映出程洲的理性和冷静，以及对市场的敬畏。 下马草军书：自下而上能择股经过长期的投资实践，程洲选股的思路也很明确，即三大标准：行业龙头、现金流好、估值低。 行业龙头公司在各自的领域内有较强的护城河和竞争优势，具备稳定的发展前景。程洲也非常关注小行业中的龙头股，虽然市值小，但是在细分领域内具有很强的话语权，即“小行业中的大公司”。 现金流好（持续为正），是为了确保企业经营业绩的真实和可靠，一方面寻找出内生增长能力强的优质企业。另一方面，可以过滤掉一些财务问题比较多的公司，比如因缺钱大量融资，负债率很高，一旦遇到危机，容易出大问题。 而估值低则提供了安全边际，如果出现个股价格明显低于内在价值的情况，经过研究后认为机会不错，就买入并长期持有。一方面以较低的持有成本享受获取企业盈利增长的收益，另一方面还有机会获得估值回归的收益。 整体来看，程洲的持仓行业较为分散，持股集中度中等，以价值投资风格为主，持仓比较稳定。在市场风格变化较大时，他也会在一定程度上适应市场的风格。这样，他在获得长期稳定业绩的同时，又不失收益弹性。 程洲投资思路非常清晰、稳定，不追逐热点行业板块，不参与机构抱团。比如2015年上半年计算机、传媒被热捧，2017年以来大消费被机构抱团，国泰聚信价值并没有刻意去配置，依然按照一贯的投资思路管理。 程洲的持股周期有长有短，有格力电器、中国平安等持股周期3年以上的，也有的持有短于一年。在长期持有（1年以上）的个股中，多数都取得了超额收益。 追求稳健收益，历史波动低、回撤小，基民可当“甩手掌柜”对大多数投资者而言，对波动和风险都比较厌恶。大家所期望的，就是能够找到一位值得托付的基金经理，以高度的责任心和专业的管理能力，为自己的资产保驾护航。自己能安心做一个“甩手掌柜”，这也是基民最大的幸福。而程洲正是这样一位不可多得的基金经理。 岁月就是一把筛子，将程洲这样优秀的基金经理筛了出来。有如一坛老酒，愈久弥香。 程洲历经多轮牛熊，对市场充满敬畏之心，以追求稳健收益为目标，控制回撤能力强，在收益指标、风险指标上均位居同类基金前列。历史上他管理的基金常常能先于大盘创新高，使买入并持有的投资者能够收获“时间的玫瑰”。赚到了真金白银，这或许也是持有人众筹请他吃饭的主要原因吧。 新基金国泰鑫睿混合（007835），这是程洲的新近发的一只基金。在整体的产品设计上和国泰聚信价值比较相似，为混合型产品。股票投资比例范围是35%-80%，有利于程洲延续一贯灵活配置的投资风格，发挥多行业（偏向特色原料药、新材料和自主可控行业）配置、极端情况下择时、严控回撤等优势。国泰鑫睿混合也将稳健管理，力争为持有人贡献稳定的回报。 在以前的文章中，我多次给大家分析过，成立于3000点附近的主动偏股基金，在长期往往有不俗的业绩。此时发新基金，对于把握低位建仓的十分有利。并且随着A股市场日益成熟，慢牛行情可期，目前就是不错的配置时点。 国泰鑫睿混合已于2019年10月21日开始发行，认购期11月8日结束，各银行、券商、第三方销售机构均有销售。期望获得长期稳健收益的投资者可以考虑参与，将资产托付给程洲这样的基金经理，从此做一个幸福的基民。]]></content>
      <categories>
        <category>基金经理</category>
      </categories>
      <tags>
        <tag>基金</tag>
        <tag>基金经理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金经理之董承非]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E7%BB%8F%E7%90%86%E4%B9%8B%E8%91%A3%E6%89%BF%E9%9D%9E%2F</url>
    <content type="text"><![CDATA[[TOC] 参考https://mp.weixin.qq.com/s?__biz=MzA5MDQxODY5MQ==&amp;mid=2247487742&amp;idx=1&amp;sn=feb16135c29e9e96889f7de394c696f6&amp;source=41#wechat_redirect 兴全新视野是一只定期开放式的偏股混合型基金，成立于2015年7月1日，成立至今（2019年10月24日），基金收益率41.16%。而同期上证指数由4053点下跌到2940点，跌幅27.46%，基金收益率大幅战胜上证指数68.62%。 而现在，从2019年10月25日-31日，新视野再次开放了！那么这只基金值得买吗？分析后的结论或许会让你大吃一惊。 优秀的基金战胜指数 上图为今年以来兴全新视野与上证指数的走势对比图，可以很明显的看出： 在上涨行情中，基金涨幅跟得上 在下跌行情中，基金下跌明显较慢 近几个月震荡行情中，显著超越上证指数 任意一次开放时买入持有至今，都是盈利的 昨天（2019年10月24日）的净值为1.4116元，是成立以来所有开放期的最高净值，这代表所有购买该基金的投资者，都是赚钱的！ 十年老将，明星基金经理董承非 董承非管理时间较长的基金共有3只，其中兴全趋势和兴全全球视野在他任职期间的复合收益率都达到了18%。 兴全新视野因为成立的点位较高，年化收益只有8.31%，但考虑同期指数表现，还是十分牛逼。 非常有实力的基金公司 兴全基金旗下所有基金，在业内排名均十分靠前，优秀是其一贯的传统。 众所周知，我一直是兴全基金的铁杆粉丝，多次推荐兴全基金公司《你应该知道的一家基金公司》。 并且我也是董承非的铁杆粉丝，我自己定投的主动型基金中，就有董承非管理的基金。 那么，兴全新视野开放了，是不是可以买入呢？ 公募外衣的私募产品设计下面我来分析一下兴全新视野这个基金。新视野是公募基金，但是从产品设计结构上讲，更像是私募。主要体现在3个地方： 封闭期 股票仓位占比 业绩报酬 封闭期兴全趋势是普通的开放式基金，随时可以进行申购赎回。而兴全新视野是定期开放型，每3个月才开放一次。 从好的方面看，有的人心态不好，容易受短期行情的影响而随意赎回基金，有封闭期可以限制短期冲动产生的赎回欲望。更容易坚持长期持有，才能真的赚到钱。 从坏的方面说，基金有封闭期，定期开放申购赎回，对投资者的便利性会有一定影响。 我认为3个月封闭期应该是中性的，有好有坏 股票仓位传统的偏股混合型基金，都有股票的最低仓位限制，比如兴全趋势最低股票持仓比例为30%。这代表即使是牛市最高点，也必须最少持有30%的股票持仓。 而兴全新视野的股票仓位为0-95%。如果基金经理认为市场行情不好，完全可以将基金仓位降低到0%，股市再怎么跌也没关系。因为兴全新视野成立于2015年7月1日，处于市场高位。 在基金成立初期，兴全新视野因为0仓位优势，基本上完全不受市场下跌的影响。净值走势十分稳健。 但是随着产品的运行，且目前处于估值低位，0仓位的意义较小 上图为兴全趋势与兴全新视野的仓位对比，可以看出，两只基金的仓位目前基本贴合到一起。 从业绩表现上看： 上图为2017年至今，兴全趋势与兴全新视野业绩对比。看图可以发现2017年以来，兴全趋势与新视野走势高度重合。并且兴全趋势还略高于兴全新视野 上图是兴全新视野与兴全趋势在2019年的业绩表现对比。延续了以前的趋势，两个基金的业绩走势图，基本完全重合。 从基金仓位和业绩表现上：兴全趋势与兴全新视野的仓位基本接近一致，且两只基金的业绩差别并不大。 业绩报酬兴全新视野最大的特点是也像私募一样会计提业绩报酬。 业绩报酬计提规则也很简单：单个封闭期收益超过1.5%且净值要创新高（高于过往开放时的最高累计净值），收益超过1.5%的部分，基金公司分成20%。 计提过程举个例子： 如果某人于2019年7月25日买入10000份兴全新视野，净值1.304元，总资产13040元。 1.5%的收益为195.6元 假设本次开放时净值为1.343元，10000份，总资产为13430元。 盈利为390元，超出部分为194.4元。需要计提业绩报酬 = 194.4 * 0.2 = 38.88元。 基金公司将对该投资收取38.88元的业绩分成，收取的方式为扣减份额。 38.88元相当于28.95份。 在计提完业绩报酬后，实际持有的份额为9971.05份，净值1.343元，总资产13391.12元。 虽然从基金收益上看，兴全趋势和兴全新视野的业绩表现差不多，但考虑业绩报酬后，兴全新视野的实际收益大幅下降。 结论 毫无疑问，董承非是长期业绩十分优秀的牛基基金经理。 其旗下管理的两只基金兴全趋势与兴全新视野业绩都很好 从业绩上看，两者近几年表现相当 兴全新视野的3个月封闭期，我认为有利有弊 从费用上说，20%的业绩报酬，将会较大影响投资者的实际收益水平。 如果看好董承非，我更推荐购买兴全趋势。]]></content>
      <categories>
        <category>基金经理</category>
      </categories>
      <tags>
        <tag>基金</tag>
        <tag>基金经理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金之优秀的基金经理]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E4%B9%8B%E4%BC%98%E7%A7%80%E7%9A%84%E5%9F%BA%E9%87%91%E7%BB%8F%E7%90%86%2F</url>
    <content type="text"><![CDATA[[TOC] 转载（二鸟说） https://mp.weixin.qq.com/s/Fz6phgA53lrWeC4Cb7T9uw https://mp.weixin.qq.com/s/npdQZWezBIvYoFdSM7KcGg 公募基金主动管理TOP50 需要说明一下，指数基金仅有体现主动管理能力的指数增强基金入选，被动管理的指数基金不在此之列。债券基金也是只有体现大类资产配置和权益管理能力的二级债基、绝对收益基金入选，纯债基金不在研究范围。 权益类公募赚钱不少，但投资者体验较差，这已经是讨论了多年的话题。除了投资者追涨杀跌、高买低卖之外，基金的实际表现与投资者预期的差距也是不容忽视的原因。 很多投资者认为：主动型基金你既然提计了1.5%的管理费，那你就应该给我把活全干了。首先你要给我做大类资产配置，还要给我择股选券，然后还要给我择时。再说白一点，既要涨的快、赚钱多，又要波动低、回撤小。这就是很多基民的要求。 要求无可厚非，但现实与理想还是有较大差距的。基金经理是人不是神。大类资产配置、择股选券、择时这三大能力很少有人能通吃的。能做好一项，就是成功的基金经理了，能做好两项绝对是TOP级。基金经理都有自己的能力圈。你要求他的能力圈无限扩大，这是违背客观规律的。 对基金经理来说，承认自己的能力圈、恪守自己的能力圈，这也是对基民负责。投资者应该是要较为深刻的了解基金经理的投资体系和能力圈，进而选择适合自己的产品。 成功的基金经理，都有自己的风格，基本都能贴个标签。这个基金是什么特点、怎么使用，基民一目了然。标签实际上就是把晦涩难懂的投资术语通俗化的过程。 下面我把《公募基金主动管理TOP50人名单，价值连城，拿走不谢》里面列的主要基金经理，都贴一个标签，供大家参考。（名单有所调整和补充） 易方达 张坤（易方达中小盘110011） 价值投资、高ROE、白酒、长期持有、长线绩优 萧楠（易方达消费行业110022） 消费、白酒、择股、长期持有、长线绩优 张清华（易方达安心回报110027） 长期绩优债基、激进二级债基、转债、大类资产配置 胡剑（易方达稳健收益110008） 长期绩优债基、稳健二级债基、大类资产配置、控制回撤 陈皓 博时 王俊（博时主题行业160505） 十年十倍、价值投资、合理估值、控制回撤、稳健 过钧（博时信用债券050011） 长期绩优债基、激进二级债基、转债、大类资产配置 华夏 蔡向阳（华夏回报002001） 十年十倍、到点分红、价值投资、稳健 广发 傅友兴（广发稳健增长270002） 股债均衡、大类资产配置、价值投资、控制回撤、长线绩优 谭昌杰（广发趋势优选000215） 绝对收益、大类资产配置、控制回撤、稳健 南方 史博（南方绩优成长202003） 二十年老将、副总、价值投资、稳健 骆帅（南方优选成长202023） 潜力新秀、价值投资、高ROE、稳健 工银瑞信 鄢耀/王君正（工银金融地产000251） 金融地产、稳健、长线绩优 袁芳（工银文体产业001714） 潜力新秀、择股、稳健、灵活 欧阳凯（工银双利485111） 长期绩优债基、稳健二级债基、大类资产配置、控制回撤 富国 朱少醒（富国天惠161006） 二十年老将、副总、十年十倍、成长股投资 李笑薇（富国沪深300增强100038） 副总、海龟、量化、指数增强 张峰（富国中国中小盘100061） 长期绩优QDII、港股及海外、择股 兴全 董承非（兴全趋势163402） 二十年老将、副总、十年十倍、稳健、择股 谢治宇（兴全合润163406） 300亿操盘手、均衡配置、择股、稳健、长线绩优 乔迁（兴全商业模式163415） 潜力新秀、择股、稳健、灵活 申庆（兴全沪深300指数163407） 二十年老将、量化、指数增强 景顺长城 刘彦春（景顺长城新兴成长260108） 十年十倍、价值投资、高ROE、择股、白酒、长期持有 黎海威（景顺长城量化新动力001974） 副总、海龟、量化、指数增强 余广（景顺长城核心竞争力260116） 价值投资、高ROE、择股、长期持有 鲍无可（景顺长城能源基建260112） 稳健、高股息、低估值、控制回撤 中欧 曹名长（中欧潜力价值001810） 价值投资、高股息、低估值 周应波（中欧时代先锋001938） 成长股投资、择股、灵活、长线绩优 周蔚文（中欧新蓝筹166022） 二十年老将、副总、成长股投资、择股 葛兰（中欧医疗健康003095） 医药、潜力新秀、医药科班、择股 汇添富 劳杰男（汇添富价值精选519069） 价值投资、长期持有、择股、高ROE、稳健、长线绩优 雷鸣（汇添富蓝筹稳健519066） 价值投资、长期持有、择股、高ROE、稳健、灵活、长线绩优 王栩（汇添富优势精选519008） 十年十倍、成长股投资、择股 赵鹏飞（汇添富高端制造001725） 暂无 郑磊（汇添富创新医药混合006113） 医药、择股、医药科班、控制回撤 长安 林忠晶（长安鑫利001281） 林忠晶：价值投资、高ROE、择股、择时 杜振业（长安鑫益002146） 杜振业：绝对收益、潜力新秀、控制回撤 华安 杨明（华安策略优选040008） 杨明：价值投资、择股、大金融、长线绩优 苏圻涵(华安沪港深通精选001581） 苏圻涵：长线绩优QDII、港股及海外、择股 胡宜斌 胡宜斌：成长股投资、潜力新秀、择股、科技、5G 交银 王崇（交银新成长518736） 王崇：均衡配置、择股、稳健、长线绩优 何帅（交银优势行业519697） 何帅：成长股投资、择股、控制回撤、长线绩优 杨浩（交银新生活力519772） 杨浩：成长股投资、择股、TMT 嘉实 归凯（嘉实泰和000595） 归凯：十年十倍、GRAP、稳健、择股 谭丽（嘉实价值优势070019） 谭丽：价值投资、高股息、低估值、稳健 张金涛（嘉实沪港深精选001878） 暂无 洪流 洪流：二十年老将、GRAP（合理估值下的成长）、稳健、择股 华泰柏瑞 田汉卿（华泰柏瑞量化增强000172） 田汉卿：副总、海龟、量化、指数增强 东方红 林鹏（东方红沪港深002803） 林鹏：二十年老将、副总、价值投资、择股、高ROE、长期持有、长线绩优 睿远 傅鹏博（睿远成长价值007119） 傅鹏博：二十年老将、副总、十年十倍、成长股投资、择股、长线绩优 中庚 丘栋荣（中庚价值领航006551） 丘栋荣：价值投资、PB-ROE、长线绩优 信达澳银1人： 冯明远（信达澳银新能源001410） 冯明远：潜力新秀、成长股投资、择股、科技、5G 国富 徐成（国富沪港深成长精选001605） 徐成：港股及海外、绩优沪港深、择股 海富通 杜晓海（海富通阿尔法对冲519062） 杜晓海：对冲、绝对收益、控制回撤、稳健 泓德 邬传雁（泓德远见回报001500） 邬传雁：副总、成长股投资、稳健、择股、长线绩优 鹏华 王宗合：价值投资、择股、高ROE、长期持有、白酒、长线绩优 前海开源 邱杰：择时、择股、大波段、长线绩优 大家可以看的出来，有4人入选的基金公司共五家：易方达、兴全、中欧、汇添富、景顺长城，这些都是比较注重主动权益产品的基金公司。 另外，东方红、睿远也都比较强调主动管理能力。东方红由于风格都差不多，只列了林鹏一人。睿远目前只有一只基金。 投资风格梳理按照投资风格和基金的类别，再次梳理一遍： 价值：张坤、王俊、蔡向阳、史博、骆帅、董承非、谢治宇、曹名长、刘彦春、余广、劳杰男、雷鸣、赵鹏飞、林忠晶、杨明、谭丽、林鹏、丘栋荣 成长：袁芳、朱少醒、乔迁、周应波、周蔚文、王栩、王崇、何帅、杨浩、归凯、傅鹏博、冯明远、邬传雁 量化及指数增强：李笑薇、黎海威、申庆、田汉卿 股债平衡：傅友兴、鲍无可 港股及海外：张峰、徐成、张金涛、苏圻涵 行业主题：萧楠（消费）、葛兰（医药）、鄢耀/王君正（金融地产） 激进二级债券：张清华、过钧 稳健二级债券：胡剑、欧阳凯 绝对收益：谭昌杰、杜振业 对冲：杜晓海 二鸟老师做组合的基金，基本来源于这50人名单。像价值五剑，有几个攻守均衡、风格稳重的老将坐镇，构成基本盘。在今年一季度行情好时，使用了像信达澳银新能源这样极富攻击性的基金去冲锋；去年行情不好时，使用了广发优企精选、富国沪港深行业精选这样比较注重控制回撤的基金去防守。在牛市走到中后期，市场喧嚣、牛熊难辨时，则可能会使用激进二级债基，让张清华、过钧这些擅长宏观研究的牛人来判断。 绝对收益目标组合，主要是使用的做绝对收益的谭昌杰、杜振业、做对冲的杜晓海以及稳健二级债基胡剑的基金。净值曲线稳步向上，接近于一条15度斜率的直线，不断创新高。 在实战角度的话，我不怕你业绩不好，就怕你没风格。你一会价值，一会成长，来回变来变去的话，这样的基金我是没法使用的。在选基金的时候，像这样的标签如果找不出三个以上的话，就不能入选。 有特点和风格的基金，即使是业绩暂时落后，我也能理解。比如老曹的中欧恒利，今年被骂的很厉害。但我是不会去骂的。老曹的择股风格是低估值、高股息，比较注重安全性。这两年市场呈“强者恒强”特征，低估值高股息策略在今年是暂时失效的。你不要因为这两年业绩不佳而质疑他的能力，只能说他的风格暂时不适合当前行情。但后面会怎样，这个说不好。风格都是轮动的，没有永远涨的股，也没有永远跌的票。他坚持自己的投资体系，这没问题。相反，如果他把那些低估值的票割肉，去追茅台之类的话，这就是瞎搞了。 再比如像冯明远的信达澳银新能源，这是一个具有鲜明科技风格的成长型基金，但波动也较大，价值五剑曾使用过一段时间。虽然现在没使用了，但我从来没否定过这只基金。这个基金长期持有没问题，这个我知道。我自己个人账户的话，肯定是不会卖的。价值五剑是公众跟投账户，完全是两码事。把这个基金替换下来，主要是出于控制组合波动考虑的，后面也不排除会阶段性再使用。类似的基金还有胡宜斌的华安媒体互联网。 对于投资者来说，鲜明的标签也便于基民识别和使用。你认可价值投资长期持有，就去找东方红、睿远、兴全；相信GRAP，就去找嘉实洪流、归凯他们；低估值高股息策略是老曹、谭丽；看好科技板块，能承受一定的波动，就使用冯明远、胡宜斌的基金；要求绝对收益，去找谭昌杰、杜振业；要做择时，还只有前海开源王宏远他们有这个能力；医药的话，得找葛兰、郑磊这些医药科班出身的基金经理；搞消费、白酒，去找萧楠、王宗合。 主动型基金的风格飘移，这是投资者最头疼的问题。建议每一位基金经理都要承认自己的能力圈、恪守自己的能力圈，进而拓展自己的能力圈。主动基金的基金经理，最好每个人都要有个标签。]]></content>
      <categories>
        <category>基金经理</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金之绝对收益基金]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E4%B9%8B%E7%BB%9D%E5%AF%B9%E6%94%B6%E7%9B%8A%E5%9F%BA%E9%87%91%2F</url>
    <content type="text"><![CDATA[[TOC] 转载（二鸟说） https://mp.weixin.qq.com/s/GzzOK5upb4uBRvcO-OBxSA https://mp.weixin.qq.com/s?__biz=MzI4MTYyODkxMw==&amp;mid=2247485851&amp;idx=1&amp;sn=a9f245e0ad556add1b25b53ff830853b&amp;chksm=eba71005dcd099138b92795378bfe66d078adafc21755ee92062aa8efaf9f728a6ae070f0382&amp;scene=21#wechat_redirect https://mp.weixin.qq.com/s?__biz=MzI4MTYyODkxMw==&amp;mid=2247485728&amp;idx=1&amp;sn=c997f8688cb49f7ba91ab249706a66d3&amp;chksm=eba710bedcd099a859aeb47d9166543f27672e2e9ae8d0d8d5360555d31e920f3b511d18626f&amp;scene=21#wechat_redirect https://mp.weixin.qq.com/s?__biz=MzI4MTYyODkxMw==&amp;mid=2247485888&amp;idx=1&amp;sn=f9e925a3ca905bcea8f5d749ca07e1ea&amp;chksm=eba7105edcd099480ae44bb824f787aa29598e988c527abf2546f8c9d815132a4562d14503f1&amp;scene=21#wechat_redirect 简介随着最后一只保本基金的转型，保本基金已经成为历史名称。但投资者对于“类保本”产品的需求并不会因此而消失，绝对收益目标基金应运而生。 所谓绝对收益目标基金，即以追求绝对收益为目标，不与某个资产价格指数进行比较，而以某个固定的收益值进行比较。由此可见，绝对收益基金在运作策略方面与追求相对收益的普通开放式基金存在明显差异。 绝对收益策略主要包括“固收+”、保本、对冲、以及多种混合策略等。保本策略对应的就是避险策略基金。这实际上是保本基金转型的产品。这类产品现在很少有基金公司愿意发行。具体原因我在《保本基金为何纷纷“大逃亡”》进行了分析。 所以，目前主要采用固收+和对冲这两种策略。“固收+”策略的本质是以中短期绝对收益率为目标、以安全资产为底仓提供基础收益，并在严格控制回撤的前提下寻求利用高弹性资产增强收益的机会。具体有固收+股票、固收+转债、固收+打新等策略。对冲策略则是利用衍生产品对投资组合进行对冲，将组合的系统性风险控制在一定的范围内，从而达到降低波动，控制回撤的效果。 无论采用哪种策略，都对基金公司和基金经理的实力提出了较高的要求。管理这类基金，不仅要同时具备固收和权益的投资能力，还需要具备大类资产配置、衍生品交易以及打新等方面的能力。因此，绝对收益目标基金并不是任何基金经理都能管得好的，由此决定了这类产品的稀缺性。 按合规要求，基金公司原则上是不能公开宣传“绝对收益”的，光从基金名称是看不出什么名堂来的。你要深入研究他的投资策略和净值表现才能确定。因此，在海量的基金中选出这类产品，就是一项极其费脑筋的工作。 为此，我们通过细致认真地研究，甄选出五个在运作中执行了绝对收益策略的产品。我们认为这几个基金还是比较靠谱的。现详细介绍如下，以供大家参考。 “固收+”策略产品 光大保德信安和/安诚债券（003109） 长城新优选混合（002227） 银华汇利灵活配置混合（001289） 这三只基金中，光大保德信的产品为二级债基，长城和银华的产品为混合基金，目前均开放申赎。从上图中三只产品的业绩走势：平缓的增长伴随极小的回撤，我们可以判断出它们都是按照绝对收益策略进行运作的。 在收益表现方面，三只产品在存续期内走势十分平稳，累积收益随时间持续增长，并在各个期间均取得了正收益。其中长城新优选表现抢眼，在各个期间内均取得了最高的收益，三年累积收益高达25.6%，年化收益超过8%。 此外，三只产品在斩获理想的收益同时，也很好的控制了净值回撤。从上图可以看出，它们在存续期内绝大多数的时候，最大回撤都控制在-0.5%以内，即便在极端市场行情中，最大回撤也都控制在了-1.5%以内。这充分体现了绝对收益目标基金在控制回撤方面的严格性，而这也正是这类产品与普通二级债基以及增强债基的区别。 从资产配置方面可以看出，三只产品均为典型的“固收+”组合，底仓均由债券构成，并少量持有股票寻求收益增强。在以固收+股票为主的同时，也少量运用了转债和打新策略。其中长城新优选持有股票比重最大，组合整体的历史波动也是最高的，这在一定程度上解释了为何它在三只产品中收益最高。 从上面持有人结构数据可以看出，机构十分青睐这类产品。一方面是由机构资金的风险偏好决定的。另一方面，这类产品受“聪明资金”追捧的程度，也能从侧面反映出其稀缺程度。其中长城优选机构资金占比高达97%，银华汇利机构资金占比也超过90%，几乎可以视为“机构定制版”产品。 另外三个优秀产品： 广发趋势优选（000215） 长安鑫益（002146） 安信稳健增值（001316） 这次为大家选出的三只样本基金是长安鑫益增强（002146，自2018年2季度定位于绝对收益）、安信稳健增值(001316)、广发趋势优选(000215)。 长安鑫益增强自2018年二季度以来，安信稳健增值、广发趋势优选自成立以来，累计净值曲线都平稳向上，实现了年年正收益。 再细化到季度。最近8个季度中，长安鑫益增强和安信稳健增值连续实现季度正收益，广发趋势优选仅有一个季度没有实现正收益，而同期同类平均和沪深300均有五次是负收益。这种绝对收益无疑给了投资者极佳的投资体验。 在回撤控制方面，这三只基金也做的很出色。2015年6月至2019年8月，A股经历了熊市、反弹、震荡等七个阶段的行情。 2015/12/26—2016/1/29期间，A股急速下跌，短短24个交易日，沪深300急跌23.2%，最大回撤24.2%，同期安信稳健增值最大回撤仅为0.7%，广发趋势优选最大回撤仅为1.7%。 2019/4/20—2019/8/7股市震荡下行期间，沪深300最大回撤为11.5%，但是三只基金最大回撤均得到了很好的控制，长安鑫益增强最大回撤为0.1%，安信稳健增值为1%，广发趋势优选为0.8%。 从阶段收益可以看出，时间周期越长，这类基金积累的收益越多。 近1年长安鑫益增强累计收益达到了20.31%，安信稳健增值为8.89%、广发趋势优选为6.75%。近2年，安信稳健增值累计收益17.02%，广发趋势优选累计收益15.75%，同期沪深300下跌2.32%。 正如广发趋势优选基金经理谭昌杰所言：只要方向正确，慢就是快。这类基金虽然每年的收益未必很高，但回撤小，长期下来经过复利的积累，做出的累积收益依然惊人。像运作时间最长的广发趋势优选，成立不到6年，累计净值已超过1.82。 上述三只基金之所以能实现绝对收益目标，主要是做好了以下三个方面：第一、做好固定收益投资，获取长期稳健收益；第二、根据股市行情变化，灵活操作股票，在严格控制回撤前提下，获取弹性收益；第三、基金经理管理思路明确，长期坚守交易纪律。 简单的说，就是股市涨时赚固定收益加股市上涨的钱；股市下跌时，只赚固定收益的钱，回避股市下跌的损失，即“债券打底，择机炒股，严控回撤，积少成多”。 长安鑫益增强自2018年2季度定位于绝对收益基金之后，坚持以债券配置为主，股票配置为辅。在债券方面根据情况在利率债和信用债之间进行灵活配置，比如2019年主要采用长久期利率债和短久期高收益信用债搭配的策略。 在股票方面，持仓范围是0-30%，操作策略是“低仓位+短线”。无论股票行情好还是不好，都保持着极低的仓位（6%以下），在趋势性、震荡、反弹行情中进行短线操作，及时止盈止损，确保不会对净值造成太大的影响。 安信稳健增值在债券配置方面，主要投资高评级的国企债券和利率债、可转债，维持一定的杠杆水平，债券市值占资产净值比长期维持在100%以上。 该基金股票持仓范围是0%–95%，采用“长期持有+择时调仓”的策略。坚持价值投资，精选优质并且估值合理的蓝筹股，长期持有，持仓基本控制在0%-15%之间。一旦有发生系统性风险的可能，便会果断降低股票仓位。比如2018年四季度末股票持仓将至6%，2019年一季度末股票持仓提升至12%，持仓比例调整比较频繁。 广发趋势优选主要是通过 “自上而下选择低相关性的、风险较低的多类别资产构建组合，重视风险控制，及时做好止损”来实现绝对收益的。 在债券配置方面，偏好于选择1-3年期AAA国企债作为底仓，为绝对收益的构建足够的安全垫。在股票配置方面，会在行业和个股层面进行分散，单只个股的集中度尽量不超过2%，此外，还制定了严格的止损标准，投资标的跌到一定程度，必须卖出。 该基金股票持仓范围是0%–95%，采用“自上而下+择时调仓”的策略。从历史数据来看，该基金股票市值占资产净值比波动较大，在2017年牛行情中维持在40%左右，2018年熊市中则迅速降低至20%以下。表明该基金是通过择时，在下跌市场降低股票仓位，在上涨市场增加股票仓位，获取绝对收益。 上述基金实现绝对收益的原理，通俗说的话就是：债券投资为收益打底，股票投资提供弹性。当然，这只是我们从投资者的角度，通过公开信息进行的研究。基金经理所采取的策略要远比这个复杂，而且实际操作难度很大。绝对收益策略堪称基金公司和基金经理的“绝活”。 在资管行业，其实大家都很希望能给投资者提供产品，收益还不错，最重要的是投资体验极佳。但到目前为止，经历过牛熊检验，真正跑出来的屈指可数，这类产品极度稀缺。道理很简单，对绝对收益产品而言玩股票就像在走钢丝绳，稍有不慎回撤控制不住就垮下去了。而不使用权益资产，仅靠债券是难以做出满意的收益的。 二鸟老师主理的“绝对收益目标组合”也是以追求绝对收益为目标，自成立以来年化收益接近10%，最大回撤低于2%，实现了“稳稳的幸福”。 对冲策略产品 南方绝对收益（000844） 汇添富绝对收益（000762） 这两只产品是市面上十分少见的在名称中包含“绝对收益”字样的产品，两者均采用了对冲策略。其中南方绝对收益是市场上屈指可数的几个纯粹对冲策略产品之一，汇添富绝对收益则属于“固收+”、对冲、打新等共存的多策略产品。关于这一点，我们可以从两只产品的资产配置上很清楚的看到。 南方绝对收益在持有股票的同时大量持有衍生品，组合中没有债券。而汇添富绝对收益组合中债券的权重接近40%，而衍生品的权重较小。 从业绩表现上看，两只产品累积收益增长平稳，且在所有区间上都获得了正收益，策略应用十分成功。其中汇添富的收益表现相对更好一些，今年对冲策略运用较少，持有的债券和股票贡献了较多的收益。 在控制回撤方面，这两只产品相对于之前介绍过的三只纯“固收+”产品而言相对较大，在多数时间里最大回撤控制在1%以内，极端市场中控制在2%左右。 这两只基金均为定期开放产品。最近一期开放日期：南方绝对收收益是2019年12月12日至19日，汇添富绝对收益是2019年11月8日至22日，过期不候。 另外还有一个海富通阿尔法对冲（519062），这是目前市场上为数不多的日常开放的对冲产品。运作时间最长，业绩非常优秀。这个基金我之前曾多次介绍过，这里不详细说了。 至于两类策略孰优孰劣，不能简单对比收益，应该进行综合评判。就反映收益/风险比的夏普比率而言，我们发现纯“固收+”的产品表现更好，而纯对冲策略的南方绝对收益较弱。介于两者之间的汇添富绝对收益，由于采取了混合策略，表现居中。当然，由于研究的样本较小，对两种策略的优劣尚不宜得出结论。 考虑到目前我国资本市场可以用于对冲的衍生品工具还不完备，且衍生品交易存在诸多限制，因此对冲组合有可能存在较大的基差，进而影响对冲的效果。因此，我们认为“固收+”具有更强的可操作性，从而成为绝对收益目标基金的主流策略。 绝对收益目标组合初心如何才能帮投资者赚到钱？这也是我写公号、开组合的初心。实践已证明，最简单可行的方法就是让跟投人长期持有。 第一个是相对收益思路：优选基金、动态管理。通过对主流指数（如沪深300）的大幅超越，树立跟投人信心。同时做好投资者陪伴服务，一起渡过市场低迷的时段。通过长期持有优质基金，静待时间玫瑰的绽放。这就是价值五剑。 第二个是绝对收益思路。如果能把净值曲线做成一条接近斜率15度的直线，回撤小，稳步上行的话，跟投人就能具备极佳的投资体验。这样甚至投资者陪伴服务都可以不需要了。因为跟投人不论何时买入都能盈利。 这种思路其实已经跑出样板了：我要稳稳的幸福。我最初的想法是：对标“我要稳稳的幸福”，适当放大一些波动，力争做出更高一点的收益。在2018年10月31日，股市一片哀嚎声中，我在且慢上开仓建了这个组合。相比于交银，我们最大的优势就是能全市场择基（我要稳稳的幸福是交银的内部FOF），择天下英才而用之。 求索做绝对收益最核心的是严控回撤，一般是采用CPPI或风险平价策略。我要稳稳的幸福就是采用风险平价策略。我本人并非金融工程科班出身，对量化、建模这一块并没有优势，我的优势在于优选基金。经过再三考虑，我采用的是优选绝对收益目标基金+大类资产配置的策略。选出优秀的绝对收益目标基金，通过基金经理的管理作为第一层保障。同时主理人进行大类资产配置和基金池的动态管理，严控组合净值的回撤，作为第二层保障。 在优选基金时，注意投资方法的分散化和多样化。最初是按配置策略、短债策略、对冲策略、综合策略去选基金。随着收益安全垫的积累，将短债策略更换为更为积极的长债策略，同时加入了小仓位的股债平衡型基金以增加组合收益的弹性。随着科创板开市，打新的制度红利较为明显，后续也加入了打新策略。 按照“固收+”的观点，“固收+股票”、“固收+转债”、“固收+打新”的思想皆有体现。同时还使用了对冲基金（市场中性策略）。绝对收益目标组合实现了投资方法和收益来源的多样化。 最初还尝试使用小仓位的基金进行波段操作以进一步增厚收益，但在实践中发现异常困难。研判的工作量、难度与仓位的大小并没有关系。哪怕是极小仓位，也不能有任何闪失。所以后面彻底放弃。 韧性绝对收益目标组合目前使用了6只成分基金，这一年股、债两市都有较大的波动，但这些基金都表现出了极强的韧性。反映在组合上面，就是净值异常坚挺。 易方达稳健收益（110008）：由易方达固收总经理胡剑老师亲自执掌，近8年时间除2011年年微亏外，其余年份全部取得正收益。这虽然是一只普通二级债基，并非采用绝对收益策略，但我仍给予高度信任。建仓初期由于股市点位较低，直接赋予了30%的仓位。现在虽有所降低，仍有25%的仓位。 广发趋势优选（000215）：这是一个默默的数钱、不声张的基金。名字里没有“绝对收益”这四个字，实际上是采用绝对收益策略。2015年2月谭昌杰接手之后，净值曲线基本就是一条斜率为15度的直线。 长安鑫益增强（002146）：这个更牛，几乎没有回撤，天天飘红不说，收益更是高的令人发指。他到底用了何种策略？神一般的存在。但他天天飘红不假，已经两年了。 海富通阿尔法对冲（519062）：运行时间最长，收益最好的且每日开放的对冲基金，极其稀缺。这个基金在成分基金里面收益虽不是最高，但对组合极其重要。他不仅自身能实现正收益，而且独立于股、债两市，经常是与股市反向波动，较好的平抑了组合的波动率。 光大保德信诚鑫（003116）：固收+打新的基金，底仓以银行股为主，相对安全。这只基金调入较晚，错过了科创板开市那几天的涨幅。不然组合的净值还能提升0.5个点。 广发稳健增长（270002）、泓德致远（004965）：两个优秀的股债平衡型基金，具备净值不断创新高的能力，具备极高的风险收益比。组合小仓位使用，用于增厚收益。 远方随着组合的影响力不断增大，也有人提出了一些质疑。比较典型的就是：组合的混合型基金占比较大，股票仓位较高，对未来能否保持高收益低回撤表示怀疑。在此我做一下解释。 先说股票仓位和回撤的问题。组合的股票总仓位我们控制在20%以内，对标二级债基，而且还使用了对冲的手段。这些基金表面上是混合基金，实际上都是以绝对收益为目标的基金，基金经理本身就在控制回撤。说股票仓位高的人，明显是对这几个基金的风格不太了解。只要这些基金不改变现有投资策略，组合的波动和回撤的控制是没有问题的。况且我还天天盯着，在进行二次把关。 再说一下收益的问题。组合成立以来，正好赶上了股债双牛的好时光，因而取得了将近10%的收益。随着时间的拉长，年化收益率可能会缓慢的降下来。这个组合的首要目标是控制回撤，收益并不是摆在首位的。在产品说明书里也写的很清楚：“力争实现5%的年化收益率”，我们也从未宣传和强调组合的高收益率。 未来随着时间的推移，组合的年化收益率如果降到5%-7%的区间，这都是很正常的事。如果未来仍维持着这个高收益，那是你们自己的福气。我找对了人，你们也找对了人。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes之存储详解]]></title>
    <url>%2FKubernetes%E4%B9%8B%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://www.kubernetes.org.cn/pvpvcstorageclass https://www.jianshu.com/p/99e610067bc8 简书介绍前面我们学习了Kubernetes中的Volume，我们可以发现前文中的Volume（无论何种类型）和使用它的Pod都是一种静态绑定关系，在Pod定义文件中，同时定义了它使用的Volume。在这种情况下，Volume是Pod的附属品，我们无法像创建其他资源（例如Pod，Node，Deployment等等）一样创建一个Volume。 因此Kubernetes提出了PersistentVolume（PV）的概念。PersistentVolume和Volume一样，代表了集群中的一块存储区域，然而Kubernetes将PersistentVolume抽象成了一种集群资源，类似于集群中的Node对象，这意味着我们可以使用Kubernetes API来创建PersistentVolume对象。PV与Volume最大的不同是PV拥有着独立于Pod的生命周期。 而PersistentVolumeClaim（PVC）代表了用户对PV资源的请求。用户需要使用PV资源时，只需要创建一个PVC对象（包括指定使用何种存储资源，使用多少GB，以何种模式使用PV等信息），Kubernetes会自动为我们分配我们所需的PV。如果把PersistentVolume类比成集群中的Node，那么PersistentVolumeClaim就相当于集群中的Pod，Kubernetes为Pod分配可用的Node，为PersistentVolumeClaim分配可用的PersistentVolume。 PV的静态创建首先是一个创建PV的简单例子： 123456789101112131415161718apiVersion: v1kind: PersistentVolumemetadata: name: pv0003spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 PV 的访问模式（accessModes）有三种： ReadWriteOnce（RWO）：是最基本的方式，可读可写，但只支持被单个 Pod 挂载。 ReadOnlyMany（ROX）：可以以只读的方式被多个 Pod 挂载。 ReadWriteMany（RWX）：这种存储可以以读写的方式被多个 Pod 共享。 不是每一种PV都支持这三种方式，例如ReadWriteMany，目前支持的还比较少。在 PVC 绑定 PV 时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。 PV 的回收策略（persistentVolumeReclaimPolicy，即 PVC 释放卷的时候 PV 该如何操作）也有三种： Retain，不清理, 保留 Volume（需要手动清理） Recycle，删除数据，即 rm -rf /thevolume/*（只有 NFS 和 HostPath 支持） Delete，删除存储资源，比如删除 AWS EBS 卷（只有 AWS EBS, GCE PD, Azure Disk 和 Cinder 支持） PVC释放卷是指用户删除一个PVC对象时，那么与该PVC对象绑定的PV就会被释放。 PV支持的类型定义PV时，我们需要指定其底层存储的类型，例如上文中创建的PV，底层使用nfs存储。目前Kuberntes支持以下类型： GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel)** FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) VMware Photon Portworx Volumes ScaleIO Volumes StorageOS PVC的创建当我们定义好一个PV后，我们希望像使用Volume那样使用这个PV，那么我们需要做的就是创建一个PVC对象，并在Pod定义中使用这个PVC。 定义一个PVC： 1234567891011121314kind: PersistentVolumeClaimapiVersion: v1metadata: name: myclaimspec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: "stable" Pod通过挂在Volume的方式应用PVC： 123456789101112131415kind: PodapiVersion: v1metadata: name: mypodspec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: "/var/www/html" name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim 下面简要分析一下定义的PVC文件的关键： 首先关注这个配置：storageClassName: slow。此配置用于绑定PVC和PV。这表明这个PVC希望使用storageClassName=slow的PV。返回到上文中PV的定义，我们可以看到PV定义中也包含storageClassName=slow的配置。 接下来是accessModes = ReadWriteOnce。这表明这个PV希望使用storageClassName=slow，并且accessModes = ReadWriteOnce的PV。 在上述条件都满足后，PVC还可以指定PV必须满足的Label，如matchLabels: release: &quot;stable&quot;。这表明此PVC希望使用storageClassName=slow，accessModes = ReadWriteOnce并且拥有Label：release: &quot;stable&quot;的PV。 最后是storage: 8Gi。这表明此PVC希望使用8G的Volume资源。 通过上面的分析，我们可以看到PVC和PV的绑定，不是简单的通过Label来进行。而是要综合storageClassName，accessModes，matchLabels以及storage来进行绑定。 PV的动态创建上文中我们通过PersistentVolume描述文件创建了一个PV。这样的创建方式我们成为静态创建。这样的创建方式有一个弊端，那就是假如我们创建PV时指定大小为50G，而PVC请求80G的PV，那么此PVC就无法找到合适的PV来绑定。因此产生了了PV的动态创建。 PV的动态创建依赖于StorageClass对象。我们不需要手动创建任何PV，所有的工作都由StorageClass为我们完成。一个例子如下： 123456789kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: slowprovisioner: kubernetes.io/aws-ebsparameters: type: io1 zones: us-east-1d, us-east-1c iopsPerGB: "10" 这个例子使用AWS提供的插件（ kubernetes.io/aws-ebs）创建了一个基于AWS底层存储的StorageClass。这意味着使用这个StorageClass，那么所有的PV都是AWSElasticBlockStore类型的。 StorageClass的定义包含四个部分： provisioner：指定 Volume 插件的类型，包括内置插件（如 kubernetes.io/aws-ebs）和外部插件（如 external-storage 提供的 ceph.com/cephfs）。 mountOptions：指定挂载选项，当 PV 不支持指定的选项时会直接失败。比如 NFS 支持 hard 和 nfsvers=4.1 等选项。 parameters：指定 provisioner 的选项，比如 kubernetes.io/aws-ebs 支持 type、zone、iopsPerGB 等参数。 reclaimPolicy：指定回收策略，同 PV 的回收策略。 手动创建的PV时，我们指定了storageClassName=slow的配置项，然后Pod定义中也通过指定storageClassName=slow，从而完成绑定。而通过StorageClass实现动态PV时，我们只需要指定StorageClass的metadata.name。 回到上文中创建PVC的例子，此时PVC指定了storageClassName=slow。那么Kubernetes会在集群中寻找是否存在metadata.name=slow的StorageClass，如果存在，此StorageClass会自动为此PVC创建一个accessModes = ReadWriteOnce，并且大小为8GB的PV。 通过StorageClass的使用，使我们从提前构建静态PV池的工作中解放出来。 PV的生命周期PV的生命周期包括 5 个阶段： Provisioning，即 PV 的创建，可以直接创建 PV（静态方式），也可以使用 StorageClass 动态创建 Binding，将 PV 分配给 PVC Using，Pod 通过 PVC 使用该 Volume，并可以通过准入控制 StorageProtection（1.9及以前版本为PVCProtection）阻止删除正在使用的 PVC Releasing，Pod 释放 Volume 并删除 PVC Reclaiming，回收 PV，可以保留 PV 以便下次使用，也可以直接从云存储中删除 Deleting，删除 PV 并从云存储中删除后段存储 根据这 5 个阶段，PV 的状态有以下 4 种 Available：可用 Bound：已经分配给 PVC Released：PVC 解绑但还未执行回收策略 Failed：发生错误 一个PV从创建到销毁的具体流程如下： 一个PV创建完后状态会变成Available，等待被PVC绑定。 一旦被PVC邦定，PV的状态会变成Bound，就可以被定义了相应PVC的Pod使用。 Pod使用完后会释放PV，PV的状态变成Released。 变成Released的PV会根据定义的回收策略做相应的回收工作。有三种回收策略，Retain、Delete 和 Recycle。Retain就是保留现场，K8S什么也不做，等待用户手动去处理PV里的数据，处理完后，再手动删除PV。Delete 策略，K8S会自动删除该PV及里面的数据。Recycle方式，K8S会将PV里的数据删除，然后把PV的状态变成Available，又可以被新的PVC绑定使用。 DefaultStorageClass前面我们说到，PVC和PV的绑定是通过StorageClassName进行的。然而如果定义PVC时没有指定StorageClassName呢？这取决与admission插件是否开启了DefaultDefaultStorageClass功能： 如果DefaultDefaultStorageClass功能开启，那么此PVC的StorageClassName就会被指定为DefaultStorageClass。DefaultStorageClass从何处而来呢？原来在定义StorageClass时，可以在Annotation中添加一个键值对：storageclass.kubernetes.io/is-default-class: true，那么此StorageClass就变成默认的StorageClass了。 如果DefaultDefaultStorageClass功能没有开启，那么没有指定StorageClassName的PVC只能被绑定到同样没有指定StorageClassName的PV。 中文文档介绍之PV/PVC/StorageClass介绍PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 PersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式。 虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 群集管理员需要能够提供多种不同于PersistentVolumes的PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。 StorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件” 例子https://kubernetes.io/docs/user-guide/persistent-volumes/walkthrough/ Lifecycle of a volume and claimPV是集群中的资源。 PVC是对这些资源的请求，也是对资源的索赔检查。 PV和PVC之间的相互作用遵循这个生命周期: Provisioning ——-&gt; Binding ——–&gt;Using——&gt;Releasing——&gt;Recycling Provisioning 这里有两种PV的提供方式:静态或者动态 1234Static集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费。Dynamic当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置 Binding 在动态配置的情况下，用户创建或已经创建了具有特定数量的存储请求和特定访问模式的PersistentVolumeClaim。 主机中的控制回路监视新的PVC，找到匹配的PV（如果可能），并将它们绑定在一起。 如果为新的PVC动态配置PV，则循环将始终将该PV绑定到PVC。 否则，用户总是至少得到他们要求的内容，但是卷可能超出了要求。 一旦绑定，PersistentVolumeClaim绑定是排他的，不管用于绑定它们的模式。 如果匹配的卷不存在，PVC将保持无限期。 随着匹配卷变得可用，PVC将被绑定。 例如，提供许多50Gi PV的集群将不匹配要求100Gi的PVC。 当集群中添加100Gi PV时，可以绑定PVC。 Using Pod使用PVC作为卷。 集群检查声明以找到绑定的卷并挂载该卷的卷。 对于支持多种访问模式的卷，用户在将其声明用作pod中的卷时指定所需的模式。 一旦用户有声明并且该声明被绑定，绑定的PV属于用户，只要他们需要它。 用户通过在其Pod的卷块中包含persistentVolumeClaim来安排Pods并访问其声明的PV。 Releasing 当用户完成卷时，他们可以从允许资源回收的API中删除PVC对象。 当声明被删除时，卷被认为是“释放的”，但是它还不能用于另一个声明。 以前的索赔人的数据仍然保留在必须根据政策处理的卷上. ReclaimingPersistentVolume的回收策略告诉集群在释放其声明后，该卷应该如何处理。 目前，卷可以是保留，回收或删除。 保留可以手动回收资源。 对于那些支持它的卷插件，删除将从Kubernetes中删除PersistentVolume对象，以及删除外部基础架构（如AWS EBS，GCE PD，Azure Disk或Cinder卷）中关联的存储资产。 动态配置的卷始终被删除 Recycling 如果受适当的卷插件支持，回收将对卷执行基本的擦除（rm -rf / thevolume / *），并使其再次可用于新的声明。但是，管理员可以使用Kubernetes控制器管理器命令行参数来配置自定义的回收站pod模板，如这里所述。 定制回收站模板必须包含卷规范，如下例所示： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pv-recycler- namespace: defaultspec: restartPolicy: Never volumes: - name: vol hostPath: path: /any/path/it/will/be/replaced containers: - name: pv-recycler image: "gcr.io/google_containers/busybox" command: ["/bin/sh", "-c", "test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/* &amp;&amp; test -z \"$(ls -A /scrub)\" || exit 1"] volumeMounts: - name: vol mountPath: /scrub Types of Persistent VolumesPV当前支持的类型 12345678910111213141516171819GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel) FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) VMware Photon Portworx Volumes ScaleIO Volumes Persistent Volumes每个PV包含了Spec和Staus, 在PV的定义中指定该内容 1234567891011121314apiVersion: v1kind: PersistentVolumemetadata: name: pv0003spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /tmp server: 172.17.0.2 Capacity 通常，PV将具有特定的存储容量。 这是使用PV的容量属性设置的。 看到库伯纳斯资源模型，以了解容量预期的单位。 目前，存储大小是唯一可以设置或请求的资源。 未来的属性可能包括IOPS，吞吐量等 Access Modes PersistentVolume可以以资源提供者支持的任何方式安装在主机上。 如下表所示，提供商将具有不同的功能，每个PV的访问模式都被设置为该特定卷支持的特定模式。 例如，NFS可以支持多个读/写客户端，但是特定的NFS PV可能会以只读方式在服务器上导出。 每个PV都有自己的一组访问模式来描述具体的PV功能。访问模式:ReadWriteOnce – the volume can be mounted as read-write by a single node (单node的读写)ReadOnlyMany – the volume can be mounted read-only by many nodes (多node的只读)ReadWriteMany – the volume can be mounted as read-write by many nodes (多node的读写)Notice:单个PV挂载的时候只支持一种访问模式PV提供插件支持的access mode参考kubernetes官方文档 Class PV可以有一个类，通过将storageClassName属性设置为StorageClass的名称来指定。 特定类的PV只能绑定到请求该类的PVC。 没有storageClassName的PV没有类，只能绑定到不需要特定类的PVC。在过去，使用了注释volume.beta.kubernetes.io/storage-class而不是storageClassName属性。 该注释仍然可以工作，但将来Kubernetes版本将不再适用。 Reclaim Policy 目前的回收政策是： 123Retain – manual reclamation Recycle – basic scrub (“rm -rf /thevolume/*”) Delete – associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted 目前，只有NFS和HostPath支持回收。 AWS EBS，GCE PD，Azure Disk和Cinder卷支持删除 Phase 卷将处于以下阶段之一： 1234Available – a free resource that is not yet bound to a claim Bound – the volume is bound to a claim Released – the claim has been deleted, but the resource is not yet reclaimed by the cluster Failed – the volume has failed its automatic reclamation PersistentVolumeClaims每个PVC包含spec和status 12345678910111213141516kind: PersistentVolumeClaimapiVersion: v1metadata: name: myclaimspec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: &quot;stable&quot; matchExpressions: - &#123;key: environment, operator: In, values: [dev]&#125; Access Modes 当请求具有特定访问模式的存储时，声明使用与卷相同的约定 Resources 声明（如pod）可以请求特定数量的资源。 在这种情况下，请求用于存储。 相同的资源模型适用于卷和声明 Selector 声明可以指定标签选择器以进一步过滤该卷集。 只有标签与选择器匹配的卷才能绑定到声明。 选择器可以由两个字段组成： 12matchLabels - 卷必须具有带此值的标签matchExpressions - 通过指定关键字和值的关键字，值列表和运算符所做的要求列表。 有效运算符包括In，NotIn，Exists和DoesNotExist。 所有来自matchLabels和matchExpressions的要求都与AND一起使用，所有这些要求都必须满足才能匹配。 Class 声明可以通过使用属性storageClassName指定StorageClass的名称来请求特定的类。只有所请求的类的PV，与PVC相同的storageClassName的PV可以绑定到PVC。 PVC不一定要求一个班级。它的storageClassName设置为等于“”的PVC总是被解释为请求没有类的PV，因此它只能绑定到没有类的PV（没有注释或一个等于“”）。没有storageClassName的PVC不完全相同，并且根据是否启用了DefaultStorageClass入门插件，集群的处理方式不同。 如果接纳插件已打开，则管理员可以指定默认的StorageClass。没有storageClassName的所有PVC只能绑定到该默认的PV。通过将StorageClass对象中的annotation storageclass.kubernetes.io/is-default-class设置为“true”来指定默认的StorageClass。如果管理员没有指定默认值，则集群会对PVC创建做出响应，就像入门插件被关闭一样。如果指定了多个默认值，则验收插件禁止创建所有PVC。如果入门插件已关闭，则不存在默认StorageClass的概念。没有storageClassName的所有PVC只能绑定到没有类的PV。在这种情况下，没有storageClassName的PVC的处理方式与将其storageClassName设置为“”的PVC相同。 根据安装方法，安装过程中可以通过addon manager在Kubernetes群集中部署默认的StorageClass。 当PVC指定一个选择器，除了请求一个StorageClass之外，这些要求被AND组合在一起：只有所请求的类和所请求的标签的PV可能被绑定到PVC。请注意，当前，具有非空选择器的PVC不能为其动态配置PV。 在过去，使用了注释volume.beta.kubernetes.io/storage-class，而不是storageClassName属性。该注释仍然可以工作，但在未来的Kubernetes版本中它将不被支持。 声明PVC作为Volumes Pods通过将声明用作卷来访问存储。 声明必须存在于与使用声明的pod相同的命名空间中。 群集在pod的命名空间中查找声明，并使用它来获取支持声明的PersistentVolume。 然后将体积安装到主机并进入Pod。 123456789101112131415kind: PodapiVersion: v1metadata: name: mypodspec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: &quot;/var/www/html&quot; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim PersistentVolumes绑定是独占的，并且由于PersistentVolumeClaims是命名空间对象，所以只能在一个命名空间内安装“许多”模式（ROX，RWX）—PVC支持被多个pod挂载 StorageClasses每个StorageClass包含字段provisioninger和参数，当属于类的PersistentVolume需要动态配置时使用。 StorageClass对象的名称很重要，用户可以如何请求特定的类。 管理员在首次创建StorageClass对象时设置类的名称和其他参数，并且在创建对象后无法更新对象。 管理员可以仅为不要求任何特定类绑定的PVC指定默认的StorageClass：有关详细信息，请参阅PersistentVolumeClaim部分。 1234567kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: standardprovisioner: kubernetes.io/aws-ebsparameters: type: gp2 Provisioner 存储类有一个供应商，它确定用于配置PV的卷插件。 必须指定此字段。 您不限于指定此处列出的“内部”供应商（其名称前缀为“kubernetes.io”并与Kubernetes一起运送）。 您还可以运行和指定外部提供程序，它们是遵循Kubernetes定义的规范的独立程序。 外部提供者的作者对代码的生命周期，供应商的运输状况，运行状况以及使用的卷插件（包括Flex）等都有充分的自主权。存储库kubernetes-incubator /外部存储库包含一个库 用于编写实施大部分规范的外部提供者以及各种社区维护的外部提供者。 Parameters 存储类具有描述属于存储类的卷的参数。 取决于供应商，可以接受不同的参数。 例如，参数类型的值io1和参数iopsPerGB特定于EBS。 当省略参数时，使用一些默认值。 AWS/GCE/Glusterfs/ OpenStack Cinder 12345678910kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: goldprovisioner: kubernetes.io/cinderparameters: type: fast availability: novatype: VolumeType created in Cinder. Default is empty. availability: Availability Zone. If not specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. 其它类型的storageClass配置参考 1https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes 配置 如果您正在编写在各种群集上运行并需要持久存储的配置模板或示例，我们建议您使用以下模式： 在您的配置文件夹（包括部署，ConfigMaps等）中包含PersistentVolumeClaim对象。在配置中不要包含PersistentVolume对象，因为实例化配置的用户可能没有创建PersistentVolumes的权限。给用户提供实例化模板时提供存储类名称的选项。１． 如果用户提供存储类名称，并且集群是1.4或更高版本，请将该值放入PVC的volume.beta.kubernetes.io/storage-class注释中。如果集群的管理员启用了StorageClasses，这将导致PVC与正确的存储类匹配。２． 如果用户不提供存储类名称或者集群是版本1.3，那么在PVC上放置一个volume.alpha.kubernetes.io/storage-class：default注释。这将导致在某些群集上为用户自动配置PV，并具有合理的默认特性。尽管在名称中使用了alpha，但这个注释背后的代码具有beta级别的支持。３． 不要使用volume.beta.kubernetes.io/storage-class：包含空字符串的任何值，因为它将阻止DefaultStorageClass接纳控制器运行（如果启用）。 在您的工具中，请注意在一段时间后未绑定的PVC，并将其表示给用户，因为这可能表明集群没有动态存储支持（在这种情况下，用户应创建匹配的PV）或集群没有存储系统（在这种情况下，用户无法部署需要PVC的配置）。 在将来，我们预计大多数集群都将启用DefaultStorageClass，并提供某种形式的存储。但是，可能没有任何存储类名可用于所有集群，因此默认情况下继续不设置。在某种程度上，alpha注释将不再有意义，但PVC上的未设置的storageClass字段将具有所需的效果。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群监控之pushgateway]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B9%8Bpushgateway%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://www.cnblogs.com/xiao987334176/p/9933963.html https://www.jianshu.com/p/e59793e8c0c2 简介Pushgateway 是 Prometheus 生态中一个重要工具，使用它的原因主要是： Prometheus 采用 pull 模式，可能由于不在一个子网或者防火墙原因，导致 Prometheus 无法直接拉取各个 target 数据。 在监控业务数据的时候，需要将不同数据汇总, 由 Prometheus 统一收集。 由于以上原因，不得不使用 pushgateway，但在使用之前，有必要了解一下它的一些弊端： 将多个节点数据汇总到 pushgateway, 如果 pushgateway 挂了，受影响比多个 target 大。 Prometheus 拉取状态 up 只针对 pushgateway, 无法做到对每个节点有效。 Pushgateway 可以持久化推送给它的所有监控数据。 因此，即使你的监控已经下线，prometheus 还会拉取到旧的监控数据，需要手动清理 pushgateway 不要的数据。 拓扑图如下： 安装基于docker 1docker pull prom/pushgateway 1234docker run -d \ --name=pg \ -p 9091:9091 \ prom/pushgateway 访问url 1http://192.168.91.132:9091/ 配置prometheus12345678910111213141516171819202122global: scrape_interval: 60s evaluation_interval: 60s scrape_configs: - job_name: prometheus static_configs: - targets: [&apos;localhost:9090&apos;] labels: instance: prometheus - job_name: linux static_configs: - targets: [&apos;192.168.91.132:9100&apos;] labels: instance: localhost - job_name: pushgateway static_configs: - targets: [&apos;192.168.91.132:9091&apos;] labels: instance: pushgateway 数据管理正常情况我们会使用 Client SDK 推送数据到 pushgateway, 但是我们还可以通过 API 来管理, 例如： shell脚本向 {job=”some_job”} 添加单条数据： 1echo &quot;some_metric 3.14&quot; | curl -X POST --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job –data-binary 表示发送二进制数据，注意：它是使用POST方式发送的！ 添加更多更复杂数据，通常数据会带上 instance, 表示来源位置： 1234567cat &lt;&lt;EOF | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance# TYPE some_metric countersome_metric&#123;label=&quot;val1&quot;&#125; 42# TYPE another_metric gauge# HELP another_metric Just an example.another_metric 2398.283EOF 注意：必须是指定的格式才行！ 删除某个组下的某实例的所有数据： 1curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance 删除某个组下的所有数据： 1curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job 可以发现 pushgateway 中的数据我们通常按照 job 和 instance 分组分类，所以这两个参数不可缺少。 因为 Prometheus 配置 pushgateway 的时候，也会指定 job 和 instance, 但是它只表示 pushgateway 实例，不能真正表达收集数据的含义。所以在 prometheus 中配置 pushgateway 的时候，需要添加 honor_labels: true 参数， 从而避免收集数据本身的 job 和 instance 被覆盖。 注意，为了防止 pushgateway 重启或意外挂掉，导致数据丢失，我们可以通过 -persistence.file 和 -persistence.interval 参数将数据持久化下来。 python脚本（flask）安装模块 12pip3 install flaskpip3 install prometheus_client metricsPrometheus提供4种类型Metrics：Counter, Gauge, Summary和Histogram Counter Counter可以增长，并且在程序重启的时候会被重设为0，常被用于任务个数，总处理时间，错误个数等只增不减的指标。 1234567891011121314151617181920212223import prometheus_clientfrom prometheus_client import Counterfrom prometheus_client.core import CollectorRegistryfrom flask import Response, Flaskapp = Flask(__name__)requests_total = Counter(&quot;request_count&quot;, &quot;Total request cout of the host&quot;)@app.route(&quot;/metrics&quot;)def requests_count(): requests_total.inc() # requests_total.inc(2) return Response(prometheus_client.generate_latest(requests_total), mimetype=&quot;text/plain&quot;)@app.route(&apos;/&apos;)def index(): requests_total.inc() return &quot;Hello World&quot;if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;) 运行该脚本，访问youhost:5000/metrics 123# HELP request_count Total request cout of the host# TYPE request_count counterrequest_count 3.0 Gauge Gauge与Counter类似，唯一不同的是Gauge数值可以减少，常被用于温度、利用率等指标。 123456789101112131415161718import randomimport prometheus_clientfrom prometheus_client import Gaugefrom flask import Response, Flaskapp = Flask(__name__)random_value = Gauge(&quot;random_value&quot;, &quot;Random value of the request&quot;)@app.route(&quot;/metrics&quot;)def r_value(): random_value.set(random.randint(0, 10)) return Response(prometheus_client.generate_latest(random_value), mimetype=&quot;text/plain&quot;)if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;) 运行该脚本，访问youhost:5000/metrics 123# HELP random_value Random value of the request# TYPE random_value gaugerandom_value 3.0 Summary/Histogram Summary/Histogram概念比较复杂，一般exporter很难用到，暂且不说。 Labels使用labels来区分metric的特征 1234567from prometheus_client import Counterc = Counter(&apos;requests_total&apos;, &apos;HTTP requests total&apos;, [&apos;method&apos;, &apos;clientip&apos;])c.labels(&apos;get&apos;, &apos;127.0.0.1&apos;).inc()c.labels(&apos;post&apos;, &apos;192.168.0.1&apos;).inc(3)c.labels(method=&quot;get&quot;, clientip=&quot;192.168.0.1&quot;).inc() REGISTRY 1234567from prometheus_client import Counter, Gaugefrom prometheus_client.core import CollectorRegistryREGISTRY = CollectorRegistry(auto_describe=False)requests_total = Counter(&quot;request_count&quot;, &quot;Total request cout of the host&quot;, registry=REGISTRY)random_value = Gauge(&quot;random_value&quot;, &quot;Random value of the request&quot;, registry=REGISTRY) 举例网卡流量先访问这篇文章《python 获取网卡实时流量》：http://www.py3study.com/Article/details/id/347.html 下面这段python脚本，主要是参考上面文章的基础上修改的 发送本机网卡流量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import prometheus_clientfrom prometheus_client import Counterfrom prometheus_client import Gaugefrom prometheus_client.core import CollectorRegistryimport psutilimport timeimport requestsimport socketdef get_key(): key_info = psutil.net_io_counters(pernic=True).keys() recv = &#123;&#125; sent = &#123;&#125; for key in key_info: recv.setdefault(key, psutil.net_io_counters(pernic=True).get(key).bytes_recv) sent.setdefault(key, psutil.net_io_counters(pernic=True).get(key).bytes_sent) return key_info, recv, sentdef get_rate(func): import time key_info, old_recv, old_sent = func() time.sleep(1) key_info, now_recv, now_sent = func() net_in = &#123;&#125; net_out = &#123;&#125; for key in key_info: # float(&apos;%.2f&apos; % a) # net_in.setdefault(key, float(&apos;%.2f&apos; %((now_recv.get(key) - old_recv.get(key)) / 1024))) # net_out.setdefault(key, float(&apos;%.2f&apos; %((now_sent.get(key) - old_sent.get(key)) / 1024))) # 计算流量 net_in.setdefault(key, now_recv.get(key) - old_recv.get(key)) net_out.setdefault(key, now_sent.get(key) - old_sent.get(key)) return key_info, net_in, net_out# def get_host_ip():# &quot;&quot;&quot;# 查询本机ip地址,针对单网卡# :return: ip# &quot;&quot;&quot;# try:# s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# s.connect((&apos;8.8.8.8&apos;, 80))# ip = s.getsockname()[0]# finally:# s.close()# return ip# 打印多网卡 mac 和 ip 信息def PrintNetIfAddr(): dic = psutil.net_if_addrs() net_dic = &#123;&#125; net_dic[&apos;no_ip&apos;] = [] # 无ip的网卡列表 for adapter in dic: snicList = dic[adapter] mac = &apos;无 mac 地址&apos; ipv4 = &apos;无 ipv4 地址&apos; ipv6 = &apos;无 ipv6 地址&apos; for snic in snicList: if snic.family.name in &#123;&apos;AF_LINK&apos;, &apos;AF_PACKET&apos;&#125;: mac = snic.address elif snic.family.name == &apos;AF_INET&apos;: ipv4 = snic.address elif snic.family.name == &apos;AF_INET6&apos;: ipv6 = snic.address # print(&apos;%s, %s, %s, %s&apos; % (adapter, mac, ipv4, ipv6)) # 判断网卡名不在net_dic中时,并且网卡不是lo if adapter not in net_dic and adapter != &apos;lo&apos;: if not ipv4.startswith(&quot;无&quot;): # 判断ip地址不是以无开头 net_dic[adapter] = ipv4 # 增加键值对 else: net_dic[&apos;no_ip&apos;].append(adapter) # 无ip的网卡 # print(net_dic) return net_dickey_info, net_in, net_out = get_rate(get_key)# ip=get_host_ip() # 本机iphostname = socket.gethostname() # 主机名REGISTRY = CollectorRegistry(auto_describe=False)input = Gauge(&quot;network_traffic_input&quot;, hostname,[&apos;adapter_name&apos;,&apos;unit&apos;,&apos;ip&apos;,&apos;instance&apos;],registry=REGISTRY) # 流入output = Gauge(&quot;network_traffic_output&quot;, hostname,[&apos;adapter_name&apos;,&apos;unit&apos;,&apos;ip&apos;,&apos;instance&apos;],registry=REGISTRY) # 流出for key in key_info: net_addr = PrintNetIfAddr() # 判断网卡不是lo(回环网卡)以及 不是无ip的网卡 if key != &apos;lo&apos; and key not in net_addr[&apos;no_ip&apos;]: # 流入和流出 input.labels(ip=net_addr[key],adapter_name=key, unit=&quot;Byte&quot;,instance=hostname).inc(net_in.get(key)) output.labels(ip=net_addr[key],adapter_name=key, unit=&quot;Byte&quot;,instance=hostname).inc(net_out.get(key))requests.post(&quot;http://192.168.91.132:9091/metrics/job/network_traffic&quot;,data=prometheus_client.generate_latest(REGISTRY))print(&quot;发送了一次网卡流量数据&quot;) 执行脚本，它会发送1次数据给Push Gateway 取到的流量没有除以1024，所以默认是字节 注意：发送的链接，约定成俗的格式如下： 1http://Pushgateway地址:9091/metrics/job/监控项目 比如监控etcd，地址就是这样的 1http://Pushgateway地址:9091/metrics/job/etcd 代码解释关键代码，就是这几行 123456REGISTRY = CollectorRegistry(auto_describe=False)input = Gauge(&quot;network_traffic_input&quot;, hostname,[&apos;adapter_name&apos;,&apos;unit&apos;,&apos;ip&apos;,&apos;instance&apos;],registry=REGISTRY) # 流入output = Gauge(&quot;network_traffic_output&quot;, hostname,[&apos;adapter_name&apos;,&apos;unit&apos;,&apos;ip&apos;,&apos;instance&apos;],registry=REGISTRY) # 流出input.labels(ip=net_addr[key],adapter_name=key, unit=&quot;Byte&quot;,instance=hostname).inc(net_in.get(key))output.labels(ip=net_addr[key],adapter_name=key, unit=&quot;Byte&quot;,instance=hostname).inc(net_out.get(key)) 1、自定义的指标收集类都必须到CollectorRegistry进行注册， 指标数据通过CollectorRegistry类的方法或者函数，返回给Prometheus.2、CollectorRegistry必须提供register()和unregister()函数，一个指标收集器可以注册多个CollectorRegistry.3、客户端库必须是线程安全的 代码第一行，声明了CollectorRegistry input和output是流入流出的流量。Metrics使用的是Gauge 1input = Gauge(&quot;network_traffic_input&quot;, hostname,[&apos;adapter_name&apos;,&apos;unit&apos;,&apos;ip&apos;,&apos;instance&apos;],registry=REGISTRY) # 流入 network_traffic_input表示键值，它必须唯一。因为在grafana图表中，要用这个键值绘制图表。 “” 为空，它其实对应的是描述信息。为了避免数据冗长，一般不写它。 [‘adapter_name’,’unit’,’ip’,’instance’] ，它是一个列表，里面每一个元素都是labels，它是用来区分metric的特征 registry=REGISTRY 把数据注册到REGISTRY中 1input.labels(ip=net_addr[key],adapter_name=key, unit=&quot;Byte&quot;,instance=hostname).inc(net_in.get(key)) 这里定义了input的labels，括号里面有3个键值对。注意：这3个键值对必须在[‘adapter_name’,’unit’,’ip’] 列表中。 如果labels中要增加键值对，那么上面的列表中，也要增加对应的元素。否则会报错！ inc表示具体值。它对应的是input 刷新Push Gateway页面IP:9091 就可以看到流入流出的数据了 python脚本（非flask）123456789101112131415from prometheus_client import CollectorRegistry, Gauge, pushadd_to_gatewaydef push2gateway(datas): registry = CollectorRegistry() g = Gauge(&apos;node_process_status_info&apos;,&apos;process monitor&apos;,[&apos;group&apos;,&apos;process_name&apos;,&apos;status&apos;,&apos;days&apos;,&apos;icon&apos;],registry=registry) for group,process,status,runtime,icon in datas: print group,process,status,runtime,icon g.labels(group,process,status,str(runtime),icon).set(icon) pushadd_to_gateway(&apos;10.10.148.34:9091&apos;, job=&apos;pushgateway&apos; ,registry=registry,timeout=200) 以上函数中， node_process_status_info 表示指标名 list参数中的值表示标签名 循环只是为了给同一个指标的不同标签设置不同的value g.labels括号中表示按顺序为标签名赋值 g.labels末尾的set表示该标签下的value值 最后推送的时候指标越多,timeout应该设置越高，否则会推送失败，具体时间根据现场网络状况测试一下就知道了 完整的案例，检测URL返回状态码并推送到pushgateway 12345678910111213141516171819202122232425#coding:utf-8import requestsfrom prometheus_client import CollectorRegistry, Gauge, pushadd_to_gatewayurl = &apos;[http://10.10.164.119:8500/v1/a/b/net.c.api.d](http://10.10.164.119:8500/v1/a/b/net.c.api.d)&apos;headers = &#123; &quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE&quot;&#125;status_code = requests.get(url,headers=headers).status_codeprint status_coderegistry = CollectorRegistry()g = Gauge(&apos;node_web_status_code&apos;,&apos;web monitor&apos;,[&apos;url&apos;],registry=registry)g.labels(url).set(status_code)pushadd_to_gateway(&apos;10.10.148.34:9091&apos;, job=&apos;pushgateway&apos; ,registry=registry,timeout=200) 注意: 使用prometheus_client推送到pushgateway的时候，如果你的指标拥有多个标签，并且在循环里写入了很多次推送，但是在pushgateway中往往只能看到最后一个，这大概是因为pushgateway推送的时候相同的指标名(job名)是以覆盖的方式进行的（具体没有更多研究和验证，我的问题解决便放下了）。所以这个时候可以将pushadd_to_gateway放在指标注册的最后，如下： 123456789101112131415def findpush(path,g): output = os.popen(&apos;hadoop fs -du -h %s&apos;%path) for line in output.readlines(): row = filter(lambda x:x,map(lambda x:x.strip().replace(&quot; &quot;,&quot;&quot;),line.split(&quot; &quot;))) row_info = (conversion(row[0]),conversion(row[1]),row[2]) g.labels(row_info[2],&apos;false&apos;).set(row_info[0]) g.labels(row_info[2],&apos;true&apos;).set(row_info[1])def main(): registry = CollectorRegistry() g = Gauge(&apos;hadoop_hdfs_du_filesize_metrics&apos;,&apos;hdfs filepath filesize from hadoop fs du&apos;,[&apos;path&apos;,&apos;backupornot&apos;],registry=registry) for path in paths: findpush(path,g) pushadd_to_gateway(&apos;pushgateway的IP地址:9091&apos;, job=&apos;custom&apos; ,registry=g,timeout=200) 总结使用Prometheus监控，有2中方式 暴露http方式的Metrics，注意：需要在Prometheus的配置文件中添加job 主动发送数据到Pushgateway，注意：只需要添加一个Pushgateway就可以了。它相当于一个API，无论有多少个服务器，发送到统一的地址。 生产环境中，一般使用Pushgateway，简单，也不需要修改Prometheus的配置文件！]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群监控之exporter]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B9%8Bexporter%2F</url>
    <content type="text"><![CDATA[[TOC] 参考https://prometheus.io/docs/instrumenting/exporters/ blackbox_exporter: https://github.com/prometheus/blackbox_exporter https://yunlzheng.gitbook.io/prometheus-book/part-ii-prometheus-jin-jie/exporter/commonly-eporter-usage/install_blackbox_exporter https://blog.csdn.net/qq_25934401/article/details/84325356 node_exporter https://github.com/prometheus/node_exporter https://www.cnblogs.com/bigberg/p/10118137.html mysql_exporter: https://github.com/prometheus/mysqld_exporter jmx_exporter: https://github.com/prometheus/jmx_exporter https://www.cnblogs.com/caizhenghui/p/9132414.html 前言大部分exporter都可以在https://github.com/prometheus中搜到 以及可以使用helm search node-exporter来进行搜索 blackbox_exporter黑盒监测Prometheus 官方提供的 exporter 之一，可以提供 http、dns、tcp、icmp 的监控数据采集 部署安装包部署 123456789101112131415161718192021222324[sss@prometheus01 ]$ cd /usr/local/blackbox_exporter/[sss@prometheus01 ]$ wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.12.0/blackbox_exporter-0.12.0.linux-amd64.tar.gz [sss@prometheus01 ]$ tar zxvf blackbox_exporter-0.12.0.linux-amd64.tar.gz[sss@prometheus01 blackbox_exporter-0.12.0.linux-amd64]$ cd blackbox_exporter-0.12.0.linux-amd64[sss@prometheus01 blackbox_exporter-0.12.0.linux-amd64]$ lltotal 15720-rwxr-xr-x. 1 1000 1000 16074005 Feb 27 2018 blackbox_exporter-rw-rw-r--. 1 1000 1000 932 Nov 21 16:05 blackbox.yml-rw-rw-r--. 1 1000 1000 11357 Feb 27 2018 LICENSE-rw-rw-r--. 1 1000 1000 94 Feb 27 2018 NOTICE[sss@prometheus01 blackbox_exporter-0.12.0.linux-amd64]$cp -r blackbox_exporter /usr/local/bin[sss@prometheus01 blackbox_exporter-0.12.0.linux-amd64]$ cat /etc/supervisord.conf|grep blackbox -A 20[program:blackbox_exporter]command=/usr/local/bin/blackbox_exporter --config.file=/usr/local/prometheus/blackbox_exporter/blackbox_exporter-0.12.0.linux-amd64/blackbox.yml stdout_logfile=/tmp/prometheus/blackbox_exporter.logautostart=trueautorestart=truestartsecs=5priority=1user=rootstopasgroup=truekillasgroup=true[sss@prometheus01 blackbox_exporter-0.12.0.linux-amd64]$ supervisorctl status |grep blackboxblackbox_exporter RUNNING pid 25343, uptime 0:19:25 blackbox的配置文件 通过 blackbox.yml 定义模块详细信息 在 Prometheus 配置文件中引用该模块以及配置被监控目标主机 12345678910111213141516171819202122232425262728293031323334353637383940414243modules: http_2xx: prober: http timeout: 10s http: preferred_ip_protocol: &quot;ip4&quot; ##如果http监测是使用ipv4 就要写上，目前国内使用ipv6很少。 http_post_2xx_query: ##用于post请求使用的模块）由于每个接口传参不同 可以定义多个module 用于不同接口（例如此命名为http_post_2xx_query 用于监测query.action接口 prober: http timeout: 15s http: preferred_ip_protocol: &quot;ip4&quot; ##使用ipv4 method: POST headers: Content-Type: application/json ##header头 body: &apos;&#123;&quot;hmac&quot;:&quot;&quot;,&quot;params&quot;:&#123;&quot;publicFundsKeyWords&quot;:&quot;xxx&quot;&#125;&#125;&apos; ##传参 tcp_connect: prober: tcp pop3s_banner: prober: tcp tcp: query_response: - expect: &quot;^+OK&quot; tls: true tls_config: insecure_skip_verify: false ssh_banner: prober: tcp tcp: query_response: - expect: &quot;^SSH-2.0-&quot; irc_banner: prober: tcp tcp: query_response: - send: &quot;NICK prober&quot; - send: &quot;USER prober prober prober :prober&quot; - expect: &quot;PING :([^ ]+)&quot; send: &quot;PONG $&#123;1&#125;&quot; - expect: &quot;^:[^ ]+ 001&quot; icmp: prober: icmp timeout: 5s icmp: 在kubernetes集群中部署 blackbox_exporter-deploy.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prometheus-blackbox-exporter namespace: monitoring labels: k8s-app: prometheus-blackbox-exporterspec: selector: matchLabels: k8s-app: prometheus-blackbox-exporter replicas: 1 template: metadata: labels: k8s-app: prometheus-blackbox-exporter spec: restartPolicy: Always containers: - name: prometheus-blackbox-exporter image: prom/blackbox-exporter:v0.16.0 imagePullPolicy: IfNotPresent ports: - name: blackbox-port containerPort: 9115 readinessProbe: tcpSocket: port: 9115 initialDelaySeconds: 5 timeoutSeconds: 5 resources: requests: memory: 50Mi cpu: 50m limits: memory: 60Mi cpu: 100m volumeMounts: - name: config mountPath: /etc/blackbox_exporter args: - --config.file=/etc/blackbox_exporter/blackbox.yml - --log.level=debug - --web.listen-address=:9115 volumes: - name: config configMap: name: prometheus-blackbox-exporter ---apiVersion: v1kind: Servicemetadata: labels: k8s-app: prometheus-blackbox-exporter name: prometheus-blackbox-exporter namespace: monitoring annotations: prometheus.io/scrape: &apos;true&apos;spec: type: ClusterIP selector: k8s-app: prometheus-blackbox-exporter ports: - name: blackbox port: 9115 targetPort: 9115 protocol: TCP ---apiVersion: v1kind: ConfigMapmetadata: labels: k8s-app: prometheus-blackbox-exporter name: prometheus-blackbox-exporter namespace: monitoringdata: blackbox.yml: |- modules: http_2xx: prober: http timeout: 10s http: valid_http_versions: [&quot;HTTP/1.1&quot;, &quot;HTTP/2&quot;] valid_status_codes: [] method: GET preferred_ip_protocol: &quot;ip4&quot; http_post_2xx: # http post 监测模块 prober: http timeout: 10s http: valid_http_versions: [&quot;HTTP/1.1&quot;, &quot;HTTP/2&quot;] method: POST preferred_ip_protocol: &quot;ip4&quot; tcp_connect: prober: tcp timeout: 10s icmp: prober: icmp timeout: 10s icmp: preferred_ip_protocol: &quot;ip4&quot; 应用场景 HTTP 测试 定义 Request Header 信息判断 Http status / Http Respones Header / Http Body 内容 TCP 测试 业务组件端口状态监听 应用层协议定义与监听 ICMP 测试 主机探活机制 POST 测试 接口联通性 SSL 证书过期时间 http测试 相关代码块添加到 Prometheus 文件内 对应 blackbox.yml文件的 http_2xx 模块 12345678910111213141516- job_name: &apos;blackbox_http_2xx&apos; scrape_interval: 45s metrics_path: /probe params: module: [http_2xx] # Look for a HTTP 200 response. static_configs: - targets: - https://www.baidu.com/ - 172.0.0.1:9090 relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 10.XXX.XX.XX:9115 # The blackbox exporter&apos;s real hostname:port. TCP 测试 监听 业务端口地址，用来判断服务是否在线，我觉的和telnet 差不多 相关代码块添加到 Prometheus 文件内 对应 blackbox.yml文件的 tcp_connect 模块 12345678910111213141516171819- job_name: &quot;blackbox_telnet_port]&quot; scrape_interval: 5s metrics_path: /probe params: module: [tcp_connect] static_configs: - targets: [ &apos;1x3.x1.xx.xx4:443&apos; ] labels: group: &apos;xxxidc机房ip监控&apos; - targets: [&apos;10.xx.xx.xxx:443&apos;] labels: group: &apos;Process status of nginx(main) server&apos; relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 10.xxx.xx.xx:9115 ICMP 测试 相关代码块添加到 Prometheus 配置文件内 对应 blackbox.yml文件的 icmp 模块 12345678910111213141516171819202122- job_name: &apos;blackbox00_ping_idc_ip&apos; scrape_interval: 10s metrics_path: /probe params: module: [icmp] #ping static_configs: - targets: [ &apos;1x.xx.xx.xx&apos; ] labels: group: &apos;xxnginx 虚拟IP&apos; relabel_configs: - source_labels: [__address__] regex: (.*)(:80)? target_label: __param_target replacement: $&#123;1&#125; - source_labels: [__param_target] regex: (.*) target_label: ping replacement: $&#123;1&#125; - source_labels: [] regex: .* target_label: __address__ replacement: 1x.xxx.xx.xx:9115 POST 测试 监听业务接口地址，用来判断接口是否在线 相关代码块添加到 Prometheus 文件内 对应 blackbox.yml文件的 http_post_2xx_query 模块（监听query.action这个接口） 1234567891011121314151617- job_name: &apos;blackbox_http_2xx_post&apos; scrape_interval: 10s metrics_path: /probe params: module: [http_post_2xx_query] static_configs: - targets: - https://xx.xxx.com/api/xx/xx/fund/query.action labels: group: &apos;Interface monitoring&apos; relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 1x.xx.xx.xx:9115 # The blackbox exporter&apos;s real hostname:port. 查看监听过程类似于 1curl http://172.16.10.65:9115/probe?target=prometheus.io&amp;module=http_2xx&amp;debug=true 告警应用测试icmp、tcp、http、post 监测是否正常可以观察probe_success 这一指标probe_success == 0 ##联通性异常probe_success == 1 ##联通性正常告警也是判断这个指标是否等于0，如等于0 则触发异常报警 123456789101112[sss@prometheus01 prometheus]$ cat rules/blackbox-alert.rules groups:- name: blackbox_network_stats rules: - alert: blackbox_network_stats expr: probe_success == 0 for: 1m labels: severity: critical annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; is down&quot; description: &quot;This requires immediate action!&quot; SSL 证书过期时间监测123456789101112131415161718192021222324252627cat &lt;&lt; &apos;EOF&apos; &gt; prometheus.ymlrule_files: - ssl_expiry.rulesscrape_configs: - job_name: &apos;blackbox&apos; metrics_path: /probe params: module: [http_2xx] # Look for a HTTP 200 response. static_configs: - targets: - example.com # Target to probe relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 # Blackbox exporter. EOF cat &lt;&lt; &apos;EOF&apos; &gt; ssl_expiry.rules groups: - name: ssl_expiry.rules rules: - alert: SSLCertExpiringSoon expr: probe_ssl_earliest_cert_expiry&#123;job=&quot;blackbox&quot;&#125; - time() &lt; 86400 * 30 for: 10mEOF 自定义探针测试HTTP服务通常会以不同的形式对外展现，有些可能就是一些简单的网页，而有些则可能是一些基于REST的API服务。 对于不同类型的HTTP的探测需要管理员能够对HTTP探针的行为进行更多的自定义设置，包括：HTTP请求方法、HTTP头信息、请求参数等。对于某些启用了安全认证的服务还需要能够对HTTP探测设置相应的Auth支持。对于HTTPS类型的服务还需要能够对证书进行自定义设置。 如下所示，这里通过method定义了探测时使用的请求方法，对于一些需要请求参数的服务，还可以通过headers定义相关的请求头信息，使用body定义请求内容： 12345678http_post_2xx: prober: http timeout: 5s http: method: POST headers: Content-Type: application/json body: &apos;&#123;&#125;&apos; 如果HTTP服务启用了安全认证，Blockbox Exporter内置了对basic_auth的支持，可以直接设置相关的认证信息即可： 12345678910http_basic_auth_example: prober: http timeout: 5s http: method: POST headers: Host: &quot;login.example.com&quot; basic_auth: username: &quot;username&quot; password: &quot;mysecret&quot; 对于使用了Bear Token的服务也可以通过bearer_token配置项直接指定令牌字符串，或者通过bearer_token_file指定令牌文件。 对于一些启用了HTTPS的服务，但是需要自定义证书的服务，可以通过tls_config指定相关的证书信息： 123456http_custom_ca_example: prober: http http: method: GET tls_config: ca_file: &quot;/certs/my_cert.crt&quot; 自定义探针行为在默认情况下HTTP探针只会对HTTP返回状态码进行校验，如果状态码为2XX（200 &lt;= StatusCode &lt; 300）则表示探测成功，并且探针返回的指标probe_success值为1。 如果用户需要指定HTTP返回状态码，或者对HTTP版本有特殊要求，如下所示，可以使用valid_http_versions和valid_status_codes进行定义： 123456http_2xx_example: prober: http timeout: 5s http: valid_http_versions: [&quot;HTTP/1.1&quot;, &quot;HTTP/2&quot;] valid_status_codes: [] 默认情况下，Blockbox返回的样本数据中也会包含指标probe_http_ssl，用于表明当前探针是否使用了SSL： 123# HELP probe_http_ssl Indicates if SSL was used for the final redirect# TYPE probe_http_ssl gaugeprobe_http_ssl 0 而如果用户对于HTTP服务是否启用SSL有强制的标准。则可以使用fail_if_ssl和fail_if_not_ssl进行配置。fail_if_ssl为true时，表示如果站点启用了SSL则探针失败，反之成功。fail_if_not_ssl刚好相反。 123456789http_2xx_example: prober: http timeout: 5s http: valid_status_codes: [] method: GET no_follow_redirects: false fail_if_ssl: false fail_if_not_ssl: false 除了基于HTTP状态码，HTTP协议版本以及是否启用SSL作为控制探针探测行为成功与否的标准以外，还可以匹配HTTP服务的响应内容。使用fail_if_matches_regexp和fail_if_not_matches_regexp用户可以定义一组正则表达式，用于验证HTTP返回内容是否符合或者不符合正则表达式的内容。 123456789http_2xx_example: prober: http timeout: 5s http: method: GET fail_if_matches_regexp: - &quot;Could not connect to database&quot; fail_if_not_matches_regexp: - &quot;Download the latest version here&quot; 最后需要提醒的时，默认情况下HTTP探针会走IPV6的协议。 在大多数情况下，可以使用preferred_ip_protocol=ip4强制通过IPV4的方式进行探测。在Bloackbox响应的监控样本中，也会通过指标probe_ip_protocol，表明当前的协议使用情况： 123# HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6# TYPE probe_ip_protocol gaugeprobe_ip_protocol 6 除了支持对HTTP协议进行网络探测以外，Blackbox还支持对TCP、DNS、ICMP等其他网络协议，感兴趣的读者可以从Blackbox的Github项目中获取更多使用信息. node_exporter功能对照表默认开启的功能 名称 说明 系统 arp 从 /proc/net/arp 中收集 ARP 统计信息 Linux conntrack 从 /proc/sys/net/netfilter/ 中收集 conntrack 统计信息 Linux cpu 收集 cpu 统计信息 Darwin, Dragonfly,FreeBSD, Linux diskstats 从 /proc/diskstats 中收集磁盘 I/O 统计信息 Linux edac 错误检测与纠正统计信息 Linux entropy 可用内核熵信息 Linux exec execution 统计信息 Dragonfly, FreeBSD filefd 从 /proc/sys/fs/file-nr 中收集文件描述符统计信息 Linux filesystem 文件系统统计信息，例如磁盘已使用空间 Darwin, Dragonfly,FreeBSD, Linux, OpenBSD hwmon 从 /sys/class/hwmon/ 中收集监控器或传感器数据信息 Linux infiniband 从 InfiniBand 配置中收集网络统计信息 Linux loadavg 收集系统负载信息 Darwin, Dragonfly, FreeBSD,Linux, NetBSD, OpenBSD, Solaris mdadm 从 /proc/mdstat 中获取设备统计信息 Linux meminfo 内存统计信息 Darwin, Dragonfly,FreeBSD, Linux netdev 网口流量统计信息，单位 bytes Darwin, Dragonfly,FreeBSD, Linux, OpenBSD netstat 从 /proc/net/netstat 收集网络统计数据，等同于 netstat -s Linux sockstat 从 /proc/net/sockstat 中收集 socket 统计信息 Linux stat 从 /proc/stat 中收集各种统计信息，包含系统启动时间，forks, 中断等 Linux textfile 通过 --collector.textfile.directory参数指定本地文本收集路径，收集文本信息 any time 系统当前时间 any uname 通过 uname 系统调用, 获取系统信息 any vmstat 从 /proc/vmstat 中收集统计信息 Linux wifi 收集 wifi 设备相关统计数据 Linux xfs 收集 xfs 运行时统计信息 Linux (kernel 4.4+) zfs 收集 zfs 性能统计信息 Linux 默认关闭的功能 名称 说明 系统 bonding 收集系统配置以及激活的绑定网卡数量 Linux buddyinfo 从 /proc/buddyinfo 中收集内存碎片统计信息 Linux devstat 收集设备统计信息 Dragonfly, FreeBSD drbd 收集远程镜像块设备（DRBD）统计信息 Linux interrupts 收集更具体的中断统计信息 Linux，OpenBSD ipvs 从 /proc/net/ip_vs 中收集 IPVS 状态信息，从 /proc/net/ip_vs_stats 获取统计信息 Linux ksmd 从 /sys/kernel/mm/ksm 中获取内核和系统统计信息 Linux logind 从 logind 中收集会话统计信息 Linux meminfo_numa 从 /proc/meminfo_numa 中收集内存统计信息 Linux mountstats 从 /proc/self/mountstat 中收集文件系统统计信息，包括 NFS 客户端统计信息 Linux nfs 从 /proc/net/rpc/nfs 中收集 NFS 统计信息，等同于 nfsstat -c Linux qdisc 收集队列推定统计信息 Linux runit 收集 runit 状态信息 any supervisord 收集 supervisord 状态信息 any systemd 从 systemd 中收集设备系统状态信息 Linux tcpstat 从 /proc/net/tcp 和 /proc/net/tcp6 收集 TCP 连接状态信息 Linux 注意：我们可以使用 --collectors.enabled 运行参数指定 node_exporter 收集的功能模块, 如果不指定，将使用默认模块。 安装下载地址：https://prometheus.io/download/#node_exporter 安装包安装安装node exporter 123tar -zxvf node_exporter-0.16.0.linux-amd64.tar.gzmv node_exporter-0.16.0.linux-amd64 /usr/local/node_exporter 创建systemd服务 1234567891011121314vim /etc/systemd/system/node_exporter.service[Unit]Description=node_exporterAfter=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/node_exporter/node_exporterRestart=on-failure[Install]WantedBy=multi-user.target 启动node_exporter 1234systemctl daemon-reloadsystemctl start node_exportersystemctl status node_exportersystemctl enable node_exporter 验证启动成功 1curl 127.0.0.1:9100/metrics kubernetes安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: prometheus-node-exporter namespace: monitoring labels: k8s-app: prometheus-node-exporterspec: template: metadata: name: prometheus-node-exporter labels: k8s-app: prometheus-node-exporter spec: containers: - image: prom/node-exporter:v0.18.0 imagePullPolicy: IfNotPresent name: prometheus-node-exporter ports: - name: prom-node-exp containerPort: 9100 hostPort: 9100 livenessProbe: failureThreshold: 3 httpGet: path: / port: 9100 scheme: HTTP readinessProbe: failureThreshold: 3 httpGet: path: / port: 9100 scheme: HTTP resources: limits: cpu: 20m memory: 2Gi requests: cpu: 10m memory: 1Gi dnsPolicy: ClusterFirst hostNetwork: true hostPID: true ---apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: &apos;true&apos; prometheus.io/app-metrics: &apos;true&apos; prometheus.io/app-metrics-path: &apos;/metrics&apos; name: prometheus-node-exporter namespace: monitoring labels: k8s-app: prometheus-node-exporterspec: ports: - name: prometheus-node-exporter port: 9100 protocol: TCP selector: k8s-app: prometheus-node-exporter type: ClusterIP 配置可以利用 Prometheus 的 static_configs 来拉取 node_exporter 的数据。 123- job_name: &apos;node&apos; static_configs: - targets: [&apos;localhost:9100&apos;] 重启prometheus，然后在Prometheus页面中的Targets中就能看到新加入的node 常用查询语句收集到 node_exporter 的数据后，我们可以使用 PromQL 进行一些业务查询和监控，下面是一些比较常见的查询 以下查询均以单个节点作为例子，如果大家想查看所有节点，将 instance=&quot;xxx&quot; 去掉即可。 cpu使用率1100 - (avg by (instance) (irate(node_cpu&#123;instance=&quot;172.16.8.153:9100&quot;, mode=&quot;idle&quot;&#125;[5m])) * 100) CPU各个mode使用率1avg by (instance, mode) (irate(node_cpu&#123;instance=&quot;172.16.8.153:9100&quot;&#125;[5m])) * 100 User：CPU一共花了多少比例的时间运行在用户态空间或者说是用户进程(running user space processes)。典型的用户态空间程序有：Shells、数据库、web服务器等 Nice：可理解为，用户空间进程的CPU的调度优先级，范围为[-20,19] System：System的含义与User相似。System表示：CPU花了多少比例的时间在内核空间运行。分配内存、IO操作、创建子进程……都是内核操作。这也表明，当IO操作频繁时，System参数会很高 ioWait：在计算机中，读写磁盘的操作远比CPU运行的速度要慢，CPU负载处理数据，而数据一般在磁盘上需要读到内存中才能处理。当CPU发起读写操作后，需要等着磁盘驱动器将数据读入内存,从而导致CPU 在等待的这一段时间内无事可做。CPU处于这种等待状态的时间由Wait参数来衡量 Idle：CPU处于空闲状态时间比例。一般而言，idel + user + nice 约等于100% 机器平均负载123node_load1&#123;instance=&quot;172.16.8.153:9100&quot;&#125; // 1分钟负载node_load5&#123;instance=&quot;172.16.8.153:9100&quot;&#125; // 5分钟负载node_load15&#123;instance=&quot;172.16.8.153:9100&quot;&#125; // 15分钟负载 内存使用率1100-(node_memory_MemFree&#123;instance=&quot;172.16.8.172:9100&quot;&#125;+node_memory_Cached&#123;instance=&quot;172.16.8.172:9100&quot;&#125;+node_memory_Buffers&#123;instance=&quot;172.16.8.172:9100&quot;&#125;)/node_memory_MemTotal&#123;instance=&quot;172.16.8.172:9100&quot;&#125; * 100 磁盘使用率1100 - node_filesystem_free&#123;instance=&quot;172.16.8.153:9100&quot;,fstype!~&quot;rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|udev|none|devpts|sysfs|debugfs|fuse.*&quot;&#125; / node_filesystem_size&#123;instance=&quot;172.16.8.153:9100&quot;,fstype!~&quot;rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|udev|none|devpts|sysfs|debugfs|fuse.*&quot;&#125; * 100 网卡出入包12345// 入包量sum by (instance) (rate(node_network_receive_bytes&#123;instance=&quot;172.16.8.153:9100&quot;,device!=&quot;lo&quot;&#125;[5m]))// 出包量sum by (instance) (rate(node_network_transmit_bytes&#123;instance=&quot;172.16.8.153:9100&quot;,device!=&quot;lo&quot;&#125;[5m])) dashboard模板可以在grafana官网中搜索对应模板 Mysql exporter安装使用helm安装部署 1helm --fetch mysql_exporter 拉取下来之后修改values文件 授权mysqld_exporter需要连接Mysql，首先为它创建用户并赋予所需要的权限： 123CREATE USER &apos;exporter&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH MAX_USER_CONNECTIONS 3;GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO &apos;exporter&apos;@&apos;%&apos;;flush privileges; prometheus配置1234- job_name: &apos;mysql&apos; static_configs: - targets: - localhost:9104 验证状态1curl localhost:9104/metrics jmx_exporter举例监控kafkajmx_prometheus_javaagent 方式收集kafka指标 下载jmx_prometheus_javaagent和kafka.yml 12wget https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-0-8-2.ymlwget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.6/jmx_prometheus_javaagent-0.6.jar 编辑kafka的启动文件 kafka-server-start.sh 添加几行代码： 12export JMX_PORT=&quot;9999&quot;export KAFKA_OPTS=&quot;-javaagent:/path/jmx_prometheus_javaagent-0.6.jar=9991:/path/kafka-0-8-2.yml&quot; 然后重启kafka。访问 http://localhost:9991/metrics 可以看到各种指标了。 举例监控hadoop下载jar包 1wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.3.1/jmx_prometheus_javaagent-0.3.1.jar 创建配置文件 创建namenode.yaml(datanode.yaml)放在任意位置，内容为你想要的metrics 1234567---startDelaySeconds: 0hostPort: master:1234 #master为本机IP（一般可设置为localhost）；1234为想设置的jmx端口（可设置为未被占用的端口）#jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:1234/jmxrmissl: falselowercaseOutputName: falselowercaseOutputLabelNames: false 参数说明 Name Description startDelaySeconds start delay before serving requests. Any requests within the delay period will result in an empty metrics set. hostPort The host and port to connect to via remote JMX. If neither this nor jmxUrl is specified, will talk to the local JVM. username The username to be used in remote JMX password authentication. password The password to be used in remote JMX password authentication. jmxUrl A full JMX URL to connect to. Should not be specified if hostPort is. ssl Whether JMX connection should be done over SSL. To configure certificates you have to set following system properties: -Djavax.net.ssl.keyStore=/home/user/.keystore -Djavax.net.ssl.keyStorePassword=changeit -Djavax.net.ssl.trustStore=/home/user/.truststore -Djavax.net.ssl.trustStorePassword=changeit lowercaseOutputName Lowercase the output metric name. Applies to default format and name. Defaults to false. lowercaseOutputLabelNames Lowercase the output metric label names. Applies to default format and labels. Defaults to false. whitelistObjectNames A list of ObjectNames to query. Defaults to all mBeans. blacklistObjectNames A list of ObjectNames to not query. Takes precedence over whitelistObjectNames. Defaults to none. rules A list of rules to apply in order, processing stops at the first matching rule. Attributes that aren’t matched aren’t collected. If not specified, defaults to collecting everything in the default format. pattern Regex pattern to match against each bean attribute. The pattern is not anchored. Capture groups can be used in other options. Defaults to matching everything. attrNameSnakeCase Converts the attribute name to snake case. This is seen in the names matched by the pattern and the default format. For example, anAttrName to an_attr_name. Defaults to false. name The metric name to set. Capture groups from the pattern can be used. If not specified, the default format will be used. If it evaluates to empty, processing of this attribute stops with no output. value Value for the metric. Static values and capture groups from the pattern can be used. If not specified the scraped mBean value will be used. valueFactor Optional number that value (or the scraped mBean value if value is not specified) is multiplied by, mainly used to convert mBean values from milliseconds to seconds. labels A map of label name to label value pairs. Capture groups from pattern can be used in each. name must be set to use this. Empty names and values are ignored. If not specified and the default format is not being used, no labels are set. help Help text for the metric. Capture groups from pattern can be used. name must be set to use this. Defaults to the mBean attribute decription and the full name of the attribute. type The type of the metric, can be GAUGE, COUNTER or UNTYPED. name must be set to use this. Defaults to UNTYPED. 修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh NameNode节点添加： 1export HADOOP_NAMENODE_OPTS=&quot;-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=1234 $HADOOP_NAMENODE_OPTS &quot; DataNode节点添加： 1export HADOOP_DATANODE_OPTS=&quot;-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=1235 $HADOOP_DATANODE_OPTS &quot; ps: 端口1234（1235）要与之前设置的jmx端口保持一致 修改$HADOOP_HOME/bin/hdfs NameNode: 1export HADOOP_NAMENODE_OPTS=&quot;$HADOOP_NAMENODE_OPTS -javaagent:/home/hadoop/jmx_prometheus_javaagent-0.3.1.jar=9200:/home/hadoop/namenode.yaml&quot; DataNode: 1export HADOOP_DATANODE_OPTS=&quot;$HADOOP_DATANODE_OPTS -javaagent:/home/hadoop/jmx_prometheus_javaagent-0.3.1.jar=9300:/home/hadoop/datanode.yaml&quot; 提示：9200（9300）为jmx_exporter提供metrics数据端口，后续Prometheus从此端口获取数据 访问http://master:9200/metrics就能获得需要的metrics数据: 12345678910# HELP jvm_buffer_pool_used_bytes Used bytes of a given JVM buffer pool.# TYPE jvm_buffer_pool_used_bytes gaugejvm_buffer_pool_used_bytes&#123;pool=&quot;direct&quot;,&#125; 1181032.0jvm_buffer_pool_used_bytes&#123;pool=&quot;mapped&quot;,&#125; 0.0# HELP jvm_buffer_pool_capacity_bytes Bytes capacity of a given JVM buffer pool.# TYPE jvm_buffer_pool_capacity_bytes gaugejvm_buffer_pool_capacity_bytes&#123;pool=&quot;direct&quot;,&#125; 1181032.0jvm_buffer_pool_capacity_bytes&#123;pool=&quot;mapped&quot;,&#125; 0.0# HELP jvm_buffer_pool_used_buffers Used buffers of a given JVM buffer pool....]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>exporter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群监控之grafana]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B9%8Bgrafana%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://segmentfault.com/a/1190000016237454 https://blog.csdn.net/wzygis/article/details/52727067 安装部署grafana-deploy.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657---apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: k8s-app: grafana name: prometheus-grafana namespace: monitoringspec: replicas: 1 selector: matchLabels: k8s-app: grafana template: metadata: labels: k8s-app: grafana spec: containers: - image: grafana/grafana:6.5.2 livenessProbe: failureThreshold: 10 httpGet: path: /api/health port: 3000 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 30 name: grafana ports: - containerPort: 3000 name: grafana protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /api/health port: 3000 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/grafana/grafana.ini name: config subPath: grafana.ini - mountPath: /var/lib/grafana name: storage volumes: - configMap: defaultMode: 420 name: prometheus-grafana name: config - emptyDir: &#123;&#125; name: storage grafana-config.yml 1234567891011121314151617181920212223242526272829303132---apiVersion: v1kind: ConfigMapmetadata: labels: k8s-app: grafana name: prometheus-grafana namespace: monitoringdata: grafana.ini: | [server] root_url = https://test.com [log] mode = console level = info [paths] data = /var/lib/grafana/data logs = /var/log/grafana plugins = /var/lib/grafana/plugins provisioning = /etc/grafana/provisioning [smtp] enabled = true host = **** user = **** password = **** cert_file = key_file = skip_verify = false from_address = **** from_name =**** [emails] welcome_email_on_sign_up = true grafana-svc.yml 1234567891011121314151617181920---apiVersion: v1kind: Servicemetadata: labels: k8s-app: grafana annotations: prometheus.io/scrape: &apos;true&apos; #prometheus.io/tcp-probe: &apos;true&apos; #prometheus.io/tcp-probe-port: &apos;80&apos; name: prometheus-grafana namespace: monitoringspec: type: NodePort ports: - port: 3000 targetPort: 3000 nodePort: 30028 selector: k8s-app: grafana 配置详解grafana后端的配置文件可以是多个以.ini结尾的配置文件，主要从三个配置文件读取配置：默认是$WORKING_DIR/conf/defaults.ini，其次用户配置是$WORKING_DIR/conf/custom.ini，用户配置则可以在命令行启动grafana时通过--config参数重新指定配置文件来覆盖。如果你是以deb或者rpm安装的，则默认的配置文件是/etc/grafana/grafana.ini，这个文件是在init.d的启动脚本中通过--config参数指定的。 所有在配置文件中的配置都可以通过环境变量来覆盖，使用的语法如下：GF_&lt;SectionName&gt;_&lt;KeyName&gt;，例如：12345[security]admin_user = admin [auth.google]client_secret = 0ldS3cretKey 如果使用环境变量，则是如下： 12export GF_SECURITY_ADMIN_USER=trueexport GF_AUTH_GOOGLE_CLIENT_SECRET=newS3cretKey 下面具体看看每个配置段的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143app_mode：应用名称，默认是production [path]data：一个grafana用来存储sqlite3、临时文件、回话的地址路径logs：grafana存储logs的路径 [server]http_addr：监听的ip地址，，默认是0.0.0.0http_port：监听的端口，默认是3000protocol：http或者https，，默认是httpdomain：这个设置是root_url的一部分，当你通过浏览器访问grafana时的公开的domian名称，默认是localhostenforce_domain：如果主机的header不匹配domian，则跳转到一个正确的domain上，默认是falseroot_url：这是一个web上访问grafana的全路径url，默认是%(protocol)s://%(domain)s:%(http_port)s/router_logging：是否记录web请求日志，默认是falsecert_file：如果使用https则需要设置cert_key：如果使用https则需要设置 [database]grafana默认需要使用数据库存储用户和dashboard信息，默认使用sqlite3来存储，你也可以换成其他数据库type：可以是mysql、postgres、sqlite3，默认是sqlite3path：只是sqlite3需要，定义sqlite3的存储路径host：只是mysql、postgres需要，默认是127.0.0.1:3306name：grafana的数据库名称，默认是grafanauser：连接数据库的用户password：数据库用户的密码ssl_mode：只是postgres使用 [security]admin_user：grafana默认的admin用户，默认是adminadmin_password：grafana admin的默认密码，默认是adminlogin_remember_days：多少天内保持登录状态secret_key：保持登录状态的签名disable_gravatar： [users]allow_sign_up：是否允许普通用户登录，如果设置为false，则禁止用户登录，默认是true，则admin可以创建用户，并登录grafanaallow_org_create：如果设置为false，则禁止用户创建新组织，默认是trueauto_assign_org：当设置为true的时候，会自动的把新增用户增加到id为1的组织中，当设置为false的时候，新建用户的时候会新增一个组织auto_assign_org_role：新建用户附加的规则，默认是Viewer，还可以是Admin、Editor [auth.anonymous]enabled：设置为true，则开启允许匿名访问，默认是falseorg_name：为匿名用户设置组织名称org_role：为匿名用户设置的访问规则，默认是Viewer [auth.github]针对github项目的，很明显，呵呵enabled = falseallow_sign_up = falseclient_id = some_idclient_secret = some_secretscopes = user:emailauth_url = https://github.com/login/oauth/authorizetoken_url = https://github.com/login/oauth/access_tokenapi_url = https://api.github.com/userteam_ids =allowed_domains =allowed_organizations = [auth.google]针对google app的，呵呵enabled = falseallow_sign_up = falseclient_id = some_client_idclient_secret = some_client_secretscopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.emailauth_url = https://accounts.google.com/o/oauth2/authtoken_url = https://accounts.google.com/o/oauth2/tokenapi_url = https://www.googleapis.com/oauth2/v1/userinfoallowed_domains = [auth.basic]enabled：当设置为true，则http api开启基本认证 [auth.ldap]enabled：设置为true则开启LDAP认证，默认是falseconfig_file：如果开启LDAP，指定LDAP的配置文件/etc/grafana/ldap.toml [auth.proxy]允许你在一个HTTP反向代理上进行认证设置enabled：默认是falseheader_name：默认是X-WEBAUTH-USERheader_property：默认是个名称usernameauto_sign_up：默认是true。开启自动注册，如果用户在grafana DB中不存在 [analytics]reporting_enabled：如果设置为true，则会发送匿名使用分析到stats.grafana.org，主要用于跟踪允许实例、版本、dashboard、错误统计。默认是truegoogle_analytics_ua_id：使用GA进行分析，填写你的GA ID即可 [dashboards.json]如果你有一个系统自动产生json格式的dashboard，则可以开启这个特性试试enabled：默认是falsepath：一个全路径用来包含你的json dashboard，默认是/var/lib/grafana/dashboards [session]provider：默认是file，值还可以是memory、mysql、postgresprovider_config：这个值的配置由provider的设置来确定，如果provider是file，则是data/xxxx路径类型，如果provider是mysql，则是user:password@tcp(127.0.0.1:3306)/database_name，如果provider是postgres，则是user=a password=b host=localhost port=5432 dbname=c sslmode=disablecookie_name：grafana的cookie名称cookie_secure：如果设置为true，则grafana依赖https，默认是falsesession_life_time：session过期时间，默认是86400秒，24小时 以下是官方文档没有，配置文件中有的[smtp]enabled = falsehost = localhost:25user =password =cert_file =key_file =skip_verify = falsefrom_address = admin@grafana.localhost [emails]welcome_email_on_sign_up = falsetemplates_pattern = emails/*.html [log]mode：可以是console、file，默认是console、file，也可以设置多个，用逗号隔开buffer_len：channel的buffer长度，默认是10000level：可以是"Trace", "Debug", "Info", "Warn", "Error", "Critical"，默认是info [log.console]level：设置级别 [log.file]level：设置级别log_rotate：是否开启自动轮转max_lines：单个日志文件的最大行数，默认是1000000max_lines_shift：单个日志文件的最大大小，默认是28，表示256MBdaily_rotate：每天是否进行日志轮转，默认是truemax_days：日志过期时间，默认是7,7天后删除 Grafana相关使用将数据源添加到Grafana 点击顶部标题中的Grafana图标打开侧边菜单 在Dashboards链接下的侧边菜单中，你应找到名为Data Sources的链接 点击顶部标题中的+ Add data source按钮 从类型下拉列表中选择Prometheus 注意：如果你没有在侧边菜单中看到Data Sources链接，则表示你当前的用户没有当前组织的Admin角色。 数据源选项 名称 描述 Name 数据源名称，这是你在面板和查询中引用数据源的方式 Default 默认数据源意味着它将为新面板预先选择 Url 你的Prometheus服务的http协议，IP和端口（默认端口通常是9090） Access Server (default) = 需要从Grafana后端/服务器访问的URL，Browser = 需要从浏览器访问的URL Basic Auth 启用对Prometheus数据源的基本身份验证 User 你的Prometheus用户名 Password 数据库用户的密码 Scrape interval 这将用作Prometheus步骤查询参数的下限，默认值为15秒 查询编辑器单击标题，以编辑模式打开图形 &gt; 编辑（或在鼠标悬停在面板上时按e键）。 名称 描述 Query expression Prometheus查询表达式，请查看Prometheus文档 Legend format 使用名称或模式控制时间系列的名称，例如，将替换为标签hostname的标签值 Min step 设置Prometheus步骤选项的下限，步骤控制Prometheus查询引擎执行范围查询时跳转的大小，遗憾的是，没有官方的prometheus文档链接到这个非常重要的选项 Resolution 控制步骤选项，小步骤可以创建高分辨率图形，但在较大的时间范围内可能会很慢，降低分辨率可以加快速度。1/2将尝试设置步骤选项以为每个其他像素生成1个数据点，值为1/10将尝试设置步长选项，因此每10个像素就有一个数据点。 Metric lookup 在此输入字段来搜索指标名称 Format as 在表格，时间序列或心跳图之间切换，表格格式仅适用于表格面板，心跳图格式适用于在心跳图面板上显示具有直方图类型的指标。在引擎盖下，它将累积的直方图转换为规则序列，并按桶绑定对序列进行排序。 模板你可以在指标查询中使用变量代替硬编码服务器、应用程序和传感器名称等内容。变量显示为仪表盘顶部的下拉选择框，这些下拉菜单可以轻松更改仪表盘中显示的数据。查看模板文档，了解模板功能和不同类型的模板变量。 查询变量查询类型的变量允许你查询Prometheus以获取指标、标签或标签值的列表，Prometheus数据源插件提供了以下可在Query输入字段中使用的函数。 名称 描述 label_values(label) 返回每个指标中label的标签值列表 label_values(metric, label) 返回指定指标中label的标签值列表 metrics(metric) 返回与指定的metric正则表达式匹配的指标列表 query_result(query) 返回query的Prometheus查询结果列表 有关指标名称、标签名称和标签值的详细信息，请参阅Prometheus文档。 使用区间和范围变量 支持$__range，$__range_s和$__range_ms，仅适用于Grafana v5.3 可以在查询变量中使用一些全局内置变量；$__interval，$__interval_ms，$__range，$__range_s和$__range_ms，有关详细信息，请参阅全局内置变量。当你需要过滤变量查询时，这些可以方便地与query_result函数一起使用，因为label_values函数不支持查询。 确保将变量的refresh触发器设置为On Time Range Change，以便在更改仪表盘上的时间范围时获取正确的实例。 用法示例： 根据仪表盘中显示的时间范围内的平均QPS，使用最繁忙的5个请求实例填充变量： 12Query: query_result(topk(5, sum(rate(http_requests_total[$__range])) by (instance)))Regex: /&quot;([^&quot;]+)&quot;/ 使用更精确的$__range_s，在仪表板中显示的时间范围内具有特定状态的实例填充变量： 12Query: query_result(max_over_time(&lt;metric&gt;[$&#123;__range_s&#125;s]) != &lt;state&gt;)Regex: 在查询中使用变量有两种语法： $ 例如：rate(http_requests_total{job=~“$job”}[5m]) [[varname]] 例如：rate(http_requests_total{job=~”[[job]]“}[5m]) 为什么两种方式？第一种语法更易于读写，但不允许你在单词的中间使用变量，启用“多值”或“包括所有值”选项后，Grafana会将标签从纯文本转换为正则表达式兼容的字符串，这意味着你必须使用=〜而不是=。 注解注解允许你在图表上叠加丰富的事件信息，你可以通过仪表盘菜单/注解视图添加注解查询。 Prometheus支持两种查询注解的方法： 常规指标查询 针对挂起和触发警报的Prometheus查询（有关详细信息，请参阅在运行时检查警报） 步骤选项可用于限制从查询返回的事件数。 将Grafana指标纳入Prometheus从4.6.0开始，Grafana在/metrics端点上为Prometheus公开了指标，我们还在Grafana中捆绑了一个仪表盘，以便你可以更快地开始查看指标。你可以通过到数据源编辑页面并点击仪表盘选项卡来导入捆绑的仪表盘，在那里你可以找到一个Grafana仪表盘和一个Prometheus仪表盘，导入并开始查看所有指标！ 使用Provisioning配置数据源现在可以使用Grafana的Provisioning系统使用配置文件配置数据源，你可以在Provisioning文档页面上阅读有关其工作原理以及可以为数据源设置的所有设置的更多信息。 以下是此数据源的一些Provisioning示例： 1234567apiVersion: 1datasources: - name: Prometheus type: prometheus access: proxy url: http://localhost:9090]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群监控之alertmanager]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B9%8Balertmanager%2F</url>
    <content type="text"><![CDATA[[toc] 参考 https://github.com/prometheus/alertmanager https://prometheus.io/docs/alerting/alertmanager/ https://www.cnblogs.com/wangxu01/articles/11654836.html 简介Alertmanager 主要用于接收 Prometheus 发送的告警信息，它支持丰富的告警通知渠道，而且很容易做到告警信息进行去重，降噪，分组等，是一款前卫的告警通知系统。 配置部署主要步骤一、部署Alertmanager二、配置Prometheus与Alertmanager通信三、配置告警 1. prometheus指定rules目录 2. configmap存储告警规则 3. configmap挂载到容器rules目录 部署alertmanageralertmanager-deploy.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: apps/v1beta2kind: Deploymentmetadata: name: alertmanager namespace: monitoringspec: replicas: 1 selector: matchLabels: k8s-app: alertmanager template: metadata: labels: k8s-app: alertmanager spec: containers: - image: prom/alertmanager:v0.19.0 name: alertmanager args: - &quot;--config.file=/etc/alertmanager/config/alertmanager.yml&quot; - &quot;--storage.path=/alertmanager&quot; - &quot;--data.retention=720h&quot; volumeMounts: - mountPath: &quot;/alertmanager&quot; name: data - mountPath: &quot;/etc/alertmanager/config&quot; name: config-volume - mountPath: &quot;/etc/alertmanager/template&quot; name: alertmanager-tmpl resources: requests: cpu: 50m memory: 500Mi limits: cpu: 100m memory: 1Gi volumes: - name: data emptyDir: &#123;&#125; - name: config-volume configMap: name: alertmanager-config - name: alertmanager-tmpl configMap: name: alertmanager-tmpl alertmanager-svc.yaml 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: name: alertmanager namespace: monitoring labels: k8s-app: alertmanager annotations: prometheus.io/scrape: &apos;true&apos;spec: ports: - name: web port: 9093 targetPort: 9093 protocol: TCP nodePort: 30027 type: NodePort selector: k8s-app: alertmanager alertmanager-config.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859apiVersion: v1kind: ConfigMapmetadata: name: alertmanager-config namespace: monitoringdata: alertmanager.yml: | global: resolve_timeout: 1m wechat_api_corp_id: &apos;*****&apos; wechat_api_url: &apos;*****&apos; wechat_api_secret: &apos;*****&apos; smtp_smarthost: &apos;*****&apos; smtp_from: &apos;****&apos; smtp_auth_username: &apos;****&apos; smtp_auth_password: &apos;****&apos; smtp_require_tls: true templates: - &apos;/etc/alertmanager/template/*.tmpl&apos; route: group_by: [&apos;alertname&apos;, &apos;job&apos;] group_wait: 20s group_interval: 20s repeat_interval: 2m receiver: &apos;email&apos; routes: - receiver: &quot;email&quot; group_wait: 10s continue: true match_re: severity: critical|error|warning #- receiver: &quot;wechat&quot; #group_wait: 10s #continue: true #match_re: #severity: critical|error|warning receivers: #- name: &quot;wechat&quot; #wechat_configs: #- send_resolved: true #to_party: &apos;1&apos; #agent_id: 1000003 #corp_id: &apos;****&apos; #api_url: &apos;****&apos; #api_secret: &apos;****&apos; - name: &quot;email&quot; email_configs: - to: &apos;****&apos; send_resolved: true inhibit_rules: - source_match: severity: &apos;critical&apos; target_match: severity: &apos;error&apos; equal: [&apos;alertname&apos;] alertmanager-tmpl.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354apiVersion: v1kind: ConfigMapmetadata: name: alertmanager-tmpl namespace: monitoringdata: wechat.tmpl: | &#123;&#123; define &quot;wechat.default.message&quot; &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @警报 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @恢复 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; 恢复: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- end &#125;&#125; email.tmpl: | &#123;&#123; define &quot;email.default.html&quot; &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @警报 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @恢复 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; 恢复: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123; end &#125;&#125; 部署之后通过IP：9093访问alertmanager UI界面 在这个页面中我们可以进行一些操作，比如过滤、分组等等，里面还有两个新的概念：Inhibition(抑制)和 Silences(静默)。 Inhibition：如果某些其他警报已经触发了，则对于某些警报，Inhibition 是一个抑制通知的概念。例如：一个警报已经触发，它正在通知整个集群是不可达的时，Alertmanager 则可以配置成关心这个集群的其他警报无效。这可以防止与实际问题无关的数百或数千个触发警报的通知，Inhibition 需要通过上面的配置文件进行配置。 Silences：静默是一个非常简单的方法，可以在给定时间内简单地忽略所有警报。Silences 基于 matchers配置，类似路由树。来到的警告将会被检查，判断它们是否和活跃的 Silences 相等或者正则表达式匹配。如果匹配成功，则不会将这些警报发送给接收者。 配置Prometheus与alertmanager通信编辑 prometheus-configmap.yaml 配置文件添加绑定信息 最后的alert模块修改一下，之前的都注释 1234alerting: alertmanagers: - static_configs: - targets: [&quot;alertmanager:80&quot;] ####需要修改alertmanger服务名字，集群内部通过服务名调用 配置告警prometheus指定rules目录编辑 prometheus-configmap.yaml 添加报警信息 123# 添加：指定读取rules配置rules_files:- /etc/config/rules/*.rules configmap存储告警规则创建yaml文件同过configmap存储告警规则 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#vim prometheus-rules.yamlapiVersion: v1kind: ConfigMapmetadata: name: prometheus-rules namespace: monitoringdata: # 通用角色 general.rules: | groups: - name: general.rules rules: - alert: InstanceDown expr: up == 0 for: 1m labels: severity: error annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; 停止工作&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125; job &#123;&#123; $labels.job &#125;&#125; 已经停止5分钟以上.&quot; # Node对所有资源的监控 node.rules: | groups: - name: node.rules rules: - alert: NodeFilesystemUsage expr: 100 - (node_filesystem_free_bytes&#123;fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_size_bytes&#123;fstype=~&quot;ext4|xfs&quot;&#125; * 100) &gt; 80 for: 1m labels: severity: warning annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; : &#123;&#123; $labels.mountpoint &#125;&#125; 分区使用率过高&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125;: &#123;&#123; $labels.mountpoint &#125;&#125; 分区使用大于80% (当前值: &#123;&#123; $value &#125;&#125;)&quot; - alert: NodeMemoryUsage expr: 100 - (node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100 &gt; 80 for: 1m labels: severity: warning annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; 内存使用率过高&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125;内存使用大于80% (当前值: &#123;&#123; $value &#125;&#125;)&quot; - alert: NodeCPUUsage expr: 100 - (avg(irate(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) by (instance) * 100) &gt; 60 for: 1m labels: severity: warning annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; CPU使用率过高&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125;CPU使用大于60% (当前值: &#123;&#123; $value &#125;&#125;)&quot; configmap挂载到容器rules目录修改挂载点位置，使用之前部署的prometheus动态PV 123456789101112131415161718192021#vim prometheus-statefulset.yaml volumeMounts: - name: config-volume mountPath: /etc/config - name: prometheus-data mountPath: /data # 添加：指定rules的configmap配置文件名称 - name: prometheus-rules mountPath: /etc/config/rules subPath: &quot;&quot; terminationGracePeriodSeconds: 300 volumes: - name: config-volume configMap: name: prometheus-config # 添加：name rules - name: prometheus-rules # 添加：配置文件 configMap: # 添加：定义文件名称 name: prometheus-rules alertmananger报警配置说明12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#cat alertmanager-configmap.yamlapiVersion: v1kind: ConfigMapmetadata: # 配置文件名称 name: alertmanager-config namespace: kube-system labels: kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: EnsureExistsdata: alertmanager.yml: | global: resolve_timeout: 5m # 告警自定义邮件 smtp_smarthost: &apos;smtp.163.com:25&apos; smtp_from: &apos;w.jjwx@163.com&apos; smtp_auth_username: &apos;w.jjwx@163.com&apos; smtp_auth_password: &apos;密码&apos; smtp_hello: &apos;163.com&apos; smtp_require_tls: false route: # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面 group_by: [&apos;job&apos;,&apos;alertname&apos;,&apos;severity&apos;] # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。 group_wait: 30s group_interval: 5m # 当第一个报警发送后，等待&apos;group_interval&apos;时间来发送新的一组报警信息。 repeat_interval: 12h # 如果一个报警信息已经发送成功了，等待&apos;repeat_interval&apos;时间来重新发送他们 #group_interval: 5m #repeat_interval: 12h receiver: default #默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器 routes: - receiver: webhook match: alertname: NodeMemoryUsage #匹配这个内存报警 receivers: - name: &apos;default&apos; email_configs: - to: &apos;314144952@qq.com&apos; send_resolved: true - name: &apos;webhook&apos; webhook_configs: - url: &apos;http://dingtalk-hook:5000&apos; send_resolved: true 部分参数说明： 12345678910111213141516route: # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面 group_by: [&apos;alertname&apos;, &apos;cluster&apos;] # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。 group_wait: 30s # 当第一个报警发送后，等待&apos;group_interval&apos;时间来发送新的一组报警信息。 group_interval: 5m # 如果一个报警信息已经发送成功了，等待&apos;repeat_interval&apos;时间来重新发送他们 repeat_interval: 5m # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器 receiver: default # 上面所有的属性都由所有子路由继承，并且可以在每个子路由上进行覆盖。 配置告警信息模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354apiVersion: v1kind: ConfigMapmetadata: name: alertmanager-tmpl namespace: monitoringdata: wechat.tmpl: | &#123;&#123; define &quot;wechat.default.message&quot; &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @警报 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @恢复 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; 恢复: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- end &#125;&#125; email.tmpl: | &#123;&#123; define &quot;email.default.html&quot; &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @警报 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @恢复 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; 恢复: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123; end &#125;&#125;]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>alertmanager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群监控之全手动部署]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B9%8B%E5%85%A8%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[参考 https://blog.csdn.net/liukuan73/article/details/78881008 https://github.com/liukuan73/kubernetes-addons 总体目标从监控平台本身的业务需求来看，至少应该通过平台获取到以下的监控数据： 性能指标(如：CPU、Memory、Load、磁盘、网络等） 容器、Pod相关的性能指标数据 主机节点相关的性能指标数据 容器内进程自己主动暴露的指标数据 k8s上应用的网络性能，如http、tcp等数据 状态指标 k8s资源对象（Deployment、Daemonset、Pod等）的运行状态指标 k8s平台组件（如kube-apiserver、kube-scheduler等）的运行状态指标 获取监控数据之后，还需要对监控进行可视化展示，以及对监控中出现的异常情况进行告警。 主流方案目前对于kubernetes的主流监控方案主要有以下几种： Heapster+InfluxDB+Grafana 每个K8S节点的Kubelet内含cAdvisor，暴露出API，Heapster通过访问这些端点得到容器监控数据。它支持多种储存方式，常用的是InfluxDB。这套方案的缺点是数据来源单一、缺乏报警功能以及InfluxDB的单点问题，而且Heapster也已经在新版本中被deprecated(被metrics server取代)了。这种实现方案的详细介绍请见这篇文章。 Metrics-Server+InfluxDB+Grafana k8s从1.8版本开始，CPU、内存等资源的metrics信息可以通过 Metrics API来获取，用户还可以通过kubectl top直接获取这些metrics信息。Metrics API需要部署Metrics-Server。 各种Exporter+Prometheus+Grafana 通过各种export采集不同维度的监控指标，并通过Prometheus支持的数据格式暴露出来，Prometheus定期pull数据并用Grafana展示，异常情况使用AlertManager告警。本方案下文详细叙述。 架构总体实现思路如下： 采集 通过cadvisor采集容器、Pod相关的性能指标数据，并通过暴露的/metrics接口用prometheus抓取 通过prometheus-node-exporter采集主机的性能指标数据，并通过暴露的/metrics接口用prometheus抓取 应用侧自己采集容器中进程主动暴露的指标数据（暴露指标的功能由应用自己实现，并添加平台侧约定的annotation，平台侧负责根据annotation实现通过Prometheus的抓取） 通过blackbox-exporter采集应用的网络性能(http、tcp、icmp等)数据，并通过暴露的/metrics接口用prometheus抓取 通过kube-state-metrics采集k8s资源对象的状态指标数据，并通过暴露的/metrics接口用prometheus抓取 通过etcd、kubelet、kube-apiserver、kube-controller-manager、kube-scheduler自身暴露的/metrics获取节点上与k8s集群相关的一些特征指标数据。 存储（汇聚），通过prometheus pull并汇聚各种exporter的监控数据 展示，通过grafana展示监控信息 告警，通过alertmanager进行告警 监控指标采集实现容器、Pod相关的性能指标数据—cAdvisorcAdvisor是谷歌开源的一个容器监控工具，cadvisor采集了主机上容器相关的性能指标数据，通过容器的指标还可进一步计算出pod的指标。 cadvisor提供的一些主要指标有： 1234567container_cpu_* container_fs_* container_memory_* container_network_* container_spec_*(cpu/memory) container_start_time_* container_tasks_state_* cadvisor接口 目前cAdvisor集成到了kubelet组件内，可以在kubernetes集群中每个启动了kubelet的节点使用cAdvisor提供的metrics接口获取该节点所有容器相关的性能指标数据。1.7.3版本以前，cadvisor的metrics数据集成在kubelet的metrics中，在1.7.3以后版本中cadvisor的metrics被从kubelet的metrics独立出来了，在prometheus采集的时候变成两个scrape的job。 cAdvisor对外提供服务的默认端口为4194，主要提供两种接口: Prometheus格式指标接口：nodeIP:4194/metrics(或者通过kubelet暴露的cadvisor接口nodeIP:10255/metrics/cadvisor); WebUI界面接口：nodeIP:4194/containers/ Prometheus作为一个时间序列数据收集，处理，存储的服务，能够监控的对象必须通过http api暴露出基于Prometheus认可的数据模型的监控数据，cAdvisor接口（nodeIP:4194/metrics）暴露的监控指标数据如下所示： 123456789101112# HELP cadvisor_version_info A metric with a constant &apos;1&apos; value labeled by kernel version, OS version, docker version, cadvisor version &amp; cadvisor revision.# TYPE cadvisor_version_info gaugecadvisor_version_info&#123;cadvisorRevision=&quot;&quot;,cadvisorVersion=&quot;&quot;,dockerVersion=&quot;1.12.6&quot;,kernelVersion=&quot;4.9.0-1.2.el7.bclinux.x86_64&quot;,osVersion=&quot;CentOS Linux 7 (Core)&quot;&#125; 1# HELP container_cpu_cfs_periods_total Number of elapsed enforcement period intervals.# TYPE container_cpu_cfs_periods_total countercontainer_cpu_cfs_periods_total&#123;container_name=&quot;&quot;,id=&quot;/kubepods/burstable/pod1b0c1f83322defae700f33b1b8b7f572&quot;,image=&quot;&quot;,name=&quot;&quot;,namespace=&quot;&quot;,pod_name=&quot;&quot;&#125; 7.062239e+06container_cpu_cfs_periods_total&#123;container_name=&quot;&quot;,id=&quot;/kubepods/burstable/pod7f86ba308f28df9915b802bc48cfee3a&quot;,image=&quot;&quot;,name=&quot;&quot;,namespace=&quot;&quot;,pod_name=&quot;&quot;&#125; 1.574206e+06container_cpu_cfs_periods_total&#123;container_name=&quot;&quot;,id=&quot;/kubepods/burstable/podb0c8f695146fe62856bc23709a3e056b&quot;,image=&quot;&quot;,name=&quot;&quot;,namespace=&quot;&quot;,pod_name=&quot;&quot;&#125; 7.107043e+06container_cpu_cfs_periods_total&#123;container_name=&quot;&quot;,id=&quot;/kubepods/burstable/podc8cf73836b3caba7bf952ce1ac5a5934&quot;,image=&quot;&quot;,name=&quot;&quot;,namespace=&quot;&quot;,pod_name=&quot;&quot;&#125; 5.932159e+06container_cpu_cfs_periods_total&#123;container_name=&quot;&quot;,id=&quot;/kubepods/burstable/podfaa9db59-64b7-11e8-8792-00505694eb6a&quot;,image=&quot;&quot;,name=&quot;&quot;,namespace=&quot;&quot;,pod_name=&quot;&quot;&#125; 6.979547e+06container_cpu_cfs_periods_total&#123;container_name=&quot;calico-node&quot;,id=&quot;/kubepods/burstable/podfaa9db59-64b7-11e8-8792-... Prometheus配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445- job_name: &apos;cadvisor&apos; # 通过https访问apiserver，通过apiserver的api获取数据 scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token #以k8s的角色(role)来定义收集，比如node,service,pod,endpoints,ingress等等 kubernetes_sd_configs: # 从k8s的node对象获取数据 - role: node relabel_configs: # 用新的前缀代替原label name前缀，没有replacement的话功能就是去掉label name前缀 # 例如：以下两句的功能就是将__meta_kubernetes_node_label_kubernetes_io_hostname # 变为kubernetes_io_hostname - action: labelmap regex: __meta_kubernetes_node_label_(.+) # replacement中的值将会覆盖target_label中指定的label name的值, # 即__address__的值会被替换为kubernetes.default.svc:443 - target_label: __address__ replacement: kubernetes.default.svc:443 # 获取__meta_kubernetes_node_name的值 - source_labels: [__meta_kubernetes_node_name] #匹配一个或多个任意字符，将上述source_labels的值生成变量 regex: (.+) # replacement中的值将会覆盖target_label中指定的label name的值, # 即__metrics_path__的值会被替换为/api/v1/nodes/$&#123;1&#125;/proxy/metrics, # 其中$&#123;1&#125;的值会被替换为__meta_kubernetes_node_name的值 target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor metric_relabel_configs: - action: replace source_labels: [id] regex: &apos;^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$&apos; target_label: rkt_container_name replacement: &apos;$&#123;2&#125;-$&#123;1&#125;&apos; - action: replace source_labels: [id] regex: &apos;^/system\.slice/(.+)\.service$&apos; target_label: systemd_service_name replacement: &apos;$&#123;1&#125;&apos; 之后，在prometheus的target(IP:Port/targets)中可以看到cadvisor相应的target PS: 关于配置文件的一些说明： 以上配置遵照官方example配置，通过apiserver提供的api做代理获取cAdvisor（ https://kubernetes.default.svc:443/api/v1/nodes/k8smaster01/proxy/metrics/cadvisor ）的监控指标(和从nodeIP:4194/metrics获取到的内容是一样的)，而不是直接从node上获取。为什么这样做，官方这样解释的：This means it will work if Prometheus is running out of cluster, or can’t connect to nodes for some other reason (e.g. because of firewalling)。 Promethues在K8S集群内通过DNS地址 https://kubernetes.default.svc访问apiserver来scrape数据 Prometheus配置文件的语法规则较复杂，为便于理解，我加了一些注释；更多语法规则请见Prometheus官方文档。 关于target中label的一些说明，target中必有的几个source label有： __address__(当static_configs时通过targets手工配置，当kubernetes_sd_configs时，值从apiserver中获取)、 __metrics_path__(默认值是/metrics)、 __scheme__(默认值是http) job 其他source label则是根据kubernetes_sd_configs时设置的- role(如endpoints、nodes、service、pod等)从k8s资源对象的label、annotation及其他一些信息中提取的 主机节点性能指标数据—node-exporterPrometheus社区提供的NodeExporter项目可以对主机的关键度量指标进行监控，通过Kubernetes的DeamonSet可以在各个主机节点上部署有且仅有一个NodeExporter实例，实现对主机性能指标数据的监控。node-exporter所采集的指标主要有: 123456789101112131415161718node_cpu_* node_disk_* node_entropy_* node_filefd_*node_filesystem_* node_forks_* node_intr_total_*node_ipvs_* node_load_* node_memory_* node_netstat_* node_network_* node_nf_conntrack_* node_scrape_* node_sockstat_* node_time_seconds_*node_timex _*node_xfs_* Prometheus配置 node-exporter-daemonset.yaml, node-exporter-svc.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: prometheus-node-exporter namespace: monitoring labels: k8s-app: prometheus-node-exporterspec: template: metadata: name: prometheus-node-exporter labels: k8s-app: prometheus-node-exporter spec: containers: - image: prom/node-exporter:v0.18.0 imagePullPolicy: IfNotPresent name: prometheus-node-exporter ports: - name: prom-node-exp containerPort: 9100 hostPort: 9100 livenessProbe: failureThreshold: 3 httpGet: path: / port: 9100 scheme: HTTP readinessProbe: failureThreshold: 3 httpGet: path: / port: 9100 scheme: HTTP resources: limits: cpu: 20m memory: 2Gi requests: cpu: 10m memory: 1Gi dnsPolicy: ClusterFirst hostNetwork: true hostPID: true ---apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: &apos;true&apos; prometheus.io/app-metrics: &apos;true&apos; prometheus.io/app-metrics-path: &apos;/metrics&apos; name: prometheus-node-exporter namespace: monitoring labels: k8s-app: prometheus-node-exporterspec: ports: - name: prometheus-node-exporter port: 9100 protocol: TCP selector: k8s-app: prometheus-node-exporter type: ClusterIP PS： 1.为了让容器里的node-exporter获取到主机上的网络、PID、IPC指标，这里设置了hostNetwork: true、hostPID: true、hostIPC: true，来与主机共用网络、PID、IPC这三个namespace。 2.此处在Service的annotations中定义标注prometheus.io/scrape: ‘true’，表明该Service需要被Promethues发现并采集数据。 通过NodeExporter暴露的metrics接口(nodeIP:9100/metrics)查看采集到的数据，可以看到是按Prometheus的格式输出的数据： 1234567891011121314151617181920212223# HELP node_arp_entries ARP entries by device# TYPE node_arp_entries gaugenode_arp_entries&#123;device=&quot;calid63983a5754&quot;&#125; 1node_arp_entries&#123;device=&quot;calid67ce395c9e&quot;&#125; 1node_arp_entries&#123;device=&quot;calid857f2bf9d5&quot;&#125; 1node_arp_entries&#123;device=&quot;calief3a4b64165&quot;&#125; 1node_arp_entries&#123;device=&quot;eno16777984&quot;&#125; 9# HELP node_boot_time Node boot time, in unixtime.# TYPE node_boot_time gaugenode_boot_time 1.527752719e+09# HELP node_context_switches Total number of context switches.# TYPE node_context_switches counternode_context_switches 3.1425612674e+10# HELP node_cpu Seconds the cpus spent in each mode.# TYPE node_cpu counternode_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;guest&quot;&#125; 0node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;guest_nice&quot;&#125; 0node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125; 2.38051096e+06node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;iowait&quot;&#125; 11904.19node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;irq&quot;&#125; 0node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;nice&quot;&#125; 2990.94node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;softirq&quot;&#125; 8038.3... Prometheus配置 12345678910111213141516171819202122232425262728293031323334353637383940414243- job_name: &apos;prometheus-node-exporter&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: #The endpoints role discovers targets from listed endpoints of a service. For each #endpoint address one target is discovered per port. If the endpoint is backed by #a pod, all additional container ports of the pod, not bound to an endpoint port, #are discovered as targets as well - role: endpoints relabel_configs: # 只保留endpoints的annotations中含有prometheus.io/scrape: &apos;true&apos;和port的name为prometheus-node-exporter的endpoint - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_endpoint_port_name] regex: true;prometheus-node-exporter action: keep # Match regex against the concatenated source_labels. Then, set target_label to replacement, # with match group references ($&#123;1&#125;, $&#123;2&#125;, ...) in replacement substituted by their value. # If regex does not match, no replacement takes place. - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+)(?::\d+);(\d+) replacement: $1:$2 # 去掉label name中的前缀__meta_kubernetes_service_label_ - action: labelmap regex: __meta_kubernetes_service_label_(.+) # 将__meta_kubernetes_namespace重命名为kubernetes_namespace - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace # 将__meta_kubernetes_service_name重命名为kubernetes_name - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name 采集应用实例中某个进程自己暴露的指标数据有的应用具有暴露容器内具体进程性能指标的需求，这些指标由应用侧实现采集并暴露，平台侧做汇聚。 如何标识哪些是主动暴露监控指标的应用并获取指标平台侧可以约定好带哪些annotation前缀的服务是自主暴露监控指标的服务。应用添加平台侧约定的这些annotations，平台侧可以根据这些annotations实现Prometheus的scrape。 例如，应用侧为自己的服务添加如下平台侧约定约定的annotation： 1234prometheus.io/scrape: &apos;true&apos;prometheus.io/app-metrics: &apos;true&apos;prometheus.io/app-metrics-port: &apos;8080&apos;prometheus.io/app-metrics-path: &apos;/metrics&apos; Prometheus可以： 根据prometheus.io/scrape: ‘true’获知对应的endpoint是需要被scrape的 根据prometheus.io/app-metrics: ‘true’获知对应的endpoint中有应用进程暴露的metrics 根据prometheus.io/app-metrics-port: ‘8080’获知进程暴露的metrics的端口号 根据prometheus.io/app-metrics-path: ‘/metrics’获知进程暴露的metrics的具体路径 如何给应用加一些标志信息，并带到Prometheus侧可能还需要根据平台和业务的需求添加其他一些以prometheus.io/app-info-为前缀的annotation，Prometheus截取下前缀，保留后半部分做key，连同value保留下来。这样满足在平台对应用做其他一些标识的需求。比如加入如下annotation来标识应用所属的的环境、租户以及应用名称 123prometheus.io/app-info-env: &apos;test&apos;prometheus.io/app-info-tenant: &apos;test-tenant&apos;prometheus.io/app-info-name: &apos;test-app&apos; Prometheus配置 1234567891011121314151617181920212223242526272829- job_name: &apos;kubernetes-app-metrics&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: #The endpoints role discovers targets from listed endpoints of a service. For each #endpoint address one target is discovered per port. If the endpoint is backed by #a pod, all additional container ports of the pod, not bound to an endpoint port, #are discovered as targets as well - role: endpoints relabel_configs: # 只保留endpoint中含有prometheus.io/scrape: &apos;true&apos;的annotation的endpoint - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_app_metrics] regex: true;true action: keep # 将用户指定的进程的metrics_path替换默认的metrics_path - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_app_metrics_path] action: replace target_label: __metrics_path__ regex: (.+) # 用pod_ip和用户指定的进程的metrics端口组合成真正的可以拿到数据的地址来替换原始__address__ - source_labels: [__meta_kubernetes_pod_ip, __meta_kubernetes_service_annotation_prometheus_io_app_metrics_port] action: replace target_label: __address__ regex: (.+);(.+) replacement: $1:$2 # 去掉label name中的前缀__meta_kubernetes_service_annotation_prometheus_io_app_info_ - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_app_info_(.+) PS: 最后两行的作用是将例如prometheus.io/app-info-tenant的annotation名切割成名为tenant的label。 通过blackbox-exporter采集应用的网络性能数据blackbox-exporter是一个黑盒探测工具，可以对服务的http、tcp、icmp等进行网络探测。 blackbox-exporter部署blackbox-exporter-deploy.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prometheus-blackbox-exporter namespace: monitoring labels: k8s-app: prometheus-blackbox-exporterspec: selector: matchLabels: k8s-app: prometheus-blackbox-exporter replicas: 1 template: metadata: labels: k8s-app: prometheus-blackbox-exporter spec: restartPolicy: Always containers: - name: prometheus-blackbox-exporter image: prom/blackbox-exporter:v0.16.0 imagePullPolicy: IfNotPresent ports: - name: blackbox-port containerPort: 9115 readinessProbe: tcpSocket: port: 9115 initialDelaySeconds: 5 timeoutSeconds: 5 resources: requests: memory: 50Mi cpu: 50m limits: memory: 60Mi cpu: 100m volumeMounts: - name: config mountPath: /etc/blackbox_exporter args: - --config.file=/etc/blackbox_exporter/blackbox.yml - --log.level=debug - --web.listen-address=:9115 volumes: - name: config configMap: name: prometheus-blackbox-exporter black-exporter-config.yml 123456789101112131415161718192021222324252627282930313233apiVersion: v1kind: ConfigMapmetadata: labels: k8s-app: prometheus-blackbox-exporter name: prometheus-blackbox-exporter namespace: monitoringdata: blackbox.yml: |- modules: http_2xx: prober: http timeout: 10s http: valid_http_versions: [&quot;HTTP/1.1&quot;, &quot;HTTP/2&quot;] valid_status_codes: [] method: GET preferred_ip_protocol: &quot;ip4&quot; http_post_2xx: # http post 监测模块 prober: http timeout: 10s http: valid_http_versions: [&quot;HTTP/1.1&quot;, &quot;HTTP/2&quot;] method: POST preferred_ip_protocol: &quot;ip4&quot; tcp_connect: prober: tcp timeout: 10s icmp: prober: icmp timeout: 10s icmp: preferred_ip_protocol: &quot;ip4&quot; black-exporter-svc.yml 12345678910111213141516171819---apiVersion: v1kind: Servicemetadata: labels: k8s-app: prometheus-blackbox-exporter name: prometheus-blackbox-exporter namespace: monitoring annotations: prometheus.io/scrape: &apos;true&apos;spec: type: ClusterIP selector: k8s-app: prometheus-blackbox-exporter ports: - name: blackbox port: 9115 targetPort: 9115 protocol: TCP PS： blackbox-exporter的配置文件为/etc/blackbox_exporter/blackbox.yml，可以运行时动态的重新加载配置文件，当重新加载配置文件失败时，不影响在运行的配置。重载方式：curl -XPOST http://IP:9115/-/reload Prometheus配置 在Prometheus的config文件中分别配置对http和tcp的探测： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879- job_name: &apos;kubernetes-service-http-probe&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: service # 将metrics_path由默认的/metrics改为/probe metrics_path: /probe # Optional HTTP URL parameters. # 生成__param_module=&quot;http_2xx&quot;的label params: module: [http_2xx] relabel_configs: # 只保留含有label为prometheus/io=scrape的service - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_http_probe] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace, __meta_kubernetes_service_annotation_prometheus_io_http_probe_port, __meta_kubernetes_service_annotation_prometheus_io_http_probe_path] action: replace target_label: __param_target regex: (.+);(.+);(.+);(.+) replacement: $1.$2:$3$4 # 用__address__这个label的值创建一个名为__param_target的label为blackbox-exporter,值为内部service的访问地址，作为blackbox-exporter采集用 #- source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_http_probe_path] # action: replace # target_label: __param_target # regex: (.+);(.+) # replacement: $1$2 # 用blackbox-exporter的service地址值”prometheus-blackbox-exporter:9115&quot;替换原__address__的值 - target_label: __address__ replacement: prometheus-blackbox-exporter:9115 - source_labels: [__param_target] target_label: instance # 去掉label name中的前缀__meta_kubernetes_service_annotation_prometheus_io_app_info_ - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_app_info_(.+) #- source_labels: [__meta_kubernetes_namespace] # target_label: kubernetes_namespace #- source_labels: [__meta_kubernetes_service_name] # target_label: kubernetes_name## kubernetes-services and kubernetes-ingresses are blackbox_exporter related# Example scrape config for probing services via the Blackbox Exporter.# # The relabeling allows the actual service scrape endpoint to be configured# for all or only some services.- job_name: &apos;kubernetes-service-tcp-probe&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: service # 将metrics_path由默认的/metrics改为/probe metrics_path: /probe # Optional HTTP URL parameters. # 生成__param_module=&quot;tcp_connect&quot;的label params: module: [tcp_connect] relabel_configs: # 只保留含有label为prometheus/io=scrape的service - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_tcp_probe] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace, __meta_kubernetes_service_annotation_prometheus_io_tcp_probe_port] action: replace target_label: __param_target regex: (.+);(.+);(.+) replacement: $1.$2:$3 # 用__address__这个label的值创建一个名为__param_target的label为blackbox-exporter,值为内部service的访问地址，作为blackbox-exporter采集用 #- source_labels: [__address__] # target_label: __param_target # 用blackbox-exporter的service地址值”prometheus-blackbox-exporter:9115&quot;替换原__address__的值 - target_label: __address__ replacement: prometheus-blackbox-exporter:9115 - source_labels: [__param_target] target_label: instance # 去掉label name中的前缀__meta_kubernetes_service_annotation_prometheus_io_app_info_ - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_app_info_(.+) 应用侧配置 应用可以在service中指定平台侧约定的annotation，实现监控平台对该应用的网络服务进行探测： http探测 1234prometheus.io/scrape: &apos;true&apos;prometheus.io/http-probe: &apos;true&apos;prometheus.io/http-probe-port: &apos;8080&apos;prometheus.io/http-probe-path: &apos;/healthz&apos; tcp探测 123prometheus.io/scrape: &apos;true&apos;prometheus.io/tcp-probe: &apos;true&apos;prometheus.io/tcp-probe-port: &apos;80&apos; Prometheus根据这些annotation可以获知相应service是需要被探测的，探测的具体网络协议是http还是tcp或其他，以及具体的探测端口。http探测的话还要知道探测的具体url。 资源对象(Deployment、Pod等)的状态—kube-state-metricskube-state-metrics采集了k8s中各种资源对象的状态信息： 1234567891011kube_daemonset_*（创建时间、所处的阶段、期望跑在几台节点上、应当正在运行的节点数量、不应该跑却跑了daemon pod的节点数、跑好了pod(ready)的节点数） kube_deployment_*（创建时间、是否将k8s的label转化为prometheus的label、所处的阶段、是不是以处于paused状态并不再被dp controller处理、期望副本数、rolling update时最多不可用副本数、dp controller观察到的阶段、实际副本数、availabel副本数、unavailabel副本数、updated副本数)kube_job_*（是否执行完成、创建时间戳...)kube_namespace_*kube_node_*kube_persistentvolumeclaim_*kube_pod_container_*kube_pod_*kube_replicaset_*kube_service_*kube_statefulset_* 部署kube-state-metrics-deploy.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: apps/v1kind: Deploymentmetadata: name: kube-state-metrics namespace: monitoringspec: selector: matchLabels: k8s-app: kube-state-metrics replicas: 1 template: metadata: labels: k8s-app: kube-state-metrics spec: serviceAccountName: prometheus containers: - name: kube-state-metrics image: bitnami/kube-state-metrics:1.8.0 ports: - protocol: TCP containerPort: 8080 readinessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 5 timeoutSeconds: 5 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 resources: limits: cpu: 50m memory: 1Gi requests: cpu: 20m memory: 512Mi kube-state-metrics-svc.yml 1234567891011121314151617apiVersion: v1kind: Servicemetadata: name: kube-state-metrics namespace: monitoring labels: k8s-app: kube-state-metrics annotations: prometheus.io/scrape: &apos;true&apos;spec: ports: - name: kube-state-metrics port: 8080 targetPort: 8080 protocol: TCP selector: k8s-app: kube-state-metrics Prometheus配置 1234567891011121314151617181920212223242526272829303132333435363738394041- job_name: &apos;kube-state-metrics&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: #The endpoints role discovers targets from listed endpoints of a service. For each #endpoint address one target is discovered per port. If the endpoint is backed by #a pod, all additional container ports of the pod, not bound to an endpoint port, #are discovered as targets as well - role: endpoints relabel_configs: # 只保留endpoint中的annotations含有prometheus.io/scrape: &apos;true&apos;和port的name为prometheus-node-exporter的endpoint - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape,__meta_kubernetes_endpoint_port_name] regex: true;kube-state-metrics action: keep - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+)(?::\d+);(\d+) replacement: $1:$2 # 去掉label name中的前缀__meta_kubernetes_service_label_ - action: labelmap regex: __meta_kubernetes_service_label_(.+) # 将__meta_kubernetes_namespace重命名为kubernetes_namespace - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace # 将__meta_kubernetes_service_name重命名为kubernetes_name - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name k8s集群组件的状态指标采集etcd、kube-controller-manager、kube-scheduler、kube-proxy、kube-apiserver、kubelet这几个k8d平台组件分别向外暴露了prometheus标准的指标接口/metrics。可通过配置prometheus来进行读取。 etcd指标获取以kubeadm启动的k8s集群中，etcd是以static pod的形式启动的，默认没有service及对应的endpoint可供集群内的prometheus访问。所以首先创建一个用来为prometheus提供接口的service（endpoint），etcd-svc.yaml文件如下： 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: namespace: kube-system name: etcd-prometheus-discovery labels: component: etcd annotations: prometheus.io/scrape: &apos;true&apos;spec: selector: component: etcd type: ClusterIP clusterIP: None ports: - name: http-metrics port: 2379 targetPort: 2379 protocol: TCP prometheus配置抓取的文件加入如下配置： 1234567891011121314151617181920- job_name: &apos;etcd&apos; # 通过https访问apiserver tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token #以k8s的角色(role)来定义收集，比如node,service,pod,endpoints,ingress等等 kubernetes_sd_configs: # 从endpoints获取apiserver数据 - role: endpoints #relabel_configs允许在抓取之前对任何目标及其标签进行修改。 relabel_configs: # 选择哪些label - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_namespace, __meta_kubernetes_service_name] # 上述选择的label的值需要与下述对应 regex: true;kube-system;etcd-prometheus-discovery # 含有符合regex的source_label的endpoints进行保留 action: keep kube-proxy指标获取kube-proxy通过10249端口暴露/metrics指标。与3.6.1同理，kube-proxy-svc.yaml如下： 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: namespace: kube-system name: kube-proxy-prometheus-discovery labels: k8s-app: kube-proxy annotations: prometheus.io/scrape: &apos;true&apos;spec: selector: k8s-app: kube-proxy type: ClusterIP clusterIP: None ports: - name: http-metrics port: 10249 targetPort: 10249 protocol: TCP prometheus配置抓取的文件加入如下配置： 1234567891011121314151617181920- job_name: &apos;kube-proxy&apos;# 通过https访问apiservertls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crtbearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token#以k8s的角色(role)来定义收集，比如node,service,pod,endpoints,ingress等等kubernetes_sd_configs:# 从endpoints获取apiserver数据- role: endpoints#relabel_configs允许在抓取之前对任何目标及其标签进行修改。relabel_configs:# 选择哪些label- source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_namespace, __meta_kubernetes_service_name] # 上述选择的label的值需要与下述对应 regex: true;kube-system;kube-proxy-prometheus-discovery # 含有符合regex的source_label的endpoints进行保留 action: keep kube-scheduler指标获取kube-scheduler通过10251端口暴露/metrics指标。与3.6.1同理，kube-scheduler-svc.yaml如下： 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: namespace: kube-system name: kube-scheduler-prometheus-discovery labels: k8s-app: kube-scheduler annotations: prometheus.io/scrape: &apos;true&apos;spec: selector: component: kube-scheduler type: ClusterIP clusterIP: None ports: - name: http-metrics port: 10251 targetPort: 10251 protocol: TCP prometheus对应的配置如下： 1234567891011121314151617181920- job_name: &apos;kube-scheduler&apos; # 通过https访问apiserver tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token #以k8s的角色(role)来定义收集，比如node,service,pod,endpoints,ingress等等 kubernetes_sd_configs: # 从endpoints获取apiserver数据 - role: endpoints #relabel_configs允许在抓取之前对任何目标及其标签进行修改。 relabel_configs: # 选择哪些label - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_namespace, __meta_kubernetes_service_name] # 上述选择的label的值需要与下述对应 regex: true;kube-system;kube-scheduler-prometheus-discovery # 含有符合regex的source_label的endpoints进行保留 action: keep kube-controller-manager指标获取kube-controller-manager通过10252端口暴露/metrics指标。与etcd同理，kube-controller-manager-svc.yaml如下： 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: namespace: kube-system name: kube-controller-manager-prometheus-discovery labels: k8s-app: kube-controller-manager annotations: prometheus.io/scrape: &apos;true&apos;spec: selector: component: kube-controller-manager type: ClusterIP clusterIP: None ports: - name: http-metrics port: 10252 targetPort: 10252 protocol: TCP prometheus配置抓取的文件加入如下配置： 1234567891011121314151617181920- job_name: &apos;kube-controller-manager&apos; # 通过https访问apiserver tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token #以k8s的角色(role)来定义收集，比如node,service,pod,endpoints,ingress等等 kubernetes_sd_configs: # 从endpoints获取apiserver数据 - role: endpoints #relabel_configs允许在抓取之前对任何目标及其标签进行修改。 relabel_configs: # 选择哪些label - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_namespace, __meta_kubernetes_service_name] # 上述选择的label的值需要与下述对应 regex: true;kube-system;kube-controller-manager-prometheus-discovery # 含有符合regex的source_label的endpoints进行保留 action: keep 以上四步配置好后可以在promethues的web UI中看到对应的targets kube-apiserver数据获取kube-apiserver与上面四个组件不同的是，部署好后集群中默认会有一个名为kubernetes的service和对应的名为kubernetes的endpoint，这个endpoint就是集群内的kube-apiserver的访问入口。可以如下配置prometheus抓取数据： 123456789101112131415161718192021- job_name: &apos;kube-apiservers&apos; # 通过https访问apiserver scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token #以k8s的角色(role)来定义收集，比如node,service,pod,endpoints,ingress等等 kubernetes_sd_configs: # 从endpoints获取apiserver数据 - role: endpoints #relabel_configs允许在抓取之前对任何目标及其标签进行修改。 relabel_configs: # 选择哪些label - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] # 上述选择的label的值需要与下述对应 regex: default;kubernetes;https # 含有符合regex的source_label的endpoints进行保留 action: keep kubelet数据获取kubelet暴露的metrics端口默认为 10255： 提供的prometheus格式指标接口：nodeIP:10255/metrics，使用Prometheus从这里取数据 kubelet提供的stats/summary接口：nodeIP:10255/stats/summary，heapster和最新的metrics-server从这里获取数据 kubelet采集的指标主要有： 12345678910111213141516171819202122232425262728apiserver_client_certificate_expiration_seconds_bucketapiserver_client_certificate_expiration_seconds_sumapiserver_client_certificate_expiration_seconds_countetcd_helper_cache_entry_countetcd_helper_cache_hit_countetcd_helper_cache_miss_countetcd_request_cache_add_latencies_summaryetcd_request_cache_add_latencies_summary_sumetcd_request_cache_add_latencies_summary_countetcd_request_cache_get_latencies_summaryetcd_request_cache_get_latencies_summary_sumetcd_request_cache_get_latencies_summary_countkubelet_cgroup_manager_latency_microsecondskubelet_containers_per_pod_countkubelet_docker_operationskubelet_network_plugin_operations_latency_microsecondskubelet_pleg_relist_interval_microsecondskubelet_pleg_relist_latency_microsecondskubelet_pod_start_latency_microsecondskubelet_pod_worker_latency_microsecondskubelet_running_container_countkubelet_running_pod_countkubelet_runtime_operations*kubernetes_build_infoprocess_cpu_seconds_totalreflector*rest_client_request_*storage_operation_duration_seconds_* 查看kubelet监控指标数据（nodeIP:10255/metrics）： 1234567891011121314151617181920# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.# TYPE apiserver_audit_event_total counterapiserver_audit_event_total 0# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.# TYPE apiserver_client_certificate_expiration_seconds histogramapiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;21600&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;43200&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;86400&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;172800&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;345600&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;604800&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;2.592e+06&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;7.776e+06&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;1.5552e+07&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;3.1104e+07&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;+Inf&quot;&#125; 4161apiserver_client_certificate_expiration_seconds_sum 1.3091542942737878e+12apiserver_client_certificate_expiration_seconds_count 4161... kubelet由于在每个节点上都有且仅有一个，所以可以通过k8s的node对象找到kubelet的指标，prometheus配置如下： 123456789101112131415161718192021222324252627282930313233343536373839- job_name: &apos;kubelet&apos; # 通过https访问apiserver，通过apiserver的api获取数据 scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token #以k8s的角色(role)来定义收集，比如node,service,pod,endpoints,ingress等等 kubernetes_sd_configs: # 从k8s的node对象获取数据 - role: node relabel_configs: # 用新的前缀代替原label name前缀，没有replacement的话功能就是去掉label_name前缀 # 例如：以下两句的功能就是将__meta_kubernetes_node_label_kubernetes_io_hostname # 变为kubernetes_io_hostname - action: labelmap regex: __meta_kubernetes_node_label_(.+) # replacement中的值将会覆盖target_label中指定的label name的值, # 即__address__的值会被替换为kubernetes.default.svc:443 - target_label: __address__ replacement: kubernetes.default.svc:443 #replacement: 10.142.21.21:6443 # 获取__meta_kubernetes_node_name的值 - source_labels: [__meta_kubernetes_node_name] #匹配一个或多个任意字符，将上述source_labels的值生成变量 regex: (.+) # 将# replacement中的值将会覆盖target_label中指定的label name的值, # 即__metrics_path__的值会被替换为/api/v1/nodes/$&#123;1&#125;/proxy/metrics, # 其中$&#123;1&#125;的值会被替换为__meta_kubernetes_node_name的值 target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics #or: #- source_labels: [__address__] # regex: &apos;(.*):10250&apos; # replacement: &apos;$&#123;1&#125;:4194&apos; # target_label: __address__ #- source_labels: [__meta_kubernetes_node_label_role] # action: replace # target_label: role Prometheus部署prometheus-rbac.yml 123456789101112131415161718192021222324---apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: prometheus name: prometheus namespace: monitoring---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheus labels: k8s-app: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: prometheus namespace: monitoring prometheus-sts.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182apiVersion: apps/v1kind: StatefulSetmetadata: name: prometheus namespace: monitoring labels: k8s-app: prometheusspec: serviceName: prometheus replicas: 1 selector: matchLabels: k8s-app: prometheus template: metadata: labels: k8s-app: prometheus spec: serviceAccountName: prometheus containers: - name: prometheus image: &quot;prom/prometheus:v2.14.0&quot; imagePullPolicy: &quot;IfNotPresent&quot; command: - &quot;/bin/prometheus&quot; args: - --config.file=/etc/prometheus/config/prometheus.yml - --storage.tsdb.path=/prometheus - --storage.tsdb.retention=14d - --web.enable-lifecycle ports: - containerPort: 9090 protocol: TCP readinessProbe: httpGet: path: /-/ready port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 livenessProbe: httpGet: path: /-/healthy port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 # based on 10 running nodes with 30 pods each resources: limits: cpu: 200m memory: 8Gi requests: cpu: 100m memory: 4Gi volumeMounts: - name: prometheus-config mountPath: /etc/prometheus/config #mountPath: /etc/prometheus/prometheus.yml #subPath: prometheus.yml - name: prometheus-rules mountPath: /etc/prometheus/rules - name: prometheus-data mountPath: /prometheus volumes: - name: prometheus-config configMap: name: prometheus-config - name: prometheus-rules configMap: name: prometheus-rules volumeClaimTemplates: - metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-rw volume.beta.kubernetes.io/storage-provisioner: flexvolume-huawei.com/fuxinfs enable: true name: prometheus-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 200Gi 对args参数做几点说明: –storage.tsdb.path：tsdb数据库存储路径 –storage.tsdb.retention：数据保留多久，可以看官方文档存储部分 –config.file：指定prometheus的config文件的路径 –web.enable-lifecycle：加上这个参数后可以向/-/reload(curl -XPOST 10.142.232.150:30006/-/reload)发送HTTP POST请求实现prometheus在config文件修改后的动态reload，更多信息请查看官方文档 –web.enable-admin-api：加上这个参数可以为一些高级用户暴露操作数据库功能的API，比如快照备份(curl -XPOST http:///api/v2/admin/tsdb/snapshot)，更多信息请查看官方文档 TSDB Admin APIs部分 prometheus-svc.yml 12345678910111213kind: ServiceapiVersion: v1metadata: name: prometheus namespace: monitoringspec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30026 selector: k8s-app: prometheus prometheus-config.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: monitoringdata: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s scrape_timeout: 10s alerting: alertmanagers: - static_configs: - targets: - alertmanager:9093 rule_files: - &quot;/etc/prometheus/rules/*.rules&quot; scrape_configs: - job_name: &apos;Prometheus&apos; static_configs: - targets: [&apos;localhost:9090&apos;] #- job_name: &apos;federate&apos; #scrape_interval: 30s #scrape_timeout: 30s #honor_labels: true #metrics_path: &apos;/federate&apos; #params: #&apos;match[]&apos;: #- &apos;&#123;job=~&quot;.+&quot;&#125;&apos; ##- &apos;&#123;job=~&quot;kubernetes-.*&quot;&#125;&apos; #static_configs: #- targets: #- &apos;prometheus.prometheus:9090&apos; - job_name: &apos;Web&apos; scrape_interval: 60s metrics_path: /probe params: module: [http_2xx] static_configs: - targets: [&quot;https://www.baidu.com&quot;] labels: service: baidu relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: localhost:9115 - job_name: &apos;Ping&apos; scrape_interval: 30s metrics_path: /probe params: module: [icmp] static_configs: - targets: [&apos;192.168.0.11&apos;] labels: suites: &apos;test&apos; relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: localhost:9115 #- job_name: &apos;Mysql&apos; #scrape_interval: 5s #static_configs: #- targets: [&apos;localhost:9104&apos;] #labels: #instance: cnhwrds01 #- job_name: ECS #static_configs: #- targets: [&apos;192.168.0.11:9100&apos;] #labels: #instance: test #- job_name: Redis #static_configs: #- targets: [&apos;localhost:9121&apos;] #labels: #instance: test #- job_name: pushgateway #scrape_interval: 1m #static_configs: #- targets: [&apos;localhost:9091&apos;] #labels: #instance: pushgateway - job_name: &apos;kubernetes-apiservers&apos; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: &apos;kubernetes-nodes&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: &apos;kubernetes-service-endpoints&apos; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: &apos;kubernetes-services&apos; kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: &apos;kubernetes-ingresses&apos; kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125; target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: &apos;kubernetes-pods&apos; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: &apos;prometheus-node-exporter&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: #The endpoints role discovers targets from listed endpoints of a service. For each #endpoint address one target is discovered per port. If the endpoint is backed by #a pod, all additional container ports of the pod, not bound to an endpoint port, #are discovered as targets as well - role: endpoints relabel_configs: # 只保留endpoints的annotations中含有prometheus.io/scrape: &apos;true&apos;和port的name为prometheus-node-exporter的endpoint - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_endpoint_port_name] regex: true;prometheus-node-exporter action: keep # Match regex against the concatenated source_labels. Then, set target_label to replacement, # with match group references ($&#123;1&#125;, $&#123;2&#125;, ...) in replacement substituted by their value. # If regex does not match, no replacement takes place. - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+)(?::\d+);(\d+) replacement: $1:$2 # 去掉label name中的前缀__meta_kubernetes_service_label_ - action: labelmap regex: __meta_kubernetes_service_label_(.+) # 将__meta_kubernetes_namespace重命名为kubernetes_namespace - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace # 将__meta_kubernetes_service_name重命名为kubernetes_name - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: &apos;kubernetes-cadvisor&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: &apos;kubernetes-service-http-probe&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_http_probe] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace, __meta_kubernetes_service_annotation_prometheus_io_http_probe_port, __meta_kubernetes_service_annotation_prometheus_io_http_probe_path] action: replace target_label: __param_target regex: (.+);(.+);(.+);(.+) replacement: $1.$2:$3$4 # 用__address__这个label的值创建一个名为__param_target的label为blackbox-exporter,值为内部service的访问地址，作为blackbox-exporter采集用 #- source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_http_probe_path] # action: replace # target_label: __param_target # regex: (.+);(.+) # replacement: $1$2 # 用blackbox-exporter的service地址值”prometheus-blackbox-exporter:9115&quot;替换原__address__的值 - target_label: __address__ replacement: prometheus-blackbox-exporter:9115 - source_labels: [__param_target] target_label: instance # 去掉label name中的前缀__meta_kubernetes_service_annotation_prometheus_io_app_info_ - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_app_info_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: &apos;kubernetes-service-tcp-probe&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [tcp_connect] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_tcp_probe] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace, __meta_kubernetes_service_annotation_prometheus_io_tcp_probe_port] action: replace target_label: __param_target regex: (.+);(.+);(.+) replacement: $1.$2:$3 # 用__address__这个label的值创建一个名为__param_target的label为blackbox-exporter,值为内部service的访问地址，作为blackbox-exporter采集用 #- source_labels: [__address__] # target_label: __param_target # 用blackbox-exporter的service地址值”prometheus-blackbox-exporter:9115&quot;替换原__address__的值 - target_label: __address__ replacement: prometheus-blackbox-exporter:9115 - source_labels: [__param_target] target_label: instance # 去掉label name中的前缀__meta_kubernetes_service_annotation_prometheus_io_app_info_ - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_app_info_(.+) - job_name: &apos;kube-state-metrics&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: # 只保留endpoint中的annotations含有prometheus.io/scrape: &apos;true&apos;和port的name为prometheus-node-exporter的endpoint - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape,__meta_kubernetes_endpoint_port_name] regex: true;kube-state-metrics action: keep # 去掉label name中的前缀__meta_kubernetes_service_label_ - action: labelmap regex: __meta_kubernetes_service_label_(.+) # 将__meta_kubernetes_namespace重命名为kubernetes_namespace - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace # 将__meta_kubernetes_service_name重命名为kubernetes_name - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: &apos;kubelet&apos; # 通过https访问apiserver，通过apiserver的api获取数据 scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: # 从k8s的node对象获取数据 - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 # 获取__meta_kubernetes_node_name的值 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: &apos;coredns&apos; # 通过https访问apiserver #scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] regex: true;kube-system;coredns-prometheus-discovery;http-metrics action: keep prometheus-rules.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359apiVersion: v1kind: ConfigMapmetadata: name: prometheus-rules namespace: monitoringdata: alertmanager.rules: |+ groups: - name: alertmanager.rules rules: - alert: AlertmanagerReloadFailed expr: alertmanager_config_last_reload_successful == 0 for: 10m labels: severity: warning annotations: description: Reloading Alertmanager&apos;s configuration has failed for &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod&#125;&#125;. summary: Alertmanager configuration reload has failed general.rules: |+ groups: - name: general.rules rules: - alert: TargetDown expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) &gt; 10 for: 10m labels: severity: warning annotations: description: &apos;&#123;&#123; $value &#125;&#125;% or more of &#123;&#123; $labels.job &#125;&#125; targets are down.&apos; summary: Targets are down - alert: DeadMansSwitch expr: vector(1) labels: severity: none annotations: description: This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional. summary: Alerting DeadMansSwitch - alert: TooManyOpenFileDescriptors expr: 100 * (process_open_fds / process_max_fds) &gt; 95 for: 10m labels: severity: critical annotations: description: &apos;&#123;&#123; $labels.job &#125;&#125;: &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; (&#123;&#123; $labels.instance &#125;&#125;) is using &#123;&#123; $value &#125;&#125;% of the available file/socket descriptors.&apos; summary: too many open file descriptors - record: instance:fd_utilization expr: process_open_fds / process_max_fds - alert: FdExhaustionClose expr: predict_linear(instance:fd_utilization[1h], 3600 * 4) &gt; 1 for: 10m labels: severity: warning annotations: description: &apos;&#123;&#123; $labels.job &#125;&#125;: &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; (&#123;&#123; $labels.instance &#125;&#125;) instance will exhaust in file/socket descriptors soon&apos; summary: file descriptors soon exhausted - alert: FdExhaustionClose expr: predict_linear(instance:fd_utilization[10m], 3600) &gt; 1 for: 10m labels: severity: critical annotations: description: &apos;&#123;&#123; $labels.job &#125;&#125;: &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; (&#123;&#123; $labels.instance &#125;&#125;) instance will exhaust in file/socket descriptors soon&apos; summary: file descriptors soon exhausted job.rules: |+ groups: - name: job.rules rules: - alert: CronJobRunning expr: time() -kube_cronjob_next_schedule_time &gt; 3600 for: 1h labels: severity: warning annotations: description: CronJob &#123;&#123;$labels.namespaces&#125;&#125;/&#123;&#123;$labels.cronjob&#125;&#125; is taking more than 1h to complete summary: CronJob didn&apos;t finish after 1h - alert: JobCompletion expr: kube_job_spec_completions - kube_job_status_succeeded &gt; 0 for: 1h labels: severity: warning annotations: description: Job completion is taking more than 1h to complete cronjob &#123;&#123;$labels.namespaces&#125;&#125;/&#123;&#123;$labels.job&#125;&#125; summary: Job &#123;&#123;$labels.job&#125;&#125; didn&apos;t finish to complete after 1h - alert: JobFailed expr: kube_job_status_failed &gt; 0 for: 5m labels: severity: warning annotations: description: Job &#123;&#123;$labels.namespaces&#125;&#125;/&#123;&#123;$labels.job&#125;&#125; failed to complete summary: Job failed kube-apiserver.rules: |+ groups: - name: kube-apiserver.rules rules: - alert: K8SApiserverDown expr: absent(up&#123;job=&quot;kubernetes-apiservers&quot;&#125; == 1) for: 5m labels: severity: critical annotations: description: Prometheus failed to scrape API server(s), or all API servers have disappeared from service discovery. summary: API server unreachable - alert: K8SApiServerLatency expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket&#123;subresource!=&quot;log&quot;,verb!~&quot;CONNECT|WATCHLIST|WATCH|PROXY&quot;&#125;) WITHOUT (instance, resource)) / 1e+06 &gt; 1 for: 10m labels: severity: warning annotations: description: 99th percentile Latency for &#123;&#123; $labels.verb &#125;&#125; requests to the kube-apiserver is higher than 1s. summary: Kubernetes apiserver latency is high kube-state-metrics.rules: |+ groups: - name: kube-state-metrics.rules rules: - alert: DeploymentGenerationMismatch expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation for: 15m labels: severity: warning annotations: description: Observed deployment generation does not match expected one for deployment &#123;&#123;$labels.namespaces&#125;&#125;/&#123;&#123;$labels.deployment&#125;&#125; summary: Deployment is outdated - alert: DeploymentReplicasNotUpdated expr: ((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas) or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas)) unless (kube_deployment_spec_paused == 1) for: 15m labels: severity: warning annotations: description: Replicas are not updated and available for deployment &#123;&#123;$labels.namespaces&#125;&#125;/&#123;&#123;$labels.deployment&#125;&#125; summary: Deployment replicas are outdated - alert: DaemonSetRolloutStuck expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 &lt; 100 for: 15m labels: severity: warning annotations: description: Only &#123;&#123;$value&#125;&#125;% of desired pods scheduled and ready for daemon set &#123;&#123;$labels.namespaces&#125;&#125;/&#123;&#123;$labels.daemonset&#125;&#125; summary: DaemonSet is missing pods - alert: K8SDaemonSetsNotScheduled expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled &gt; 0 for: 10m labels: severity: warning annotations: description: A number of daemonsets are not scheduled. summary: Daemonsets are not scheduled correctly - alert: DaemonSetsMissScheduled expr: kube_daemonset_status_number_misscheduled &gt; 0 for: 10m labels: severity: warning annotations: description: A number of daemonsets are running where they are not supposed to run. summary: Daemonsets are not scheduled correctly - alert: PodFrequentlyRestarting expr: increase(kube_pod_container_status_restarts_total[1h]) &gt; 5 for: 10m labels: severity: warning annotations: description: Pod &#123;&#123;$labels.namespaces&#125;&#125;/&#123;&#123;$labels.pod&#125;&#125; is was restarted &#123;&#123;$value&#125;&#125; times within the last hour summary: Pod is restarting frequently - alert: KubeNodeNotReady expr: kube_node_status_condition&#123;job=&quot;kube-state-metrics&quot;,condition=&quot;Ready&quot;,status=&quot;true&quot;&#125; == 0 for: 1h labels: severity: warning annotations: message: &apos;&#123;&#123; $labels.node &#125;&#125; has been unready for more than an hour&apos; runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready - alert: K8SManyNodesNotReady expr: count(kube_node_status_condition&#123;condition=&quot;Ready&quot;,status=&quot;true&quot;&#125; == 0) &gt; 1 and (count(kube_node_status_condition&#123;condition=&quot;Ready&quot;,status=&quot;true&quot;&#125; == 0) / count(kube_node_status_condition&#123;condition=&quot;Ready&quot;,status=&quot;true&quot;&#125;)) &gt; 0.5 for: 1m labels: severity: critical annotations: description: &apos;&#123;&#123; $value &#125;&#125; Kubernetes nodes (more than 10% are in the NotReady state).&apos; summary: Many Kubernetes nodes are Not Ready kubelet.rules: |+ groups: - name: kubelet.rules rules: - alert: K8SKubeletDown expr: count(up&#123;job=&quot;kubelet&quot;&#125; == 0) / count(up&#123;job=&quot;kubelet&quot;&#125;) &gt; 0.03 for: 1h labels: severity: warning annotations: description: Prometheus failed to scrape &#123;&#123; $value &#125;&#125;% of kubelets. summary: Many Kubelets cannot be scraped - alert: K8SManyKubeletDown expr: absent(up&#123;job=&quot;kubelet&quot;&#125; == 1) or count(up&#123;job=&quot;kubelet&quot;&#125; == 0) / count(up&#123;job=&quot;kubelet&quot;&#125;) &gt; 0.5 for: 1h labels: severity: critical annotations: description: Prometheus failed to scrape &#123;&#123; $value &#125;&#125;% of kubelets, or all Kubelets have disappeared from service discovery. summary: Many Kubelets cannot be scraped - alert: K8SKubeletTooManyPods expr: kubelet_running_pod_count &gt; 100 labels: severity: warning annotations: description: Kubelet &#123;&#123;$labels.instance&#125;&#125; is running &#123;&#123;$value&#125;&#125; pods, close to the limit of 110 summary: Kubelet is close to pod limit kubernetes.rules: |+ groups: - name: kubernetes.rules rules: - record: cluster_namespace_controller_pod_container:spec_memory_limit_bytes expr: sum(label_replace(container_spec_memory_limit_bytes&#123;container_name!=&quot;&quot;&#125;, &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name) - record: cluster_namespace_controller_pod_container:spec_cpu_shares expr: sum(label_replace(container_spec_cpu_shares&#123;container_name!=&quot;&quot;&#125;, &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name) - record: cluster_namespace_controller_pod_container:cpu_usage:rate expr: sum(label_replace(irate(container_cpu_usage_seconds_total&#123;container_name!=&quot;&quot;&#125;[5m]), &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name) - record: cluster_namespace_controller_pod_container:memory_usage:bytes expr: sum(label_replace(container_memory_usage_bytes&#123;container_name!=&quot;&quot;&#125;, &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name) - record: cluster_namespace_controller_pod_container:memory_working_set:bytes expr: sum(label_replace(container_memory_working_set_bytes&#123;container_name!=&quot;&quot;&#125;, &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name) - record: cluster_namespace_controller_pod_container:memory_rss:bytes expr: sum(label_replace(container_memory_rss&#123;container_name!=&quot;&quot;&#125;, &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name) - record: cluster_namespace_controller_pod_container:memory_cache:bytes expr: sum(label_replace(container_memory_cache&#123;container_name!=&quot;&quot;&#125;, &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name) - record: cluster_namespace_controller_pod_container:memory_pagefaults:rate expr: sum(label_replace(irate(container_memory_failures_total&#123;container_name!=&quot;&quot;&#125;[5m]), &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name, scope, type) - record: cluster_namespace_controller_pod_container:memory_oom:rate expr: sum(label_replace(irate(container_memory_failcnt&#123;container_name!=&quot;&quot;&#125;[5m]), &quot;controller&quot;, &quot;$1&quot;, &quot;pod_name&quot;, &quot;^(.*)-[a-z0-9]+&quot;)) BY (cluster, namespace, controller, pod_name, container_name, scope, type) - record: cluster:memory_allocation:percent expr: 100 * sum(container_spec_memory_limit_bytes&#123;pod_name!=&quot;&quot;&#125;) BY (cluster) / sum(machine_memory_bytes) BY (cluster) - record: cluster:memory_used:percent expr: 100 * sum(container_memory_usage_bytes&#123;pod_name!=&quot;&quot;&#125;) BY (cluster) / sum(machine_memory_bytes) BY (cluster) - record: cluster:cpu_allocation:percent expr: 100 * sum(container_spec_cpu_shares&#123;pod_name!=&quot;&quot;&#125;) BY (cluster) / sum(container_spec_cpu_shares&#123;id=&quot;/&quot;&#125; * ON(cluster, instance) machine_cpu_cores) BY (cluster) - record: cluster_resource_verb:apiserver_latency:quantile_seconds expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket) BY (le, cluster, job, resource, verb)) / 1e+06 labels: quantile: &quot;0.99&quot; - record: cluster_resource_verb:apiserver_latency:quantile_seconds expr: histogram_quantile(0.9, sum(apiserver_request_latencies_bucket) BY (le, cluster, job, resource, verb)) / 1e+06 labels: quantile: &quot;0.9&quot; - record: cluster_resource_verb:apiserver_latency:quantile_seconds expr: histogram_quantile(0.5, sum(apiserver_request_latencies_bucket) BY (le, cluster, job, resource, verb)) / 1e+06 labels: quantile: &quot;0.5&quot; node.rules: |+ groups: - name: node.rules rules: - alert: NodeExporterDown expr: absent(up&#123;job=&quot;prometheus-node-exporter&quot;&#125; == 1) for: 10m labels: severity: warning annotations: description: Prometheus could not scrape a node-exporter for more than 10m, or node-exporters have disappeared from discovery. summary: node-exporter cannot be scraped - alert: K8SNodeOutOfDisk expr: kube_node_status_condition&#123;condition=&quot;OutOfDisk&quot;,status=&quot;true&quot;&#125; == 1 labels: service: k8s severity: critical annotations: description: &apos;&#123;&#123; $labels.node &#125;&#125; has run out of disk space.&apos; summary: Node ran out of disk space. - alert: K8SNodeMemoryPressure expr: kube_node_status_condition&#123;condition=&quot;MemoryPressure&quot;,status=&quot;true&quot;&#125; == 1 labels: service: k8s severity: warning annotations: description: &apos;&#123;&#123; $labels.node &#125;&#125; is under memory pressure.&apos; summary: Node is under memory pressure. - alert: K8SNodeDiskPressure expr: kube_node_status_condition&#123;condition=&quot;DiskPressure&quot;,status=&quot;true&quot;&#125; == 1 labels: service: k8s severity: warning annotations: description: &apos;&#123;&#123; $labels.node &#125;&#125; is under disk pressure.&apos; summary: Node is under disk pressure. - alert: NodeCPUUsage expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total&#123;job=&quot;prometheus-node-exporter&quot;,mode=&quot;idle&quot;&#125;[5m])) * 100)) &gt; 90 for: 30m labels: severity: warning annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 90% (current value is: &#123;&#123; $value &#125;&#125;)&quot; - alert: NodeMemoryUsage expr: (((node_memory_MemTotal_bytes-node_memory_MemFree_bytes-node_memory_Cached_bytes)/(node_memory_MemTotal_bytes)*100)) &gt; 90 for: 30m labels: severity: warning annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: High memory usage detected&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 90% (current value is: &#123;&#123; $value &#125;&#125;)&quot; prometheus.rules: |+ groups: - name: prometheus.rules rules: - alert: PrometheusReloadFailed expr: prometheus_config_last_reload_successful == 0 for: 10m labels: severity: warning annotations: description: Reloading Prometheus&apos; configuration has failed for &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod&#125;&#125;. summary: Prometheus configuration reload has failed 可视化展示Grafana部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111---apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: k8s-app: grafana name: prometheus-grafana namespace: monitoringspec: replicas: 1 selector: matchLabels: k8s-app: grafana template: metadata: labels: k8s-app: grafana spec: containers: - image: grafana/grafana:6.5.2 livenessProbe: failureThreshold: 10 httpGet: path: /api/health port: 3000 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 30 name: grafana ports: - containerPort: 3000 name: grafana protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /api/health port: 3000 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/grafana/grafana.ini name: config subPath: grafana.ini - mountPath: /var/lib/grafana name: storage volumes: - configMap: defaultMode: 420 name: prometheus-grafana name: config - emptyDir: &#123;&#125; name: storage ---apiVersion: v1kind: ConfigMapmetadata: labels: k8s-app: grafana name: prometheus-grafana namespace: monitoringdata: grafana.ini: | [server] root_url = https://www.baidu.com [log] mode = console level = info [paths] data = /var/lib/grafana/data logs = /var/log/grafana plugins = /var/lib/grafana/plugins provisioning = /etc/grafana/provisioning [smtp] enabled = true host = user user = user password = password cert_file = key_file = skip_verify = false from_address = user from_name = user [emails] welcome_email_on_sign_up = true ---apiVersion: v1kind: Servicemetadata: labels: k8s-app: grafana annotations: prometheus.io/scrape: &apos;true&apos; #prometheus.io/tcp-probe: &apos;true&apos; #prometheus.io/tcp-probe-port: &apos;80&apos; name: prometheus-grafana namespace: monitoringspec: type: NodePort ports: - port: 3000 targetPort: 3000 nodePort: 30028 selector: k8s-app: grafana grafana配置 添加数据源 添加dashboard 一些比较实用的模板： 315这个模板是cadvisor采集的各种指标的图表1860这个模板是node-exporter采集的各种主机相关的指标的图表6417这个模板是kube-state-metrics采集的各种k8s资源对象的状态的图表4859和4865这两个模板是blackbox-exporter采集的服务的http状态指标的图表（两个效果基本一样，选择其一即可）5345这个模板是blackbox-exporter采集的服务的网络状态指标的图表 告警设置警报和通知的主要步骤： 安装配置Alertmanager 配置Prometheus与Alertmanager通信 在Prometheus中创建告警规则 部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183apiVersion: apps/v1beta2kind: Deploymentmetadata: name: alertmanager namespace: monitoringspec: replicas: 1 selector: matchLabels: k8s-app: alertmanager template: metadata: labels: k8s-app: alertmanager spec: containers: - image: prom/alertmanager:v0.19.0 name: alertmanager args: - &quot;--config.file=/etc/alertmanager/config/alertmanager.yml&quot; - &quot;--storage.path=/alertmanager&quot; - &quot;--data.retention=720h&quot; volumeMounts: - mountPath: &quot;/alertmanager&quot; name: data - mountPath: &quot;/etc/alertmanager/config&quot; name: config-volume - mountPath: &quot;/etc/alertmanager/template&quot; name: alertmanager-tmpl resources: requests: cpu: 50m memory: 500Mi limits: cpu: 100m memory: 1Gi volumes: - name: data emptyDir: &#123;&#125; - name: config-volume configMap: name: alertmanager-config - name: alertmanager-tmpl configMap: name: alertmanager-tmpl ---apiVersion: v1kind: ConfigMapmetadata: name: alertmanager-config namespace: monitoringdata: alertmanager.yml: | global: resolve_timeout: 1m wechat_api_corp_id: &apos;****&apos; wechat_api_url: &apos;****&apos; wechat_api_secret: &apos;****&apos; smtp_smarthost: &apos;****&apos; smtp_from: &apos;****&apos; smtp_auth_username: &apos;****&apos; smtp_auth_password: &apos;****&apos; smtp_require_tls: true templates: - &apos;/etc/alertmanager/template/*.tmpl&apos; route: group_by: [&apos;alertname&apos;, &apos;job&apos;] group_wait: 20s group_interval: 20s repeat_interval: 2m receiver: &apos;email&apos; routes: - receiver: &quot;email&quot; group_wait: 10s continue: true match_re: severity: critical|error|warning #- receiver: &quot;wechat&quot; #group_wait: 10s #continue: true #match_re: #severity: critical|error|warning receivers: #- name: &quot;wechat&quot; #wechat_configs: #- send_resolved: true #to_party: &apos;1&apos; #agent_id: 1000003 #corp_id: &apos;****&apos; #api_url: &apos;****&apos; #api_secret: &apos;****&apos; - name: &quot;email&quot; email_configs: - to: &apos;cumt_gongzhao@163.com&apos; send_resolved: true inhibit_rules: - source_match: severity: &apos;critical&apos; target_match: severity: &apos;error&apos; equal: [&apos;alertname&apos;] ---apiVersion: v1kind: Servicemetadata: name: alertmanager namespace: monitoring labels: k8s-app: alertmanager annotations: prometheus.io/scrape: &apos;true&apos;spec: ports: - name: web port: 9093 targetPort: 9093 protocol: TCP nodePort: 30027 type: NodePort selector: k8s-app: alertmanager ---apiVersion: v1kind: ConfigMapmetadata: name: alertmanager-tmpl namespace: monitoringdata: wechat.tmpl: | &#123;&#123; define &quot;wechat.default.message&quot; &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @警报 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @恢复 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; 恢复: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- end &#125;&#125; email.tmpl: | &#123;&#123; define &quot;email.default.html&quot; &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @警报 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125; @恢复 状态: &#123;&#123; .Status &#125;&#125; 名称: &#123;&#123; .Labels.alertname &#125;&#125; 级别: &#123;&#123; .Labels.severity &#125;&#125; 实例: &#123;&#123; .Labels.instance &#125;&#125; 信息: &#123;&#123; .Annotations.summary &#125;&#125; 详情: &#123;&#123; .Annotations.description &#125;&#125; 时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; 恢复: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125; &#123;&#123; end &#125;&#125; 关联Prometheus与alertmanager在Prometheus的架构中被划分成两个独立的部分。Prometheus负责产生告警，而Alertmanager负责告警产生后的后续处理。因此Alertmanager部署完成后，需要在Prometheus中设置Alertmanager相关的信息。 编辑Prometheus配置文件prometheus.yml,并添加以下内容 1234alerting: alertmanagers: - static_configs: - targets: [&apos;alertmanager:9093&apos;] reload Prometheus后可以在AlertManager的webui上查看告警信息：http://IP:Port 设置报警规则详见prometheus-rules.yml]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群监控之Prometheus Operator]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B9%8BPrometheus-Operator%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://github.com/helm/charts/tree/master/stable/prometheus-operator https://github.com/coreos/prometheus-operator https://www.servicemesher.com/blog/prometheus-operator-manual/ https://blog.csdn.net/travellersY/article/details/84632679 Kubernetes Operator 介绍Operator是由CoreOS公司开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的一些专业知识，比如创建一个数据库的Operator，则必须对创建的数据库的各种运维方式非常了解，创建Operator的关键是CRD（自定义资源）的设计。 CRD是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在YAML文件里定义的那些spec都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。 Operator是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。目前CoreOS官方提供了几种Operator的实现，其中就包括我们今天的主角：Prometheus Operator，Operator的核心实现就是基于 Kubernetes 的以下两个概念： 资源：对象的状态定义 控制器：观测、分析和行动，以调节资源的分布 当然我们如果有对应的需求也完全可以自己去实现一个Operator，接下来我们就来给大家详细介绍下Prometheus-Operator的使用方法。 Prometheus Operator介绍Kubernetes的Prometheus Operator为Kubernetes服务和Prometheus实例的部署和管理提供了简单的监控定义。 安装完毕后，Prometheus Operator提供了以下功能： 创建/毁坏: 在Kubernetes namespace中更容易启动一个Prometheus实例，一个特定的应用程序或团队更容易使用Operator。 简单配置: 配置Prometheus的基础东西，比如在Kubernetes的本地资源versions, persistence, retention policies, 和replicas。 Target Services通过标签: 基于常见的Kubernetes label查询，自动生成监控target 配置；不需要学习普罗米修斯特定的配置语言。 Prometheus Operator 架构图如下： 以上架构中的各组成部分以不同的资源方式运行在 Kubernetes 集群中，它们各自有不同的作用： Operator： Operator 资源会根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。 Prometheus： Prometheus 资源是声明性地描述 Prometheus 部署的期望状态。 Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。 ServiceMonitor： ServiceMonitor 也是一个自定义资源，它描述了一组被 Prometheus 监控的 targets 列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。 Service： Service 资源主要用来对应 Kubernetes 集群中的 Metrics Server Pod，来提供给 ServiceMonitor 选取让 Prometheus Server 来获取信息。简单的说就是 Prometheus 监控的对象，例如 Node Exporter Service、Mysql Exporter Service 等等。 Alertmanager： Alertmanager 也是一个自定义资源类型，由 Operator 根据资源描述内容来部署 Alertmanager 集群。 为什么需要prometheus-operator因为是prometheus主动去拉取的，所以在k8s里pod因为调度的原因导致pod的ip会发生变化，人工不可能去维持，自动发现有基于DNS的，但是新增还是有点麻烦。 Prometheus-operator的本职就是一组用户自定义的CRD资源以及Controller的实现，Prometheus Operator这个controller有BRAC权限下去负责监听这些自定义资源的变化，并且根据这些资源的定义自动化的完成如Prometheus Server自身以及配置的自动化管理工作。 在Kubernetes中我们使用Deployment、DamenSet、StatefulSet来管理应用Workload，使用Service、Ingress来管理应用的访问方式，使用ConfigMap和Secret来管理应用配置。我们在集群中对这些资源的创建，更新，删除的动作都会被转换为事件(Event)，Kubernetes的Controller Manager负责监听这些事件并触发相应的任务来满足用户的期望。这种方式我们成为声明式，用户只需要关心应用程序的最终状态，其它的都通过Kubernetes来帮助我们完成，通过这种方式可以大大简化应用的配置管理复杂度。 而除了这些原生的Resource资源以外，Kubernetes还允许用户添加自己的自定义资源(Custom Resource)。并且通过实现自定义Controller来实现对Kubernetes的扩展，不需要用户去二开k8s也能达到给k8s添加功能和对象。 因为svc的负载均衡，所以在K8S里监控metrics基本最小单位都是一个svc背后的pod为target，所以prometheus-operator创建了对应的CRD: kind: ServiceMonitor ，创建的ServiceMonitor里声明需要监控选中的svc的label以及metrics的url路径的和namespaces即可。 什么是metrics例如我们要查看etcd的metrics，先查看etcd的运行参数找到相关的值，这里我是所有参数写在一个yml文件里，非yml自行查看systemd文件或者运行参数找到相关参数和值即可。 1234567891011[root@k8s-m1 ~]# ps aux | grep -P '/etc[d] 'root 13531 2.8 0.8 10631072 140788 ? Ssl 2018 472:58 /usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml[root@k8s-m1 ~]# cat /etc/etcd/etcd.config.yml...listen-client-urls: 'https://172.16.0.2:2379'...client-transport-security: ca-file: '/etc/etcd/ssl/etcd-ca.pem' cert-file: '/etc/etcd/ssl/etcd.pem' key-file: '/etc/etcd/ssl/etcd-key.pem'... 我们需要两部分信息： listen-client-urls的httpsurl，我这里是https://172.16.0.2:2379 允许客户端证书信息 然后使用下面的curl，带上各自证书路径访问https的url执行 1curl --cacert /etc/etcd/ssl/etcd-ca.pem --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem https://172.16.0.2:2379/metrics 也可以etcd用选项和值--listen-metrics-urls http://interface_IP:port设置成非https的metrics端口可以不用证书即可访问，我们会看到etcd的metrics输出信息如下： 1234567891011121314151617181920212223242526272829303132....grpc_server_started_total&#123;grpc_method="RoleList"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="RoleRevokePermission"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="Snapshot"，grpc_service="etcdserverpb.Maintenance"，grpc_type="server_stream"&#125; 0grpc_server_started_total&#123;grpc_method="Status"，grpc_service="etcdserverpb.Maintenance"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="Txn"，grpc_service="etcdserverpb.KV"，grpc_type="unary"&#125; 259160grpc_server_started_total&#123;grpc_method="UserAdd"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="UserChangePassword"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="UserDelete"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="UserGet"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="UserGrantRole"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="UserList"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="UserRevokeRole"，grpc_service="etcdserverpb.Auth"，grpc_type="unary"&#125; 0grpc_server_started_total&#123;grpc_method="Watch"，grpc_service="etcdserverpb.Watch"，grpc_type="bidi_stream"&#125; 86# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.# TYPE process_cpu_seconds_total counterprocess_cpu_seconds_total 28145.45# HELP process_max_fds Maximum number of open file descriptors.# TYPE process_max_fds gaugeprocess_max_fds 65536# HELP process_open_fds Number of open file descriptors.# TYPE process_open_fds gaugeprocess_open_fds 121# HELP process_resident_memory_bytes Resident memory size in bytes.# TYPE process_resident_memory_bytes gaugeprocess_resident_memory_bytes 1.46509824e+08# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.# TYPE process_start_time_seconds gaugeprocess_start_time_seconds 1.54557786888e+09# HELP process_virtual_memory_bytes Virtual memory size in bytes.# TYPE process_virtual_memory_bytes gaugeprocess_virtual_memory_bytes 1.0886217728e+10 同理kube-apiserver也有metrics信息 1234567891011121314151617181920212223$ kubectl get --raw /metrics...rest_client_request_latency_seconds_bucket&#123;url="https://[::1]:6443/apis?timeout=32s"，verb="GET"，le="0.512"&#125; 39423rest_client_request_latency_seconds_bucket&#123;url="https://[::1]:6443/apis?timeout=32s"，verb="GET"，le="+Inf"&#125; 39423rest_client_request_latency_seconds_sum&#123;url="https://[::1]:6443/apis?timeout=32s"，verb="GET"&#125; 24.781942557999795rest_client_request_latency_seconds_count&#123;url="https://[::1]:6443/apis?timeout=32s"，verb="GET"&#125; 39423# HELP rest_client_requests_total Number of HTTP requests， partitioned by status code， method， and host.# TYPE rest_client_requests_total counterrest_client_requests_total&#123;code="200"，host="[::1]:6443"，method="GET"&#125; 2.032031e+06rest_client_requests_total&#123;code="200"，host="[::1]:6443"，method="PUT"&#125; 1.106921e+06rest_client_requests_total&#123;code="201"，host="[::1]:6443"，method="POST"&#125; 38rest_client_requests_total&#123;code="401"，host="[::1]:6443"，method="GET"&#125; 17378rest_client_requests_total&#123;code="404"，host="[::1]:6443"，method="GET"&#125; 3.546509e+06rest_client_requests_total&#123;code="409"，host="[::1]:6443"，method="POST"&#125; 29rest_client_requests_total&#123;code="409"，host="[::1]:6443"，method="PUT"&#125; 20rest_client_requests_total&#123;code="422"，host="[::1]:6443"，method="POST"&#125; 1rest_client_requests_total&#123;code="503"，host="[::1]:6443"，method="GET"&#125; 5# HELP ssh_tunnel_open_count Counter of ssh tunnel total open attempts# TYPE ssh_tunnel_open_count counterssh_tunnel_open_count 0# HELP ssh_tunnel_open_fail_count Counter of ssh tunnel failed open attempts# TYPE ssh_tunnel_open_fail_count counterssh_tunnel_open_fail_count 0 这种就是prometheus的定义的metrics格式规范，缺省是在http(s)的url的/metrics输出。 而metrics要么程序定义输出(模块或者自定义开发)，要么用官方的各种exporter(node-exporter，mysqld-exporter，memcached_exporter…)采集要监控的信息占用一个web端口然后输出成metrics格式的信息，prometheus server去收集各个target的metrics存储起来(tsdb)。 用户可以在prometheus的http页面上用promQL(prometheus的查询语言)或者(grafana数据来源就是用)api去查询一些信息，也可以利用pushgateway去统一采集然后prometheus从pushgateway采集(所以pushgateway类似于zabbix的proxy) 部署使用helm安装部署拉取安装包 12helm search prometheus-operatorhelm fetch stable/prometheus-operator 解压之后修改values文件(请参考官方github) 1helm install --name prometheus-operator --namespace monitoring -f values.yaml ./ 源码部署先获取相关文件后面跟着文件来讲，直接用git客户端拉取即可，不过文件大概30多M，没梯子基本拉不下来。 1git clone https://github.com/coreos/prometheus-operator.git 拉取不下来可以在katacoda的网页上随便一个课程的机器都有docker客户端，可以git clone下来后把文件构建进一个alpine镜像然后推到dockerhub上，再在自己的机器docker run这个镜像的时候docker cp到宿主机上。 Prometheus Operator引入的自定义资源包括： Prometheus ServiceMonitor Alertmanager 用户创建了prometheus-operator(也就是上面监听三个CRD的各种事件的controller)后，用户可以利用kind: Prometheus这种声明式创建对应的资源。 下面我们部署简单的例子学习prometheus-operator 创建prometheus-operator的pod拉取到文件后我们先创建prometheus-operator： 123456$ cd prometheus-operator$ kubectl apply -f bundle.yamlclusterrolebinding.rbac.authorization.k8s.io/prometheus-operator createdclusterrole.rbac.authorization.k8s.io/prometheus-operator createddeployment.apps/prometheus-operator createdserviceaccount/prometheus-operator created 确认pod运行，以及我们可以发现operator的pod在有RBAC下创建了一个APIService： 12345$ kubectl get podNAME READY STATUS RESTARTS AGEprometheus-operator-6db8dbb7dd-djj6s 1/1 Running 0 1m$ kubectl get APIService | grep monitorv1.monitoring.coreos.com 2018-10-09T10:49:47Z 查看这个APISerivce 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172$ kubectl get --raw /apis/monitoring.coreos.com/v1&#123; "kind": "APIResourceList"， "apiVersion": "v1"， "groupVersion": "monitoring.coreos.com/v1"， "resources": [ &#123; "name": "alertmanagers"， "singularName": "alertmanager"， "namespaced": true， "kind": "Alertmanager"， "verbs": [ "delete"， "deletecollection"， "get"， "list"， "patch"， "create"， "update"， "watch" ] &#125;， &#123; "name": "prometheuses"， "singularName": "prometheus"， "namespaced": true， "kind": "Prometheus"， "verbs": [ "delete"， "deletecollection"， "get"， "list"， "patch"， "create"， "update"， "watch" ] &#125;， &#123; "name": "servicemonitors"， "singularName": "servicemonitor"， "namespaced": true， "kind": "ServiceMonitor"， "verbs": [ "delete"， "deletecollection"， "get"， "list"， "patch"， "create"， "update"， "watch" ] &#125;， &#123; "name": "prometheusrules"， "singularName": "prometheusrule"， "namespaced": true， "kind": "PrometheusRule"， "verbs": [ "delete"， "deletecollection"， "get"， "list"， "patch"， "create"， "update"， "watch" ] &#125; ]&#125; 这个是因为bundle.yml里有如下的CLusterRole和对应的ClusterRoleBinding来让prometheus-operator有权限对monitoring.coreos.com这个apiGroup里的这些CRD进行所有操作 12345678910111213141516171819202122apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheus-operatorrules:- apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - '*'- apiGroups: - monitoring.coreos.com resources: - alertmanagers - prometheuses - prometheuses/finalizers - alertmanagers/finalizers - servicemonitors - prometheusrules verbs: - '*' 同时我们查看到pod里的log发现operator也在集群里创建了对应的CRD 12345678910111213141516$ kubectl logs prometheus-operator-6db8dbb7dd-dkhxcts=2018-10-09T11:21:09.389340424Z caller=main.go:165 msg="Starting Prometheus Operator version '0.26.0'."level=info ts=2018-10-09T11:21:09.491464524Z caller=operator.go:377 component=prometheusoperator msg="connection established" cluster-version=v1.11.3level=info ts=2018-10-09T11:21:09.492679498Z caller=operator.go:209 component=alertmanageroperator msg="connection established" cluster-version=v1.11.3level=info ts=2018-10-09T11:21:12.085147219Z caller=operator.go:624 component=alertmanageroperator msg="CRD created" crd=Alertmanagerlevel=info ts=2018-10-09T11:21:12.085265548Z caller=operator.go:1420 component=prometheusoperator msg="CRD created" crd=Prometheuslevel=info ts=2018-10-09T11:21:12.099210714Z caller=operator.go:1420 component=prometheusoperator msg="CRD created" crd=ServiceMonitorlevel=info ts=2018-10-09T11:21:12.118721976Z caller=operator.go:1420 component=prometheusoperator msg="CRD created" crd=PrometheusRulelevel=info ts=2018-10-09T11:21:15.182780757Z caller=operator.go:225 component=alertmanageroperator msg="CRD API endpoints ready"level=info ts=2018-10-09T11:21:15.383456425Z caller=operator.go:180 component=alertmanageroperator msg="successfully synced all caches"$ kubectl get crdNAME CREATED ATalertmanagers.monitoring.coreos.com 2018-10-09T11:21:11Zprometheuses.monitoring.coreos.com 2018-10-09T11:21:11Zprometheusrules.monitoring.coreos.com 2018-10-09T11:21:12Zservicemonitors.monitoring.coreos.com 2018-10-09T11:21:12Z 相关CRD介绍这四个CRD作用如下 Prometheus: 由 Operator 依据一个自定义资源kind: Prometheus类型中，所描述的内容而部署的 Prometheus Server 集群，可以将这个自定义资源看作是一种特别用来管理Prometheus Server的StatefulSets资源。 ServiceMonitor: 一个Kubernetes自定义资源(和kind: Prometheus一样是CRD)，该资源描述了Prometheus Server的Target列表，Operator 会监听这个资源的变化来动态的更新Prometheus Server的Scrape targets并让prometheus server去reload配置(prometheus有对应reload的http接口/-/reload)。而该资源主要通过Selector来依据 Labels 选取对应的Service的endpoints，并让 Prometheus Server 通过 Service 进行拉取（拉）指标资料(也就是metrics信息)，metrics信息要在http的url输出符合metrics格式的信息，ServiceMonitor也可以定义目标的metrics的url。 Alertmanager：Prometheus Operator 不只是提供 Prometheus Server 管理与部署，也包含了 AlertManager，并且一样通过一个 kind: Alertmanager 自定义资源来描述信息，再由 Operator 依据描述内容部署 Alertmanager 集群。 PrometheusRule:对于Prometheus而言，在原生的管理方式上，我们需要手动创建Prometheus的告警文件，并且通过在Prometheus配置中声明式的加载。而在Prometheus Operator模式中，告警规则也编程一个通过Kubernetes API 声明式创建的一个资源.告警规则创建成功后，通过在Prometheus中使用想servicemonitor那样用ruleSelector通过label匹配选择需要关联的PrometheusRule即可。 部署kind: Prometheus现在我们有了prometheus这个CRD，我们部署一个prometheus server只需要如下声明即可。 12345678910111213141516171819$ cat&lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: ServiceAccountmetadata: name: prometheus---apiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata: name: prometheusspec: serviceMonitorSelector: matchLabels: team: frontend serviceAccountName: prometheus resources: requests: memory: 400MiEOF 因为负载均衡，一个svc下的一组pod是监控的最小单位，要监控一个svc的metrics就声明创建一个servicemonitors即可。 部署一组pod及其svc首先，我们部署一个带metrics输出的简单程序的deploy，该镜像里的主进程会在8080端口上输出metrics信息。 12345678910111213141516171819$ cat&lt;&lt;EOF | kubectl apply -f -apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: example-appspec: replicas: 3 template: metadata: labels: app: example-app spec: containers: - name: example-app image: zhangguanzhang/instrumented_app ports: - name: web containerPort: 8080EOF 创建对应的svc。 1234567891011121314$ cat&lt;&lt;EOF | kubectl apply -f -kind: ServiceapiVersion: v1metadata: name: example-app labels: app: example-appspec: selector: app: example-app ports: - name: web port: 8080EOF 部署kind: ServiceMonitor现在创建一个ServiceMonitor来告诉prometheus server需要监控带有label app: example-app的svc背后的一组pod的metrics。 1234567891011121314$ cat&lt;&lt;EOF | kubectl apply -f -apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata: name: example-app labels: team: frontendspec: selector: matchLabels: app: example-app endpoints: - port: webEOF 默认情况下ServiceMonitor和监控对象必须是在相同Namespace下的，如果要关联非同ns下需要下面这样设置值。 1234spec: namespaceSelector: matchNames: - target_ns_name 如果希望ServiceMonitor可以关联任意命名空间下的标签，则通过以下方式定义： 123spec: namespaceSelector: any: true 如果需要监控的Target对象启用了BasicAuth认证，那在定义ServiceMonitor对象时，可以使用endpoints配置中定义basicAuth如下所示basicAuth中的password和username值来源于同ns下的一个名为basic-auth的Secret。 12345678910111213141516171819spec endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: web---apiVersion: v1kind: Secretmetadata: name: basic-authtype: Opaquedata: user: dXNlcgo= # base64编码后的用户名 password: cGFzc3dkCg== # base64编码后的密码 上面要注意的是我创建prometheus server的时候有如下值。 123serviceMonitorSelector: matchLabels: team: frontend 该值字面意思可以知道就是指定prometheus server去选择哪些ServiceMonitor，这个概念和svc去选择pod一样，可能一个集群跑很多prometheus server来监控各自选中的ServiceMonitor，如果想一个prometheus server监控所有的则spec.serviceMonitorSelector: {}为空即可，而namespaces的范围同样的设置spec.serviceMonitorSelector: {}，后面官方的prometheus实例里我们可以看到设置了这两个值。 给prometheus server设置相关的RBAC权限。 123456789101112131415161718192021222324252627282930313233$ cat&lt;&lt;EOF | kubectl apply -f -apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [""] resources: - nodes - services - endpoints - pods verbs: ["get"， "list"， "watch"]- apiGroups: [""] resources: - configmaps verbs: ["get"]- nonResourceURLs: ["/metrics"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: defaultEOF 创建svc使用NodePort方便我们访问prometheus的web页面，生产环境不建议使用NodePort。 12345678910111213141516$ cat&lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: name: prometheusspec: type: NodePort ports: - name: web nodePort: 30900 port: 9090 protocol: TCP targetPort: web selector: prometheus: prometheusEOF 打开浏览器访问ip:30900进入target发现已经监听起来了，对应的config里也有配置生成和导入。 先清理掉上面的，然后我们使用官方提供的全套yaml正式部署prometheus-operator。 12345678kubectl delete svc prometheus example-appkubectl delete ClusterRoleBinding prometheus kubectl delete ClusterRole prometheuskubectl delete ServiceMonitor example-appkubectl delete deploy example-appkubectl delete sa prometheuskubectl delete prometheus prometheuskubectl delete -f bundle.yaml 部署官方的prometheus-operator官方把所有文件都放在一起，这里我分类下。 1234567891011121314151617181920212223cd contrib/kube-prometheus/manifests/mkdir -p operator node-exporter alertmanager grafana kube-state-metrics prometheus serviceMonitor adaptermv *-serviceMonitor* serviceMonitor/mv 0prometheus-operator* operator/mv grafana-* grafana/mv kube-state-metrics-* kube-state-metrics/mv alertmanager-* alertmanager/mv node-exporter-* node-exporter/mv prometheus-adapter* adapter/mv prometheus-* prometheus/$ lltotal 40drwxr-xr-x 9 root root 4096 Jan 6 14:19 ./drwxr-xr-x 9 root root 4096 Jan 6 14:15 ../-rw-r--r-- 1 root root 60 Jan 6 14:15 00namespace-namespace.yamldrwxr-xr-x 3 root root 4096 Jan 6 14:19 adapter/drwxr-xr-x 3 root root 4096 Jan 6 14:19 alertmanager/drwxr-xr-x 2 root root 4096 Jan 6 14:17 grafana/drwxr-xr-x 2 root root 4096 Jan 6 14:17 kube-state-metrics/drwxr-xr-x 2 root root 4096 Jan 6 14:18 node-exporter/drwxr-xr-x 2 root root 4096 Jan 6 14:17 operator/drwxr-xr-x 2 root root 4096 Jan 6 14:19 prometheus/drwxr-xr-x 2 root root 4096 Jan 6 14:17 serviceMonitor/ 部署operator 先创建ns和operator，quay.io仓库拉取慢，可以使用我脚本拉取，其他镜像也可以这样去拉，不过在apply之前才能拉，一旦被docker接手拉取就只能漫长等。 123kubectl apply -f .curl -s https://zhangguanzhang.github.io/bash/pull.sh | bash -s -- quay.io/coreos/prometheus-operator:v0.26.0kubectl apply -f operator/ 确认状态运行正常再往后执行，这里镜像是quay.io仓库的可能会很慢耐心等待或者自行修改成能拉取到的。 123$ kubectl -n monitoring get podNAME READY STATUS RESTARTS AGEprometheus-operator-56954c76b5-qm9ww 1/1 Running 0 24s 部署整套CRD 创建相关的CRD，这里镜像可能也要很久。 1234567kubectl apply -f adapter/kubectl apply -f alertmanager/kubectl apply -f node-exporter/kubectl apply -f kube-state-metrics/kubectl apply -f grafana/kubectl apply -f prometheus/kubectl apply -f serviceMonitor/ 可以通过get查看整体状态，这里镜像原因会等待很久，我们可以先往后看几个坑的地方。 1kubectl -n monitoring get all 踩坑坑一这里要注意有一个坑，二进制部署k8s管理组件和新版本kubeadm部署的都会发现在prometheus server的页面上发现kube-controller和kube-schedule的target为0/0也就是上图所示。这是因为serviceMonitor是根据label去选取svc的，我们可以看到对应的serviceMonitor是选取的ns范围是kube-system。 123456789101112131415161718$ grep -2 selector serviceMonitor/prometheus-serviceMonitorKube*serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml- matchNames:serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml- - kube-systemserviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml: selector:serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml- matchLabels:serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml- k8s-app: kube-controller-manager--serviceMonitor/prometheus-serviceMonitorKubelet.yaml- matchNames:serviceMonitor/prometheus-serviceMonitorKubelet.yaml- - kube-systemserviceMonitor/prometheus-serviceMonitorKubelet.yaml: selector:serviceMonitor/prometheus-serviceMonitorKubelet.yaml- matchLabels:serviceMonitor/prometheus-serviceMonitorKubelet.yaml- k8s-app: kubelet--serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml- matchNames:serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml- - kube-systemserviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml: selector:serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml- matchLabels:serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml- k8s-app: kube-scheduler 而kube-system里默认只有这俩svc，且没有符合上面的label。 1234$ kubectl -n kube-system get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP，53/TCP 139mkubelet ClusterIP None &lt;none&gt; 10250/TCP 103m 但是却有对应的ep(没有带任何label)被创建，这点想不通官方什么鬼操作，另外这里没有kubelet的ep，我博客部署的二进制的话会有。 12345$ kubectl get ep -n kube-systemNAME ENDPOINTS AGEkube-controller-manager &lt;none&gt; 139mkube-dns 10.244.1.2:53，10.244.8.10:53，10.244.1.2:53 + 1 more... 139mkube-scheduler &lt;none&gt; 139m 解决办法 所以这里我们创建两个管理组建的svc，名字无所谓，关键是svc的label要能被servicemonitor选中，svc的选择器的label是因为kubeadm的staticPod的label是这样，如果是二进制部署的这俩svc的selector部分不能要。 1234567891011121314151617181920212223242526272829303132333435apiVersion: v1kind: Servicemetadata: namespace: kube-system name: kube-controller-manager labels: k8s-app: kube-controller-managerspec: selector: component: kube-controller-manager type: ClusterIP clusterIP: None ports: - name: http-metrics port: 10252 targetPort: 10252 protocol: TCP---apiVersion: v1kind: Servicemetadata: namespace: kube-system name: kube-scheduler labels: k8s-app: kube-schedulerspec: selector: component: kube-scheduler type: ClusterIP clusterIP: None ports: - name: http-metrics port: 10251 targetPort: 10251 protocol: TCP 二进制的话需要我们手动填入svc对应的ep的属性，我集群是HA的，所有有三个，仅供参考，别傻傻得照抄，另外这个ep的名字得和上面的svc的名字和属性对应上： 123456789101112131415161718192021222324252627282930313233apiVersion: v1kind: Endpointsmetadata: labels: k8s-app: kube-controller-manager name: kube-controller-manager namespace: kube-systemsubsets:- addresses: - ip: 172.16.0.2 - ip: 172.16.0.7 - ip: 172.16.0.8 ports: - name: http-metrics port: 10252 protocol: TCP---apiVersion: v1kind: Endpointsmetadata: labels: k8s-app: kube-scheduler name: kube-scheduler namespace: kube-systemsubsets:- addresses: - ip: 172.16.0.2 - ip: 172.16.0.7 - ip: 172.16.0.8 ports: - name: http-metrics port: 10251 protocol: TCP 这里不知道为啥kubeadm部署的没有kubelet这个ep，我博客二进制部署后是会有kubelet这个ep的(好像metrics server创建的)，下面仅供参考，IP根据实际写。另外kubeadm部署下kubelet的readonly的metrics端口(默认是10255)不会开放可以删掉ep的那部分port： 123456789101112131415161718192021222324252627282930313233343536373839apiVersion: v1kind: Endpointsmetadata: labels: k8s-app: kubelet name: kubelet namespace: kube-systemsubsets:- addresses: - ip: 172.16.0.14 targetRef: kind: Node name: k8s-n2 - ip: 172.16.0.18 targetRef: kind: Node name: k8s-n3 - ip: 172.16.0.2 targetRef: kind: Node name: k8s-m1 - ip: 172.16.0.20 targetRef: kind: Node name: k8s-n4 - ip: 172.16.0.21 targetRef: kind: Node name: k8s-n5 ports: - name: http-metrics port: 10255 protocol: TCP - name: cadvisor port: 4194 protocol: TCP - name: https-metrics port: 10250 protocol: TCP 至于prometheus server的服务访问，别再用效率不行的NodePort了，上ingress controller吧，怎么部署参照我博客IngressController。 1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: extensions/v1beta1kind: Ingressmetadata: name: prometheus-ing namespace: monitoringspec: rules: - host: prometheus.monitoring.k8s.local http: paths: - backend: serviceName: prometheus-k8s servicePort: 9090---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: grafana-ing namespace: monitoringspec: rules: - host: grafana.monitoring.k8s.local http: paths: - backend: serviceName: grafana servicePort: 3000---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: alertmanager-ing namespace: monitoringspec: rules: - host: alertmanager.monitoring.k8s.local http: paths: - backend: serviceName: alertmanager-main servicePort: 9093 坑二访问prometheus server的web页面我们发现即使创建了svc和注入对应ep的信息在target页面发现prometheus server请求被拒绝。 在宿主机上我们发现127.0.0.1才能访问，网卡ip不能访问(这里是另一个环境找的，所以ip是192不是前面的172) 123456789$ hostname -i192.168.15.223$ curl -I http://192.168.15.223:10251/metricscurl: (7) Failed connect to 192.168.15.223:10251; Connection refused$ curl -I http://127.0.0.1:10251/metricsHTTP/1.1 200 OKContent-Length: 30349Content-Type: text/plain; version=0.0.4Date: Mon， 07 Jan 2019 13:33:50 GMT 解决办法 修改管理组件bind的ip。 如果使用kubeadm启动的集群，初始化时的config.yml里可以加入如下参数 1234controllerManagerExtraArgs: address: 0.0.0.0schedulerExtraArgs: address: 0.0.0.0 已经启动后的使用下面命令更改就会滚动更新 1sed -ri '/--address/s#=.+#=0.0.0.0#' /etc/kubernetes/manifests/kube-* 二进制的话查看是不是bind的0.0.0.0如果不是就修改成0.0.0.0，多块网卡如果只想bind一个网卡就写对应的主机上的网卡ip，写0.0.0.0就会监听所有网卡的对应端口。 监控mysql曾经 想象一下，我们以传统的方式去监控一个mysql服务，首先需要安装mysql-exporter，获取mysql metrics，并且暴露一个端口，等待prometheus服务来拉取监控信息，然后去Prometheus Server的prometheus.yaml文件中在scarpe_config中添加mysql-exporter的job，配置mysql-exporter的地址和端口等信息，再然后，需要重启Prometheus服务，就完成添加一个mysql监控的任务 现在 现在我们以Prometheus-Operator的方式来部署Prometheus，当我们需要添加一个mysql监控我们会怎么做，首先第一步和传统方式一样，部署一个mysql-exporter来获取mysql监控项，然后编写一个ServiceMonitor通过labelSelector选择刚才部署的mysql-exporter，由于Operator在部署Prometheus的时候默认指定了Prometheus选择label为：prometheus: kube-prometheus的ServiceMonitor，所以只需要在ServiceMonitor上打上prometheus: kube-prometheus标签就可以被Prometheus选择了，完成以上两步就完成了对mysql的监控，不需要改Prometheus配置文件，也不需要重启Prometheus服务，是不是很方便，Operator观察到ServiceMonitor发生变化，会动态生成Prometheus配置文件，并保证配置文件实时生效 安装mysql_exporter获取安装包 1helm fetch stable/prometheus-mysql-exporter 解压 1tar xf prometheus-mysql-exporter-0.5.2.tgz 进入解压出来的文件夹修改values文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126# Default values for prometheus-mysql-exporter.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 1image: repository: "prom/mysqld-exporter" tag: "v0.11.0" pullPolicy: "IfNotPresent"service: name: mysql-exporter type: ClusterIP externalPort: 9104 internalPort: 9104serviceMonitor: # enabled should be set to true to enable prometheus-operator discovery of this service enabled: true # interval is the interval at which metrics should be scraped # interval: 30s # scrapeTimeout is the timeout after which the scrape is ended # scrapeTimeout: 10s # additionalLabels is the set of additional labels to add to the ServiceMonitor additionalLabels: &#123;&#125;# release: prometheus jobLabel: "" targetLabels: [] podTargetLabels: []resources: # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. limits: cpu: 400m memory: 500Mi requests: cpu: 300m memory: 200MinodeSelector: &#123;&#125;tolerations: []affinity: &#123;&#125;podLabels: &#123;&#125;annotations: prometheus.io/scrape: "true" prometheus.io/path: "/metrics" prometheus.io/port: "9104"collectors: &#123;&#125; # auto_increment.columns: false # binlog_size: false # engine_innodb_status: false # engine_tokudb_status: false # global_status: true # global_variables: true # info_schema.clientstats: false # info_schema.innodb_metrics: false # info_schema.innodb_tablespaces: false # info_schema.innodb_cmp: false # info_schema.innodb_cmpmem: false # info_schema.processlist: false # info_schema.processlist.min_time: 0 # info_schema.query_response_time: false # info_schema.tables: true # info_schema.tables.databases: '*' # info_schema.tablestats: false # info_schema.schemastats: false # info_schema.userstats: false # perf_schema.eventsstatements: false # perf_schema.eventsstatements.digest_text_limit: 120 # perf_schema.eventsstatements.limit: false # perf_schema.eventsstatements.timelimit: 86400 # perf_schema.eventswaits: false # perf_schema.file_events: false # perf_schema.file_instances: false # perf_schema.indexiowaits: false # perf_schema.tableiowaits: false # perf_schema.tablelocks: false # perf_schema.replication_group_member_stats: false # slave_status: true # slave_hosts: false # heartbeat: false # heartbeat.database: heartbeat # heartbeat.table: heartbeat# mysql connection params which build the DATA_SOURCE_NAME env var of the docker containermysql: db: "" host: "192.168.0.1" param: "" pass: "*****" port: 3306 protocol: "" user: "exporter" existingSecret: false# cloudsqlproxy https://cloud.google.com/sql/docs/mysql/sql-proxycloudsqlproxy: enabled: false image: repo: "gcr.io/cloudsql-docker/gce-proxy" tag: "1.14" pullPolicy: "IfNotPresent" instanceConnectionName: "project:us-central1:dbname" port: "3306" credentials: '&#123; "type": "service_account", "project_id": "project", "private_key_id": "KEYID1", "private_key": "-----BEGIN PRIVATE KEY-----\sdajsdnasd\n-----END PRIVATE KEY-----\n", "client_email": "user@project.iam.gserviceaccount.com", "client_id": "111111111", "auth_uri": "https://accounts.google.com/o/oauth2/auth", "token_uri": "https://accounts.google.com/o/oauth2/token", "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs", "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/user%40project.iam.gserviceaccount.com" &#125;' 安装 1helm install --name me-release -f values.yaml . 如果values文件中的serviceMonitor.enabled为false，则需要我们自己创建serviceMonitor 创建监控接下来让我看看自己如何编写servicemonitor.yaml 接下来编写ServiceMonitor文件,执行命令vim servicemonitor.yaml 12345678910111213141516171819apiVersion: monitoring.coreos.com/v1kind: ServiceMonitor #资源类型为ServiceMonitormetadata: labels: prometheus: kube-prometheus #prometheus默认通过 prometheus: kube-prometheus发现ServiceMonitor，只要写上这个标签prometheus服务就能发现这个ServiceMonitor name: prometheus-exporter-mysqlspec: jobLabel: app #jobLabel指定的标签的值将会作为prometheus配置文件中scrape_config下job_name的值，也就是Target，如果不写，默认为service的name selector: matchLabels: #该ServiceMonitor匹配的Service的labels，如果使用mathLabels，则下面的所有标签都匹配时才会匹配该service，如果使用matchExpressions，则至少匹配一个标签的service都会被选择 app: prometheus-mysql-exporter # 由于前面查看mysql-exporter的service信息中标签包含了app: prometheus-mysql-exporter这个标签，写上就能匹配到 namespaceSelector: any: true #表示从所有namespace中去匹配，如果只想选择某一命名空间中的service，可以使用matchNames: []的方式 # mathNames： [] endpoints: - port: mysql-exporter #前面查看mysql-exporter的service信息中，提供mysql监控信息的端口是Port: mysql-exporter 9104/TCP，所以这里填mysql-exporter interval: 30s #每30s获取一次信息 # path: /metrics HTTP path to scrape for metrics，默认值为/metrics honorLabels: true 保存并退出文件，然后执行命令：kubectl create -f servicemonitor.yaml，创建成功之后执行命令kubectl get serviceMonitor查看是否有刚才创建的serviceMonitor： 1234567891011121314151617181920root@k8s1:~/mysql-exporter# kubectl create -f servicemonitor.yamlservicemonitor.monitoring.coreos.com/prometheus-exporter-mysql createdroot@k8s1:~/mysql-exporter# kubectl get serviceMonitorNAME AGEgrafana 4dkafka-release 5hkafka-release-exporter 5hkube-prometheus 23hkube-prometheus-alertmanager 23hkube-prometheus-exporter-coredns 23hkube-prometheus-exporter-kube-controller-manager 23hkube-prometheus-exporter-kube-etcd 23hkube-prometheus-exporter-kube-scheduler 23hkube-prometheus-exporter-kube-state 23hkube-prometheus-exporter-kubelets 23hkube-prometheus-exporter-kubernetes 23hkube-prometheus-exporter-node 23hprometheus-exporter-mysql 13sprometheus-operator 4droot@k8s1:~/mysql-exporter# 可以看到Prometheus-exporter-mysql已经存在了，表示创建成功了，过1分钟左右，在prometheus的界面中查看Targets，可以看到已经成功添加了mysql监控。 上面提到prometheus通过标签prometheus: kube-prometheus选择ServiceMonitor，该配置写在这里, 当然，你可以通过在values.yaml中配置serviceMonitorsSelector来指定按照自己的规则选择serviceMonitor，关于如何配置serviceMonitorsSelector将放在后文统一讲解 动态添加告警规则当我们动态添加了监控对象，一般会对该对象配置告警规则，采用prometheus-operator的架构模式下，当我们需要动态配置告警规则的时候，可以使用另一种自定义资源（CRD）PrometheusRule，PrometheusRule和ServiceMonitor都是一种自定义资源，ServiceMonitor用于动态添加监控实例，而PrometheusRule则用于动态添加告警规则，下面依然通过动态添加mysql的告警规则为例来演示如何使用PrometheusRule资源。 执行命令vim mysql-rule.yaml，输入以下内容 12345678910111213141516171819202122232425apiVersion: monitoring.coreos.com/v1 #这和ServiceMonitor一样kind: PrometheusRule #该资源类型是Prometheus，这也是一种自定义资源（CRD）metadata: labels: app: "prometheus-rule-mysql" prometheus: kube-prometheus #同ServiceMonitor，ruleSelector也会默认选择标签为prometheus: kube-prometheus的PrometheusRule资源 name: prometheus-rule-mysqlspec: groups: #编写告警规则，和prometheus的告警规则语法相同 - name: mysql.rules rules: - alert: TooManyErrorFromMysql expr: sum(irate(mysql_global_status_connection_errors_total[1m])) &gt; 10 labels: severity: critical annotations: description: mysql产生了太多的错误. summary: TooManyErrorFromMysql - alert: TooManySlowQueriesFromMysql expr: increase(mysql_global_status_slow_queries[1m]) &gt; 10 labels: severity: critical annotations: description: mysql一分钟内产生了&#123;&#123; $value &#125;&#125;条慢查询日志. summary: TooManySlowQueriesFromMysql Prometheus选择PrometheusRule资源是通过ruleSelector来选择，默认也是通过标签：prometheus: kube-prometheus来选择，在这里可以看到，ruleSelector和ServiceMonitorsSelector都是可以配置的，如何配置将放在后文的配置统一讲解 保存以上文件之后执行kubectl create -f mysql-rule.yaml，创建成功之后执行命令kubectl get prometheusRule可以看到刚才创建的PrometheusRule资源prometheus-rule-mysql： 123456789101112131415root@k8s1:~/mysql-exporter# kubectl create -f mysql-rule.yamlprometheusrule.monitoring.coreos.com/prometheus-rule-mysql createdroot@k8s1:~/mysql-exporter# kubectl get prometheusRuleNAME AGEkube-prometheus 1hkube-prometheus-alertmanager 1hkube-prometheus-exporter-kube-controller-manager 1hkube-prometheus-exporter-kube-etcd 1hkube-prometheus-exporter-kube-scheduler 1hkube-prometheus-exporter-kube-state 1hkube-prometheus-exporter-kubelets 1hkube-prometheus-exporter-kubernetes 1hkube-prometheus-exporter-node 1hkube-prometheus-rules 1hprometheus-rule-mysql 8s 等待1分钟左右，在prometheus图形界面中可以找到刚才添加的mysql.rule的内容了 如何动态更新Alertmanager配置原理 Operator部署Alertmanager的时候会生成一个statefulset类型对象，通过命令kubectl get statefulset –all-namespaces可以找到这个statefulset，可以看到name是alertmanager-kube-prometheus 12345678910111213root@k8s1:~/prometheus-operator/helm/alertmanager/templates# kubectl get statefulset --all-namespacesNAMESPACE NAME DESIRED CURRENT AGEelk elastic-release-elasticsearch-data 2 2 8delk elastic-release-elasticsearch-master 3 3 8delk logstash-release 1 1 9dkafka kafka-release 3 3 1dkafka zookeeper-release 3 3 12dkube-system mongodb-release-arbiter 1 1 16dkube-system mongodb-release-primary 1 1 16dkube-system mongodb-release-secondary 1 1 16dkube-system my-release-mysqlha 3 3 15dmonitoring alertmanager-kube-prometheus 1 1 9hmonitoring prometheus-kube-prometheus 1 1 9h 然后执行命令kubectl describe statefulset alertmanager-kube-prometheus -n monitoring可以看到该statefulset的详细信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162root@k8s1:~/prometheus-operator/helm/alertmanager/templates# kubectl describe statefulset alertmanager-kube-prometheus -n monitoringName: alertmanager-kube-prometheusNamespace: monitoringCreationTimestamp: Wed, 05 Sep 2018 09:46:08 +0800Selector: alertmanager=kube-prometheus,app=alertmanagerLabels: alertmanager=kube-prometheus app=alertmanager chart=alertmanager-0.1.6 heritage=Tiller release=kube-prometheusAnnotations: &lt;none&gt;Replicas: 1 desired | 1 totalUpdate Strategy: RollingUpdatePods Status: 1 Running / 0 Waiting / 0 Succeeded / 0 FailedPod Template: Labels: alertmanager=kube-prometheus app=alertmanager Containers: alertmanager: Image: intellif.io/prometheus-operator/alertmanager:v0.15.1 Ports: 9093/TCP, 6783/TCP Host Ports: 0/TCP, 0/TCP Args: --config.file=/etc/alertmanager/config/alertmanager.yaml --cluster.listen-address=$(POD_IP):6783 --storage.path=/alertmanager --web.listen-address=:9093 --web.external-url=http://192.168.11.178:30903 --web.route-prefix=/ --cluster.peer=alertmanager-kube-prometheus-0.alertmanager-operated.monitoring.svc:6783 Requests: memory: 200Mi Liveness: http-get http://:web/api/v1/status delay=0s timeout=3s period=10s #success=1 #failure=10 Readiness: http-get http://:web/api/v1/status delay=3s timeout=3s period=5s #success=1 #failure=10 Environment: POD_IP: (v1:status.podIP) Mounts: /alertmanager from alertmanager-kube-prometheus-db (rw) /etc/alertmanager/config from config-volume (rw) #挂载的secert目录 config-reloader: Image: intellif.io/prometheus-operator/configmap-reload:v0.0.1 Port: &lt;none&gt; Host Port: &lt;none&gt; Args: -webhook-url=http://localhost:9093/-/reload -volume-dir=/etc/alertmanager/config Limits: cpu: 5m memory: 10Mi Environment: &lt;none&gt; Mounts: /etc/alertmanager/config from config-volume (ro) Volumes: config-volume: Type: Secret (a volume populated by a Secret) SecretName: alertmanager-kube-prometheus Optional: false alertmanager-kube-prometheus-db: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium:Volume Claims: &lt;none&gt;Events: &lt;none&gt; 该statefulset挂载了一个名为alertmanager-kube-prometheus的secret资源到alertmanager容器内部的/etc/alertmanager/config/alertmanager.yaml，上面的Volumes:下面的config-volume：标签下可以看到，Type:字段的值为Secret表示挂载一个secret资源，secrect的name是alertmanager-kube-prometheus，通过一下命令查看该secret： kubectl describe secrets alertmanager-kube-prometheus -n monitoring 123456789101112131415root@k8s1:~# kubectl describe secrets alertmanager-kube-prometheus -n monitoringName: alertmanager-kube-prometheusNamespace: monitoringLabels: alertmanager=kube-prometheus app=alertmanager chart=alertmanager-0.1.6 heritage=Tiller release=kube-prometheusAnnotations: &lt;none&gt;Type: OpaqueData====alertmanager.yaml: 567 bytes 可以看到该secert的Data项里面有一个key为alertmanager.yaml的属性，其value包含567bytes，而这个alertmanager.yaml的值其实就是alertmanager容器的/etc/alertmanager/config/alertmanager.yaml中的内容，statefulset通过挂载的方式将/etc/alertmanager/config挂载成一个secret，执行kubectl edit secrets -n monitoring alertmanager-kube-prometheus可以看到该secret的内容： 123456789101112131415161718apiVersion: v1data: alertmanager.yaml: Z2xvYmFsOgogIHJlc29sdmVfdGltZW91dDogNW0KICBzbXRwX2F1dGhfcGFzc3dvcmQ6ICoqKioKICBzbXRwX2F1dGhfdXNlcm5hbWU6IGlub3JpX3lpbmprQDE2My5jb20KICBzbXRwX2Zyb206IGlub3JpX3lpbmprMUAxNjMuY29tCiAgc210cF9yZXF1aXJlX3RsczogZmFsc2UKICBzbXRwX3NtYXJ0aG9zdDogc210cC4xNjMuY29tOjI1CnJlY2VpdmVyczoKLSBlbWFpbF9jb25maWdzOgogIC0gaGVhZGVyczoKICAgICAgU3ViamVjdDogJ1tFUlJPUl0gcHJvbWV0aGV1cy4uLi4uLi4uLi4uLicKICAgIHRvOiB4eHh4QHFxLmNvbQogIG5hbWU6IHRlYW0tWC1tYWlscwotIG5hbWU6ICJudWxsIgpyb3V0ZToKICBncm91cF9ieToKICAtIGFsZXJ0bmFtZQogIC0gY2x1c3RlcgogIC0gc2VydmljZQogIGdyb3VwX2ludGVydmFsOiA1bQogIGdyb3VwX3dhaXQ6IDYwcwogIHJlY2VpdmVyOiB0ZWFtLVgtbWFpbHMKICByZXBlYXRfaW50ZXJ2YWw6IDI0aAogIHJvdXRlczoKICAtIG1hdGNoOgogICAgICBhbGVydG5hbWU6IERlYWRNYW5zU3dpdGNoCiAgICByZWNlaXZlcjogIm51bGwikind: Secretmetadata: creationTimestamp: 2018-09-05T01:46:08Z labels: alertmanager: kube-prometheus app: alertmanager chart: alertmanager-0.1.6 heritage: Tiller release: kube-prometheus name: alertmanager-kube-prometheus namespace: monitoring resourceVersion: "5820063" selfLink: /api/v1/namespaces/monitoring/secrets/alertmanager-kube-prometheus uid: 75a589e8-b0ad-11e8-8746-005056bf1d6etype: Opaque 其中data:下面的alertmanager.yaml这个key对应的值是一串base64编码过后的字符串，将这段字符串复制出来通过base64反编码之后内容如下： 123456789101112131415161718192021222324252627global: resolve_timeout: 5m smtp_auth_password: xxxxxx smtp_auth_username: inori_yinjk@163.com smtp_from: inori_yinjk@163.com smtp_require_tls: false smtp_smarthost: smtp.163.com:25receivers:- email_configs: - headers: Subject: '[ERROR] prometheus............' to: 1121562648@qq.com name: team-X-mails- name: "null"route: group_by: - alertname - cluster - service group_interval: 5m group_wait: 60s receiver: team-X-mails repeat_interval: 24h routes: - match: alertname: DeadMansSwitch receiver: "null" 这其实就是alertmanager的config配置，上面说到，该内容会被挂载到alertmanager容器的/etc/alertmanager/config/alertmanager.yaml，我们进入alertmanager容器去看看该文件，执行命令kubectl exec -it alertmanager-kube-prometheus-0 -n monitoring sh进入到容器（可能你的容器名和我的不同，可以通过kubectl get pods –all-namespaces命令查看所有的容器），然后进入目录/etc/alertmanager/config,然后ls可以看到该目录下有一个叫alertmanager.yaml的文件，而该文件的内容就是上面base64反编译之后的内容，我们通过修改名为alertmanager-kube-prometheus的secret的data属性中的alertmanager.yaml字段对应的值就相当于修改了该文件中的内容，所以现在问题就变简单了，在alertmanager的pod中还有另一个container叫做config-reloader，它会监听/etc/alertmanager/config目录，当该目录下的文件发生变化的时候，config-reloader会向alertmanager发起http://localhost:9093/-/reloadPOST请求，alertmanager会重新加载该目录下的配置文件，从而实现了动态配置更新 如何操作 在理解了alertmanager动态配置的原理之后，问题就很清晰了，我们需要动态配置alertmanager只需要更新名为alertmanager-kube-prometheus(你的secret名不一定为这个名字，但一定是alertmanager-{*}格式)的secret的data属性中的alertmanager.yaml字段的值就可以了，更新secret有两种方法，一是通过kubectl edit secret的方式，一种是通过kubectl patch secret的方式，但是两种方式更新secret都需要输入base64编码之后的字符串，这里通过linux下的base64命令进行编码： 首先修改上面base64反编译后的文件，比如smtp_from改成另一个邮箱发送，修改完成之后保存文件，然后通过命令base64 file &gt; test.txt的方式将配置通过base64编码并将编码结果输出到test.txt文件中，然后进入test.txt文件中复制编码之后的字符串,如果通过第一种方式更新secret，执行命令 kubectl edit secrets -n monitoring alertmanager-kube-prometheus 然后data下面的alertmanager.yaml的值为刚才复制的字符串，保存并退出就可以了。如果通过第二种方式更新secert，执行命令 kubectl patch secert alertmanager-kube-prometheus -n -n monitoring -p &#39;{&quot;data&quot;:{&quot;alertmanager.yaml&quot;:&quot;此处填写刚才复制的base64编码之后的配置字符串&quot;}&#39; 即可完成更新，该命令中 -p参数后面跟的是一个JSON字符串，将刚才复制的base64编码后的字符串填入正确的位置可以了 在完成更新之后可以访问alertmanager的界面http://192.168.11.178:30903/#/status，查看配置已经生效了 通过上面的操作我们已经实现了监控对象的动态发现，监控告警规则的动态添加，告警配置（发送邮件）的动态配置，基本上已经实现了所有配置的动态配置 Prometheus-Operator配置配置Prometheus一般情况下只需要配置kube-prometheus下的values.yaml就能实现对alertmanager、promethes的配置，该文件该如何配置在配置项上一般都有说明，这里主要讲解上文提到的几个配置以及其他比较常用的几个配置，避免占用太多篇幅，我已经将比较简单或不常用的配置以省略号代替： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140...alertmanager: #所有alertmanager的配置都在这个标签下面 config: #alermanager的config，和传统的alertmanager的config配置相同 global: resolve_timeout: 5m route: group_by: ['job'] group_wait: 30s group_interval: 5m repeat_interval: 12h receiver: 'null' routes: - match: alertname: DeadMansSwitch receiver: 'null' receivers: - name: 'null' ## 外部url，报警发送邮件后可以通过该Url访问Alertmanager的界面 externalUrl: "http://192.168.11.178:30903"... ## Node labels for Alertmanager pod assignment 通过该选择器将会选择alertmanager在哪个node上面部署 ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: &#123;&#125;... ## List of Secrets in the same namespace as the AlertManager ## object, which shall be mounted into the AlertManager Pods. ## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec ## secrets: [] service:... ## Port to expose on each node ## Only used if service.type is 'NodePort' ## nodePort: 30903 #暴露的端口 ## Service type ## type: NodePort # 以NodePort的方式部署alertmanager 的Service，这将可以使用node的ip访问服务prometheus: #所有的prometheus的配置都在这里编写 ## Alertmanagers to which alerts will be sent ## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints ## alertingEndpoints: [] #alertmanager地址，不填写默认会使用同时部署的alertmanager，在helm/prometheus/templates/prometheus.yaml文件第40行， #github中https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L40 # - name: "" # namespace: "" # port: 9093 # scheme: http... ## External URL at which Prometheus will be reachable ##同上，外部访问地址 externalUrl: "" ## List of Secrets in the same namespace as the Prometheus ## object, which shall be mounted into the Prometheus Pods. ## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec ## secrets: [] ## How long to retain metrics ## prometheus时序数据库数据保存时间 retention: 24h ## Namespaces to be selected for PrometheusRules discovery. ## If unspecified, only the same namespace as the Prometheus object is in is used. ## 选择PrometheusRule的namespace，如何配置any:true则回去所有命名空间中去寻找PrometheusRule资源 ruleNamespaceSelector: &#123;&#125; ## any: true ## or ## ## Rules PrometheusRule CRD selector ## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md ## ## 1. If `matchLabels` is used, `rules.additionalLabels` must contain all the labels from ## `matchLabels` in order to be be matched by Prometheus ## 2. If `matchExpressions` is used `rules.additionalLabels` must contain at least one label ## from `matchExpressions` in order to be matched by Prometheus ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels ## 上文提到的ruleNamespaceSelector，不填写默认会使用prometheus: &#123;&#123;.Values.prometheusLabelValue&#125;&#125;, ## 而prometheusLabel定义在helm/prometheus/values.yaml中，默认为.Release.Name即releaseName(kube-prometheus) ## 我们可以通过修改helm/prometheus/values.yaml中的prometheusLabel值来修改默认选择的标签，也可以在这里定义自己的标签， ## 在这里定义了标签之后默认的会失效，比如在这里定义一个comment: prometheus标签，默认的prometheus: kube-prometheus就会失效，prometheus也将根据新定义的标签来选择PrometheusRule资源 rulesSelector: &#123;&#125; # rulesSelector: &#123; # matchExpressions: [&#123;key: prometheus, operator: In, values: [example-rules, example-rules-2]&#125;] # &#125; ### OR # rulesSelector: &#123; # matchLabels: [&#123;role: example-rules&#125;] # &#125; ## Prometheus alerting &amp; recording rules ## Ref: https://prometheus.io/docs/querying/rules/ ## Ref: https://prometheus.io/docs/alerting/rules/ ## rules: #可以在这里配置PrometheusRule资源，一般不在这里配置，而是单独编写PrometheusRule这样可以实现动态配置 specifiedInValues: true ## What additional rules to be added to the PrometheusRule CRD ## You can use this together with `rulesSelector` additionalLabels: &#123;&#125; # prometheus: example-rules # application: etcd value: &#123;&#125; service: ## Port to expose on each node ## Only used if service.type is 'NodePort' ## nodePort: 30900 #同上，暴露的端口 ## Service type ## type: NodePort #同上，将Prometheus Service映射到外网 ## Service monitors selector ## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md ## 同上，prometheus默认会选择带有prometheus: kube-prometheus标签的ServiceMonitor资源，也可以在这里配置自定义的标签，一旦在这里配置了标签，默认的将会失效，prometheus会按照新的serviceMonitorSelector中定义的标签来选择对应的ServiceMonitor serviceMonitorsSelector: &#123;&#125;# matchLabels:# - comment: prometheus# - release: kube-prometheus ## ServiceMonitor CRDs to create &amp; be scraped by the Prometheus instance. ## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/service-monitor.md ## serviceMonitors: [] #可以在这里配置serviceMonitor资源，一般不再这里配置,参见如何编写ServiceMonitor章节]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群监控之Prometheus]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B9%8BPrometheus%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 http://dockone.io/article/9269 http://dockone.io/article/5716 https://www.jianshu.com/p/fb5c82de935d https://github.com/prometheus/prometheus https://yunlzheng.gitbook.io/prometheus-book https://prometheus.io/docs/introduction/overview/ https://github.com/samber/awesome-prometheus-alerts 监控目的在《SRE：Google运维解密》一书中指出，监控系统需要能够有效的支持白盒监控和黑盒监控。通过白盒能够了解其内部的实际运行状态，通过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优化。而黑盒监控，常见的如HTTP探针，TCP探针等，可以在系统或者服务在发生故障时能够快速通知相关的人员进行处理。通过建立完善的监控体系，从而达到以下目的： 长期趋势分析：通过对监控样本数据的持续收集和统计，对监控指标进行长期趋势分析。例如，通过对磁盘空间增长率的判断，我们可以提前预测在未来什么时间节点上需要对资源进行扩容。 对照分析：两个版本的系统运行资源使用情况的差异如何？在不同容量情况下系统的并发和负载变化如何？通过监控能够方便的对系统进行跟踪和比较。 告警：当系统出现或者即将出现故障时，监控系统需要迅速反应并通知管理员，从而能够对问题进行快速的处理或者提前预防问题的发生，避免出现对业务的影响。 故障分析与定位：当问题发生后，需要对问题进行调查和处理。通过对不同监控指标以及历史数据的分析，能够找到并解决根源问题。 数据可视化：通过可视化仪表盘能够直接获取系统的运行状态、资源使用情况、以及服务运行状态等直观的信息。 而对于上一代监控系统而言，在使用过程中往往会面临以下问题： 与业务脱离的监控：监控系统获取到的监控指标与业务本身也是一种分离的关系。好比客户可能关注的是服务的可用性、服务的SLA等级，而监控系统却只能根据系统负载去产生告警； 运维管理难度大：Nagios这一类监控系统本身运维管理难度就比较大，需要有专业的人员进行安装，配置和管理，而且过程并不简单； 可扩展性低： 监控系统自身难以扩展，以适应监控规模的变化； 问题定位难度大：当问题产生之后（比如主机负载异常增加）对于用户而言，他们看到的依然是一个黑盒，他们无法了解主机上服务真正的运行情况，因此当故障发生后，这些告警信息并不能有效的支持用户对于故障根源问题的分析和定位。 在上述需求中，我们可以提取出以下对于一个完善的监控解决方案的几个关键词：数据分析、趋势预测、告警、故障定位、可视化。 除此以外，当前越来越多的产品公司迁移到云或者容器的情况下，对于监控解决方案而言还需要另外一个关键词：云原生。 Prometheus简介Prometheus已经被广泛应用于数据中心监控，尤其是和Kubernetes结合的容器监控。本文主要从架构分析到落地实践，详细介绍Prometheus原理和使用。对比Prometheus与其他监控工具（Zabbix、Open-Falcon）的特点与使用场景。然后介绍Prometheus与Kubernetes集成，主要从监控和自动伸缩两个方面。最后通过企业案例，分享实践经验和注意事项。 Kubernetes从2014年开源以来，迅速成为容器管理的领头羊，它是Google Borg系统的开源实现。和Kubernetes一起火起来的还有另一个开源项目Prometheus，它是Google BorgMon的开源实现。 2016年，由Google发起的Linux基金会旗下的原生云基金会（Cloud Native Computing Foundation）将Prometheus纳入其第二大开源项目。Prometheus在开源社区也十分活跃，在GitHub上拥有两万多Star，并且系统每隔一两周就会有一个小版本的更新。 Prometheus是由SoundCloud开发的开源监控报警系统和时序列数据库。从字面上理解，Prometheus由两个部分组成，一个是监控报警系统，另一个是自带的时序数据库（TSDB）。 上图是Prometheus整体架构图，左侧是各种符合Prometheus数据格式的exporter，除此之外为了支持推动数据类型的Agent，可以通过Pushgateway组件，将Push转化为Pull。Prometheus甚至可以从其它的Prometheus获取数据，组建联邦集群。Prometheus的基本原理是通过HTTP周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口并且符合Prometheus定义的数据格式，就可以接入Prometheus监控。 上侧是服务发现，Prometheus支持监控对象的自动发现机制，从而可以动态获取监控对象。 图片中间是Prometheus Server，Retrieval模块定时拉取数据，并通过Storage模块保存数据。PromQL为Prometheus提供的查询语法，PromQL模块通过解析语法树，调用Storage模块查询接口获取监控数据。图片右侧是告警和页面展现，Prometheus将告警推送到alertmanger，然后通过alertmanger对告警进行处理并执行相应动作。数据展现除了Prometheus自带的WebUI，还可以通过Grafana等组件查询Prometheus监控数据。 数据类型理解时间序列 在Node Exporter的/metrics接口中返回的每一行监控数据，在Prometheus下称为一个样本。采集到的样本由以下三部分组成： 指标（metric）：指标和一组描述当前样本特征的labelsets唯一标识； 时间戳（timestamp）：一个精确到毫秒的时间戳，一般由采集时间决定； 样本值（value）： 一个folat64的浮点型数据表示当前样本的值。 Prometheus会将所有采集到的样本数据以时间序列（time-series）的方式保存在内存数据库中，并且定时保存到硬盘上。每条time-series通过指标名称（metrics name）和一组标签集（labelset）命名。如下所示，可以将time-series理解为一个以时间为X轴的二维矩阵： 这种多维度的数据存储方式，可以衍生出很多不同的玩法。 比如，如果数据来自不同的数据中心，那么我们可以在样本中添加标签来区分来自不同数据中心的监控样本，例如： 1node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;, dc=&quot;dc0&quot;&#125; 从内部实现上来看Prometheus中所有存储的监控样本数据没有任何差异，均是一组标签，时间戳以及样本值。 从存储上来讲所有的监控指标metric都是相同的，但是在不同的场景下这些metric又有一些细微的差异。 例如，在Node Exporter返回的样本中指标node_load1反应的是当前系统的负载状态，随着时间的变化这个指标返回的样本数据是在不断变化的。而指标node_cpu所获取到的样本数据却不同，它是一个持续增大的值，因为其反应的是CPU的累积使用时间，从理论上讲只要系统不关机，这个值是会无限变大的。 为了能够帮助用户理解和区分这些不同监控指标之间的差异，Prometheus定义了4中不同的指标类型（metric type）：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要） Counter：只增不减的计数器 Counter是一个简单但有强大的工具，例如我们可以在应用程序中记录某些事件发生的次数，通过以时序的形式存储这些数据，我们可以轻松的了解该事件产生速率的变化。PromQL内置的聚合操作和函数可以用户对这些数据进行进一步的分析： 例如，通过rate()函数获取HTTP请求量的增长率： 1rate(http_requests_total[5m]) Gauge：可增可减的仪表盘 与Counter不同，Gauge类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。常见指标如：node_memory_MemFree（主机当前空闲的内容大小）、node_memory_MemAvailable（可用内存大小）都是Gauge类型的监控指标。 通过Gauge指标，用户可以直接查看系统的当前状态： 1node_memory_MemFree 对于Gauge类型的监控指标，通过PromQL内置函数delta()可以获取样本在一段时间返回内的变化情况。例如，计算CPU温度在两个小时内的差异： 1delta(cpu_temp_celsius&#123;host=&quot;zeus&quot;&#125;[2h]) 还可以使用deriv()计算样本的线性回归模型，甚至是直接使用predict_linear()对数据的变化趋势进行预测。例如，预测系统磁盘空间在4个小时之后的剩余情况： 1predict_linear(node_filesystem_free&#123;job=&quot;node&quot;&#125;[1h], 4 * 3600) 使用Histogram和Summary分析数据分布情况 在大多数情况下人们都倾向于使用某些量化指标的平均值，例如CPU的平均使用率、页面的平均响应时间。这种方式的问题很明显，以系统API调用的平均响应时间为例：如果大多数API请求都维持在100ms的响应时间范围内，而个别请求的响应时间需要5s，那么就会导致某些WEB页面的响应时间落到中位数的情况，而这种现象被称为长尾问题。 为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组。例如，统计延迟在010ms之间的请求数有多少而1020ms之间的请求数又有多少。通过这种方式可以快速分析系统慢的原因。Histogram和Summary都是为了能够解决这样问题的存在，通过Histogram和Summary类型的监控指标，我们可以快速了解监控样本的分布情况。 例如，指标prometheus_tsdb_wal_fsync_duration_seconds的指标类型为Summary。 它记录了Prometheus Server中wal_fsync处理的处理时间，通过访问Prometheus Server的/metrics地址，可以获取到以下监控样本数据： 12345prometheus_tsdb_wal_fsync_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 0.012352463prometheus_tsdb_wal_fsync_duration_seconds&#123;quantile=&quot;0.9&quot;&#125; 0.014458005prometheus_tsdb_wal_fsync_duration_seconds&#123;quantile=&quot;0.99&quot;&#125; 0.017316173prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002prometheus_tsdb_wal_fsync_duration_seconds_count 216 从上面的样本中可以得知当前Promtheus Server进行wal_fsync操作的总次数为216次，耗时2.888716127000002s。其中中位数（quantile=0.5）的耗时为0.012352463，9分位数（quantile=0.9）的耗时为0.014458005s。 Prometheus对于数据的存储方式就意味着，不同的标签就代表着不同的特征维度。用户可以通过这些特征维度对查询，过滤和聚合样本数据。 例如，通过node_load1，查询出当前时间序列数据库中所有名为node_load1的时间序列： 1node_load1 如果找到满足某些特征维度的时间序列，则可以使用标签进行过滤： 1node_load1&#123;instance=&quot;localhost:9100&quot;&#125; 通过以标签为核心的特征维度，用户可以对时间序列进行有效的查询和过滤，当然如果仅仅是这样，显然还不够强大，Prometheus提供的丰富的聚合操作以及内置函数，可以通过PromQL轻松回答以下问题： 当前系统的CPU使用率？ 1avg(irate(node_cpu&#123;mode!=&quot;idle&quot;&#125;[2m])) without (cpu, mode) CPU占用率前5位的主机有哪些？ 1topk(5, avg(irate(node_cpu&#123;mode!=&quot;idle&quot;&#125;[2m])) without (cpu, mode)) 预测在4小时候后，磁盘空间占用大致会是什么情况？ 1predict_linear(node_filesystem_free&#123;job=&quot;node&quot;&#125;[2h], 4 * 3600) 其中avg()，topk()等都是PromQL内置的聚合操作，irate()，predict_linear()是PromQL内置的函数，irate()函数可以计算一段时间返回内时间序列中所有样本的单位时间变化率。predict_linear函数内部则通过简单线性回归的方式预测数据的变化趋势。 以Grafana为例，在Grafana中可以通过将Promtheus作为数据源添加到系统中，后再使用PromQL进行数据可视化。在Grafana v5.1中提供了对Promtheus 4种监控类型的完整支持，可以通过Graph Panel，Singlestat Panel，Heatmap Panel对监控指标数据进行可视化。 数据采集 Prometheus通过HTTP接口的方式从各种客户端获取数据，这些客户端必须符合Prometheus监控数据格式，通常有两种方式，一种是侵入式埋点监控，通过在客户端集成，如果Kubernetes API直接通过引入Prometheus go client，提供/metrics接口查询kubernetes API各种指标；另一种是通过exporter方式，在外部将原来各种中间件的监控支持转化为Prometheus的监控数据格式，如redis exporter将Reids指标转化为Prometheus能够识别的HTTP请求。 HTTP返回Header和Body如上图所示，指标前面两行#是注释，标识指标的含义和类型。指标和指标的值通过空格分割，开发者通常不需要自己拼接这种个数的数据， Prometheus提供了各种语言的SDK支持。 Prometheus并没有采用json的数据格式，而是采用text/plain纯文本的方式 ，这是它的特殊之处。 Exporter Prometheus为了支持各种中间件以及第三方的监控提供了exporter，大家可以把它理解成监控适配器，将不同指标类型和格式的数据统一转化为Prometheus能够识别的指标类型。 譬如Node exporter主要通过读取Linux的/proc以及/sys目录下的系统文件获取操作系统运行状态，reids exporter通过Reids命令行获取指标，mysql exporter通过读取数据库监控表获取MySQL的性能数据。他们将这些异构的数据转化为标准的Prometheus格式，并提供HTTP查询接口。 数据存储 Prometheus提供了两种数据持久化方式：一种是本地存储，通过Prometheus自带的TSDB（时序数据库），将数据保存到本地磁盘，为了性能考虑，建议使用SSD。但本地存储的容量毕竟有限，建议不要保存超过一个月的数据。Prometheus本地存储经过多年改进，自Prometheus 2.0后提供的V3版本TSDB性能已经非常高，可以支持单机每秒1000w个指标的收集。 Prometheus本地数据存储能力一直为大家诟病，但Prometheus本地存储设计的初衷就是为了监控数据的查询，Facebook发现85％的查询是针对26小时内的数据。所以Prometheus本地时序数据库的设计更多考虑的是高性能而非分布式大容量。 另一种是远端存储，适用于大量历史监控数据的存储和查询。通过中间层的适配器的转化，Prometheus将数据保存到远端存储。适配器实现Prometheus存储的remote write和remote read接口，并把数据转化为远端存储支持的数据格式。目前，远端存储主要包括OpenTSDB、InfluxDB、Elasticsearch、M3DB等，其中M3DB是目前非常受欢迎的后端存储。 PromQL Prometheus数据展现除了自带的WebUI还可以通过Grafana，他们本质上都是通过HTTP + PromQL的方式查询Prometheus数据。和关系型数据库的SQL类似，Prometheus也内置了数据查询语言PromQL，它提供对时间序列数据丰富的查询，聚合以及逻辑运算的能力。数据运算包括了： +（加法） -（减法） *（乘法） /（除法） %（求余） ^（幂运算） 聚合包括了： sum（求和） min（最小值） max（最大值） avg（平均值） stddev（标准差） stdvar（标准差异） count（计数） count_values（对value进行计数） bottomk（后n条） topk（前n条） quantile（分布统计） 如果需要获取某个时刻的数据可以通过curl ‘http://Prometheus地址:9090/api/v1/query?query=up&amp;time=xx&#39;查询监控数据，其中query参数就是一个PromQL表达式。除此之外，还支持范围查询query_range，需要额外添加下面的参数：start（起始时间）、end（结束时间）、step=（查询步长） 当接收到请求参数后，通过PromQL引擎解析PromQL，确定查询的数据序列和时间范围，通过tsdb接口获取对应数据块（chunks），最后根据聚合函数处理监控数据并返回。 告警 如果监控数据达到告警阈值Prometheus Server会通过HTTP将告警发送到告警模块alertmanger。Prometheus告警配置也是通过yaml文件，核心是上面的expr表达式（告警规则）和查询一样也是一个PromQL表达式。 for代表持续时间，如果在for时间内持续触发Prometheus才发出告警告警组件alertmanger地址是在Prometheus的配置文件中指定，告警经过alertmanger去重、抑制等操作，最后执行告警动作，目前支持邮件、slack、微信和webhook，如果是对接钉钉，便可以通过webhook方式触发钉钉的客户端发送告警。 联邦 为了扩展单个Prometheus的采集能力和存储能力，Prometheus引入了“联邦”的概念。多个Prometheus节点组成两层联邦结构，如图所示，上面一层是联邦节点，负责定时从下面的Prometheus节点获取数据并汇总，部署多个联邦节点是为了实现高可用以及数据汇聚存储。下层的Prometheus节点又分别负责不同区域的数据采集，在多机房的事件部署中，下层的每个Prometheus节点可以被部署到单独的一个机房，充当代理。 Improbable开源的Thanos提供了Prometheus集群化能力，感兴趣的朋友可以深入了解一下。 其他监控分布式熟悉zabbix的朋友可能知道，zabbix中有主动模式和被动模式，主动模式可以实现agent节点自动向server节点汇报，这样就减轻了server端的压力。被动模式中也有一种添加代理节点方式实现分布式监控，实现跨机房异地监控目标。具体方法可以参考本人zabbix监控文章。 prometheus联邦机制prometheus的分布式类似于nginx的负载均衡模式，主节点配置文件可以配置从节点的地址池，主节点只要定时向从节点拉取数据即可，主节点的作用就是存贮实时数据，并提供给grafana 使用。而从节点作用就是分别从不同的各个采集端中抽取数据，可以实现分机器或者分角色。这种由一个中心的prometheus负则聚合多个prometheus数据中的监控模式，称为prometheus联邦集群。例如：集群规模200台，两个从节点，可以每台机器监控100台，也可以每台机器监控200台，但是分别监控不同角色，第一个从节点监控hdfs，第二个节点监控hbase这种方式，反正想怎么监控就看个人配置了。 部署配置分布式的部署就是找多台机器分别部署prometheus，部署方式都是一致，只有配置文件不同。联邦集群核心在于每一个prometheus server都包含一个用于获取当前实例中监控样本的接口 /federate 。对于中心prometheus server无论是从其他prometheus实例还是node_exporter采集端获取数据，事实上没有任何差异的。 参数 作用 honor_labels 防止采集到监控指标冲突，配置true可以确保采集到指标冲突时自动忽略冲突指标；配置false会自动将冲突指标替换为exported_的形式。还可以添加标签区分不同监控目标 metrics_path 联邦集群用于获取监控样本参数配置 /federate match[ ] 指定需要获取的时间序列，个人认为也就是填写从节点的角色标签或者环境变量。可以填写job=”zookeeper”或者name=~“instance.*”，模糊匹配可以使用通配符。将你想要展示的角色或者变量写入prometheus主节点才可以获取从节点上信息，否则无法获取 static_configs 在此填写从节点地址池即可 主节点配置文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global &apos;evaluation_interval&apos;.rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&apos;s Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.# - job_name: &apos;prometheus&apos; # metrics_path defaults to &apos;/metrics&apos; # scheme defaults to &apos;http&apos;.# static_configs:# - targets: [&apos;localhost:9090&apos;] - job_name: &apos;node_workers&apos; honor_labels: true metrics_path: &apos;/federate&apos; params: &apos;match[]&apos;: - &apos;&#123;job=&quot;zookeeper&quot;&#125;&apos; - &apos;&#123;job=&quot;hbase&quot;&#125;&apos; - &apos;&#123;job=&quot;hdfs&quot;&#125;&apos; - &apos;&#123;job=&quot;hive&quot;&#125;&apos; - &apos;&#123;job=&quot;kafka&quot;&#125;&apos; - &apos;&#123;job=&quot;linux_server&quot;&#125;&apos; - &apos;&#123;job=&quot;spark&quot;&#125;&apos; - &apos;&#123;job=&quot;yarn&quot;&#125;&apos; - &apos;&#123;__name__=~&quot;instance.*&quot;&#125;&apos; static_configs: - targets: - &apos;192.168.1.1:9090&apos; - &apos;192.168.1.2:9090&apos; 从节点1配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global &apos;evaluation_interval&apos;.rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&apos;s Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: &apos;prometheus&apos; # metrics_path defaults to &apos;/metrics&apos; # scheme defaults to &apos;http&apos;. static_configs: - targets: [&apos;localhost:9090&apos;] - job_name: &apos;linux_server&apos; file_sd_configs: - files: - configs/linux.json - job_name: &apos;hdfs&apos; file_sd_configs: - files: - configs/hdfs.json - job_name: &apos;hbase&apos; file_sd_configs: - files: - configs/hbase.json - job_name: &apos;yarn&apos; file_sd_configs: - files: - configs/yarn.json 从节点2配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global &apos;evaluation_interval&apos;.rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&apos;s Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: &apos;prometheus&apos; # metrics_path defaults to &apos;/metrics&apos; # scheme defaults to &apos;http&apos;. static_configs: - targets: [&apos;localhost:9090&apos;] - job_name: &apos;zookeeper&apos; file_sd_configs: - files: - configs/zookeeper.json - job_name: &apos;hive&apos; file_sd_configs: - files: - configs/hive.json - job_name: &apos;kafka&apos; file_sd_configs: - files: - configs/kafka.json - job_name: &apos;spark&apos; file_sd_configs: - files: - configs/spark.json 服务发现与云原生：以Kubernetes为例 Prometheus有两种方式配置监控对象，一种是通过静态文件配置，另一种是动态发现机制。 目前动态发现目前已经支持Kubernetes、etcd、Consul等多种服务，动态发现可以减少运维人员手动配置，在容器运行环境中尤为重要，容器集群通常在几千甚至几万的规模，如果每个容器都需要单独配置监控项不仅需要大量工作量，而且容器经常变动，后续维护更是异常麻烦。针对Kubernetes环境的动态发现，Prometheus通过Watch Kubernetes API动态获取当前集群所有主机、容器以及服务的变化情况。 对于Kubernetes而言，如上图所示，我们可以把当中所有的资源分为几类： 基础设施层（Node）：集群节点，为整个集群和应用提供运行时资源 容器基础设施（Container）：为应用提供运行时环境 用户应用（Pod）：Pod中会包含一组容器，它们一起工作，并且对外提供一个（或者一组）功能 内部服务负载均衡（Service）：在集群内，通过Service在集群暴露应用功能，集群内应用和应用之间访问时提供内部的负载均衡。 外部访问入口（Ingress）：通过Ingress提供集群外的访问入口，从而可以使外部客户端能够访问到部署在Kubernetes集群内的服务。 因此，在不考虑Kubernetes自身组件的情况下，如果要构建一个完整的监控体系，我们应该考虑，以下5个方面： 集群节点状态监控：从集群中各节点的kubelet服务获取节点的基本运行状态； 集群节点资源用量监控：通过Daemonset的形式在集群中各个节点部署Node Exporter采集节点的资源使用情况； 节点中运行的容器监控：通过各个节点中kubelet内置的cAdvisor中获取个节点中所有容器的运行状态和资源使用情况； 从黑盒监控的角度在集群中部署Blackbox Exporter探针服务，检测Service和Ingress的可用性； 如果在集群中部署的应用程序本身内置了对Prometheus的监控支持，那么我们还应该找到相应的Pod实例，并从该Pod实例中获取其内部运行状态的监控指标。 而对于Prometheus这一类基于Pull模式的监控系统，显然也无法继续使用的static_configs的方式静态的定义监控目标。而对于Prometheus而言其解决方案就是引入一个中间的代理人（服务注册中心），这个代理人掌握着当前所有监控目标的访问信息，Prometheus只需要向这个代理人询问有哪些监控目标控即可， 这种模式被称为服务发现。 Prometheus提供了对Kubernetes的完整支持，通过与Kubernetes的API进行交互，Prometheus可以自动的发现Kubernetes中所有的Node、Service、Pod、Endpoints以及Ingress资源的相关信息。 通过服务发现找到所有的监控目标后，并通过Prometheus的Relabling机制对这些资源进行过滤，metrics地址替换等操作，从而实现对各类资源的全自动化监控。 例如，通过以下流程任务配置，可以自动从集群节点的kubelet服务中内置的cAdvisor中获取容器的监控数据： 12345678910111213141516171819- job_name: &apos;kubernetes-cadvisor&apos;scheme: httpstls_config:ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crtbearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role: noderelabel_configs:- action: labelmapregex: __meta_kubernetes_node_label_(.+)- target_label: __address__replacement: kubernetes.default.svc:443- source_labels: [__meta_kubernetes_node_name]regex: (.+)target_label: __metrics_path__replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor 由或者是通过集群中部署的blackbox exporter对服务进行网络探测： 123456789101112131415161718192021- job_name: &apos;kubernetes-services&apos;metrics_path: /probeparams:module: [http_2xx]kubernetes_sd_configs:- role: servicerelabel_configs:- source_labels: [__address__]target_label: __param_target- target_label: __address__replacement: blackbox-exporter.example.com:9115- source_labels: [__param_target]target_label: instance- action: labelmapregex: __meta_kubernetes_service_label_(.+)- source_labels: [__meta_kubernetes_namespace]target_label: kubernetes_namespace- source_labels: [__meta_kubernetes_service_name]target_label: kubernetes_name 规模化监控解决方案Prometheus周期性的从Target中获取监控数据并保存到本地的time-series中，并且通过PromQL对外暴露数据查询接口。 内部周期性的检查告警规则文件，产生告警并有Alertmanager对告警进行后续处理。 那么问题来了，这里Prometheus是单点，Alertmanager也是单点。 这样的结构能否支持大规模的监控量？ 对于Prometheus而言，要想完全理解其高可用部署模式，首先我们需要理解Prometheus的数据存储机制。 如上所示，Prometheus 2.x采用自定义的存储格式将样本数据保存在本地磁盘当中。按照两个小时为一个时间窗口，将两小时内产生的数据存储在一个块（Block）中，每一个块中包含该时间窗口内的所有样本数据（chunks），元数据文件（meta.json）以及索引文件（index）。 当前时间窗口内正在收集的样本数据，Prometheus则会直接将数据保存在内存当中。为了确保此期间如果Prometheus发生崩溃或者重启时能够恢复数据，Prometheus启动时会从写入日志（WAL）进行重播，从而恢复数据。此期间如果通过API删除时间序列，删除记录也会保存在单独的逻辑文件当中（tombstone）。 通过时间窗口的形式保存所有的样本数据，可以明显提高Prometheus的查询效率，当查询一段时间范围内的所有样本数据时，只需要简单的从落在该范围内的块中查询数据即可。而对于历史数据的删除，也变得非常简单，只要删除相应块所在的目录即可。 对于单节点的Prometheus而言，这种基于本地文件系统的存储方式能够让其支持数以百万的监控指标，每秒处理数十万的数据点。为了保持自身管理和部署的简单性，Prometheus放弃了管理HA的复杂度。 因此首先，对于这种存储方式而言，我们需要明确的几点： Prometheus本身不适用于持久化存储长期的历史数据，默认情况下Prometheus只保留15天的数据。 本地存储也意味着Prometheus自身无法进行有效的弹性伸缩。 而当监控规模变得巨大的时候，对于单台Prometheus而言，其主要挑战包括以下几点： 服务的可用性，如何确保Prometheus不会发生单点故障； 监控规模变大的意味着，Prometheus的采集Job的数量也会变大（写）操作会变得非常消耗资源； 同时也意味着大量的数据存储的需求。 简单HA：服务可用性由于Prometheus的Pull机制的设计，为了确保Prometheus服务的可用性，用户只需要部署多套Prometheus Server实例，并且采集相同的Exporter目标即可。 基本的HA模式只能确保Prometheus服务的可用性问题，但是不解决Prometheus Server之间的数据一致性问题以及持久化问题（数据丢失后无法恢复），也无法进行动态的扩展。因此这种部署方式适合监控规模不大，Promthues Server也不会频繁发生迁移的情况，并且只需要保存短周期监控数据的场景。 基本HA + 远程存储在基本HA模式的基础上通过添加Remote Storage存储支持，将监控数据保存在第三方存储服务上。 当Prometheus在获取监控样本并保存到本地的同时，会将监控数据发送到Remote Storage Adaptor，由Adaptor完成对第三方存储的格式转换以及数据持久化。 当Prometheus查询数据的时候，也会从Remote Storage Adaptor获取数据，合并本地数据后进行数据查询。 在解决了Prometheus服务可用性的基础上，同时确保了数据的持久化，当Prometheus Server发生宕机或者数据丢失的情况下，可以快速的恢复。 同时Prometheus Server可能很好的进行迁移。因此，该方案适用于用户监控规模不大，但是希望能够将监控数据持久化，同时能够确保Prometheus Server的可迁移性的场景。 基本HA + 远程存储 + 联邦集群当单台Prometheus Server无法处理大量的采集任务时，用户可以考虑基于Prometheus联邦集群的方式将监控采集任务划分到不同的Prometheus实例当中即在任务级别功能分区。 这种部署方式一般适用于两种场景： 场景一：单数据中心 + 大量的采集任务 这种场景下Prometheus的性能瓶颈主要在于大量的采集任务，因此用户需要利用Prometheus联邦集群的特性，将不同类型的采集任务划分到不同的Prometheus子服务中，从而实现功能分区。例如一个Prometheus Server负责采集基础设施相关的监控指标，另外一个Prometheus Server负责采集应用监控指标。再有上层Prometheus Server实现对数据的汇聚。 场景二：多数据中心 这种模式也适合与多数据中心的情况，当Prometheus Server无法直接与数据中心中的Exporter进行通讯时，在每一个数据中部署一个单独的Prometheus Server负责当前数据中心的采集任务是一个不错的方式。这样可以避免用户进行大量的网络配置，只需要确保主Prometheus Server实例能够与当前数据中心的Prometheus Server通讯即可。 中心Prometheus Server负责实现对多数据中心数据的聚合。 高可用方案选择上面的部分，根据不同的场景演示了3种不同的高可用部署方案。当然对于Prometheus部署方案需要用户根据监控规模以及自身的需求进行动态调整，下表展示了Prometheus和高可用有关3个选项各自解决的问题，用户可以根据自己的需求灵活选择。 对于Alertmanager而言，Alertmanager集群之间使用Gossip协议相互传递状态，因此对于Prometheus而言，只需要关联多个Alertmanager实例即可，关于Alertmanager集群的详细详细可以参考：https://github.com/yunlzheng/p … ty.md 配置文件解析Prometheus通过命令行标志和配置文件进行配置。 虽然命令行标志配置了不可变的系统参数（例如存储位置，保留在磁盘和内存中的数据量等），但配置文件定义了与抓取作业及其实例相关的所有内容，以及哪些规则文件 载入。 要查看所有可用的命令行参数，执行./prometheus -h Prometheus可以在运行时重新加载其配置。 如果新配置格式不正确，则不会应用更改。 通过向Prometheus进程发送SIGHUP或向/-/reload端点发送HTTP POST请求（启用--web.enable-lifecycle标志时）来触发配置重新加载。 这也将重新加载任何已配置的规则文件。 配置文件 要指定要加载的配置文件， 请使用–config.file标志。 该文件以YAML格式编写，由下面描述的方案定义。 括号表示参数是可选的。 对于非列表参数，该值设置为指定的默认值。 通用占位符定义如下： ：一个可以取值为true或false的布尔值 ：与正则表达式匹配的持续时间[0-9] +（ms | [smhdwy]） ：与正则表达式匹配的字符串[a-zA-Z _] [a-zA-Z0-9 _] * ：一串unicode字符 ：当前工作目录中的有效路径 ：由主机名或IP后跟可选端口号组成的有效字符串 ：有效的URL路径 ：一个可以取值http或https的字符串 ：常规字符串 ：一个秘密的常规字符串，例如密码 ：在使用前进行模板扩展的字符串 其他占位符是单独指定的。 可以在此处找到有效的示例文件。 全局配置指定在所有其他配置上下文中有效的参数。 它们还可用作其他配置节的默认值 1234567891011121314151617181920212223242526272829303132333435363738global: # 默认情况下抓取目标的频率. [ scrape_interval: &lt;duration&gt; | default = 1m ] # 抓取超时时间. [ scrape_timeout: &lt;duration&gt; | default = 10s ] # 评估规则的频率. [ evaluation_interval: &lt;duration&gt; | default = 1m ] # 与外部系统通信时添加到任何时间序列或警报的标签 #（联合，远程存储，Alertma# nager）. external_labels: [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ]# 规则文件指定了一个globs列表. # 从所有匹配的文件中读取规则和警报.rule_files: [ - &lt;filepath_glob&gt; ... ]# 抓取配置列表.scrape_configs: [ - &lt;scrape_config&gt; ... ]# 警报指定与Alertmanager相关的设置.alerting: alert_relabel_configs: [ - &lt;relabel_config&gt; ... ] alertmanagers: [ - &lt;alertmanager_config&gt; ... ]# 与远程写入功能相关的设置.remote_write: [ - &lt;remote_write&gt; ... ]# 与远程读取功能相关的设置.remote_read: [ - &lt;remote_read&gt; ... ] 部分指定一组描述如何刮除它们的目标和参数。 在一般情况下，一个scrape配置指定单个作业。 在高级配置中，这可能会改变。 目标可以通过参数静态配置，也可以使用其中一种支持的服务发现机制动态发现。 此外，允许在抓取之前对任何目标及其标签进行高级修改。 其中在所有scrape配置中必须是唯一的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# 默认分配给已抓取指标的job名称。job_name: &lt;job_name&gt;# 从job中抓取目标的频率.[ scrape_interval: &lt;duration&gt; | default = &lt;global_config.scrape_interval&gt; ]# 抓取此job时，每次抓取超时时间.[ scrape_timeout: &lt;duration&gt; | default = &lt;global_config.scrape_timeout&gt; ]# 从目标获取指标的HTTP资源路径.[ metrics_path: &lt;path&gt; | default = /metrics ]# honor_labels控制Prometheus如何处理已经存在于已抓取数据中的标签与Prometheus将附加服务器端的标签之间的冲突（&quot;job&quot;和&quot;instance&quot;标签，手动配置的目标标签以及服务发现实现生成的标签）。# # 如果honor_labels设置为&quot;true&quot;，则通过保留已抓取数据的标签值并忽略冲突的服务器端标签来解决标签冲突。## 如果honor_labels设置为&quot;false&quot;，则通过将已抓取数据中的冲突标签重命名为&quot;exported_ &lt;original-label&gt;&quot;（例如&quot;exported_instance&quot;，&quot;exported_job&quot;）然后附加服务器端标签来解决标签冲突。 这对于联合等用例很有用，其中应保留目标中指定的所有标签。# # 请注意，任何全局配置的&quot;external_labels&quot;都不受此设置的影响。 在与外部系统通信时，它们始终仅在时间序列尚未具有给定标签时应用，否则将被忽略。# [ honor_labels: &lt;boolean&gt; | default = false ]# 配置用于请求的协议方案.[ scheme: &lt;scheme&gt; | default = http ]# 可选的HTTP URL参数.params: [ &lt;string&gt;: [&lt;string&gt;, ...] ]# 使用配置的用户名和密码在每个scrape请求上设置`Authorization`标头。 password和password_file是互斥的。basic_auth: [ username: &lt;string&gt; ] [ password: &lt;secret&gt; ] [ password_file: &lt;string&gt; ]# 使用配置的承载令牌在每个scrape请求上设置`Authorization`标头。 它`bearer_token_file`和是互斥的。[ bearer_token: &lt;secret&gt; ]# 使用配置的承载令牌在每个scrape请求上设置`Authorization`标头。 它`bearer_token`和是互斥的。[ bearer_token_file: /path/to/bearer/token/file ]# 配置scrape请求的TLS设置.tls_config: [ &lt;tls_config&gt; ]# 可选的代理URL.[ proxy_url: &lt;string&gt; ]# Azure服务发现配置列表.azure_sd_configs: [ - &lt;azure_sd_config&gt; ... ]# Consul服务发现配置列表.consul_sd_configs: [ - &lt;consul_sd_config&gt; ... ]# DNS服务发现配置列表。dns_sd_configs: [ - &lt;dns_sd_config&gt; ... ]# EC2服务发现配置列表。ec2_sd_configs: [ - &lt;ec2_sd_config&gt; ... ]# OpenStack服务发现配置列表。openstack_sd_configs: [ - &lt;openstack_sd_config&gt; ... ]# 文件服务发现配置列表。file_sd_configs: [ - &lt;file_sd_config&gt; ... ]# GCE服务发现配置列表。gce_sd_configs: [ - &lt;gce_sd_config&gt; ... ]# Kubernetes服务发现配置列表。kubernetes_sd_configs: [ - &lt;kubernetes_sd_config&gt; ... ]# Marathon服务发现配置列表。marathon_sd_configs: [ - &lt;marathon_sd_config&gt; ... ]# AirBnB的神经服务发现配置列表。nerve_sd_configs: [ - &lt;nerve_sd_config&gt; ... ]# Zookeeper Serverset服务发现配置列表。serverset_sd_configs: [ - &lt;serverset_sd_config&gt; ... ]# Triton服务发现配置列表。triton_sd_configs: [ - &lt;triton_sd_config&gt; ... ]# 此job的标记静态配置目标列表。static_configs: [ - &lt;static_config&gt; ... ]# 目标重新标记配置列表。relabel_configs: [ - &lt;relabel_config&gt; ... ]# 度量标准重新配置列表。metric_relabel_configs: [ - &lt;relabel_config&gt; ... ]# 对每个将被接受的样本数量的每次抓取限制。# 如果在度量重新标记后存在超过此数量的样本，则整个抓取将被视为失败。 0表示没有限制。[ sample_limit: &lt;int&gt; | default = 0 ] 12345678910111213# 用于验证API服务器证书的CA证书。[ ca_file: &lt;filename&gt; ]# 用于服务器的客户端证书身份验证的证书和密钥文件。[ cert_file: &lt;filename&gt; ][ key_file: &lt;filename&gt; ]# ServerName扩展名，用于指示服务器的名称。# https://tools.ietf.org/html/rfc4366#section-3.1[ server_name: &lt;string&gt; ]# 禁用服务器证书的验证。[ insecure_skip_verify: &lt;boolean&gt; ] 基于DNS的服务发现配置允许指定一组DNS域名，这些域名会定期查询以发现目标列表。 要联系的DNS服务器从/etc/resolv.conf中读取。 此服务发现方法仅支持基本的DNS A，AAAA和SRV记录查询，但不支持RFC6763中指定的高级DNS-SD方法。 在重新标记阶段，元标签__meta_dns_name在每个目标上可用，并设置为生成已发现目标的记录名称。 123456789101112# 要查询的DNS域名列表。names: [ - &lt;domain_name&gt; ]# 要执行的DNS查询的类型。[ type: &lt;query_type&gt; | default = &apos;SRV&apos; ]# 查询类型不是SRV时使用的端口号。[ port: &lt;number&gt;]# 提供名称后刷新的时间。[ refresh_interval: &lt;duration&gt; | default = 30s ] 其中是有效的DNS域名。 其中是SRV，A或AAAA。 Kubernetes SD配置允许从Kubernetes的RESTAPI中检索scrape目标，并始终与群集状态保持同步。 可以配置以下role类型之一来发现目标 node node角色发现每个群集节点有一个目标，其地址默认为Kubelet的HTTP端口。 目标地址默认为NodeInternalIP，NodeExternalIP，NodeLegacyHostIP和NodeHostName的地址类型顺序中Kubernetes节点对象的第一个现有地址。 可用元标签： __meta_kubernetes_node_name：节点对象的名称。 _meta_kubernetes_node_label ：节点对象中的每个标签。 _meta_kubernetes_node_annotation`：节点对象中的每个注释。 _meta_kubernetes_node_address：每个节点地址类型的第一个地址（如果存在）。 此外，节点的instance标签将设置为从API服务器检索的节点名称。 service service角色为每个服务发现每个服务端口的目标。 这对于服务的黑盒监控通常很有用。 该地址将设置为服务的Kubernetes DNS名称和相应的服务端口。 可用元标签： __meta_kubernetes_namespace：服务对象的命名空间。 _meta_kubernetes_service_annotation：服务对象的注释。 __meta_kubernetes_service_cluster_ip：服务的群集IP地址。 （不适用于ExternalName类型的服务） __meta_kubernetes_service_external_name：服务的DNS名称。 （适用于ExternalName类型的服务） _meta_kubernetes_service_label ：服务对象的标签。 __meta_kubernetes_service_name：服务对象的名称。 __meta_kubernetes_service_port_name：目标服务端口的名称。 __meta_kubernetes_service_port_number：目标的服务端口号。 __meta_kubernetes_service_port_protocol：目标服务端口的协议。 pod pod角色发现所有pod并将其容器暴露为目标。 对于容器的每个声明端口，将生成单个目标。 如果容器没有指定端口，则会创建每个容器的无端口目标，以通过重新标记手动添加端口。 可用元标签： __meta_kubernetes_namespace：pod对象的命名空间。 __meta_kubernetes_pod_name：pod对象的名称。 __meta_kubernetes_pod_ip：pod对象的pod IP。 _meta_kubernetes_pod_label ：pod对象的标签。 _meta_kubernetes_pod_annotation ：pod对象的注释。 __meta_kubernetes_pod_container_name：目标地址指向的容器的名称。 __meta_kubernetes_pod_container_port_name：容器端口的名称。 __meta_kubernetes_pod_container_port_number：容器端口号。 __meta_kubernetes_pod_container_port_protocol：容器端口的协议。 __meta_kubernetes_pod_ready：对于pod的就绪状态，设置为true或false。 __meta_kubernetes_pod_phase：在生命周期中设置为Pending，Running，Succeeded，Failed或Unknown。 __meta_kubernetes_pod_node_name：将pod安排到的节点的名称。 __meta_kubernetes_pod_host_ip：pod对象的当前主机IP。 __meta_kubernetes_pod_uid：pod对象的UID。 __meta_kubernetes_pod_controller_kind：对象类型的pod控制器。 __meta_kubernetes_pod_controller_name：pod控制器的名称。 endpoints endpoints角色从列出的服务端点发现目标。 对于每个端点地址，每个端口发现一个目标。 如果端点由pod支持，则pod的所有其他容器端口（未绑定到端点端口）也会被发现为目标。 可用元标签： __meta_kubernetes_namespace：端点对象的命名空间。 __meta_kubernetes_endpoints_name：端点对象的名称。对于直接从端点列表中发现的所有目标（不是从底层pod中另外推断的那些），附加以下标签： __meta_kubernetes_endpoint_ready：对端点的就绪状态设置为true或false。 __meta_kubernetes_endpoint_port_name：端点端口的名称。 __meta_kubernetes_endpoint_port_protocol：端点端口的协议。 __meta_kubernetes_endpoint_address_target_kind：端点地址目标的种类。 __meta_kubernetes_endpoint_address_target_name：端点地址目标的名称。 如果端点属于某个服务，则会附加角色：服务发现的所有标签。 对于由pod支持的所有目标，将附加角色的所有标签：pod发现。 ingress ingress角色发现每个入口的每个路径的目标。 这通常用于黑盒监控入口。 地址将设置为入口规范中指定的主机。 可用元标签： __meta_kubernetes_namespace：入口对象的名称空间。 __meta_kubernetes_ingress_name：入口对象的名称。 _meta_kubernetes_ingress_label ：入口对象的标签。 _meta_kubernetes_ingress_annotation：入口对象的注释。 __meta_kubernetes_ingress_scheme：入口的协议方案，如果设置了TLS配置，则为https。 默认为http。 __meta_kubernetes_ingress_path：来自入口规范的路径。 默认为/。 有关Kubernetes发现的配置选项，请参见下文： 123456789101112131415161718192021222324252627282930313233# 访问Kubernetes API的信息。# API服务器地址。 如果保留为空，则假定Prometheus在集群内部运行并自动发现API服务器，并在/var/run/secrets/kubernetes.io/serviceaccount/上使用pod的CA证书和不记名令牌文件。[ api_server: &lt;host&gt; ]# 应该被发现的实体的Kubernetes角色。role: &lt;role&gt;# 用于向API服务器进行身份验证的可选身份验证信息。请注意，`basic_auth`，`bearer_token`和`bearer_token_file`选项是互斥的.password和password_file是互斥的。# 可选的HTTP基本认证信息。basic_auth: [ username: &lt;string&gt; ] [ password: &lt;secret&gt; ] [ password_file: &lt;string&gt; ]# 可选的承载令牌认证信息。[ bearer_token: &lt;secret&gt; ]# 可选的承载令牌文件认证信息。[ bearer_token_file: &lt;filename&gt; ]# 可选的代理URL。[ proxy_url: &lt;string&gt; ]# TLS配置。tls_config: [ &lt;tls_config&gt; ]# 可选命名空间发现 如果省略，则使用所有名称空间。namespaces: names: [ - &lt;string&gt; ] 其中``必须是endpoints，service，pod，node或ingress。 有关为Kubernetes配置Prometheus的详细示例，请参阅此示例Prometheus配置文件。 您可能希望查看第三方Prometheus操作，它可以在Kubernetes上自动执行Prometheus设置。 static_config允许指定目标列表和它们的公共标签集。 这是在scrape配置中指定静态目标的规范方法。 1234567# 静态配置指定的目标。targets: [ - &apos;&lt;host&gt;&apos; ]# 分配给从目标中已抓取的所有指标的标签。labels: [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ] 重新标记是一种强大的工具，可以在抓取目标之前动态重写目标的标签集。 每个抓取配置可以配置多个重新标记步骤。 它们按照它们在配置文件中的出现顺序应用于每个目标的标签集。 最初，除了配置的每目标标签之外，目标的作业标签设置为相应的scrape配置的job_name值。 __address__标签设置为目标的&lt;host&gt;:&lt;port&gt;地址。 重新标记后，如果在重新标记期间未设置实例标签，则实例标签默认设置为__address__的值。 __scheme__和__metrics_path__标签分别设置为目标的方案和度量标准路径。 __param_&lt;name&gt;标签设置为名为的第一个传递的URL参数的值。 在重新标记阶段，可以使用带有__meta_前缀的附加标签。 它们由提供目标的服务发现机制设置，并在不同机制之间变化。 在目标重新标记完成后，将从标签集中删除以__开头的标签。 如果重新标记步骤仅需临时存储标签值（作为后续重新标记步骤的输入），请使用__tmp标签名称前缀。 保证Prometheus本身不会使用此前缀。 123456789101112131415161718192021# 源标签从现有标签中选择值。 它们的内容使用已配置的分隔符进行连接，并与已配置的正则表达式进行匹配，以进行替换，保留和删除操作。[ source_labels: &apos;[&apos; &lt;labelname&gt; [, ...] &apos;]&apos; ]# 分隔符放置在连接的源标签值之间。[ separator: &lt;string&gt; | default = ; ]# 在替换操作中将结果值写入的标签。# 替换操作是强制性的。 正则表达式捕获组可用。[ target_label: &lt;labelname&gt; ]# 与提取的值匹配的正则表达式。[ regex: &lt;regex&gt; | default = (.*) ]# 采用源标签值的散列的模数。[ modulus: &lt;uint64&gt; ]# 如果正则表达式匹配，则执行正则表达式替换的替换值。 正则表达式捕获组可用。[ replacement: &lt;string&gt; | default = $1 ]# 基于正则表达式匹配执行的操作。[ action: &lt;relabel_action&gt; | default = replace ] 是任何有效的RE2正则表达式。 它是replace，keep，drop，labelmap，labeldrop和labelkeep操作所必需的。 正则表达式固定在两端。 要取消锚定正则表达式，请使用。* .*。 确定要采取的重新签名行动： replace：将regex与连接的source_labels匹配。 然后，将target_label设置为replacement，将匹配组引用（${1}，${2}，…）替换为其值。 如果正则表达式不匹配，则不进行替换。 keep：删除regex与连接的source_labels不匹配的目标。 drop：删除regex与连接的source_labels匹配的目标。 hashmod：将target_label设置为连接的source_labels的哈希模数。 labelmap：将regex与所有标签名称匹配。 然后将匹配标签的值复制到替换时给出的标签名称，替换为匹配组引用（${1}，{2}，…）替换为其值。 labeldrop：将regex与所有标签名称匹配。匹配的任何标签都将从标签集中删除。 labelkeep：将regex与所有标签名称匹配。任何不匹配的标签都将从标签集中删除。 必须小心使用labeldrop和labelkeep，以确保在删除标签后仍然对指标进行唯一标记。 度量重新标记应用于样本，作为摄取前的最后一步。 它具有与目标重新标记相同的配置格式和操作。 度量标准重新标记不适用于自动生成的时间序列，例如up。 一个用途是将黑名单时间序列列入黑名单，这些时间序列太昂贵而无法摄取。 警报重新标记在发送到Alertmanager之前应用于警报。 它具有与目标重新标记相同的配置格式和操作。 外部标签后应用警报重新标记。 这样做的一个用途是确保具有不同外部标签的HA对Prometheus服务器发送相同的警报。 alertmanager_config部分指定Prometheus服务器向其发送警报的Alertmanager实例。 它还提供参数以配置如何与这些Alertmanagers进行通信。 Alertmanagers可以通过static_configs参数静态配置，也可以使用其中一种支持的服务发现机制动态发现。 此外，relabel_configs允许从发现的实体中选择Alertmanagers，并对使用的API路径提供高级修改，该路径通过__alerts_path__标签公开。 write_relabel_configs是在将样本发送到远程端点之前应用于样本的重新标记。 在外部标签之后应用写入重新标记。 这可用于限制发送的样本。 有一个如何使用此功能的小型演示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 要发送样本的端点的URL.url: &lt;string&gt;# 对远程写端点的请求超时。[ remote_timeout: &lt;duration&gt; | default = 30s ]# 远程写入重新标记配置列表。write_relabel_configs: [ - &lt;relabel_config&gt; ... ]# 使用配置的用户名和密码在每个远程写请求上设置`Authorization`标头.password和password_file是互斥的。basic_auth: [ username: &lt;string&gt; ] [ password: &lt;string&gt; ] [ password_file: &lt;string&gt; ]# 使用配置的承载令牌在每个远程写请求上设置`Authorization`头。 它与`bearer_token_file`互斥。[ bearer_token: &lt;string&gt; ]# 使用配置的承载令牌在每个远程写请求上设置`Authorization`头。 它与`bearer_token`互斥。[ bearer_token_file: /path/to/bearer/token/file ]# 配置远程写入请求的TLS设置。tls_config: [ &lt;tls_config&gt; ]# 可选的代理URL。[ proxy_url: &lt;string&gt; ]# 配置用于写入远程存储的队列。queue_config: # 在我们开始删除之前每个分片缓冲的样本数。 [ capacity: &lt;int&gt; | default = 10000 ] # 最大分片数，即并发数。 [ max_shards: &lt;int&gt; | default = 1000 ] # 最小分片数，即并发数。 [ min_shards: &lt;int&gt; | default = 1 ] # 每次发送的最大样本数。 [ max_samples_per_send: &lt;int&gt; | default = 100] # 样本在缓冲区中等待的最长时间。 [ batch_send_deadline: &lt;duration&gt; | default = 5s ] # 在可恢复错误上重试批处理的最大次数。 [ max_retries: &lt;int&gt; | default = 3 ] # 初始重试延迟。 每次重试都会加倍。 [ min_backoff: &lt;duration&gt; | default = 30ms ] # 最大重试延迟。 [ max_backoff: &lt;duration&gt; | default = 100ms ] 有一个与此功能集成的列表。 12345678910111213141516171819202122232425262728293031# 要发送样本的端点的URL.url: &lt;string&gt;# 可选的匹配器列表，必须存在于选择器中以查询远程读取端点。required_matchers: [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ]# 对远程读取端点的请求超时。[ remote_timeout: &lt;duration&gt; | default = 1m ]# 本地存储应该有完整的数据。[ read_recent: &lt;boolean&gt; | default = false ]# 使用配置的用户名和密码在每个远程写请求上设置`Authorization`标头.password和password_file是互斥的。basic_auth: [ username: &lt;string&gt; ] [ password: &lt;string&gt; ] [ password_file: &lt;string&gt; ]# 使用配置的承载令牌在每个远程写请求上设置`Authorization`头。 它与`bearer_toke_filen`互斥。[ bearer_token: &lt;string&gt; ]# 使用配置的承载令牌在每个远程写请求上设置`Authorization`头。 它与`bearer_token`互斥。[ bearer_token_file: /path/to/bearer/token/file ]# 配置远程写入请求的TLS设置。tls_config: [ &lt;tls_config&gt; ]# 可选的代理URL。[ proxy_url: &lt;string&gt; ] 有一个与此功能集成的列表。 Prometheus API清理prometheus数据 确保 prometheus 启动的时候， 加了参数 –web.enable-admin-api 清理这个key的全部的数据 12curl -X POST \ -g &apos;http://192.168.2.100:9090/api/v1/admin/tsdb/delete_series?match[]=up&amp;match[]=mysql_global_status_threads_running&#123;instance=&quot;test-db13:9104&quot;,job=&quot;mysql&quot;&#125;&apos; 清理这个key指定时间段的数据 （清理的时间戳区间：1557903714 到 155790395 ） 12curl -X POST \ -g &apos;http://192.168.2.100:9090/api/v1/admin/tsdb/delete_series?start=1557903714&amp;end=1557903954&amp;match[]=mysql_global_status_threads_running&#123;instance=&quot;test-db13:9104&quot;,job=&quot;mysql&quot;&#125;&apos; Prometheus Alerts可以参考这个https://github.com/samber/awesome-prometheus-alerts]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之5G基金]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B5G%E5%9F%BA%E9%87%91%2F</url>
    <content type="text"><![CDATA[5G相关指数市场上涉及5G相关的指数有三个： （1）中证全指通信设备指数（931160） 中证官网：http://www.csindex.com.cn/zh-CN/indices/index-detail/931160 选取中证全指样本股中的通信设备行业股票组成，以反映该行业股票的整体表现。 场内：国泰中证全指通信设备ETF（515880） 场外：国泰中证全指通信设备ETF联接A（007817），国泰中证全指通信设备ETF联接C（007818） （2）中证5G通信主题指数（931079） 中证官网：http://www.csindex.com.cn/zh-CN/indices/index-detail/931079 选取产品和业务与5G通信技术相关的上市公司作为样本股，包括但不限于电信服务、通信设备、计算机及电子设备和计算机运用等细分行业，旨在反映相关领域的A股上市公司整体表现 场内：华夏中证5G通信主题ETF（515050） 场外：华夏中证5G通信主题ETF联接A（008086），华夏中证5G通信主题ETF联接C（008087） （3）中证通信技术主题指数（931144） 中证官网：http://www.csindex.com.cn/zh-CN/indices/index-detail/931144 中证通信技术主题指数选取产品或业务与5G、物联网、光通信、量子通信和卫星通信等新兴通信技术相关的上市公司，包括但不限于通信传输设备、通信终端设备、半导体、电子设备及制造商、光电子器件等行业，旨在反映相关领域的A股上市公司整体表现。 场外：东财中证通信A（008326），东财中证通信C（008327） 区别聊聊两个指数的区别： （1）中证全指通信设备指数是一个二级行业指数，中证5G通信主题指数是一个主题指数，5G主题的意思就是专注于5G相关行业； （2）通信设备指数的前10大权重股合计权重为52.08%，5G主题指数的前10大权重股合计权重为63.27，相比而言后者的5G持股更集中； （3）主要成分股权重比较： 中兴通讯：两个指数持有5G龙头中兴通讯的权重差不多，通讯设备指数持有9.51%，5G主题指数持有9.62%，相比而言后者持有权重略高； 信维通信：通讯设备指数持有7.09%，5G主题指数持有10.33%，相比较而言后者持有权重高不少。信维通讯的主营产品为天线、无线充电模组等射频元器件，可应用于移动终端、基站端以及汽车等领域，华为mate30采用21颗天线，其中14颗支持5G连接，超出市场预期；而iPhone 已经开始采用44 MIMO天线，5G时代44MIMO天线渗透持续提升，部分机型甚至采用8*8MIMO天线，带来天线价值量大幅提升。 另外，5G主题指数相比通讯设备指数的重仓股没有闻泰科技与工业富联，因为这两家公司是传统的设备生产企业5G概念并不突出（闻泰科技是中国领先的移动终端和智能硬件产业生态平台，工业富联公司是全球领先的通信网络设备、云服务设备、精密工具及工业机器人专业设计制造服务商） 再重复一次：中证5G通信主题指数，选取产品和业务与5G通信技术相关的上市公司作为样本股。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之互联网基金]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E4%BA%92%E8%81%94%E7%BD%91%E5%9F%BA%E9%87%91%2F</url>
    <content type="text"><![CDATA[中证海外中国互联网50人民币指数（H30533）易方达中概互联50ETF，中国互联网50指数，跟踪中证海外中国互联网50人民币指数，选取海外上市的50家中国互联网企业作为样本股，反映在海外交易所上市知名中国互联网企业的投资机会。 虽然，中国互联网50指数，是想选取50家公司作为成分股，但是呢，符合条件的公司，只有30多家，所以这个50指数就有点名不符实了哈。 需要说明的是，中国互联网50指数成分股业绩基数已经很高，尽管现在的业绩增速仍然很快，但未来应该会逐渐降低，但何时会降低，还是一个未知数。因此，如果有朋友也看好这只指数的话，建议尽量放低收益预期，尽管我相信互联网行业未来仍然会快速发展，但不要期待，未来能像之前那样高速地增长。 中国互联网50指数的构成非常简单，腾讯、阿里、百度、网易、京东，这5家公司占据了指数80%的权重。下图是该指数的前大权重股，他们占据了这个指数90%的权重。 互联网是一个强者恒强的行业，这些年来，回报最好的还是其中的龙头股公司，因为这一行有个特点，就是用户越多产品越好用，产品越好用用户也就越多，这样龙头公司的优势就可以不断自我强化，形成强大的护城河，最终转化为巨大的利润。国内的腾讯、阿里、百度，国外的Facebook、Google都是非常好的例子。 互联网这样的行业格局，有极高的概率会继续维持下去。如果看好互联网行业发展的话，长期投资其中的龙头股应该是不错的选择，而中国互联网的行业龙头股，都集中在了中国互联网50这只指数中。 投资工具 可以在场内1.30以下，场外1.00以下，坚持定投中概互联和它的场外联接基金。 跟踪中国互联网50指数的指数基金， 场内：易方达中概互联50ETF（代码：513050） 场外联接基金：易中概ETF联接人民币（A份额：006327，C份额：006328） 准备长期（半年以上）持有的朋友，可以申购A份额（006327），想短期持有的朋友，可以选择申购C份额（006328） 中证海外中国互联网指数（H11136）中证海外中国互联网指数选取海外交易所上市的中国互联网企业作为样本股，采用自由流通市值加权计算，以反映在海外交易所上市中国互联网企业的整体走势。 我个人认为性价比远远比不上中概互联。 原因一，是互联网是一个强者恒强的行业，这些年来，发展最好，投资回报最高的，还是腾讯阿里这几家龙头公司，而且，在相当长的一段时间内，我们还看不到这种行业规律的改变。因此，看好互联网行业的话，最好的选择还是投资腾讯阿里这些龙头公司。 交银中证海外中国互联网（164906）中，龙头公司的占比是大幅低于中概互联的。下图是交银中证海外中国互联网（164906）的前十大重仓股，BAT的仓位只有中概互联的一半。 原因二，是交银中证海外中国互联网（164906）的费率要比中概互联高得多。管理费+托管费，中概互联每年是0.85%，交银则是1.45%。 场内： 场外： 脚印中证海外中国互联网指数（164906） 中证科技龙头指数（931087）市场中第一个名字里面含有“科技”的ETF就是这个515000，上市规模10亿，没几个月的时现在规模都增长到60多亿了。 科技ETF的跟踪标的为科技龙头指数，这指数由沪深两市中电子、计算机、通信、生物科技等科技领域中规模大、市占率高、成长能力强、研发投入高的50只龙头公司股票组成，以反映沪深两市科技领域内龙头公司股票的整体表现，为指数化产品提供新的标的。 场内： 华宝中证科技龙头ETF（515000） 场外： 华宝中证科技龙头ETF联接A（007873）, 华宝中证科技龙头ETF联接C（007874） 中证嘉实新兴科技盈利成长100策略指数（931165）科技100跟踪的中证新兴科技100策略指数，该指数从沪深A股新兴科技相关产业中选取高盈利能力、高成长且兼具估值水平低的股票作为指数样本股，采用基本面加权。 从指数介绍发行采用的基本面加权，国内最早的基本面指数就是嘉实基金的，没错，这个科技100也是嘉实基金，目前最新规模为12亿左右。 科技100策略相比科技龙头而言，权重股里面没有恒瑞医药（难道是嫌弃恒瑞80倍PE估值高？），科技100策略的成份股为100只股票，科技龙头的成份股为50只股票，所以科技100策略的前10权重股的具体权重相对较低。但是海康威视、立讯精密、大华股份、长春医药这个几个核心资产还是在的。 场内： 嘉实新兴科技100ETF(515860) 场外：嘉实新兴科技100ETF联接A(007815) 中证科技100指数好名字都靠抢，科技ETF被人占了，科技100也被人占了，但是中证科技也不错啊，中证科技的管理人是华泰柏瑞基金公司，目前规模为22亿左右。 中证科技跟踪中证科技100指数，该指数从沪深两市的科技主题空间选取100只研发强度较高、盈利能力较强且兼具成长特征的科技龙头公司股票作为指数样本股，采用自由流通市值加权，以反映沪深两市研发强度较高、盈利能力较强且兼具成长特性的科技龙头公司的整体表现 科技100是按自由流通市值加权的，跟科技100策略采用基本面加权的方法不一样，此外科技100选股行业范围比科技龙头更宽，例如在成份股里面看到: 京东方、三一中国、上汽集团、东方财富、中国中车等等，并非向科技龙头成份股主要集中在电子、计算机、通信、生物科技中。 场内： 华泰柏瑞中证科技100ETF（515580） 场外： 中证研发创新100指数创新ETF跟踪是中证研发创新100指数由沪深两市中信息技术、通信、航空航天与国防、医药、汽车与汽车零部件等科技相关行业中研发营收比例高、行业代表性高的100只股票构成样本股，以反映科技行业代表性上市公司股票在A股市场的整体走势。 科技的名字用完了，可以用“创新”了…… 看看成份股，创新100与科技100还是非常相像的，但是科技100里面恒瑞是6.19%权重，创新100里面恒瑞是11.65%的权重，也都有中国中车、三一重工、东方财富啥的。 此外还要一个申万菱信的515200也跟踪中证研发创新100指数，唯一的区别银华的是深交所上市的，申万的是上交所上市的。 场内： 银华中证研发创新100ETF(159987),申万菱信中证研发创新100ETF(515200)]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔记之正则表达式]]></title>
    <url>%2FPython%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://www.runoob.com/python/python-reg-expressions.html Python 正则表达式正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。 re 模块使 Python 语言拥有全部的正则表达式功能。 compile 函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。 re 模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串做为它们的第一个参数。 本章节主要介绍Python中常用的正则表达式处理函数。 re.match函数re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。 函数语法： 1re.match(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 匹配成功re.match方法返回一个匹配的对象，否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 实例123456#!/usr/bin/python# -*- coding: UTF-8 -*- import reprint(re.match(&apos;www&apos;, &apos;www.runoob.com&apos;).span()) # 在起始位置匹配print(re.match(&apos;com&apos;, &apos;www.runoob.com&apos;)) # 不在起始位置匹配 以上实例运行输出结果为： 12(0, 3)None 实例12345678910111213#!/usr/bin/pythonimport re line = &quot;Cats are smarter than dogs&quot; matchObj = re.match( r&apos;(.*) are (.*?) .*&apos;, line, re.M|re.I) if matchObj: print &quot;matchObj.group() : &quot;, matchObj.group() print &quot;matchObj.group(1) : &quot;, matchObj.group(1) print &quot;matchObj.group(2) : &quot;, matchObj.group(2)else: print &quot;No match!!&quot; 以上实例执行结果如下： 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter 检索和替换Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。 语法： 1re.sub(pattern, repl, string, count=0, flags=0) 参数： pattern : 正则中的模式字符串。 repl : 替换的字符串，也可为一个函数。 string : 要被查找替换的原始字符串。 count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 实例1234567891011121314#!/usr/bin/python# -*- coding: UTF-8 -*- import re phone = &quot;2004-959-559 # 这是一个国外电话号码&quot; # 删除字符串中的 Python注释 num = re.sub(r&apos;#.*$&apos;, &quot;&quot;, phone)print &quot;电话号码是: &quot;, num # 删除非数字(-)的字符串 num = re.sub(r&apos;\D&apos;, &quot;&quot;, phone)print &quot;电话号码是 : &quot;, num 以上实例执行结果如下： 12电话号码是: 2004-959-559 电话号码是 : 2004959559 re.compile 函数compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern ）对象，供 match() 和 search() 这两个函数使用。 语法格式为： 1re.compile(pattern[, flags]) 参数： pattern : 一个字符串形式的正则表达式 flags : 可选，表示匹配模式，比如忽略大小写，多行模式等，具体参数为： re.I 忽略大小写 re.L 表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境 re.M 多行模式 re.S 即为 . 并且包括换行符在内的任意字符（. 不包括换行符） re.U 表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于 Unicode 字符属性数据库 re.X 为了增加可读性，忽略空格和 # 后面的注释 实例12345678910111213141516171819&gt;&gt;&gt;import re&gt;&gt;&gt; pattern = re.compile(r&apos;\d+&apos;) # 用于匹配至少一个数字&gt;&gt;&gt; m = pattern.match(&apos;one12twothree34four&apos;) # 查找头部，没有匹配&gt;&gt;&gt; print mNone&gt;&gt;&gt; m = pattern.match(&apos;one12twothree34four&apos;, 2, 10) # 从&apos;e&apos;的位置开始匹配，没有匹配&gt;&gt;&gt; print mNone&gt;&gt;&gt; m = pattern.match(&apos;one12twothree34four&apos;, 3, 10) # 从&apos;1&apos;的位置开始匹配，正好匹配&gt;&gt;&gt; print m # 返回一个 Match 对象&lt;_sre.SRE_Match object at 0x10a42aac0&gt;&gt;&gt;&gt; m.group(0) # 可省略 0&apos;12&apos;&gt;&gt;&gt; m.start(0) # 可省略 03&gt;&gt;&gt; m.end(0) # 可省略 05&gt;&gt;&gt; m.span(0) # 可省略 0(3, 5) 在上面，当匹配成功时返回一个 Match 对象，其中： group([group1, …]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)； start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0； end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0； span([group]) 方法返回 (start(group), end(group))。 再看看一个例子： 实例1234567891011121314151617181920212223&gt;&gt;&gt;import re&gt;&gt;&gt; pattern = re.compile(r&apos;([a-z]+) ([a-z]+)&apos;, re.I) # re.I 表示忽略大小写&gt;&gt;&gt; m = pattern.match(&apos;Hello World Wide Web&apos;)&gt;&gt;&gt; print m # 匹配成功，返回一个 Match 对象&lt;_sre.SRE_Match object at 0x10bea83e8&gt;&gt;&gt;&gt; m.group(0) # 返回匹配成功的整个子串&apos;Hello World&apos;&gt;&gt;&gt; m.span(0) # 返回匹配成功的整个子串的索引(0, 11)&gt;&gt;&gt; m.group(1) # 返回第一个分组匹配成功的子串&apos;Hello&apos;&gt;&gt;&gt; m.span(1) # 返回第一个分组匹配成功的子串的索引(0, 5)&gt;&gt;&gt; m.group(2) # 返回第二个分组匹配成功的子串&apos;World&apos;&gt;&gt;&gt; m.span(2) # 返回第二个分组匹配成功的子串(6, 11)&gt;&gt;&gt; m.groups() # 等价于 (m.group(1), m.group(2), ...)(&apos;Hello&apos;, &apos;World&apos;)&gt;&gt;&gt; m.group(3) # 不存在第三个分组Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;IndexError: no such group findall在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 注意： match 和 search 是匹配一次 findall 匹配所有。 语法格式为： 1findall(string[, pos[, endpos]]) 参数： string : 待匹配的字符串。 pos : 可选参数，指定字符串的起始位置，默认为 0。 endpos : 可选参数，指定字符串的结束位置，默认为字符串的长度。 查找字符串中的所有数字： 实例12345678910# -*- coding:UTF8 -*- import re pattern = re.compile(r&apos;\d+&apos;) # 查找数字result1 = pattern.findall(&apos;runoob 123 google 456&apos;)result2 = pattern.findall(&apos;run88oob123google456&apos;, 0, 10) print(result1)print(result2) 输出结果： 12[&apos;123&apos;, &apos;456&apos;][&apos;88&apos;, &apos;12&apos;] re.finditer和 findall 类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回。 1re.finditer(pattern, string, flags=0) 参数： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 实例1234567# -*- coding: UTF-8 -*- import re it = re.finditer(r&quot;\d+&quot;,&quot;12a32bc43jf3&quot;) for match in it: print (match.group() ) 输出结果： 123412 32 43 3 re.splitsplit 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下： 1re.split(pattern, string[, maxsplit=0, flags=0]) 参数： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 maxsplit 分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 实例12345678910&gt;&gt;&gt;import re&gt;&gt;&gt; re.split(&apos;\W+&apos;, &apos;runoob, runoob, runoob.&apos;)[&apos;runoob&apos;, &apos;runoob&apos;, &apos;runoob&apos;, &apos;&apos;]&gt;&gt;&gt; re.split(&apos;(\W+)&apos;, &apos; runoob, runoob, runoob.&apos;) [&apos;&apos;, &apos; &apos;, &apos;runoob&apos;, &apos;, &apos;, &apos;runoob&apos;, &apos;, &apos;, &apos;runoob&apos;, &apos;.&apos;, &apos;&apos;]&gt;&gt;&gt; re.split(&apos;\W+&apos;, &apos; runoob, runoob, runoob.&apos;, 1) [&apos;&apos;, &apos;runoob, runoob, runoob.&apos;] &gt;&gt;&gt; re.split(&apos;a*&apos;, &apos;hello world&apos;) # 对于一个找不到匹配的字符串而言，split 不会对其作出分割[&apos;hello world&apos;] 正则表达式对象re.RegexObjectre.compile() 返回 RegexObject 对象。 re.MatchObjectgroup() 返回被 RE 匹配的字符串。 start() 返回匹配开始的位置 end() 返回匹配结束的位置 span() 返回一个元组包含匹配 (开始,结束) 的位置 正则表达式修饰符 - 可选标志正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志： 修饰符 描述 re.I 使匹配对大小写不敏感 re.L 做本地化识别（locale-aware）匹配 re.M 多行匹配，影响 ^ 和 $ re.S 使 . 匹配包括换行在内的所有字符 re.U 根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B. re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 正则表达式模式模式字符串使用特殊的语法来表示一个正则表达式： 字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。 多数字母和数字前加一个反斜杠时会拥有不同的含义。 标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。 反斜杠本身需要使用反斜杠转义。 由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’\t’，等价于 ‘\t’)匹配相应的特殊字符。 下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。 模式 描述 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 […] 用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’ [^…] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。 re* 匹配0个或多个的表达式。 re+ 匹配1个或多个的表达式。 re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 re{ n} 精确匹配 n 个前面表达式。例如， o{2} 不能匹配 “Bob” 中的 “o”，但是能匹配 “food” 中的两个 o。 re{ n,} 匹配 n 个前面表达式。例如， o{2,} 不能匹配”Bob”中的”o”，但能匹配 “foooood”中的所有 o。”o{1,}” 等价于 “o+”。”o{0,}” 则等价于 “o*”。 re{ n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a| b 匹配a或b (re) 对正则表达式分组并记住匹配的文本 (?imx) 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。 (?-imx) 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。 (?: re) 类似 (…), 但是不表示一个组 (?imx: re) 在括号中使用i, m, 或 x 可选标志 (?-imx: re) 在括号中不使用i, m, 或 x 可选标志 (?#…) 注释. (?= re) 前向肯定界定符。如果所含正则表达式，以 … 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。 (?! re) 前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功 (?&gt; re) 匹配的独立模式，省去回溯。 \w 匹配字母数字及下划线 \W 匹配非字母数字及下划线 \s 匹配任意空白字符，等价于 [\t\n\r\f]. \S 匹配任意非空字符 \d 匹配任意数字，等价于 [0-9]. \D 匹配任意非数字 \A 匹配字符串开始 \Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。 \z 匹配字符串结束 \G 匹配最后匹配完成的位置。 \b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \B 匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \n, \t, 等. 匹配一个换行符。匹配一个制表符。等 \1…\9 匹配第n个分组的内容。 \10 匹配第n个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式。 正则表达式实例字符匹配 实例 描述 python 匹配 “python”. 字符类 实例 描述 [Pp]ython 匹配 “Python” 或 “python” rub[ye] 匹配 “ruby” 或 “rube” [aeiou] 匹配中括号内的任意一个字母 [0-9] 匹配任何数字。类似于 [0123456789] [a-z] 匹配任何小写字母 [A-Z] 匹配任何大写字母 [a-zA-Z0-9] 匹配任何字母及数字 [^aeiou] 除了aeiou字母以外的所有字符 [^0-9] 匹配除了数字外的字符 特殊字符类 实例 描述 . 匹配除 “\n” 之外的任何单个字符。要匹配包括 ‘\n’ 在内的任何字符，请使用象 ‘[.\n]’ 的模式。 \d 匹配一个数字字符。等价于 [0-9]。 \D 匹配一个非数字字符。等价于 [^0-9]。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。 \w 匹配包括下划线的任何单词字符。等价于’[A-Za-z0-9_]’。 \W 匹配任何非单词字符。等价于 ‘[^A-Za-z0-9_]’。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔记之json字符串相互转换]]></title>
    <url>%2FPython%E7%AC%94%E8%AE%B0%E4%B9%8Bjson%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[json和字符串之间的转换，主要使用到json库中的json.loads()函数json.dumps()含函数。 所以我们首先要导入json库。 1import json 一、字符串转为json,使用 json.loads() 函数 首先我们新建一个字符串，考虑全面一点，里面包含中文和英文，这会涉及到后面字符串转json时候的中文编码问题。 123456789101112131415string = """[&#123; "name": "冯振振", "age": "23", "job": "Python engineer", "motto": "I like coding"&#125;,&#123; "name": "康康", "age": "23", "job": "web engineer", "motto": "专业前端，不至于前端"&#125;] 字符串转成json，只需要使用json.loads()函数即可，传入字符串，输出json格式 123json_list = json.loads(string)# 输入结果如下[&#123;'name': '冯振振', 'age': '23', 'job': 'Python engineer', 'motto': 'I like coding'&#125;, &#123;'name': '康康', 'age': '23', 'job': 'web engineer', 'motto': '专业前端，不至于前端'&#125;] 使用json.loads()将字符串转换为json之后，所属的数据类型是list类型，即`` 二、json转字符串,使用 json.dumps() 函数，必要时需要传入ensure_ascii=False, indent=2参数 我们使用上面转换得到的json格式数据，将它转换为字符串。 1[&#123;'name': '冯振振', 'age': '23', 'job': 'Python engineer', 'motto': 'I like coding'&#125;, &#123;'name': '康康', 'age': '23', 'job': 'web engineer', 'motto': '专业前端，不至于前端'&#125;] 使用json.dumps()，可以将json格式转换为字符串格式。但是，如果我们的字符串中包含中文，转换后的字符串中，中文不会显示出来，只会显示中文的编码。 1[&#123;"name": "\u51af\u632f\u632f", "age": "23", "job": "Python engineer", "motto": "I like coding"&#125;, &#123;"name": "\u5eb7\u5eb7", "age": "23", "job": "web engineer", "motto": "\u4e13\u4e1a\u524d\u7aef\uff0c\u4e0d\u81f3\u4e8e\u524d\u7aef"&#125;] 这个时候，我们可以给json.dumps()函数传入ensure_ascii=False参数，即可解决这个问题。 123new_string = json.dumps(json_list,ensure_ascii=False)# 输入的结果如下[&#123;"name": "冯振振", "age": "23", "job": "Python engineer", "motto": "I like coding"&#125;, &#123;"name": "康康", "age": "23", "job": "web engineer", "motto": "专业前端，不至于前端"&#125;] 这个时候我们发现，转换后的字符串全部都显示在一行上面，看上去不直观。我们可以再给json.dumps()函数传入indent=2参数，设置字符串的缩进。 12345678910111213141516new_string = json.dumps(json_list,ensure_ascii=False,indent=2)# 输出的结果如下[ &#123; "name": "冯振振", "age": "23", "job": "Python engineer", "motto": "I like coding" &#125;, &#123; "name": "康康", "age": "23", "job": "web engineer", "motto": "专业前端，不至于前端" &#125;]]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之中证500]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E4%B8%AD%E8%AF%81500%2F</url>
    <content type="text"><![CDATA[参考 https://mp.weixin.qq.com/s?__biz=MzU4Mzg0NDIyMA==&amp;mid=2247486579&amp;idx=1&amp;sn=24446e21f6f2c7c74c526520a1ab46c9&amp;chksm=fda3a460cad42d764d5b07c006353722dac9895b0f93a8965af83b4fcba62f1c99ad2cd837d3&amp;scene=21#wechat_redirect https://xueqiu.com/7084328572/133811192 概念中证500 中证500指数由全部A股中剔除沪深300指数成份股及总市值排名前300名的股票后，总市值排名靠前的500只股票组成，综合反映中国A股市场中一批中小市值公司的股票价格表现。 500低波动 中证500行业中性低波动指数在中证500指数二级行业内选取低波动特征的股票为样本，保持行业中性的同时，行业内股票采用波动率倒数加权。 点评中证500 行业分布很平均，属于中小市值指数，高弹性，涨很快，跌得也快。 500低波动 指数成立时间较短，历史数据上看，相比中证500有一定优势。 比较https://xueqiu.com/7084328572/133811192 结论 500低波动的历史收益比中证500的收益高不少。 牛市期间中证500的涨、跌会大于500低波动。 在现阶段，500低波动对比中证500，可谓又好又便宜，所以500低波动更值得投资。 选基中证500 场外基金：推荐161017富国中证500，长期业绩稳定跑赢中证500指数。跌的少，涨的多，选他就是这么简单。 场内基金：159922嘉实500ETF，三费合计0.2%，便宜便宜成交量不小，是买他的主要理由。 500低波动 场外基金：003318景顺长城中证500低波 场内基金：512260华安中证低波ETF]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[债券之可转债]]></title>
    <url>%2F%E5%80%BA%E5%88%B8%E4%B9%8B%E5%8F%AF%E8%BD%AC%E5%80%BA%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://mp.weixin.qq.com/s?__biz=MzU4Mzg0NDIyMA==&amp;mid=2247485137&amp;idx=1&amp;sn=ee281a07df7ea2ef8c39689a0d2c2648&amp;chksm=fda3aec2cad427d474fd1765a1bae41e5d8cc23dd80db70dacbc5ddcc859d4733fcef5034ab0&amp;scene=21#wechat_redirect https://mp.weixin.qq.com/s/NRvgiR7HajUIbS8QeF7Yxg https://mp.weixin.qq.com/s/dsYye1IpcAYetNB6WzWfPg 最近可转债发行又再次密集，在当前行情，我建议积极参与网上可转债申购。可转债申购的黄金期：风险低&amp;中签高 现在申购可转债：1. 中签率高 2. 潜在收益还可以 3.风险较小（3200申购，2800上市平均亏损25元） 同时可转债是非常好的投资品种，如果想进一步了解可转债的特点，可以详细看这一篇。主要讲解可转债的条款，对理解可转债的风险和机会很有帮助。 下面以113024核建转债为例进行介绍，可转债条款基本都是大同小异，大体逻辑都是一样的。 定义可转换债券是债券持有人可按照发行时约定的价格将债券转换成公司的普通股票的债券。如果债券持有人不想转换，则可以继续持有债券，直到偿还期满时收取本金和利息，或者在流通市场出售变现。 利率可转债本质是债券，但多了一个认购期权 可转债 = 债券 + 认购期权 转股的权力并非免费，而是通过债券降低债券的利息体现 可转债利率 = 普通债券利率 - 认购期权的成本 可转债的利率会比普通债券低 利率又分为两个部分：票面利率和利率补偿 以核建转债为例 票面利率：第1年0.2%,第2年0.4%,第3年1%,第4年1.5%,第5年1.8%,第6年2% 利率补偿：可转债期满后5个交易日内,公司将按本次发行的可转债的票面面值的105%(含最后一期年度利息)的价格向投资者赎回全部未转股的可转债。 如果持有到期，持有6年实际利息为：0.2+0.4+1+1.5+1.8+5=9.9 有3个地方需要注意： 最后一年的实际利息是利率补偿，是5%。但是为什么还要弄一个票面利率2%？带着问题看后面的条款你就知道了。 6年利息仅9.9%，相比普通债券这个利息是很低的，低的原因就是认购期权的费用。 利率虽然低，但是作为一个债券，到期偿还的特性使得价格在100以下时，会有非常强的抗跌性。 假设价格跌到90元，但是持有6年还本付息109.9。 实际收益率 = （109.9 - 90） / 90 = 22.11% 收益率大幅提高了 假设可转债到期时间仅剩3年，最后3年的利息是8.3元，如果价格还是90元。 实际收益率 = （108.3 - 90） / 90 = 20.33%。 3年收益率高达20.33%，非常有吸引力了 所以当可转债价格跌下100元后，特别的抗跌！因为持有到期是最极端的假设，绝大部分可转债都是以转股结束，转股的权力很有价值。 转股价转股价：可转债可以按照转股价将债券转换成股票 1张可转债可以转换多少股股票，按照面值100元**除以转股价。** 和交易价格没关系，即使价格130，转股也是按照面值100元转换。 核建转债的转股价是9.93元，即100/9.93=10.07 每1张可转债可以转换10.07股中国核建的股票。 当前中国核建股价7.99元，10.07股价值 = 10.07 * 7.99 = 80.46元 80.46元，就是核建转债的转股价值。 当前核建转债的交易价格为101.8元。 交易价格远高于转股价值，如果此时把可转债换成股票卖掉，亏得妈妈都不认识了。 交易价格这么高的原因： 债券的特点在托底 绝大部分可转债，最后都是转股结束，所以转股的权力值钱。 后面还有其他条款，继续看 可转债的抗跌性有多强，看这个溢价就很有感觉了，目前新发的可转债转股价值一般在100左右。 转股价调整当公司出现因派送股票股利、转增股本、增发新股或配股等情况(不包括因本次发行的可转债转股而增加的股本)使公司股份发生变化或派送现金股利时,公司将按下述公式进行转股价格的调整 。 这个条款是当正股股价因为各种原因除权而向下调整时，转股价同步调整 转股期限2019-10-14至2025-04-07 一般情况下，发行半年后进入转股期，核建转债还未满半年，是转不了股的 转股价特别向下修正条款当公司股票在任何连续三十个交易日中至少有十五个交易日的收盘价低于当期转股价格的80%时，公司董事会有权提出转股价格向下修正方案并提交公司股东大会审议表决 修正后的转股价格应不低于审议上述方案的股东大会召开日前二十个交易日和前一交易日公司股票交易均价,同时修正后的转股价格不低于公司最近一期经审计的每股净资产值和股票面值 核建转债转股价9.93元的80%为7.94元，如果中国核建的股价15天/30天低于8.06元，上市公司就有可能会把转股价格下调。 这是可转债的核心条款，不同的可转债的触发条件略有不同，但本质都是一样的，当股价大幅下跌时，上市公司就有可能把转股价格向下调整，让可转债依然具有投资价值。 问题：要靠上市公司主动调整，如果不调整投资者又能怎么办？继续往下看。 回售条款本次发行的可转债最后两个计息年度,如果公司股票在任何连续三十个交易日收盘价格低于当期转股价格的70%时,可转债持有人有权将其持有的可转债全部或部分按债券面值加上当期应计利息的价格回售给公司。 如果股价很低，但上市公司一直不修正转股价怎么办？大部分可转债都有回售条款（小部分没有）。 如果股价太低，又一直不修正转股价，投资者可以按照面值+利息，把可转债回售给公司，不陪你玩了！ 这里就是投资者和上市公司互相博弈的结果，逼迫上市公司一定要向下修正转股价。回售条款越宽松，债券的利率就会越低。回售条款越难，或者没有回售条款的，债券利息就越高。 核建转债回售触发的时间是最后2年，所以： 在回售触发之前，上市公司不修正转股价，投资者是拿他们没办法的。 在最后2年，如果还不修正，再见，不陪你玩了！ 股价下跌–&gt;触发向下修正条款–&gt;上市公司修正转股价，转股价值上升–&gt;转股概率提高 股价下跌–&gt;触发向下修正条款–&gt;上市公司不修正转股价–&gt;继续下跌–&gt;触发回售–&gt;将债券回售，不陪你玩了 核建转债转股价9.93元的70%为6.95元，存续期最后2年如果中国核建的股价30天低于7.05元，投资者就可以把债券回售给上市公司。 提前赎回条款在本次发行可转债的转股期内,如果公司股票在任何连续三十个交易日中至少有十五个交易日的收盘价格不低于**当期转股价格的130%(含130%),或未转股金额不足人民币3,000万元时,公司有权按照债券面值加当期应计利息**的价格赎回全部或部分未转股的可转债 这个是上市公司要求可转债投资者在上涨的时候赶紧转股的条款，转股之后上市公司就不用还钱了。 所以转股价值高于130后，可转债的交易价格与正股价格高度同步。因为随时可能被逼转股。 同时，交易价格远离100元面值，安全垫丧失，下跌时缺少债性保护。 这时候的可转债，和股票是差不多的。 前面的小问题：为什么第6年的利率2%，但是又要搞个利率补偿5%出来呢？ 利率补偿5%是实际的最后一年利率，但是因为赎回和回售条款，都是按照面值+当期利率来确定价格。上市公司在赎回和回售的时候可以节省成本。 申购可转债以123006东财转债为例，在上涨与下跌行情时，可转债的表现如下 当股价上涨时 红绿色K线是东财转债，蓝色K线是东方财富股价走势。在上涨时，东财转债涨的速度和东方财富一样快。 当股价下跌时 2018年熊市时的走势，在东方财富下跌35%时，东财转债跌幅不到10% 小结 股市下跌的时候，是债券 股市上涨的时候，是股票 这就是可转债 网上申购什么是网上申购可转债？ 在可转债上市之前，按照100元面值进行申购。申购的过程就像抽奖，平均每3-5次顶格申购，可中签1次，即10张100元面值共1000元的可转债。 为什么要申购？ 为了赚钱。 截止2019年4月19日，今年新上市的可转债一共38只，上市首日开盘价及成交均价在111元以上。100元面值的可转债，平均可按照111元的价格卖出。即每中签1000元，平均盈利约108元。 小结：所以网上申购可转债，就是抽奖买100元的可转债，上市第一天直接卖掉，赚取中间的差价。目前平均盈利为80元左右. 如何操作第一步：必须在有一个证券账户，有的话可以直接操作 开户后第二天，可以申购可转债。开户当天无法申购。 第二步：不同的交易软件如果操作有所不同。以兴业证券软件为例，如下图： 第三步： 可转债是否有申购价值，请参考公众号的跟踪表，我每个交易日都会进行推送。 按照软件指引进行申购 申购价格：100元 申购数量（上限）：10000张 注1：这里的申购上限金额是100万，不需要交钱。平均3-5次才会中签1000元，如未中签，不会产生任何费用。 注2：每个人单只可转债仅第一次申购有效，不能重复申购。但可以用家人账户多开户，多次申购。 第四步：等中签结果 假设 周一进行申购 如果没有中签，那什么都不会发生。 如果中签：周二晚，会收到中签短信 第五步：转钱 周三，根据提示转入中签金额，一般是1000元，个别情况会2000元或更多。 注：如果中签后不想交钱，可以弃缴。一个人12个月内可放弃2次。如果3次不缴款，则暂停申购资格6个月。 第六步：等待上市 今年可转债从申购日到上市日之间的等待时间，平均为24天。也就是申购后不到一个月的时间就可以卖出。 第七步：卖出 上市后，直接用股票账户卖出，结束一次循环。 2019年可转债中签后平均盈利108元左右。 举个例子，定投君中签的一只可转债： 中签10张通威转债，1000元盈利191.2元。 风险在哪 12个月内3次没有缴款，会禁止6个月的申购权限。 今年上市73只可转债，有13只亏损 亏损的13只中，仅雅化转债，永鼎转债一直没涨上100，其他的都有100元以上的机会卖出 常见问题 为什么开了户，申购可转债是废单？ 开户后第二天，可以申购可转债。开户当天无法申购。 申购10000张，就要100万，这也太多了吧，没钱交怎么办。 申购上限金额是100万，申购时不需要交钱，0元门槛。平均3-5次申购才会中签1000元。中签后需要交的钱就是1000元。 中签之后没有钱交怎么办？ 每个人在12个月内，有2次机会不缴款。3次不缴款也没有实质性处罚，处罚为暂停6个月申购权限。 我有几个账户，可以多申购几次吗？ 每个人仅第一次申购有效，重复申购无效。但可用亲人的账户开户，进行申购。 怎么开户证券账户？ 任意证券账户都可以申购可转债，如果还没有账户，可以点击阅读原文进行开户。需要准备身份证+银行卡。 创业板转债不能申购？ 创业板权限要临柜开通，找到你最近的营业部，去开户。或者咨询客户经理。]]></content>
      <categories>
        <category>债券</category>
      </categories>
      <tags>
        <tag>债券</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金投资九月策略]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E6%8A%95%E8%B5%84%E4%B9%9D%E6%9C%88%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[参考 https://mp.weixin.qq.com/s/aJWreUvmtiSZIcBoL5z0JA 总体策略八月份市场整体维持震荡行情，就目前来看，暂时还无法打破这一格局。无论从上市公司的盈利情况、市场的流动性，还是内外环境，都不具备立即发起一轮牛市的条件。就指数而言，上证指数四月份创造的3288，很可能就是全年的最高点。我在五月底就说了：一季度把全年的饭都吃完了。 但这并不意味着个股就没有机会。白酒、消费、医药等板块依然维持长牛的格局，近期科技板块也在蠢蠢欲动，只是金融地产等板块比较疲软，导致指数的形态不太好看。这种行情就是主动管理型基金大展身手的好时机，一批优秀基金的净值近期也在迭创新高。最近三个多月，价值五剑也把沪深300指数甩了七、八个点。 最近出现的新情况是成长股的表现优于价值股、题材股要好于蓝筹股。很多人问：市场是否会转向成长风格？最近一个多月出现的这种情况是正常的。以中证500、创业板为代表的小盘股已经连续三年落后于以沪深300为代表的大盘股了，两种风格交替表现也是正常的。但要形成像2015年那种极致的成长股行情可能性不大。大的趋势还是业绩驱动股价，只要业绩好就有机会，不论大盘还是小盘。基金风格配置还是以均衡配置为宜。 还有一个问题是H股是否值得布局。像恒生国企指数就估值而言确实是比较低了，但就交易形态而言还是属于左侧交易。要想走出右侧交易的形态，可能还有待时日。最起码，中国香港地区的局势要有好转。所以目前还只能按左侧交易来对待。 仓位A股长期看好，可保持适度积极的仓位，但不满仓。 基金推荐1、经典A股价值投资基金 华安策略优选（040008） 东方红沪港深（002803） 易方达蓝筹精选（005827） 汇添富价值精选（519069） 嘉实价值优势（070019） 2、A股量化和指数增强基金 景顺长城量化新动力（001974） 兴全沪深300（163407） 富国沪深300增强（100038） 3、沪港深基金 华安沪港深通精选（001581） 国富沪港深成长（001605） 4、QDII基金（港股为主） 工银全球配置（QDII）（486001） 广发全球精选股票（QDII）（270023） 5、经典A股成长风格基金 中欧时代先锋（001938） 兴全商业模式（163415） 睿远成长价值（007119） 博时新兴消费（004505） 富国天惠成长（161005） 6、股债均衡型基金 广发稳健增长（270002） 景顺长城能源基建（260112） 7、二级债基 易方达稳健收益（110007） 工银双利（485111） 8、对冲基金 海富通阿尔法对冲（519062）]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理财相关公众号博客]]></title>
    <url>%2F%E7%90%86%E8%B4%A2%E7%9B%B8%E5%85%B3%E5%85%AC%E4%BC%97%E5%8F%B7%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[公众号 研报社 银行螺丝钉：定投十年赚十倍（低估表） 一石二鸟：二鸟说 望京博格：绿巨人星球 ETF拯救世界：长盈指数投资 定投从零开始 投基行动派（低估表） AI搬砖工养基 蚂蚁财学社 思哲与创富（低估表） 唐书房 麦可撩财 博客 https://www.jianshu.com/p/8fd1a116a572 https://www.jianshu.com/u/3d9a3c3cf8a8 常用网站 理杏仁 天天基金 晨星网 且慢 同花顺 神奇公式]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>理财</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从余额宝收益率看债券牛熊市]]></title>
    <url>%2F%E4%BB%8E%E4%BD%99%E9%A2%9D%E5%AE%9D%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9C%8B%E5%80%BA%E5%88%B8%E7%89%9B%E7%86%8A%E5%B8%82%2F</url>
    <content type="text"><![CDATA[参考 https://mp.weixin.qq.com/s/kdSCq5XfYBK_uxMo_cCl3A 债券的利率与价格我们知道债券的特点就是按照固定的频率，付固定的利息。如果我们投资一个债券，未来能够收到多少利息以及本金是固定的，未来的现金流是不会改变的。 假设某债券，现在价格100元，一年之后还本付息105元。投资这个债券的收益如下： 在正常市场： 持有1年，赚5元的利息收入。 利率为：105 / 100 -100% = 5% 在债券牛市： 牛市中债券价格从100元涨到102元 盈利为两部分， 价格上升，因为随时按照102元卖出，所以价差2元立刻确认为盈利。 如果继续持有1年， 未来的本息收入还是105元。但是目前的市场价格变为102元。利率降低为 3 /102 = 2.94% 短期有较高的价差盈利，但是继续持有债券的收益从5%下降为2.94%，长期收益率下降。 这里比较绕，希望能理解： 债券处于牛市状态时，利率下降，债券价格上涨。 因为未来的本息收入是固定的，短期价格上涨，是把未来的收益提前变现。继续持有债券的收益将会变低。 当债券价格处于熊市时，债券的价格会下跌，短期表现不好，甚至亏钱。但是因为利率提高了，未来的收益率将会上升。 余额宝收益率与债券牛熊市债券市场虽然非常巨大，但因为专业性太强，我们普通人对债券牛熊市没有直接接触。债券的价格，利率更是毫无感觉。 事实上感受债券市场牛市熊市最简单直观的指标，就是余额宝收益率。 当余额宝利率下降时，债券市场处于牛市，债券基金表现很好 当余额宝利率上升时，债券市场处于熊市，债券基金表现不好 当余额宝利率不变时，处于横盘状态，投资债券就是赚利息 上图为余额宝7日年化收益率的变化与长期纯债基金收益率的对比。 2014年，2015年余额宝收益率大幅下降，长期纯债债券基金的表现也非常好。2014年平均收益12.63%，2015年平均收益9.82% 2016年，2017年，余额宝利率上升时，债券基金的收益率就很不理想。平均收益低于2%，买债券基金还不如存余额宝。 2018年，2019年余额宝利率也有所下降，目前债券基金的表现也很好。 上图为2014年至今，余额宝的收益率走势图。 但是，目前余额宝收益率2.3%左右。目前的收益率接近2015年最低水平。这说明： 当前利率水平已经很低，有可能短期会继续降低，但空间已经不大。 如果利率水平维持不变。当前低利率环境下，仅靠持有债券的收利息，收益率也会很低。 万一利率上升，买债券基金也有亏损风险。 总结 余额宝利率可以比较直观的反应债券市场。 可以看看债券基金在余额宝利率上升的2016年2017年，表现是不是比较差 市场利率较低，牛市继续上涨空间不大，长期持有利率又太低，万一利率上涨，债券基金也会亏钱。 整体上看，投资债券基金应该谨慎]]></content>
      <categories>
        <category>债券</category>
      </categories>
      <tags>
        <tag>债券</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之主动型基金]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E4%B8%BB%E5%8A%A8%E5%9E%8B%E5%9F%BA%E9%87%91%2F</url>
    <content type="text"><![CDATA[参考 https://mp.weixin.qq.com/s/XOW3z_SfnGCdaTVZyHsDmA https://mp.weixin.qq.com/s?__biz=MzU4Mzg0NDIyMA==&amp;mid=2247484343&amp;idx=1&amp;sn=306c2a3d950876bea169be8d8ce084fb&amp;chksm=fda3aba4cad422b2f577d87f60f407316b892096e597dda2f7c9ab04856c1b1df1978cf26eb8&amp;scene=21#wechat_redirect 区别主动基金和被动基金有什么区别？ 主动基金的持仓是不可跟踪的 被动指数基金：指数基金，因为是复制所跟踪的指数走势。指数基金的持仓基本上和指数的成分股及权重类似，投资策略是透明的。 主动基金：每季度仅公布一次十大重仓股，基金经理有权对持有的股票进行调仓。并且季报年报的公布时间也有延迟。比如很多基金8月份才公布半年报，显示的是6月30日的十大重仓。说不定7月份基金经理就把重仓股卖了。 主动基金的仓位是会变动的 被动指数基金：基本永远保持在95%的高仓位运行，上图为兴全沪深300指数的仓位，基本保持不变。 主动基金：有可能50%仓位，也可能95%的仓位。基金经理根据对未来行情的判断，可以主动的调整仓位。上图是兴全趋势的仓位，在2015年下跌行情时，仓位一度下降到了32%。 主动基金涨跌，并不完全跟随指数 被动指数基金：指数涨，指数基金必然涨，指数跌，指数基金必然跌。 主动基金：主动基金虽然有业绩基准，但是涨跌并不跟随。有可能指数今天涨了，基金净值却按兵不动。也有可能指数不动，基金净值却蹭蹭上涨。 主动基金分化巨大 被动指数基金：虽然有好有坏，但都是跟踪指数的，烂也烂不到哪。 主动基金：好基金，非常好。比如兴全趋势，净值在过去十几年不断的创历史新高。 烂基金：非常烂。某主动基金，跌又跌得多，涨又涨的少。指数都没他这么惨。 选基金经理最核心的观点：买主动型基金，就是买基金经理。 基金经理有不同的风格，当市场风格与基金经理风格不一致时，短期跑输指数是很正常的。因为主动基金是黑盒策略，如果无法建立起基本的信任，给予基金经理一定的时间和忍耐度，我不建议你买主动基金，要不会一直很痛苦。 你要知道，在网上到处咨询，现在是不是高点要不要卖，是不是低点，要不要买。问大V小V的观点 真实水平无法验证，是嘴炮键盘侠还是真正的民间高手？ 即使一时收益高，是因为赌对了还是水平高呢？ 专业水平真有这些十几年从业的基金经理厉害吗？ 你只需要支付一点点管理费，买了他管理的基金，他就成了你的打工仔，需要帮你管理资金，多简单。 太细节的事就交给这些打工仔基金经理去操作就好了。把短期高与低的判断，仓位高低，未来什么行业什么股票表现好，谁能真的预测呢？把烦心事交给基金经理去烦恼吧。 专业的事交给最专业的人来做，不用再每天纠结今天涨了怎么办，要不要卖，跌了怎么办，要不要买。 思考点作为养鸡集团的董事长，你需要思考的只有两点： 1. 现在是熊市低点，还是牛市高点？决定你整体的仓位高低 在牛市高位，人人都知道是高点，但是基金仓位限制，无法完全空仓，有部分基金经理的风格就是满仓穿越牛熊（比如朱少醒），需要我们在高位进行减仓。 在熊市低位，人人都知道是低点，但是不买的话，牛市来了，也和你无关啊。 所以牛市高位与低位，需要我们主动操作。 至于3000点3200点这种不高不低的时候要怎么办？你看这些基金经理净值不是都新高了吗，他们做的挺好，不需要我们操心。 2. 以年为周期，对旗下子公司的负责人进行定期考核。**表现烂的就开除，换人。** 基金的分化十分巨大，有的基金，基金经理会换人。有的基金经理业绩好是一时赌对了运气好，而不是水平高。我们就需要对基金进行定期的考核。 以年为周期，不要过于关注短期的表现。如果确实能力不行，就卖掉。 判断持有基金经理 看基金经理变动 看新任基金经理过往业绩是否优秀 长期业绩看该基金的长期每年的收益和基金排名是否比较稳定，比如前1/2,。 定投模拟器计算定投收益率 总结挑选主动管理基金的注意点如下： 首先长期业绩要优秀 基金经理是否频繁变更，新上任的基金经理是否有过往业绩可以参考 没有过往业绩的新手基金经理需特别注意 长期业绩很差的基金可直接卖出 基金经理业绩时好时坏不稳定，也建议卖出。 我们希望的主动管理型基金应该是这样的： 基金经理稳定，不频繁变更 有3年以上的过往业绩可供参考 业绩排名稳定靠前，最好每年都在前1/2，不要忽上忽下。 关注点： 当养鸡集团的董事长很简单，抓大放小 只关注战略上的高点：牛市高点与熊市低点。 只抓人事权：挑选最优秀的基金经理来给你打工 其他的琐碎小事，让打工仔去做就好了。因为他们更专业。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之基金经理与大V]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E5%9F%BA%E9%87%91%E7%BB%8F%E7%90%86%E4%B8%8E%E5%A4%A7V%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://www.jisilu.cn/question/303754 大V银行螺丝钉管理组合“螺丝钉指数基金组合”，简称钉大 ETF拯救世界管理“长盈指数投资计划”，简称E大 望京博格管理组合“绿巨人”，简称望京博格 基金经理朱少醒富国天惠成长LOF（161005） 董程非兴全趋势（163402） 曹名长中欧价值发现混合A（166005） 傅友兴广发稳健增长（270002） 李晓星银华中小盘精选混合（180031） ##谢治宇 参考： https://mp.weixin.qq.com/s?__biz=MzU4Mzg0NDIyMA==&amp;mid=2247486343&amp;idx=1&amp;sn=4e38d5a5700783d78037c32039407e53&amp;chksm=fda3a394cad42a820fabe326045929870a899548ec8393efe7b4446e41cf72b182686f372c0f&amp;scene=21#wechat_redirect 兴全合宜混合A（163417） 兴全合润分级混合 张慧参考： https://mp.weixin.qq.com/s/duT1bbC6U3Hza0PA7SVm1A 华泰柏瑞创新升级混合型基金（000566） 华泰柏瑞创新动力（000967） 华泰柏瑞战略新兴产业（005409） 新作： 华泰柏瑞研究精选混合（007968） 焦巍参考： https://mp.weixin.qq.com/s/qWmh6OLqeA6cuNUHrXTevg 银华富裕混合 董伟炜 https://mp.weixin.qq.com/s/OrCSnCR_jxvpQdysXraVqg 光大保德信行业轮动 光大景气先锋混合基金(007854)]]></content>
      <categories>
        <category>基金经理</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之投资策略组合]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E6%8A%95%E8%B5%84%E7%AD%96%E7%95%A5%E7%BB%84%E5%90%88%2F</url>
    <content type="text"><![CDATA[参考 投基行动派公众号 且慢投资组合 价值五剑主理人 一石二鸟 购买渠道 且慢 天天基金 绿巨人主理人 望京博格 购买渠道 且慢 天天基金 蛋卷 盈米股基精选组合主理人 盈米 购买渠道 且慢 盈米稳健八心八箭主理人 盈米 购买渠道 且慢 我要稳稳的幸福主理人 交银施罗德基金 购买渠道 天天基金 且慢 蛋卷 长赢指数投资计划（150份与S定投）主理人 ETF拯救世界 购买渠道 且慢 极简投资组合主理人 简七理财 购买渠道 且慢 #蛋卷投资组合 螺丝钉指数基金组合主理人 银行螺丝钉 购买渠道 蛋卷 大树底下好乘凉主理人 孥孥的大树 购买渠道 蛋卷 十年定投成长主理人 揭幕者 购买渠道 蛋卷 奶爸教育基金主理人 趋势交易的奶爸 购买渠道 蛋卷 辣妈精选基金组合主理人 辣妈有财商 购买渠道 蛋卷 林奇的指数基金组合主理人 林奇 购买渠道 蛋卷 全球精选主理人 Alex 购买渠道 蛋卷 天天基金投资组合老司基风云一号主理人 老司基一枚 购买渠道 天天基金 行动派均线定投主理人 投基行动派 购买渠道 天天基金]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之50类指数]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B50%E7%B1%BB%E6%8C%87%E6%95%B0%2F</url>
    <content type="text"><![CDATA[参考 https://mp.weixin.qq.com/s/LMhAYiwT0UCvrTFaUKfpYw 指数科普1.上证50：2004年1月成立，挑选上海证券市场规模大、流动性好的最具代表性的50 只股票组成样本股。 二个条件： 1、来自上证180指数样本股 2、沪市总市值、成交金额综合排名前50位。 2.上证50AH优选：2004年12月成立，使用AH价（A指A股、H指港股）差投资策略的整体表现，为投资者提供建立在上证50基础上的额外回报。个人理解：和上证50选的公司一样，但50AH优选会选择同时在A股H股上市、股价更便宜的公司。 二个条件： 1、 选的50家公司与上证50一致。 2、 会在A股和H股中做选择，那边便宜买那个 3.央视50：2012年6月成立，央视财经50指数从“成长、创新、回报、公司治理、社会责任”5个维度对上市公司进行评价，每个维度选出10家、合计50家A股公司构成样本股。在指数中，5个维度具有相同的初始权重，均为20%。在维度内，单只样本股的权重不超过30%。 由中央电视台财经频道联合北京大学、复旦大学、中国人民大学、南开大学、中央财经大学五大院校，以及中国注册会计师协会、大公国际资信评估有限公司等专业机构，共同评价遴选。 三个条件： 1、 财务绩效优异、治理水平较高、回报丰厚，拥有一流管理团队、品牌、战略的优秀上市公司（说的比较虚）。 2、 入选由五大院校学术单位设计的“创新、回报、成长、治理、责任”五大指标体系。 3、 券商研究所、公募基金、私募基金的50家权威机构+五大院校+中央电视台财经频道再评选。 一张图总结上证50、基本面50、50AH优选、央视50： 收益 基金挑选 基金对比 最终选择 易方达上证50指数A(110003) 华夏上证50AH优选指数A(501050) 招商央视财经50指数A(217027)]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之投资组合]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E6%8A%95%E8%B5%84%E7%BB%84%E5%90%88%2F</url>
    <content type="text"><![CDATA[稳如狗系列一上证50+中证红利或上证红利（1:1） 这套组合主打一个稳。 红利指数往往拥有较高的股息率（接近4%），股息率=股息/市值，在市场持续下跌的环境下，股价下跌分母下降，股息率不断攀升，反向推高资产的收益率，因此往往展现出极强的抗跌属性，所以红利指数是稳健型投资者定投必备。 如果是场内定投可以选择上证红利，如果是场外定投可以选择中证红利。 而上证50指数包含几家A股最大的上市公司，市值大的公司往往更加抗跌，在市场反弹的浪潮中也经常打头阵，比如前段时间上证50指数就狂涨15%。 组合优点：长期定投盈利确定性极高，收益率稳健。 组合缺点：缺乏收益想象空间。 总结：适合收益率要求不高，需要跑赢通胀，稳健投资的人群。 相关标的： 大成中证红利（090010）； 华泰柏瑞上证红利（510880）； 博时上证50ETF联接A（001237）。 稳如狗系列二上证50AH优选：标普A股红利机会或中证红利低波动（1：1） 这套组合是策略型指数，是对稳如狗系列一的替换。 50AH优选本质和上证50并无太大区别，但比50指数更加聪明，要知道部分上证50成分股是同时在A股和港股上市的，但是两者会出现估值差异。 50AH指数把上证50中同时在港股和A股上市的股票中更加低估的筛选出来，也就是港股比A股便宜买港股，反之则持有A股，进行切换轮动获取超额收益。 而标普红利机会和中证红利指数关联度较高，但由于红利机会要求股票利润、盈利增长均为正，且严格分散个股权重，因此也是优质的红利策略指数，近十年历史收益率为16%，显著高于前者。 另外，除了标普红利之外，中证红利低波动也是不错的选择，它将中证红利指数中波动率较低的股票筛选出来，而低波动代表着不容易暴涨暴跌，价值投资者更愿意长期持有，因此历史收益也跑赢了同期中证红利指数。 所以红利策略指数我们可以将标普红利和中证红利低波动进行二选一来替代中证红利。 组合优点：收益率想象空间更足。 组合缺点：长期定投盈利不确定性高于被动指数组合。 相关标的： 华夏上证50AH优选（501050）； 标普A股红利机会（501029）； 红利低波（512890）。 稳中求进系列一沪深300：中证500（1:1） 这套组合简直和泰坦尼克号一样经典，沪深300代表着沪深两市具有良好流动性，市值前300的300只股票，而中证500则代表着排除沪深300成分股之外，具备良好流动性，市值前500的股票。 此组合可以说是攻守兼备，中证500目前PE、PB分位等指标已经降低到历史空前的低位，而中证500成分股属于中小盘股，而且中证500近几年的利润增速一直保持一个不错的态势，成长性不错。 而沪深300虽然估值尚未下滑到历史低位，但贵在一直表现稳健，从成立至今也可以获得年化10%左右的回报率。 如果你打算按照1：1定投的话，直接买中证800也未尝不可，我个人的建议还是分开买，哪个涨的少可以多买些，另一个则少买些，保证沪深300和中证500的市值在1:1左右。 组合优点：涵盖A股绝大部分标的，完美追踪A股大盘走势。 组合缺点：中规中矩。 总结：预期收益率适中，风险波动适中。 相关标的： 天弘中证500（000962）； 易方达沪深300（110020）。 稳中求进系列二沪深300：500低波动或价值回报指数：标普红利机会或红利低波（1:1:1） 这个组合的区别在于把中证500换替换成中证500行业中性低波动指数或者价值回报指数（神奇公式指数），然后再加上一只红利指数型基金进行定投。 500低波动主要是筛选中证500成分股中波动率较低的股票组合，500低波的实盘回测近十年年化收益率接近16%，远高于500指数。 同理，价值回报指数主要是选择高ROE，低PE的股票仓位进行组合，近十年实盘回测也超过16%。 价值回报指数和500低波动都和中证500关联性很高，两者都是投资中小盘股为主，因此可以二选一作为中证500的替代产品。 优点：投资范围广阔，涵盖大部分标的同时，优中选优，有机会获得更高的超额收益机会。 缺点：价值回报指数的追踪费用较高，其他策略指数有失效的可能性。 相关标的： 易方达沪深300（110020）； 天弘中证500（000962）； 价值回报（006255）； 500低波（003318）； 标普红利（501029）； 红利低波（005561）。 突飞猛进系列一沪深300：中证500：医药300：中证环保：新能源车（2:2:1:1:1） 这个组合就是在稳中求进一系列再布局了环保、新能源和养老三只高成长性的指数，新能源指数15~17年的利润增速为42.8%，环保为14.2%，医药300主要布局医药行业龙头股，面临老龄化时代将有很大增长潜力。 但高速发展的产业往往带有剧烈波动，其中环保还为强周期行业，因此高收益背后同样蕴含着高风险，此组合适合风险承受能力更强，且有长期布局计划的人群。 优点：行业指数成长性高，布局长线，想象空间大。 缺点：短期波动、风险较大。 总结：适合长期布局，能够接受短期大幅回撤，预期收益率较高的人群。 相关标的： 天弘中证500（000962）； 易方达沪深300（110020）； 300医药（001344）； 广发中证环保（001064）； 国泰国证新能源车（160225）。 突飞猛进系列二中证红利：上证50：医药300：中证环保：新能源车（2:2:2:1:1:1） 此组合就是在核星组合上加上三个行业指数，逻辑同上。 小结行业指数的成长性是高度不确定的，大家如果不愿意接受太大的波动，我优先建议选择稳如狗系和稳中求进系，毕竟行业指数的波动和风险远高于宽基类指数。 这里大家会发现，其实很多策略指数和宽基指数可以相互转化和替代。 比如中证红利和标普红利机会，中证500和500低波，上证50和50AH等等。 策略指数基金牛掰的地方在历史业绩好，选股逻辑也站得住脚，但缺点在于费率通常更高，而且容易出现回撤牛逼，实盘超鬼的现象。 至于如何做搭配选择，就见仁见智了。 另外还有很多人问我，一个定投组合才两、三只宽基，怎么那么少！ 实际上完全够用，要清楚我们投的并非是单一股票，也不是单一行业指数，我们定投的可是宽基指数，宽基指数里面已经包含几十上百只股票了，所以在这个基础上你再分散很多只宽基，有意义吗？ 大家只要银子掏出来，坚持反人性的投宽基指数，那么必然会获得丰厚的回报。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之XIRR公式计算年化收益率]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8BXIRR%E5%85%AC%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%B9%B4%E5%8C%96%E6%94%B6%E7%9B%8A%E7%8E%87%2F</url>
    <content type="text"><![CDATA[买了这么多理财，你清楚收益多少吗？ 不会算收益率，就容易被表象迷惑。 就像年金险，看似到期后返还的钱翻了一倍。拿起计算机一算，还达不到余额宝的收益。 一年下来，想必七友们都投了不少理财产品。每个产品的收益都不一样，我们怎么计算自己的年化收益率呢？ 年化收益率是指投资期限为1年所获得的收益率。在很多固定收益类理财产品中经常会看到这个名词，但其实很多人不知道怎么去计算。 一笔投资的年化收益率计算起来相对简单些，套用公式，计算机一按结果就出来了。 年化收益率=(投资内收益/本金)×（365/投资天数）×100% 比如说花了10000元买入某只股票，90天后卖出，赚了502元。 取得的年化收益率是：(502/10000)×（365/90）×100%=20.36% 实际上，我们投资往往是分批，分期进行的，不止收益率不一样，投资的时间长度也不一样，所以我们要算的就是综合年化收益率。 看起来貌似很复杂，其实借助Excel里的一个公式——XIRR函数就可以全部搞定！ 这个公式能帮助我们计算不同产品的投资时间、投资金额不定的投资收益。 XIRR（Values,Dates,Guess），即收益率=（现金流，时间，预估值） guess用处不大，可以忽略不计。那么，XIRR公式需要两组数据，一组是现金流，一组是投资日期。 让我们来看一个例子： 1、打开Excel，输入投资金额和对应的投资日期。支出为负数，收回为正数（注意：投资日期要使用日期格式）； 2、用XIRR函数算出这半年的投资回报率； 3、选择计算区间； 4、得出结果]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之基本面指数]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E5%9F%BA%E6%9C%AC%E9%9D%A2%E6%8C%87%E6%95%B0%2F</url>
    <content type="text"><![CDATA[[TOC] 基本概念1、基本面60（399701）：以深市A股为样本空间，分别挑选基本面价值最大的60家上市公司作为样本。基本面价值由四个财务指标（营业收入、现金流、净资产、分红）来衡量，并决定了样本股的权重。 2、基本面120（399702）：以深市A股为样本空间，分别挑选基本面价值最大的120家上市公司作为样本。 3、基本面200：以深市A股为样本空间，分别挑选基本面价值最大的200家上市公司作为样本。 4、基本面50（000925）：以沪深A股为样本空间，分别挑选基本面价值最大的50家上市公司作为样本。 三个条件： 1、上市时间超过一个季度，除非该股票自上市以来的日均 A 股总市值在 全部沪深 A 股中排在前 30 位； 2、非 ST、*ST 股票，非暂停上市股票。 3、 按照四个财务指标（营业收入、现金流、净资产、分红）来衡量，选取排名在前60 名的股票作为深证基本面 60 指数的样本股；选取排名在前 120 名的股票作为深证基本面 120 指数的样本股；选取排名在前 200 名的股票作为深证基本面 200指数的样本股。 一张图总结深证基本面指数： 往年收益基本面50 基本面60 基本面120 结论： 基本面50 基本面50+基本面60/基本面120 选择 比较 结论 嘉实中证锐联基本面50指数(LOF)A（160017） 建信深证基本面60ETF联接A（530015） 什么时候买尽管基本面50的基期是2004 年12月31日，但指数的正式发布是在2009年2月26日，因此，在wind上查询到的基本面50指数估值数据，也是从2009年开始的。 从2009年到现在，蓝筹股经历了3次牛市，分别是2009年、2014年和2016年3月至2018年1月（这轮蓝筹股的牛市有没有终结，不好说）；2次熊市，分别是2010年初至2014年中和2015年中至2016年2月底。 我个人认为，一个指数如果经历了2轮牛熊的考验，且时间跨度接近10年，其估值数据，在一定程度上就具备了参考价值。 下图是基本面50指数自发布以来，市盈率与市净率走势的变化。 因为基本面50指数中，金融、能源、地产等强周期性行业，占据了近70%的权重，因此我认为用市净率（PB）进行估值，更为合理。 基本面50指数自发布以来，平均市净率为1.59，市净率中位数为1.36，市净率最高值为3.68，出现在2009年8月3日，最低值0.95，出现在2014年5月7日。 目前基本面50指数的市净率为1.25，低于历史均值和中位值，高于历史最低值31%；2016年年初，A股熔断大跌时，基本面50的最低PB值为1.05，比现在低约16%。 考虑到基本面50的资产质地和估值，我认为，可以通过战略底仓+定投的方式进行投资。我们可以先制定一个投资计划，想好大致会用多少钱来投资，然后用其中20%的资金先用于一次性买入，剩下的钱，再每个月进行定投。当然，具体首次买入比例、定投时间跨度等等细节，大家还是根据自己的实际状况来确定比较好。 目前跟踪基本面50指数的基金，只有一只，就是嘉实基本面50，这只基金既有场外份额，也有场内份额，但因为场内的流动性非常差，昨天一天的成交额才22万，所以，如果想投资嘉实基本面50的话，还是从基金账户里，直接申购比较好，申购代码：160716。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之沪深300]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E6%B2%AA%E6%B7%B1300%2F</url>
    <content type="text"><![CDATA[[TOC] 基本概念沪深300：2005年4月成立，由沪深证券交易所发布，所选取沪深两市规模大、流动性好的300支股票作为样本股。（指数基日是2004.12，也就是收集数据从2004.12起） 五个条件： 股市交易时间超过一个季度 非ST、*ST股票，非暂停上市股票 公司经营状况良好，最近一年无重大违法违规事件、财务报告无重大问题;股票价格无明 显的异常波动或市场操纵，剔除其他经专家认定不能进入指数的股 选样标准为选取规模大、流动性好的股票作为样本股。 300价值指数：2004年12月成立，由中证指数有限公司发布，以沪深300指数样本股中价值因子评分最高的100只股票为成分股。300价值指数可以理解成沪深300的低估值增强策略指数。 二个条件： 从沪深300中去选100支价值成份最高的公司。 价值因子包含四个变量：股息收益率（D/P），每股净资产与价格比率（B/P），每股净现金流与价格比率（CF/P）、每股收益与价格比率（E/P）。 比较沪深300 VS 300价值是否应该把沪深300指数换成300价值指数，核心逻辑是看涨幅。 2010年至2019年8月2日，300价值指数上涨28.27%，同期沪深300指数上涨4.8%，300价值指数战胜沪深300指数。 在大部分时候，300价值的表现都是优于300价值指数的，跌得比沪深300少，涨的比沪深300多。 但是在2010年以及2019年，沪深300的表现又比300价值强。 结论： 10年中有8年跑赢，仅2年跑输。理论上讲，如果你买的是完全跟踪指数的被动型指数基金，那就有必要换成300价值指数。但是转换也并不是毫无风险的，比如2010年和2019年，300价值表现就比沪深300差。 增强型沪深300 VS 300价值买跟踪300价值的被动指数基金，就相当于买了自带增强策略的沪深300增强指数基金。 那么有必要从现有的沪深300增强指数基金，换成跟踪300价值的指数基金吗？ 以2只沪深300指数基金163407兴全沪深300，以及100038富国沪深300与老牌300价值指数基金519671银河300价值指数为例，收益对比如下： 从2011年至今，兴全沪深300与富国沪深300的收益率都在103%左右，高于银河沪深300价值。 从2010年至今，富国沪深300收益率84.96%，也高于银河沪深300指数。 从各个单独的年份上看，各有优略，增强型指数基金收益率并不比300价值指数基金差，互有高低。 这很容易理解，300价值指数是自带增强策略的指数基金。其他的指数增强基金也有自己的增强策略。相比锁定低估值增强策略的300价值指数，普通的沪深300增强指数基金增强策略更灵活，可以根据市场风格进行调整。 优秀的沪深300增强指数基金可以做到每一年都跑赢沪深300指数。 而不像300价值一样，有时跑赢，有时跑输。 结论： 300价值指数 = 沪深300的增强指数 跟踪300价值的指数基金 = 沪深300的增强指数基金 结论 从游戏规则上看，300价值指数估值必定是低于沪深300指数的 从涨幅上看，300价值指数大部分时候表现都优于沪深300指数 个别年份（2010/2019年），300价值指数表现也会比沪深300差 如果你买的是纯被动指数型基金，可以考虑换成300价值指数，但应该知道第3条风险。 如果投资的已经是沪深300增强指数基金，就没有必要换成300价值指数基金了。 300价值只是沪深300增强策略中的一种而已。 选择沪深300 按照成立时间，资产规模，涨跌幅来对基金进行排序，筛选出较好的基金 知乎、雪球、且慢、天天基金等等搜索 序号 基金名称 代码 1 申万菱信沪深300价值指数 310398 2 兴全沪深300指数增强 163407 3 景顺长城沪深300指数增强 000311 4 富国沪深300指数增强 100038 5 易方达沪深300量化增强 110030 6 易方达沪深300ETF联接 110020 8 博时沪深300指数A 050002 9 华夏沪深300ETF联接 000051 按照规模、跟踪误差、收益率、基金经理、费率等列出excel表格 最终选择 兴全沪深300指数增强（163407） 景顺长城沪深300指数增强（000311） 300价值 根据如下几点进行判断 基金收益 基金规模 基金成立时间 基金经理 基金评级 最终选择 银河沪深300指数基金（519671） 什么时候买大家习惯用市盈率和市净率来进行估值，因为沪深300指数的成分股中金融股权重很高，我认为用市净率进行估值相对合理。 现在余额宝的年化收益率大约是4%，如果沪深300指数未来可以保持10%的净资产收益率，并考虑20%风险溢价的话，那么，在2倍市净率以下投资沪深300指数基金，要比投资余额宝划算得多。 我们再看下相对估值。自沪深300指数发布以来，到2018-04-13收盘，沪深300指数每天市净率的平均值是2.31；中位数是1.85，这个数值在2005年、2006年、2011年、2012年和2015年初多次出现；最小值为1.17，这个数值在2014年5月多次出现；最大值为7.46，出现在2007年10月16日，具体可以看下面图表。 怎么买 如果波动承受能力弱，并且认为沪深300指数会低估很久，可以选择定投，具体方法有很多，比如说定期不定额，每个月都会投资，但数量会有不同，低估的时候多一些，估值贵了就少一些或干脆不投；也可以根据估值的变化，制定定投计划，比如说，在沪深300 1.8倍市净率以下可买入40%的仓位，1.5倍市净率时可加仓到60%，1.2倍市净率以下时，可满仓，当然我这只是举一个例子，具体还是大家自己来决定。 需要说明的是，定投可以降低账户的波动，但在单边上涨中也会降低整体的收益率，在低估时间较短时，也会因为没完成建仓，而充满遗憾。没办法，盈亏同源，甘蔗没有两头甜，这是必然的事情。 如果波动承受能力强，可以一次性建仓，但要承受巨大波动，一次性买入之前，一定要衡量好自己的波动承受能力，切实考虑好自己的现金流、负债等情况。 市场是有效的，好机会总是稀缺的，指数基金出现极佳买入机会的时间窗口，是非常短暂的，一旦错过，留给自己的就只剩下悔恨，因此我个人倾向于战略底仓（一次性建立底仓，比如说计划仓位的50%）与日常现金流定投（用工资等收入定期加仓）相结合的投资方法。 跟踪沪深300的指数基金有很多，场内场外都有，如果定投的话，建议大家选场外自动扣款的那种，因为这样可以克服自己人性的弱点，不受股市短期涨跌的影响，能真正坚持下来，对上班族来说，也更省心。 场内成交量最大的应该是300ETF(SH510300)，场外的就更多了。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投之红利指数]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E4%B9%8B%E7%BA%A2%E5%88%A9%E6%8C%87%E6%95%B0%2F</url>
    <content type="text"><![CDATA[科普1.上证红利：2017年1月成立，由股息率最高、现金分红最多的50只股票组成，是上证A股市场真正的核心优质资产，并每年根据最新排名优胜劣汰，具有极高的投资价值。 三个加入条件： (1)过去两年内连续现金分红而且每年的现金股息率(税后)均大于0; (2)过去一年内日均流通市值排名在上海A股的前50%; (3)过去一年内日均成交金额排名在上海A股的前50% 2.深证红利：2002年11月成立，40支深证成熟的绩优股或分红能力较强的成长股。 五个加入条件： （1）在深圳证券交易所上市交易; （2）有一定上市交易日期(一般为三个月); （3）非ST、*ST 股票; （4）公司最近一年无重大违规、财务报告无重大问题; （5）为保证深市市场指数的稳定和发展，各类指数都保持与深证成指和中小板指一定的重合率。 3.中证红利：2005年5月，由上海、深圳交易所中现金股息率高、分红比较稳定、具有一定规模及流动性的100只股票作为样本，反映A股市场高红利股票的整体状况和走势。 4.标普A股红利：标普红利指数，也叫标普红利机会指数，请注意，这不是跟踪美股的指数，而是标普公司针对中国股市开发的指数。这个指数也是选取股息率最高的100只股票。 四个加入条件： （1）过去3年盈利增长必须为正 （2）过去12个月的净利润必须为正 （3）每只股票权重不超过3% （4）单个行业不超过33%等等条件。 一张图总结四个红利指数： 深证红利采取的是股息和市值加权，而中证红利采取的是股息率加权，这就导致了深证红利主要还是看规模选股，选出来的主要是格力，美的，五粮液等估值相对比较高的股票，如果你选择深证红利，就等于选择市值较大，拥有稳定分红能力的蓝筹股 中证红利则主要看股息率大小选股，选出来的是中国神华，江铃汽车，新希望这类估值比较低的股票，如果你选择中证红利，就等于拥抱高股息策略，但市值相对偏小的股票 收益 年化收益率排名：标普A股红利&gt;深证红利&gt;中证红利&gt;上证红利 稳定性排名：上证红利&gt;中证红利&gt;标普A股红利&gt;深证红利 小结：综合风险与收益，可以考虑标普A股红利和中证红利指数基金。 估值以下数据来源理杏仁网站，收集时间为2019-09-17。 上证红利 PE-TTM:7.31，小于机会值7.45，判断为低估状态，分位点11.23%。 中证红利 PE-TTM:7.80，小于机会值7.97，判断为低估状态，分位点12.19%。 深证红利 PE-TTM:16.04，大于中位值15.68，判断为高估状态，分位点62.74%。 标普A股红利 无数据 对比天天基金网对比 晨星网对比 最终选择 大成中证红利指数（090010） 富国中证红利指数（100032）]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金之行业指数]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E4%B9%8B%E8%A1%8C%E4%B8%9A%E6%8C%87%E6%95%B0%2F</url>
    <content type="text"><![CDATA[参考链接 https://mp.weixin.qq.com/s/ncACmzT8GoIBLCatvxu_xA 很多朋友都热衷于行业主题或行业指数基金。原因是这类基金持仓集中，爆发力强，一旦押中风口便能赚快钱。但一个硬币都有两个面，涨跌同源。这些基金由于持股高度集中于某个板块，波动较大，一旦跌起来也是毫不含糊的。 对于新手和小白，不建议过多参与这类基金。对于段位较高的投资者，投资这类基金也是有很多技巧的。其中有两个问题，大家要先想清楚。 一是选哪个行业？也就是要找准赛道。有的人很自信，来回切换去追风口。对追风口这种方法，我从来不提倡。太多的事实已证明：偶尔可能会做成一、两次，但长期下来不可避免的陷入追涨杀跌的怪圈。 二是选主动基金还是选被动基金。不同行业由于行业性质和研究壁垒的不同，主被动基金的业绩差别也很大。即使是同一行业，选主动基金还是选被动基金，这个也是有很大技巧的。 选行业我不建议去追风口。最简单的方法就是选具有“长牛特征”的行业。用现在流行的话来说就是“核心资产”。由于这些行业能穿越牛熊、不断创新高，即使高位被套，后面也还有机会解套。 下面我就四个具有“长牛特征”行业的有关主被动基金给大家分析一下。 消费行业消费行业具有长牛属性，这个争议不大。实际上消费是一个很大的行业，和人类衣食住行、吃喝拉撒有关的上市公司都可以纳入大消费的范畴。消费行业的选股范围可以比肩宽基基金。 主动基金 易方达消费行业（110022）、汇添富消费行业（000083）、富国消费主题（519915） 被动基金（跟踪指数：中证食品饮料指数，中证主要消费指数） 天弘中证食品饮料指数（001631）、汇添富中证主要消费ETF联接（000248） 主被动ＰＫ：消费行业主被动基金基本打了个平手。以汇添富为例，近三年涨幅汇添富消费行业是96.14％，汇添富中证主要消费ETF联接是104.96％，同一家公司的主被动基金基本相当。 选择：消费行业选主被动基金都行。被动基金攻击性要强一些；主动基金可以自带防守，攻守均衡一点。 医药行业医药行业也是属于大消费的范畴，而且具有更为强烈的刚需属性，也是牛股辈出的沃土。相比于大消费行业，医药行业的选股范围要窄很多，所以波动要大一些。 主动基金 中欧医疗健康（003095）、汇添富创新医药（006113）、中海医疗保健（399011） 被动基金（跟踪指数：中证医药卫生指数，沪深300医药卫生指数） 鹏华中证医药卫生（160635）、易方达沪深300医药ETF（512010） 主被动ＰＫ：医药行业主动基金完虐被动基金。原因很简单：医药行业研究壁垒较高，且子行业众多，差别较大，主动基金具有较大的操作空间。特别是具有医药背景的基金经理更是如鱼得水。 选择：医药行业建议选择主动基金。 科技行业科技行业具有长牛属性，这一点在以美国的成熟市场表现非常明显。在A股由于各种原因，科技类股票的表现远不如消费类那么显眼，但未来仍有较大的发展空间。科技类股票具有较强的成长属性，同时波动也较大。和消费一样，科技行业定义也较为宽泛，选股面较宽。 主动基金 信达澳银新能源（001410）、华安媒体互联网（001071） 被动基金（跟踪指数：中证科技龙头指数，中证人工智能主题指数） 华宝中证科技龙头ETF（515000）、平安人工智能ETF（512930） 主被动PK：科技类主被动基金不具有可比性。因为科技的定义太宽泛了，主被动基金都很少有比较纯的科技类的。上面列的两个被动指数是相对比较纯的科技类ETF，都是今年推出的，还没有长期业绩。 选择：被动基金对科技的定义要相对纯正一些，比较透明，不确定因素较少。从确定性的角度考虑，建议选择被动基金。另外，科技类指数越是后面推出的，编制原则可能会更契合市场风格一些。 金融行业以银行、保险为代表的金融行业具备较高的ROE，且盈利比较稳健。拉长时间来看，走势是震荡向上的。而且这也是A股的国情和特色。因为金融行业在指数占权重较大，“无金融，不牛市”。金融行业在A股确实是走出了长牛行情。 主动基金 工银瑞信金融地产（000251）、嘉实金融精选（005662） 被动基金（跟踪指数：中证银行指数，中证800证券保险指数，沪深300金融地产指数） 天弘中证银行指数（001594）、鹏华证券保险（160625）、国投瑞银沪深300金融地产联接（161211） 主被动PK：主动基金里金融行业主题基金比较少，上述两个主动基金都完全跑赢了被动基金。上面两个基金是在银行、保险、证券地产四个行业里选股，相比于单一的银行或证券、保险，主动管理有一定的操作空间。 选择：金融行业建议选择主动基金。 总结生存和发展是人类永恒的主题，人类的一切行为都可以归结到上述两个主题。消费和科技，分别对应着人类的生存和发展，具备永恒的驱动动力，因此这两个行业是股市的常青树。而金融行业是“万业之母”，也是A股极具特色的主题。 上述四个行业就是我给大家推荐的赛道。目前公认的具备“核心资产”特征的股票，绝大多数都是出自这四个行业。投资行业主题或指数基金，你不要东一下西一下，长期持有（或定投）这四个主题的相关基金就行了。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投方法]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[TOC] 参考链接公众号 银行螺丝钉公众号（蛋卷基金APP 螺丝钉组合+股票开户） 简七读财公众号（且慢极简投资组合+股票开户） 基金豆公众号（小目标） 二鸟说公众号（天天基金 价值五剑） 三大估值方法 1、三大估值方法都是通过分析历史数据，从而判断指数当前估值。但是中国是不成熟的投资市场，它的发展特别快，仅靠分析历史数据得到的结论并不一定靠谱哟。 2、没有万能的公式，这三种方法得到的数据有一定的参考价值。 3、三大估值方法都有自己的适用范围，大家在使用时都要注意适用的指数，不要滥用。 4、目前，蚂蚁推崇的是银行螺丝钉的指数表。也有人通过分析指数市盈率和市净率所处的分位点来判断指数估值，蚂蚁并不看好通过这种方法做出来的指数估值表。 5、蚂蚁使用三大估值法数据都是来自理杏仁，但是也有一些其它的网站也在做指数估值数据，在数据上也有一定的差别。 6、对于我们来讲，我们无须每天计算指数的估值数据，只要在投资指数基金时计算一次即可。蚂蚁希望大家投资时要做个简单的估值表，而不是看着别人的估值表，看到低估就去买。 盈利收益率法盈利收益率是格雷厄姆常用的一个估值指标，它所代表的含义是 ，假如我们把一家公司全部买下来，这家公司一年的盈利能够带给我们的收益率，就是盈利收益率。 举个例子，假设一家公司的盈利是1亿元，公司的市值规模是8亿元，那么盈利收益率就是1亿元除以8亿元，也就是12.5%。换句话说，如果我们出8亿元买下这个公司，那这个公司的盈利可以每年带给我们12.5%的收益率。一般来说，盈利收益率越高，代表公司估值越低，公司越有可能被低估。 盈利收益率是用公司的盈利除以公司市值，也就是盈利收益率 = E/P 如果你炒股的话，应该会经常听到“A股的市盈率都好高”的说法，这里的的市盈率是用公司市值除以公司的盈利，也就是市盈率 = P/E ，俗称 PE，是不是看着很眼熟？没错， 市盈率是盈利收益率的倒数！！例如市盈率是8，那么盈利收益率就是12.5% 盈利收益率法，可简单描述为在盈利收益率高的时候开始定投，在盈利收益率低的时候停止定投，甚至卖出。 使用条件条件比较苛刻，只适合流通性比较好、盈利比较稳定的品种。如果是盈利增长速度比较快，或者盈利波动比较大的指数基金，则不适合使用盈利收益率法。 目前适合盈利收益率的品种，国内主要是上证红利、中证红利、上证50、基本面50、上证50AH优选、央视50、恒生指数和恒生中国企业指数等。 使用方法1）当盈利收益率大于10%时，分批投资 2）当盈利收益率小于10%时，但大于6.4%时，坚定持有已经买入的基金份额 3）当盈利收益率小于6.4%时，分批卖出基金 指标获取人区别于动物的一个主要特征就是人会使用工具，我们投资理财可不是为了去搞明白如何计算盈利收益率的，盈利收益率已经有专业人士和机构帮我们计算好了，我们只需要知道如何获取即可。当然，如果你非要钻牛角尖，也可以出门左转，如何计算 我也有总结。 方法1:部分关注指数基金投资的微信公众号 推荐 银行螺丝钉的 定投十年赚十倍 从这张估值表来看，绿色部分的指数基金流通性比较好、盈利比较稳定适合采用盈利收益率法进行估值，因为盈利收益率均大于10%，所以可以进行分批投资。而黄色部分中的上证50 也属于流通性比较好、盈利比较稳定的指数基金，但是它的盈利收益率为9.81%，不建议进行投资，如果之前已经购买了，则建议稳定持有。 方法2:指数官网 点我带你飞到 指数官网 方法3:金融终端 一般是付费的金融终端，如果你不是致力于深入研究的话，大可不必购买。 博格公式法在长期的投资投资过程中，约翰·博格发现决定股市长期回报的最关键的三个因素，分别是：初始投资时刻的股息率、投资期内的市盈率变化、投资期内的盈利增长率。 初始投资时刻的股息率影响我们的分红收益，投资期内的市盈率变化、投资期内的盈利增长率影响我们的资本利得收益。 基于这三个因素，博格提出了指数基金的收益公式 【未来年复合收益率】 = 初期股息率 + （平均）每年的市盈率变化率 + （平均）每年的盈利变化率 其中：股息率 = 现金分红 / 公司市值 ，市盈率 = 公司市值 / 公司盈利 又因为 分红率 = 股息/ 公司盈利，盈利收益率 = 公司盈利 / 公司市值 所以 股息率 = 盈利收益率 * 分红率 ，而一般公司的分红率都是多年保持不变的，因此股息率和盈利收益率正相关 使用条件盈利高速增长或盈利周期性的指数基金，如果指数当前的市盈率处于它历史市盈率波动范围的较低区域，就可以投资它。 目前适合博格公式法的指数基金：沪深300、中证500、创业板指数、红利机会指数、必需消费品指数、医药行业指数、可选消费指数、养老产业指数 使用方法从公式来看，未来年复合收益率和3个因素正相关：初期股息率、（平均）每年的市盈率变化率、（平均）每年的盈利变化率。投资开始的时候，能确定的两个因素分别是 初期的股息率和当前市盈率处于历史波动范围的位置；无法确定的因素只有一个——未来什么时候市盈率会从低到高恢复正常，以及未来盈利的增速将会如何。 市盈率从长期来看会在一个范围内周期性变化，例如恒生指数的市盈率，从20世纪70年代以来，在6～30倍成周期性波动，上证综指市盈率从20世纪90年代以来，在9～50倍之间呈周期性的波动。 由于市盈率呈周期性波动，因此如果当前市盈率处于历史较低位置，那么未来市盈率大概率是会上涨的。也就是说，可以在市盈率较低的时候买入，这样通过市盈率的变化，未来市盈率大概率会上涨，就能获取正的收益。 对指数基金来说，只要国家经济长期发展、盈利就会长期上涨，当经济景气，盈利增速就会比较快，当经济不景气，盈利增速则会放缓，但是我们无法预测未来盈利的上涨速度。我们只需要知道，正常情况下，只要国家经济正常发展，长期看盈利大概率是会上涨的。 未知的事情我们无法控制，但我们可以在已知的事情上下功夫。 基于这个思路，博格公式的使用方法如下： 1）在股息率高的时候买入 一般来说，指数基金越是低估，也就是当其价格越低于其内在价值时，股息率一般越高。 2）在市盈率处于历史较低位置时候买入 3）买入之后耐心等待“均值回归”，即等待市盈率从低到高 指标获取方法1:部分关注指数基金投资的微信公众号 推荐 银行螺丝钉的 定投十年赚十倍 方法2:指数官网 点我带你飞到 指数官网 博格公式的变种如果指数基金背后的公司盈利下滑，或者盈利周期性的变化，就会导致市盈率的分母“盈利”失去参考价值。例如强周期性行业中的证券行业，在熊市的时候，盈利都会短期大幅下滑，这样市盈率、盈利、收益率、股息率等与盈利关系很大的指标都无法参考。 这种情况下，我们使用博格公式的变种，也就是使用市净率来对指数基金进行分析。 【未来的年复合收益率】= 指数基金每年市净率的变化率 + 指数基金每年净资产的变化率 市净率 = P / B 其中P代表指数背后公司的平均股价, B代表平均净资产。 博格变种使用条件1）指数基金背后的公司盈利下滑，或者盈利周期性变化。 2）公司盈利虽然不稳定，但是要求公司在经营困难的时候也不会亏损。这样进公司的净资产价值才会有保障，我们才可以使用市净率来劲，进行估值。 目前适合博格变种的指数：证券行业指数、金融行业指数、非银金融行业指数、地产行业指数 博格变种使用使用方法1）在市净率低的时候买入 2）买入之后耐心等待市净率回归 按照证券业行业的历史市净率，证券业行业在1.8PB以下进入低估区域，在3.3PB以上进入高估区域。所以我们最好在1.8PB以下再开始考虑投资证券业行业指数，3.3PB以上开始卖出证券业行业。 以下是银行螺丝钉提供的指数基金估值数据，从下图看到证券行业目前市净率仅有1.26处于低估区域，可以买入，然后耐心等待市净率回归到3.3PB。 查询平台官方平台中证指数官网 http://www.csindex.com.cn/zh-CN 理杏仁花1元钱即可查询市盈率、市净率、盈利情况等详细财务数据，这个网站是几个年轻有想法懂投资又懂技术的人做的，我记得创始人的一句初心话：希望有一天当用户使用这个网站的时候，从牙缝里飙出一句话，这个网站，真TM好。 雪球APP聪明的投资人都在用雪球，这里是投资者俱乐部，里面有很大V公开自己的投资实盘，和发表投资研究文章，为了方便投资者直观的查看估值，在APP上一个估值数据入口，可能很多人不知道，我指给你看。（打开APP–行情–指数估值） 且慢指数估值且慢在业内也是一家很有情怀的公司，在微信小程序里面搜索且慢指数估值，可以直接查看。 蚂蚁财富指数红绿灯对，就是你每天都在用的支付宝，里面有一个指数红绿灯，具体位置在（财富–基金–指数红绿灯） 支付宝没有给出详细的估值数据，而是使用内部的判断方法，直观的说明当前是低估、正常还是高估；投资者根据自己倾向的投资标的，在这里再查询一下当前的状态。 天天基金（打开APP–指数宝–低估榜单） 蛋卷基金（打开APP–估值排行） 银行螺丝钉估值表利用银行螺丝钉估值表买入绿色低估基金 行动派均线定投表利用指数行动派定投表买入绿色低估基金 集思录主打低风险投资工具型社区，可转债、套利、打新、指数基金估值查询，等等，都可以用集思录获取最直接信息，不需要计算，一目了然。https://www.jisilu.cn/data/etf/#index 各类证券APP各类证券APP，例如万得、choice数据等，主打专业付费查询，但是一般常见的指数估值数据免费也可以看得到。 挑选基金 通过近1年，近2年，近3年这种业绩回顾方式去看基金，因为近期的业绩表现影响很大，很容易一涨遮百丑。 风格激进的基金，押宝某行业，基金的弹性变强。只要赌对行业，业绩爆炸，一下就成了牛基。 但是这种突然爆发的业绩，很有可能是因为运气好，而不是能力强。 当你发现某基金基金涨的很好，买入。第二年又很大的概率变垫底的瘟基了。 通过业绩选基金时，更重要的是对比过去多年的业绩是否稳定，是否连续。基金经理的专业能力能够长期持续的为我们赚钱（可以对比沪深300）。 对这种极好极坏的激进基金，在他表现最好的时候买入。很有可能是去接盘而已。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金定投学习笔记]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[常见指数场内基金一般是159***， 或510*** 场外基金是非159或510开头 上证50：大盘股代表，大盘股统称蓝筹股 场内：华夏上证50ETF 510050 场外：易方达上证50 110003 沪深300：大盘股代表，大盘股统称蓝筹股 场内：华泰柏瑞沪深300ETF 510300 场外：嘉实沪深300ETF联接 160706 中证500：中盘股代表 场内：南方中证500ETF 510500 场外：南方中证500ETF联接 160119 上证红利指数(上交50家公司)： 场内：红利ETF 510880 中证红利指数（上深100家公司，大盘股）： 场外：富国中证红利 100032 红利机会指数（类似中证红利，对股票盈利能力有要求，中小盘较高）： 场内/场外 华宝标普中国A股红利机会（LOF） 201029 恒生指数（香港50家企业）： 香港本地： 盈富基金 02800 场内：华夏恒生ETF 159920 场外：华夏恒生ETF联接 000071 H股指数（50只公司主营业务在内地，在香港上市的公司）： 场内：易方达H股ETF 510900 场外：易方达H股ETF联接 110031 挑选方式 基金概况 基金类型 交易状态 成立时间 基金规模 手续费 行业配置和重仓股票、债券 追踪误差小 基金经理、基金公司、基金业绩 定投表 日期 买入/卖出 基金 净值 份额 金额 手续费 备注 养老定投计划高分红指数基金 工资定投计划教育定投计划市值加权型指数中证系列指数：中证100 中证200 沪深300 中证500 中证1000 创业板指数深圳交易所创业板市值最大的100家公司 策略加权型指数红利类上证红利 中证红利 深证红利 红利机会指数 基本面类原则： 企业营业收入 企业的现金流 企业的净资产 企业的分红 基本面50：沪市交易所50只 纳入大量银行股 基本面60：深市60只 多为格力等制造业 基本面120 价值类原则： 市盈率低 市净率低 市现率低 股息率高 300价值 低波动类单日涨跌幅标准差 500低波动指数 红利低波动指数 其他类型指数央视50指数 50AH优选指数]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基金基本概念]]></title>
    <url>%2F%E5%9F%BA%E9%87%91%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[[TOC] 参考 https://zhuanlan.zhihu.com/p/61885234 https://mp.weixin.qq.com/s?__biz=MzU0Nzg1MzU3OA==&amp;mid=2247484012&amp;idx=1&amp;sn=76dae3edadd8e7a79386e5627a417c9e&amp;scene=19#wechat_redirect https://www.jianshu.com/p/f077b27e995e 市盈率什么是市盈率 首先我们要知道什么是市盈率。 市盈率（PE）的计算公式为：股票的价格/每股盈利。（另一种算法是：股票总市值/总盈利，两种算法是一样的，公式的变形而已） 它反映的是投资者愿意为当前的每股盈利支付多少钱，或者说当公司的盈利全部发放股利时，投资者通过多少年才能收回全部投资。 其实仔细想一想就能理解，你不能单看投多少钱就去判断一个项目值不值得投资，最起码得知道这个项目每年能带来多少现金流，投资需要多少年回本，对吧。 这么说可能比较抽象，举个例子就明白了。 假设A公司的股价为10元，每股盈利为0.5元（每股盈利等于一年的净利润除以发行股数），那么它的市盈率就是20倍，假设盈利全部分红的情况下，投资这个公司需要20年才能收回成本。 现在有一家同行业的B公司，股价为100元，每股盈利是10元，即B公司的市盈率为10倍，也就是说投资B公司只要10年就能回本。 你看，虽然A公司股价更低，但实际上B公司更有投资价值，因为B公司的估值要比A公司低的多，通俗点说就是B公司卖的更便宜。 投资B公司有两个好处，一是B公司通过股利收回成本的时间更短（假设盈利全部发放股利），另一个是因为A,B是同一个行业的公司，A公司的估值远远高于B公司，因为同一个行业的公司估值近似，所以未来B公司的股价很可能上涨。 什么决定了投资者愿意以高市盈率买一个公司呢？ 主要原因有两个，一个是公司的前景，另一个是投资者的情绪。 我们还拿A、B公司举例子，之所以A的市盈率比较高，可能是因为A的利润增速比B快得多，投资者更愿意为A公司出更高的价格。 比如A公司开发出了新的技术，开拓出新的市场，未来前景一片大好，预期利润会大幅增加。 假设A公司每年利润增速为100%（夸张了），而B每年利润增速为0%，那么只需要3年时间，A的市盈率就会降低为5倍（10/（0.522），假设股价不变），而B还是10倍市盈率。 显然A的未来前景要比B好得多，也就意味着投资者更看好A，愿意以更高的市盈率购买A。 这也是为什么科技股的市盈率普遍比食品饮料行业高得多，因为大家预期科技公司未来会高速增长，而食品饮料公司的未来比较确定，增速比较稳定，难以产生爆发式增长。 除了这个原因以外，在牛市的时候，投资者普遍对股市非常乐观，这个时候公司的增长潜力和业务跟原来没什么变化，但是投资者就是更愿意给予公司更高的估值，这是由牛市中投资者的风险偏好和乐观情绪决定的。 通常，在牛市中我们会看到几乎所有股票估值都要比平时高很多。 到这里，我们可以得出一个结论： 市盈率越低，估值越低，越具有投资价值。 但同时有可能反映了市场的投资者觉得该公司前景一般。 市盈率越高，估值越高，泡沫越大。 但同时有可能也反映了市场投资者觉得该公司具有高速发展的可能性。 为什么用市盈率为指数估值 这里必须提一些市盈率估值法的缺点了。 比如，在对比市盈率的时候必须同行业对比，比如白酒股和科技股通过市盈率去判断估值，这是没有意义的，因为行业不一样，发展的潜力也不一样，也就意味着增速不一样。 市盈率估值法更加适用一些利润增长比较稳定的行业，比如医药，食品饮料等等，一些类型行业是不能用市盈率估值法的，比如周期股、银行股这些就不行。至于为什么不行，这里就不展开说了。 除了这两条之外，不得不说的一个重要缺点，就是单个公司经营的不确定性，会导致市盈率估值法失效。 假设我们以10倍市盈率买入C公司的股票，这时候股票价格是10元，每股盈利1元，我们对比了这支股票的历史走势，认为10倍市盈率是一个非常合适的价格，所以我们果断选择买入。 但是意想不到的情况发生了，由于竞争对手开发出了新的技术，使得C公司的业务急剧萎缩，C公司的每股盈利变为0.2元。 这时候你发现，在股票价格不变的情况下，公司的市盈率一下变成了50倍，瞬间变得老高。 自己本来是以一个低估价格买入的股票，突然就变得高估了。 更不幸的是，在实际情况中，如果公司的基本面发生如此重大的改变，股价也会急剧下跌。我们假设股价跌为4元，这个时候你会发现自己亏损了60%，并且市盈率还是在历史高位。 这种情况之所以发生，是因为个股会面临着很多不确定性，导致公司的基本面发生根本性的改变。 也许突然之间变好，也许突然之间变坏。这时候你使用市盈率判断一家公司的估值高低就失效了。 市盈率这种估值方法最适于那些基本面长期比较稳定，业绩增长比较稳定，没有周期性的公司。 虽然市盈率估值法在个股估值时有很多的局限性，但是这种方法却非常适合指数估值。 指数的成分股数量多，种类丰富，从而使个股的风险相互对冲掉了，当然行业和个股的不同特性也被完全对冲掉了。 这样整个指数的走势其实只受到市场情绪的影响，而不受个别公司经营风险的影响，指数的走势就是各行各业股票走势的平均值，相当稳定。 用一个稍微专业点的名词就是指数只有系统性风险，没有非系统性风险。 这刚好满足了市盈率估值法对业绩稳定性的要求，所以使用市盈率对指数估值是相当靠谱的一种方法。 这样，对于指数估值我们得出了一个更简单的结论： 市盈率越低，市场情绪越悲观，估值越低，越具有投资价值。 市盈率越高，市场情绪越乐观，估值越高，泡沫越大。 市盈率的种类在使用市盈率的时候还有一个问题，比如半年报刚出来，这时候我想要知道标的的市盈率，应该用哪个时期的每股盈利来计算市盈率呢？ 这就衍生出3种不同的市盈率计算方法。 如果用过去一年的每股盈利来计算市盈率，这种方法算出来的市盈率叫做“静态市盈率”。缺点是没有把今年的利润增加的部分算进去，会产生偏差。 如果用前两个季度的利润增速，去推算后两个季度的每股盈利，那么算出来的市盈率包含了本年度的整个利润增速，用这种方法算出来的市盈率叫做“动态市盈率”。但是问题是推算出来的那两个季度的业绩是不准的，也可能会出现偏差。 如果使用已经出来的4个季度的业绩去算，即上半年的业绩和去年下半年的业绩，用这种方法算出来的叫做“滚动市盈率”。这种方法相当于折中一点，就事论事，我只用已经出来的业绩去计算。虽然也会有误差，但是相对来说更为准确一点。 当然，并不是说“滚动市盈率”就最好，在不同的情况下，这三种市盈率都有发挥的余地，具体的用法不是本文重点，我们只要知道在指数估值的时候，使用哪一种就可以了。 在指数估值的时候，我采用的是“滚动市盈率”，因为本身滚动市盈率更能反映目前市场的估值，这部分简单了解一下就可以。 市盈率分位点好了，我们知道给指数估值需要看指数的市盈率，并且知道了市盈率越低，估值也就越低。但是多少算高，多少算低呢？ 我们在说低估和高估时，实际上是默认有一个参照物进行对比。 这个参照物其实就是上文所说的“价值”，但是问题是一听可乐的价值很容易搞清楚，但是一个公司或者一个指数怎么确定其价值呢？ 可以告诉大家，这是非常难的，即使专业的分析师也只能大致估算出“价值”，因为所有的估算方法中都有一部分的数据是预测出来了，这就注定会造成偏差。 那如果不好估算价值，还有没有别的办法呢？ 我们知道指数的历史的市盈率是千千万万不同投资者互相博弈出来的结果，也就是在当时某一个时刻所有参与交易的投资者共同认为指数在这个时候值多少钱。 那么我们根本不用去算指数到底价值多少，只要用当前的市盈率和历史上各个时期的市盈率进行对比，就能够判断出来它相对的高低。 对于指数来说，用历史的自己来充当这个参照物再合适不过了。所以我们使用市盈率分位点这种统计方法来判断指数的估值情况。 请注意，市盈率分位点是一个极其重要的概念，整个指数估值基本都是围绕着市盈率分位点来进行的。 市盈率分位点：即该指数当前的市盈率在历史上战胜了多少的自己。 假设按照每周对指数的市盈率进行取样，并按照从小到大进行排序，当前的市盈率在排序中所处的位置，并计算出所在位置的百分比。 这个说起来有些拗口，简单的说，分位点越低，排名越靠前，证明在过去的历史中，打败的自己越少，估值越低。反之则估值越高。 比如，创业板在2018年10月12日这天市盈率是28.61倍，市盈率分位点为0.05%，这意味着28.61倍的市盈率几乎是创业板开板以来最低的市盈率，按照从小到大排序几乎排在第一位。 创业板在2015年6月5日这天，也就是大牛市的顶点，市盈率高达135倍，市盈率分位点是100%，为历史之最，也就是说135倍这个市盈率在创业板开板以来的所有市盈率（每周取样）中最高，按照从小到大的排列，排在最后一位。 如果大牛市来了，创业板的市盈率超过135呢？假设创业板的市盈率涨到140倍，那么140倍时的市盈率分位点会变为100%，而135倍市盈率的分位点会低于100%。 请注意，这里的分位值并不是简单的当前市盈率占最高的百分之多少，而是历史中排序的位置。 比如中位值，如果按照简单数学计算，应该是135倍除以2，计算得出的中值为67.5。但是实际的中位值是48.6，这是因为在大多数情况下创业板的市盈率都是在40-60之间，只有在牛市的时候，创业板的市盈率才突然一下子变得特别高，而这样的时间特别短，也就是估值特别高的采样特别少，所以计算出来中间的那个值只有40多倍。这个理解不了也没有关系，只要知道怎么看就行了。 机会值、中位值和危险值为了更加直观的观察分位值的高低，我们可以人为规定出来三个值，分别为机会值，中位值，危险值。 以市盈率分位点的20%作为机会值，分位值低于20%，认为当前指数低估，有投资价值。 以市盈率分位点的50%作为中位值，为历史市盈率排序的中间值。 以市盈率分位点的80%作为危险值，分位值高于80%，认为当前指数高估，有泡沫。 至于为什么选择20%和80%这两个位置作为标尺，其实也没什么特别的原因，就是人为划分的。你也可以采取10%和90%，或者30%和70%都可以，完全凭借个人喜好。 如何使用估值表每周五我会更新指数估值表，估值表的中会依次显示指数当前的滚动市盈率，市盈率分位点，机会值，中位值，危险值和股息率。 大家可以参考估值表来判断指数估值的相对高低。低估买入，高估卖出。 这里我必须强调一点，低估和高估的值是人为规定的。 比如在指数表里的机会值为市盈率低于20%，这个20%是我自己划分的。 当然你也可以认为30%以下就算低估，开始买入，这个没有定数，要结合自己的交易系统。 就像一些朋友问我，现在中证500位置低估，能买吗？ 这个问题说实话没法回答，对于我来说，按照我的定投系统，我会无脑买入，并且不管几年，坚持拿到高估再卖出，这个过程中的短期盈利和亏损我会通通忽略。 就像2018年亏损的时候和现在盈利的时候，对于我来说都是账户中的波动，只要没有卖出落袋，就不算是盈利。 虽然现在中证500低估，但如果买入之后，恰好赶上股市回调，而且回调幅度还比较大，瞬间亏损，有些朋友可能承受不住割肉离场了。 这种情况2018年发生的太多了，那时候指数极其低估，但很多人要么割肉，要么停止定投。 虽然都是低估买入，不同的人和不同的系统，最后的结果可能会大相径庭。 低买高卖，这话说起来简单，但做起来真心不易。 说来说去，估值表只是工具，知道了估值必须和你自己的交易系统结合才能发挥最大的价值。 1、假如一些很有前景，但目前没有利润的创业型公司就不适合用市盈率法估值，因为负值的市盈率毫无意义。**而拥有大量固定资产的制造行业和轻资产的新兴行业来说，市净率又变得不再适用。 2、在财务报表中，可以调节利润的其他科目非常多，因此，上市公司的利润很容易被操控，从而导致市盈率指标失真。而市净率也对固定资产依赖越来越少，看不见摸不着的知识财产权，商誉，品牌等在当今社会越来越重要，可这类资产却很难估值。另外，由于科技进步或通胀导致的企业账面净资产与市场值会相差很多。 比如，一家生产4寸手机屏的生产线价值1亿，然而现在5寸以上的大屏手机越来越流行，因此导致生产线因技术革新而贬值；再如，上市公司持有的现金资产，如果投资不力，也会因通胀而贬值。 3、市盈率和市净率更适合横向对比和业内的纵向对比，因为不同行业之间的指标差别实在太大了。就拿第一条举例吧！像制造业是重资产行业，企业往往拥有大量实物净资产，这样一下子就会拉低市净率。 市净率市净率=公司市值/公司净资产，即PB=P/B PB：市净率 P：公司市值 B：公司净资产 市净率=每股股价/每股净资产（市净率另一种算法） 市净率的意义：从买资产的角度，衡量当前企业的市值与企业净资产的关系，市净率越低代表公司低估。市净率估值优点在于净资产比净利润更稳定。 举个例子，公司净资产100块，上市后由于大家对公司盈利能力有信心，股价上涨，市值变成了120块。此时，蚂蚁公司的市净率就为1.2。多出来的20块市值是市场对于公司的估值，当这个估值较低时，蚂蚁公司处于低估；当这个估值较高时，蚂蚁公司处于高估。 盈利收益率盈利收益率=公司盈利/公司市值，即EP=E/P EP：盈率收益率 P：公司市值 E：公司盈利 盈利收益率是投资大师格雷厄姆常用的一种估值指标，适用于投资指数基金，盈利收益率是市盈率的倒数。 盈利收益率的意义：假如我们把一定公司买下来，它一年的盈利带给我们的收益率。 举个例子： 公司市值100块，2018年盈利20块，那么盈利收益率就是20/100，也就是20%。也就是说花100块买下蚂蚁公司，公司的盈利可以给我们带来20%的收益率。 盈利收益率的买入标准： 1、盈利收益率要大于10%。 2、盈利收益率超过国债收益率的两倍。 参考十年国债收益率3.268%，两倍就是6.536%，两个买入标准符合一个就值得投资。就目前而言，指数的盈利收益率大于6.536%就可以考虑投资，超过10%则更好。 盈利收益率的卖出标准： 盈利收益率低于债券收益率。 债券收益大概是6.4%，盈利收益率低于6.4%就可以考虑分批卖出了，也就代表它的投资价值不大了。 股息率股息率=现金分红/公司市值 股息率的意义：衡量企业过去一年的现金分红带来的收率。企业每年的盈利，需要拿出一部分再投入生产，另一部分可以现金分红给股东。 分红比例=现金分红/公司盈利 国内分红比例一般在30-40%，成熟市场（如欧美市场）能达到40-60%。 净资产收益率（ROE）净资产收益率（ROE）=公司税后盈利/公司净资产 净资产收益率意义：衡量公司对股东投入资本的利用效率，它弥补了每股税后利润指标的不足。ROE越高，说明投资带来的收益越高。该指标体现了自有资本获得净收益的能力。 市现率市现率是总市值与税息折旧摊销前收益（EBITDA）（每股股价/每股现金流）的比值，而EBITDA为税后净利润、所得税、利息费用、折旧和摊销之和。该指标越小，往往说明上市公司增加的现金流多，经营压力就越小。 市销率市销率则是总市值与销售收入（每股股价/每股销售收入）的比值。指标越小，往往说明上市公司销售收入越多，市场竞争力越大，产品成熟度越高。 A类C类基金费用不同的基金申赎费率略有差别，A类带有申购赎回费用，C类在七天之外没有申购赎回费，但每年有个销售服务费。 那么我们只需要比较这个销售服务费和申购费用哪个划算就可以了。 假设 A类申购费为0.08%，七天以内赎回为1.5%，七天以外赎回为0.1%，托管费0.1%和管理费0.3% C类申购费为0，赎回费率和A类一样，托管费和管理费和A类一样，销售服务费0.3% 那么让A类申购费=C类每日收取销售服务费*天数 天数=0.08%/0.3%/365=96天 那么结果就是持有时间大于96天选A类，小于96天选C类。 LOF基金LOF基金，场内场外代码一致，场内场外都可以交易。 夏普比率夏普比率（Sharpe Ratio）是衡量基金风险调整后收益的指标之一，由诺贝尔经济学奖得住威廉·夏普提出，夏普比率反映了基金承担单位风险所获得的超额回报率，即基金总回报率高于同期无风险收益率的部分，一般情况下，该比率越高，基金承担单位风险得到的超额回报率越高。夏普比率代表投资人每多承担一分风险，可以拿到几分报酬；若为正值，代表基金报酬率高过波动风险；若为负值，代表基金操作风险大过于报酬率。 使用 夏普比率的比较必须在同一类基金当中进行，因为不同的基金所对应的业绩基准不同。你不能拿股票型基金的夏普比率来跟债券型基金的夏普比率比较。也不能拿主要投资大蓝筹股的基金的夏普比率来跟投资中小盘股的基金的夏普比率来比较。 阿尔法系数阿尔法系数（α）是基金的实际收益和按照β系数计算的期望收益之间的差额。其计算方法如下：超额收益是基金的收益减去无风险投资收益（在中国为1年期银行定期存款收益）；期望收益是贝塔系数β和市场收益的乘积，反映基金由于市场整体变动而获得的收益；超额收益和期望收益的差额即α系数。 简单的理解就是阿尔法系数代表了主动型基金超过业绩基准的那部分，超过业绩基准越大相应的阿尔法系数就越大。而主动型基金存在的目的就是为了跑赢业绩基准所对应的指数收益。如果跑不赢业绩基准对应的指数收益，那与其购买主动型基金还不如购买被动型的指数基金。 基金申购费用前端收费是指买后即刻扣除申购所需要的费用，此外前端收费的费用会随着申购金额的不断上升而不断降低。 后端收费是指买的时候不付钱，而在赎回时一次性付清所需费用的行为，后端收费的申购费用会随着投资者持有资金的时限越长而越少。 管理费 债券型0.6%-0.7%左右，混合型和股票型1-2%。 货币基金交易是免费的，免费，不收手续费也不收管理费。 基金后面的字母大概有四种含义 基金的收费方式，分为前端收费和后端收费； 基金的申购门槛 基金的销售渠道 基金的风险等级 比如 货币基金A：起购100元 货币基金B：起购50w元 债券基金A：前端收费 债券基金B：后端后费 债券基金C：无申购费但有销售费 股票基金A：低风险低收益 股票基金B：高风险高收益 债券与股票基金后面的A/C基金中最常见的分类是A类份额和C类份额，比如上面提到天弘中证500指数A和天弘中证500指数C，这里的A/C字母不同代表着不同的收费方式。 A类份额是前端收费，在申购时时收取申购费。 C类份额在申购赎回时都不收取费用，但在基金存续期内按日计提销售服务费。 以天弘中证500指数A和天弘中证500指数C，他们的收费如下： 如上图所示，因为目前在主流的网络平台申购基金，手续费都会打1折，所以只要是打算长期持有的，购买A类份额会比较划算哦。因为申购赎回费用是一次性收取，而销售服务费持有时间越长，收得越多。 货币基金后面的A/B货币基金比较常见的分类有A/B类份额。A类最为常见，针对普通的投资者，申购起点一般1-1000元不等。而B类通常针对机构客户或大资金客户，在过去申购的门槛较高，通常500万起，而且基金的费用也较低。 较低的收费意味着同样的投资收益率，给投资者会带来更高的回报。在过去，B类份额一般是机构专享，但是现在B类的申购门槛也在慢慢降低了。比如支付宝目前唯一代销的B类货币基金，004137博时合惠货币B在支付宝上的申购门槛只要5万起。 而比如蛋卷基金甚至已经连5万的申购门槛都没有了。所以买货币基金时，同样的一只货基，尽量买B类总没错。]]></content>
      <categories>
        <category>基金</category>
      </categories>
      <tags>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见证书详解]]></title>
    <url>%2F%E5%B8%B8%E8%A7%81%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[SSL 证书提供了一种在互联网上身份验证的方式,是用来标识和证明通信双方身份的数字信息文件。使用SSL 证书的网站，可以保证用户和服务器间信息交换的保密性，具有不可窃听、不可更改、不可否认、不可冒充的功能。 SSL证书由浏览器中“受信任的根证书颁发机构”在验证服务器身份后颁发,具有网站身份验证和加密传输双重功能. SSL工具箱：https://www.myssl.cn/tools/ 常见证书格式及相互转化PKCS 全称是 Public-Key Cryptography Standards ，是由 RSA 实验室与其它安全系统开发商为促进公钥密码的发展而制订的一系列标准，PKCS 目前共发布过 15 个标准。 常用的有：PKCS#7 Cryptographic Message Syntax StandardPKCS#10 Certification Request StandardPKCS#12 Personal Information Exchange Syntax Standard X.509是常见通用的证书格式。所有的证书都符合为Public Key Infrastructure (PKI) 制定的 ITU-T X509 国际标准。 123PKCS#7 常用的后缀是： .P7B .P7C .SPCPKCS#12 常用的后缀有： .P12 .PFX pfx/p12用于存放个人证书/私钥，他通常包含保护密码，2进制方式p10是证书请求p7r是CA对证书请求的回复，只用于导入p7b以树状展示证书链(certificate chain)，同时也支持单个证书，不含私钥。 123X.509 DER 编码(ASCII)的后缀是： .DER .CER .CRTX.509 PAM 编码(Base64)的后缀是： .PEM .CER .CRT der,cer文件一般是二进制格式的，只放证书，不含私钥 crt文件可能是二进制的，也可能是文本格式的，应该以文本格式居多，功能同der/cer pem文件一般是文本格式的，可以放证书或者私钥，或者两者都有pem如果只含私钥的话，一般用.key扩展名，而且可以有密码保护 openssl常用命令用openssl创建CA证书的RSA密钥(PEM格式)： 1openssl genrsa -des3 -out ca.key 1024 用openssl创建CA证书(PEM格式,假如有效期为一年)： (openssl是可以生成DER格式的CA证书的，最好用IE将PEM格式的CA证书转换成DER格式的CA证书。) 1openssl req -new -x509 -days 365 -key ca.key -out ca.crt -config openssl.cnf x509到pfx 1pkcs12 -export –in keys/client1.crt -inkey keys/client1.key -out keys/client1.pfx PEM格式的ca.key转换为Microsoft可以识别的pvk格式。 1pvk -in ca.key -out ca.pvk -nocrypt -topvk PKCS#12 到 PEM 的转换 1openssl pkcs12 -nocerts -nodes -in cert.p12 -out private.pem 从 PFX 格式文件中提取私钥格式文件 (.key) 1openssl pkcs12 -in mycert.pfx -nocerts -nodes -out mycert.key 转换 pem 到到 spc 1openssl crl2pkcs7 -nocrl -certfile venus.pem -outform DER -out venus.spc PEM 到 PKCS#12 的转换 1openssl pkcs12 -export -in Cert.pem -out Cert.p12 -inkey key.pem 合并证书文件（crt）和私钥文件（key） 1cat client.crt client.key &gt; client.pem]]></content>
      <tags>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes-使用私有仓库镜像]]></title>
    <url>%2FKubernetes-%E4%BD%BF%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[[TOC] Docker私有仓库在介绍k8s拉取私有仓库之前，需要配置docker拉取私有仓库。默认docker拉取私有仓库是使用https协议，如果需要使用http，需要进行以下配置。编辑/etc/docker/daemon.json，添加内容： 123&#123; &quot;insecure-registries&quot; : [&quot;registry.xxxx.com&quot;]&#125;123 重启docker，然后通过以下操作拉取私有仓库镜像： 12docker login registry.xxxx.comdocker pull registry.xxxx.com/xxx/xx:xxx K8s私有仓库配置好docker后，这里继续介绍k8s通过secret配置来拉取私有仓库镜像。如果不进行设置的话，创建RC拉取镜像时就会提示找不到镜像，报以下错误： 12345678910Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 49s 49s 1 &#123;default-scheduler &#125; Normal Scheduled Successfully assigned webapp-xq03t to master 48s 11s 3 &#123;kubelet master&#125; spec.containers&#123;webapp&#125; Normal Pulling pulling image &quot;e5:8889/tomcat&quot; 48s 11s 3 &#123;kubelet master&#125; spec.containers&#123;webapp&#125; Warning Failed Failed to pull image &quot;e5:8889/tomcat&quot;: Error: image tomcat:latest not found 48s 11s 3 &#123;kubelet master&#125; Warning FailedSync Error syncing pod, skipping: failed to &quot;StartContainer&quot; for &quot;webapp&quot; with ErrImagePull: &quot;Error: image tomcat:latest not found&quot; 47s 0s 3 &#123;kubelet master&#125; spec.containers&#123;webapp&#125; Normal BackOff Back-off pulling image &quot;e5:8889/tomcat&quot; 47s 0s 3 &#123;kubelet master&#125; Warning FailedSync Error syncing pod, skipping: failed to &quot;StartContainer&quot; for &quot;webapp&quot; with ImagePullBackOff: &quot;Back-off pulling image \&quot;e5:8889/tomcat\&quot;&quot; 创建secret1kubectl create secret docker-registry registrysecret --docker-server=server_host:8889 --docker-username=admin --docker-password=xxxx --docker-email=lusyoe@163.com RC中使用secret1234567891011121314151617181920apiVersion: v1kind: ReplicationControllermetadata: name: webappspec: replicas: 2 template: metadata: name: webapp labels: app: webapp spec: containers: - name: webapp imagePullPolicy: Always image: e5:8889/tomcat:latest ports: - containerPort: 80 imagePullSecrets: - name: registrysecret 关键就在于imagePullSecrets。 如果仅仅是这样的话，每次编写yaml脚本都需要添加这2行配置也太麻烦了。我们需要使其默认就自动可以从私有仓库中下载还需要几步。 配置默认规则将该密钥设置到k8s的默认账号中： 1kubectl patch sa default -p &apos;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;registrysecret&quot;&#125;]&#125; 查看默认账号配置：kubectl get serviceaccounts default -o yaml 看看默认账户的详细配置： 12345678910111213apiVersion: v1imagePullSecrets:- name: registrysecretkind: ServiceAccountmetadata: creationTimestamp: 2018-03-11T15:28:06Z name: default namespace: default resourceVersion: &quot;997965&quot; selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: cc20a274-2540-11e8-8755-3497f600e8edsecrets:- name: default-token-5qvkp 我们发现已经添加了imagePullSecrets，这样我们后续就不用在每个yaml脚本中都添加这个配置啦，自动会加上去的。不同的namespace命名空间secret是隔离的，这里只演示了default命名空间。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDN原理]]></title>
    <url>%2FCDN%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[什么是CDNCDN（Content Delivery Network，内容分发网络）是构建在现有互联网基础之上的一层智能虚拟网络，通过在网络各处部署节点服务器，实现将源站内容分发至所有CDN节点，使用户可以就近获得所需的内容。CDN服务缩短了用户查看内容的访问延迟，提高了用户访问网站的响应速度与网站的可用性，解决了网络带宽小、用户访问量大、网点分布不均等问题。 加速原理当用户访问使用CDN服务的网站时，本地DNS服务器通过CNAME方式将最终域名请求重定向到CDN服务。CDN通过一组预先定义好的策略(如内容类型、地理区域、网络负载状况等)，将当时能够最快响应用户的CDN节点IP地址提供给用户，使用户可以以最快的速度获得网站内容。使用CDN后的HTTP请求处理流程如下： CDN节点有缓存场景 图1 HTTP请求处理流程（节点有缓存场景） HTTP请求流程说明： 用户在浏览器输入要访问的网站域名，向本地DNS发起域名解析请求。 域名解析的请求被发往网站授权DNS服务器。 网站DNS服务器解析发现域名已经CNAME到了www.example.com.c.cdnhwc1.com。 请求被指向CDN服务。 CDN对域名进行智能解析，将响应速度最快的CDN节点IP地址返回给本地DNS。 用户获取响应速度最快的CDN节点IP地址。 浏览器在得到速度最快节点的IP地址以后，向CDN节点发出访问请求。 CDN节点将用户所需资源返回给用户。 CDN节点无缓存场景 图2 HTTP请求处理流程（节点无缓存场景） HTTP请求流程说明： 用户在浏览器输入要访问的网站域名，向本地DNS发起域名解析请求。 域名解析的请求被发往网站授权DNS服务器。 网站DNS服务器解析发现域名已经CNAME到了www.example.com.c.cdnhwc1.com。 请求被指向CDN服务。 CDN对域名进行智能解析，将响应速度最快的CDN节点IP地址返回给本地DNS。 用户获取响应速度最快的CDN节点IP地址。 浏览器在得到速度最快节点的IP地址以后，向CDN节点发出访问请求。 CDN节点回源站拉取用户所需资源。 将回源拉取的资源缓存至节点。 将用户所需资源返回给用户。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>cdn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Git命令清单]]></title>
    <url>%2F%E5%B8%B8%E7%94%A8Git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[以下是整理的常用 Git 命令清单，几个专用名词的译名如下： Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 [TOC] 一、新建代码库# 在当前目录新建一个 Git 代码库$ git init # 新建一个目录，将其初始化为 Git 代码库$ git init [project-name] # 下载一个项目和它的整个代码历史$ git clone [url] 二、配置Git 的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的 Git 配置$ git config –list # 编辑 Git 配置文件$ git config -e [–global] # 设置提交代码时的用户信息$ git config [–global] user.name “[name]”$ git config [–global] user.email “[email address]” 三、增加/删除文件# 添加指定文件到暂存区$ git add file1 file2… # 添加指定目录到暂存区，包括子目录$ git add [dir] # 添加当前目录的所有文件到暂存区$ git add . # 删除工作区文件，并且将这次删除放入暂存区$ git rm file1 file2… # 停止追踪指定文件，但该文件会保留在工作区$ git rm –cached [file] # 改名文件，并且将这个改名放入暂存区$ git mv file-original file-renamed 四、代码提交# 提交暂存区到仓库区$ git commit -m [message] # 提交暂存区的指定文件到仓库区$ git commit file1 file2 … -m [message] # 提交工作区自上次 commit 之后的变化，直接到仓库区$ git commit -a # 提交时显示所有 diff 信息$ git commit -v # 使用一次新的 commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次 commit 的提交信息$ git commit –amend -m [message] # 重做上一次 commit，并包括指定文件的新变化$ git commit –amend … 五、分支# 列出所有本地分支$ git branch # 列出所有远程分支$ git branch -r # 列出所有本地分支和远程分支$ git branch -a # 新建一个分支，但依然停留在当前分支$ git branch [branch-name] # 新建一个分支，并切换到该分支$ git checkout -b [branch] # 新建一个分支，指向指定 commit$ git branch branch commit # 新建一个分支，与指定的远程分支建立追踪关系$ git branch –track branch remote-branch # 切换到指定分支，并更新工作区$ git checkout [branch-name] # 建立追踪关系，在现有分支与指定的远程分支之间$ git branch –set-upstream branch remote-branch # 合并指定分支到当前分支$ git merge [branch] # 选择一个 commit，合并进当前分支$ git cherry-pick [commit] # 删除分支$ git branch -d [branch-name] # 删除远程分支$ git push origin –delete $git push origin :[branch-name]$ git branch -dr 六、标签# 列出所有 tag$ git tag # 新建一个 tag 在当前 commit$ git tag [tag] # 新建一个 tag 在指定 commit$ git tag tag commit # 查看 tag 信息$ git show [tag] # 提交指定 tag$ git push remote tag # 提交所有 tag$ git push [remote] –tags # 新建一个分支，指向某个 tag$ git checkout -b branch tag 七、查看信息# 显示有变更的文件$ git status # 显示当前分支的版本历史$ git log # 显示 commit 历史，以及每次 commit 发生变更的文件$ git log –stat # 显示某个文件的版本历史，包括文件改名$ git log –follow [file]$ git whatchanged [file] # 显示指定文件相关的每一次 diff$ git log -p [file] # 显示指定文件是什么人在什么时间修改过$ git blame [file] # 显示暂存区和工作区的差异$ git diff # 显示暂存区和上一个 commit 的差异$ git diff –cached [] # 显示工作区与当前分支最新 commit 之间的差异$ git diff HEAD # 显示两次提交之间的差异$ git diff [first-branch]…[second-branch] # 显示某次提交的元数据和内容变化$ git show [commit] # 显示某次提交发生变化的文件$ git show –name-only [commit] # 显示某次提交时，某个文件的内容$ git show [commit]:[filename] # 显示当前分支的最近几次提交$ git reflog 八、远程同步# 下载远程仓库的所有变动$ git fetch [remote] # 显示所有远程仓库$ git remote -v # 显示某个远程仓库的信息$ git remote show [remote] # 增加一个新的远程仓库，并命名$ git remote add shortname url # 删除远程仓库$ git remote remove [shortname] # 取回远程仓库的变化，并与本地分支合并$ git pull remote branch # 上传本地指定分支到远程仓库$ git push remote branch # 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] –force # 推送所有分支到远程仓库$ git push [remote] –all 九、撤销# 恢复暂存区的指定文件到工作区$ git checkout [file] # 恢复某个 commit 的指定文件到工作区$ git checkout commit file # 恢复上一个 commit 的所有文件到工作区$ git checkout . # 重置暂存区的指定文件，与上一次 commit 保持一致，但工作区不变$ git reset [file] # 重置暂存区与工作区，与上一次 commit 保持一致$ git reset –hard # 重置当前分支的指针为指定 commit，同时重置暂存区，但工作区不变$ git reset [commit] # 重置当前分支的 HEAD 为指定 commit，同时重置暂存区和工作区，与指定 commit 一致$ git reset –hard [commit] # 重置当前 HEAD 为指定 commit，但保持暂存区和工作区不变$ git reset –keep [commit] # 新建一个 commit，用来撤销指定 commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit] 十、其他# 生成一个可供发布的压缩包$ git archive Git撤销修改自然，你是不会犯错的。不过现在是凌晨两点，你正在赶一份工作报告，你在readme.txt中添加了一行： 123456$ cat readme.txtGit is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.My stupid boss still prefers SVN. 在你准备提交前，一杯咖啡起了作用，你猛然发现了stupid boss可能会让你丢掉这个月的奖金！ 既然错误发现得很及时，就可以很容易地纠正它。你可以删掉最后一行，手动把文件恢复到上一个版本的状态。如果用git status查看一下： 123456789$ git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 你可以发现，Git会告诉你，git checkout -- file可以丢弃工作区的修改： 1$ git checkout -- readme.txt 命令git checkout -- readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 现在，看看readme.txt的文件内容： 12345$ cat readme.txtGit is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files. 文件内容果然复原了。 git checkout -- file命令中的--很重要，没有--，就变成了“切换到另一个分支”的命令，我们在后面的分支管理中会再次遇到git checkout命令 Git fetch和pull的详解及区别git fetch和pull的区别 Git中从远程的分支获取最新的版本到本地有这样2个命令： git fetch：相当于是从远程获取最新版本到本地，不会自动merge 123Git fetch origin mastergit log -p master..origin/mastergit merge origin/master 以上命令的含义： 首先从远程的origin的master主分支下载最新的版本到origin/master分支上；然后比较本地的master分支和origin/master分支的差别；最后进行合并。上述过程其实可以用以下更清晰的方式来进行： 123git fetch origin master:tmpgit diff tmp git merge tmp 从远程获取最新的版本到本地的tmp分支上之后再进行比较合并 git pull：相当于是从远程获取最新版本并merge到本地 1git pull origin master 上述命令其实相当于git fetch 和 git merge 在实际使用中，git fetch更安全一些。因为在merge前，我们可以查看更新情况，然后再决定是否合并结束。 Git merge 与 rebasegit merge来看两种场景中merge的不同方式。 场景一：切出特性分支后，develop分支上没有新的提交。 fast-forward，若无分歧，会直接移动文件指针。看不出特性分支的起始点。 no-fast-forward（–no-ff），保留提交链的完整性。 squash，压缩不必要的commit；无法看出feature分支合到develop；feature，develop保持相对独立。 场景二：切出特性分支后，develop分支上提交了C6，C7。 develop分支上提交了C6，C7，无法fast forward。 three way merge， 找到develop分支的最新节点C7； 找到feature分支的最新节点C5； 找到develop分支和feature分支的共同祖先节点C3； 对C3，C5，C7进行三方合并，生成最新的C8。 git rebase变基/衍合，从旧的base变基到新的base，在新base的基础上重现另一个分支的修改过程，版本树像两个链条串在一起变成一个链条，以达到线性的效果。 rebase，对指定的base本身并没有什么影响；只是重写base之后的commit历史。 来看几个场景： 在继续开发的过程中和主分支develop同步 从develop上拉出分支做一些开发工作，在这个过程中develop分支可能继续往前走了，那我们需要经常和develop保持下同步（可能是别人提交了公用的模块，或者修复了对大家都有影响的bug）。之前我会通过pull操作来达到这一目的，但pull往往内置了merge（pull的效果看起来就像fetch+merge，想想提到过的“merge应该反映业务层面的合并，而非技术行为”，并且此时merge会产生一些杂乱的历史记录）。 这就相当于我们本地的开发工作（一系列的commits）是在旧的base上展开的，应该使用rebase操作，将本地的开发工作变基到develop的最新节点上。 1git pull --rebase 重新拾起搁置的工作 很久之前可能启动一个并行工作（开发不着急上线的新特性或者优化一些功能），但一直没有时间处理所以就搁置了，现在又有时间重新拾起，然后就发现当时基于的base实在是太太太老了。现在肯定是希望基于最新的base展开工作，这样就可以从已解决的bugfix或已完成的新特性中中受益。 在push之前整理我的本地历史 1git rebase -i &lt;myBaseCommit&gt; 这是一个使用更频繁的场景：并不是为了变基，而是清理本地的commit。很多情况我们都需要commit： 可能需要多个连续的commit才能完成一次bugfix； 切换分支需要保存本地修改（当然按理说这个时候应该使用stash或者idea提供的shelve了）； 某些历史commit写错了msg（最近一次commit的msg可以通过 commit –amend 修改）； 通过-i（–interactive，交互式的），我们可以干预rebase将要执行的脚本化过程，从而达到清理本地commit历史的目的。 熟练使用rebase，可以轻松的进行commit，只需要在最终push之前做一次清理，不必担心影响到公共仓库。 1234git fetch origin developgit rebase origin/develop... do sth ...git push rebase的场景和用法还有待探索，慢慢更新了。 记住这个： 只能rebase私有分支，一旦发布到公共仓库，不要再rebase了。 merge V.S. rebase什么时候用merge；基于上述不同的merge行为（fast-forward，–no-ff，squash），什么场景下用哪种merge： merge执行一个合并，这个合并应该反应业务层面的合并，而非技术行为。我们希望在当前分支上往前走，这样它就包含了其他分支的工作。 所以问题的关键是：这个“其他分支”是什么样的分支？这个“其他分支”需要在历史图谱中展示么？ 本地的、临时的分支，使用它仅仅是为了使master保持clean。 1.若拉出本地分支后，master往前走了，此时在本地分支： 1git rebase -i master 将本地分支变基到最新的master节点（重新梳理本地历史提交信息比如合并成一个commit），好似本地分支就是在最新的master节点上做的开发工作，以保证合并到master后呈现线性增长。 2.在master上： 1git merge (fast-forward) 最终以一个或几个commit展现在master上。 知名分支，团队明确定义的，可能是用来追踪bug/feature。 永远不要用rebase，而是用 1git merge --no-ff 以保留清晰完整的历史图谱。 master分支执行merge不应该存在冲突，merge过程需要加入--no-ff，记录合并过程，保留分支历史记录 在个人分支上，落后于master，先执行rebase，并且整理好commit，再回到master执行如上的merge rebase示例参考：https://www.jianshu.com/p/4a8f4af4e803 合并多个commit为一个完整commit当我们在本地仓库中提交了多次，在我们把本地提交push到公共仓库中之前，为了让提交记录更简洁明了，我们希望把如下分支B、C、D三个提交记录合并为一个完整的提交，然后再push到公共仓库。 现在我们在测试分支上添加了四次提交，我们的目标是把最后三个提交合并为一个提交： 这里我们使用命令: 1git rebase -i [startpoint] [endpoint] 其中-i的意思是--interactive，即弹出交互式的界面让用户编辑完成合并操作，[startpoint] [endpoint]则指定了一个编辑区间，如果不指定[endpoint]，则该区间的终点默认是当前分支HEAD所指向的commit(注：该区间指定的是一个前开后闭的区间)。在查看到了log日志后，我们运行以下命令： 1git rebase -i 36224db 或: 1git rebase -i HEAD~3 然后我们会看到如下界面: 上面未被注释的部分列出的是我们本次rebase操作包含的所有提交，下面注释部分是git为我们提供的命令说明。每一个commit id 前面的 1pick 表示指令类型，git 为我们提供了以下几个命令: pick：保留该commit（缩写:p） reword：保留该commit，但我需要修改该commit的注释（缩写:r） edit：保留该commit, 但我要停下来修改该提交(不仅仅修改注释)（缩写:e） squash：将该commit和前一个commit合并（缩写:s） fixup：将该commit和前一个commit合并，但我不要保留该提交的注释信息（缩写:f） exec：执行shell命令（缩写:x） drop：我要丢弃该commit（缩写:d） 根据我们的需求，我们将commit内容编辑如下: 然后是注释修改界面: 编辑完保存即可完成commit的合并了： 注意事项：如果这个过程中有操作错误，可以使用 git rebase --abort来撤销修改，回到没有开始操作合并之前的状态。 git多人合作流程针对于merge合并分支的流程 做几个假设:线上只有一条origin。 张三, 李四，王五都从origin上clone一个master分支 张三在本地master分支上，创建一个zhangsan分支 张三在zhangan分支上进行工作.工作完成后，切换到master分支 假如这时候origin分支上可能已经被李四或者wa王五提交了多个版本，假如有4个版本了，张三在master分支上pull一下，这时候肯定没有冲突 张三在master分支上，git merge zhangsan, 解决完冲突， 然后将master， push上去,这时候张三任务完成，这时候张三如果想继续在zhangsan分支上开发，直接在zhangsan分支上，让指针指向master的最顶端的commit即可 针对rebase合并分支的流程 做几个假设:线上只有一条origin 张三, 李四，王五都从origin上clone一个master分支 张三在本地master分支上，创建一个zhangsan分支 张三在zhangan分支上进行工作.工作完成后，切换到master分支 假如这时候origin分支上可能已经被李四或者wa王五提交了多个版本，假如有4个版本了，张三在master分支上pull一下，这时候肯定没有冲突， pull之后，切换会zhangsan分支 张三在zhangsan分支上，git rebase master, 这时候可能有多个冲突(zhangsan分支commit过几次，就会有几次冲突)， 然后 一直continue，直到解决完冲突为止。 然后切换回master， 在master分支上，指针指向zhangsan分支的顶端（操作方法 git reset –hard zhangsan顶端的commit值）,然后push master上 远程仓库 这时候张三任务完成]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile详解]]></title>
    <url>%2FDockerfile%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是DockerfileDocker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。有了 Dockerfile，当我们需要定制自己额外的需求时，只需在 Dockerfile 上添加或者修改指令，重新生成 image 即可，省去了敲命令的麻烦。 例： 1docker build -f /path/to/a/Dockerfile Dockerfile的基本结构Dockerfile 一般分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令，’#’ 为 Dockerfile 中的注释。 四部分 指令 基础镜像信息 FROM 维护者信息 MAINTAINER 镜像操作指令 RUN、COPY、ADD、EXPOSE等 容器启动时执行指令 CMD、ENTRYPOINT Dockerfile文件的第一条指令必须是FROM，其后可以是各种镜像的操作指令，最后是CMD或ENTRYPOINT指定容器启动时执行的命令。 下面引用yeasy/docker_practice对Dockerfile中各个指令的介绍， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081指令指令的一般格式为 INSTRUCTION arguments，指令包括 FROM、MAINTAINER、RUN 等。FROM格式为 FROM &lt;image&gt;或FROM &lt;image&gt;:&lt;tag&gt;。第一条指令必须为 FROM 指令。并且，如果在同一个Dockerfile中创建多个镜像时，可以使用多个 FROM 指令（每个镜像一次）。MAINTAINER格式为 MAINTAINER &lt;name&gt;，指定维护者信息。RUN格式为 RUN &lt;command&gt; 或 RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]。前者将在 shell 终端中运行命令，即 /bin/sh -c；后者则使用 exec 执行。指定使用其它终端可以通过第二种方式实现，例如 RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;]。每条 RUN 指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用 \ 来换行。CMD支持三种格式 CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] 使用 exec 执行，推荐方式； CMD command param1 param2 在 /bin/sh 中执行，提供给需要交互的应用； CMD [&quot;param1&quot;,&quot;param2&quot;] 提供给 ENTRYPOINT 的默认参数；指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条命令，只有最后一条会被执行。如果用户启动容器时候指定了运行的命令，则会覆盖掉 CMD 指定的命令。EXPOSE格式为 EXPOSE &lt;port&gt; [&lt;port&gt;...]。告诉 Docker 服务端容器暴露的端口号，供互联系统使用。在启动容器时需要通过 -P，Docker 主机会自动分配一个端口转发到指定的端口。ENV格式为 ENV &lt;key&gt; &lt;value&gt;。 指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持。例如ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATHADD格式为 ADD &lt;src&gt; &lt;dest&gt;。该命令将复制指定的 &lt;src&gt; 到容器中的 &lt;dest&gt;。 其中 &lt;src&gt; 可以是Dockerfile所在目录的一个相对路径；也可以是一个 URL；还可以是一个 tar 文件（自动解压为目录）。COPY格式为 COPY &lt;src&gt; &lt;dest&gt;。复制本地主机的 &lt;src&gt;（为 Dockerfile 所在目录的相对路径）到容器中的 &lt;dest&gt;。当使用本地目录为源目录时，推荐使用 COPY。ENTRYPOINT两种格式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT command param1 param2（shell中执行）。配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。VOLUME格式为 VOLUME [&quot;/data&quot;]。创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。USER格式为 USER daemon。指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如：RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres。要临时获取管理员权限可以使用 gosu，而不推荐 sudo。WORKDIR格式为 WORKDIR /path/to/workdir。为后续的 RUN、CMD、ENTRYPOINT 指令配置工作目录。可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如WORKDIR /aWORKDIR bWORKDIR cRUN pwd则最终路径为 /a/b/c。ONBUILD格式为 ONBUILD [INSTRUCTION]。配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。例如，Dockerfile 使用如下的内容创建了镜像 image-A。[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...]如果基于 image-A 创建新的镜像时，新的Dockerfile中使用 FROM image-A指定基础镜像时，会自动执行 ONBUILD 指令内容，等价于在后面添加了两条指令。FROM image-A #Automatically run the followingADD . /app/srcRUN /usr/local/bin/python-build --dir /app/src使用 ONBUILD 指令的镜像，推荐在标签中注明，例如 ruby:1.9-onbuild。 Dockerfile文件说明Docker以从上到下的顺序运行Dockerfile的指令。为了指定基本映像，第一条指令必须是FROM。一个声明以＃字符开头则被视为注释。可以在Docker文件中使用RUN，CMD，FROM，EXPOSE，ENV等指令。 在这里列出了一些常用的指令。 FROM：指定基础镜像，必须为第一个命令 12345678格式： FROM &lt;image&gt; FROM &lt;image&gt;:&lt;tag&gt; FROM &lt;image&gt;@&lt;digest&gt;示例： FROM mysql:5.6注： tag或digest是可选的，如果不使用这两个值时，会使用latest版本的基础镜像 MAINTAINER: 维护者信息 123456格式： MAINTAINER &lt;name&gt;示例： MAINTAINER Jasper Xu MAINTAINER sorex@163.com MAINTAINER Jasper Xu &lt;sorex@163.com&gt; RUN：构建镜像时执行的命令 12345678910111213RUN用于在镜像容器中执行命令，其有以下两种命令执行方式：shell执行格式： RUN &lt;command&gt;exec执行格式： RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]示例： RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] RUN apk update RUN [&quot;/etc/execfile&quot;, &quot;arg1&quot;, &quot;arg1&quot;]注： RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache ADD：将本地文件添加到容器中，tar类型文件会自动解压(网络压缩资源不会被解压)，可以访问网络资源，类似wget 12345678格式： ADD &lt;src&gt;... &lt;dest&gt; ADD [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] 用于支持包含空格的路径示例： ADD hom* /mydir/ # 添加所有以&quot;hom&quot;开头的文件 ADD hom?.txt /mydir/ # ? 替代一个单字符,例如：&quot;home.txt&quot; ADD test relativeDir/ # 添加 &quot;test&quot; 到 `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # 添加 &quot;test&quot; 到 /absoluteDir/ COPY：功能类似ADD，但是是不会自动解压文件，也不能访问网络资源 CMD：构建容器后调用，也就是在容器启动时才进行调用。 123456789格式： CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (执行可执行文件，优先) CMD [&quot;param1&quot;,&quot;param2&quot;] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数) CMD command param1 param2 (执行shell内部命令)示例： CMD echo &quot;This is a test.&quot; | wc - CMD [&quot;/usr/bin/wc&quot;,&quot;--help&quot;]注： CMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。 ENTRYPOINT：配置容器，使其可执行化。配合CMD可省去”application”，只使用参数。 123456789格式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (可执行文件, 优先) ENTRYPOINT command param1 param2 (shell内部命令)示例： FROM ubuntu ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;] CMD [&quot;-c&quot;]注： ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令。 LABEL：用于为镜像添加元数据 123456格式： LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...示例： LABEL version=&quot;1.0&quot; description=&quot;这是一个Web服务器&quot; by=&quot;IT笔录&quot;注： 使用LABEL指定元数据时，一条LABEL指定可以指定一或多条元数据，指定多条元数据时不同元数据之间通过空格分隔。推荐将所有的元数据通过一条LABEL指令指定，以免生成过多的中间镜像。 ENV：设置环境变量 1234567格式： ENV &lt;key&gt; &lt;value&gt; #&lt;key&gt;之后的所有内容均会被视为其&lt;value&gt;的组成部分，因此，一次只能设置一个变量 ENV &lt;key&gt;=&lt;value&gt; ... #可以设置多个变量，每个变量为一个&quot;&lt;key&gt;=&lt;value&gt;&quot;的键值对，如果&lt;key&gt;中包含空格，可以使用\来进行转义，也可以通过&quot;&quot;来进行标示；另外，反斜线也可以用于续行示例： ENV myName John Doe ENV myDog Rex The Dog ENV myCat=fluffy EXPOSE：指定于外界交互的端口 12345678格式： EXPOSE &lt;port&gt; [&lt;port&gt;...]示例： EXPOSE 80 443 EXPOSE 8080 EXPOSE 11211/tcp 11211/udp注： EXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口 VOLUME：用于指定持久化目录 1234567格式： VOLUME [&quot;/path/to/dir&quot;]示例： VOLUME [&quot;/data&quot;] VOLUME [&quot;/var/www&quot;, &quot;/var/log/apache2&quot;, &quot;/etc/apache2&quot;]注： 一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能： 123451 卷可以容器间共享和重用2 容器并不一定要和其它容器共享卷3 修改卷后会立即生效4 对卷的修改不会对镜像产生影响5 卷会一直存在，直到没有任何容器在使用它 WORKDIR：工作目录，类似于cd命令 12345678格式： WORKDIR /path/to/workdir示例： WORKDIR /a (这时工作目录为/a) WORKDIR b (这时工作目录为/a/b) WORKDIR c (这时工作目录为/a/b/c)注： 通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行。在使用docker run运行容器时，可以通过-w参数覆盖构建时所设置的工作目录。 USER:指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。使用USER指定用户时，可以使用用户名、UID或GID，或是两者的组合。当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户 1234567891011格式: USER user USER user:group USER uid USER uid:gid USER user:gid USER uid:group 示例： USER www 注： 使用USER指定用户后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT都将使用该用户。镜像构建完成后，通过docker run运行容器时，可以通过-u参数来覆盖所指定的用户。 ARG：用于指定传递给构建运行时的变量 12345格式： ARG &lt;name&gt;[=&lt;default value&gt;]示例： ARG site ARG build_user=www ONBUILD：用于设置镜像触发器 1234567格式： ONBUILD [INSTRUCTION]示例： ONBUILD ADD . /app/src ONBUILD RUN /usr/local/bin/python-build --dir /app/src注： 当所构建的镜像被用做其它镜像的基础镜像，该镜像中的触发器将会被钥触发 以下是一个小例子： 123456789101112131415161718192021222324252627282930313233# This my first nginx Dockerfile# Version 1.0# Base images 基础镜像FROM centos#MAINTAINER 维护者信息MAINTAINER tianfeiyu #ENV 设置环境变量ENV PATH /usr/local/nginx/sbin:$PATH#ADD 文件放在当前目录下，拷过去会自动解压ADD nginx-1.8.0.tar.gz /usr/local/ ADD epel-release-latest-7.noarch.rpm /usr/local/ #RUN 执行以下命令 RUN rpm -ivh /usr/local/epel-release-latest-7.noarch.rpmRUN yum install -y wget lftp gcc gcc-c++ make openssl-devel pcre-devel pcre &amp;&amp; yum clean allRUN useradd -s /sbin/nologin -M www#WORKDIR 相当于cdWORKDIR /usr/local/nginx-1.8.0 RUN ./configure --prefix=/usr/local/nginx --user=www --group=www --with-http_ssl_module --with-pcre &amp;&amp; make &amp;&amp; make installRUN echo &quot;daemon off;&quot; &gt;&gt; /etc/nginx.conf#EXPOSE 映射端口EXPOSE 80#CMD 运行以下命令CMD [&quot;nginx&quot;] 最后用一张图解释常用指令的意义 常见问题ADD VS COPY从Dockerfile构建Docker镜像时，您可以选择两个指令将目录/文件添加到镜像：ADD和COPY。两条指令都遵循相同的基本形式，并完成了几乎相同的事情： 12ADD &lt;src&gt;... &lt;dest&gt;COPY &lt;src&gt;... &lt;dest&gt; ADD 与COPY指令不同的是，ADD从一开始就是Docker的一部分，除了从构建上下文中复制文件之外，还支持一些其他技巧。 ADD指令允许您使用URL作为参数。提供URL时，将从URL下载文件并将其复制到。 12ADD http://foo.com/bar.go /tmp/main.go复制代码 上面的文件将从指定的URL下载并添加到容器的文件系统/tmp/main.go中。另一种形式可以让你简单地为下载的文件指定目标目录： 12ADD http://foo.com/bar.go /tmp/复制代码 由于参数以尾部”/“结尾，因此Docker会从URL中推断出文件名并将其添加到指定的目录中。在这种情况下，一个名为/tmp/bar.go的文件将被添加到容器的文件系统中。 ADD的另一个功能是能够自动解压缩压缩文件。如果参数是一个识别压缩格式（tar，gzip，bzip2等）的本地文件，那么它将被解压到容器文件系统中的指定处。 12ADD /foo.tar.gz /tmp/复制代码 上面的命令会导致foo.tar.gz归档文件的内容被解压到容器的/ tmp目录中。 有趣的是，URL下载和解压功能不能一起使用。任何通过URL复制的压缩文件都不会自动解压缩。 COPY 当版本1.0的Docker发布时，包含了新的COPY指令。与ADD不同的是，COPY直接将文件和文件夹从构建上下文复制到容器中。 COPY不支持URL作为参数，因此它不能用于从远程位置下载文件。任何想要复制到容器中的东西都必须存在于本地构建上下文中。 另外，COPY对压缩文件没有特别的处理。如果您复制归档文件，它将完全按照出现在构建上下文中的方式落入容器中，而不会尝试解压缩它。 COPY实际上只是ADD的精简版本，旨在满足大部分“复制文件到容器”的使用案例而没有任何副作用。 使用哪个？ 如果现在还不明显，Docker团队的建议是在几乎所有情况下都使用COPY。 真的，使用ADD的唯一原因是当你有一个压缩文件，你一定想自动解压到镜像中。理想情况下，ADD将被重新命名为EXTRACT之类的内容，以真正将这一点引入Docker生态（同样，出于向后兼容的原因，这不太可能发生）。 好的，但是从远程URL获取软件包的方法不是仍然有用吗？技术上，是的，但在大多数情况下，您可能会更好地运行curl或wget。考虑下面的例子： 12345ADD http://foo.com/package.tar.bz2 /tmp/RUN tar -xjf /tmp/package.tar.bz2 \ &amp;&amp; make -C /tmp/package \ &amp;&amp; rm /tmp/package.tar.bz2复制代码 这里我们有一条ADD指令，它从一个URL中检索一个包，然后是一条RUN指令，它将它解包，构建它，然后尝试清理下载的存档。 不幸的是，由于软件包检索和rm命令在单独的镜像层中，我们实际上并没有在最终镜像中节省任何空间（有关此现象的更详细的解释，请参阅我的优化docker镜像文章）。 在这种情况下,你最好像下面这样做： 12345RUN curl http://foo.com/package.tar.bz2 \ | tar -xjC /tmp/package \ &amp;&amp; make -C /tmp/package复制代码 这里我们使用curl命令下载压缩包，然后通过管道传递给tar命令解压。这样我们就不会在我们需要清理的文件系统上留下压缩文件。 将远程文件添加到镜像中可能仍有正当理由，但这应该是明确的决定，而不是您的默认选择。 最终，规则是这样的：使用COPY（除非你确定你需要ADD）。 CMD VS ENTRYPOINT 参考：https://www.cnblogs.com/rainwang/p/7274254.html CMD指令和ENTRYPOINT指令的作用都是为镜像指定容器启动后的命令，那么它们两者之间有什么各自的优点呢？ 为了更好地对比CMD指令和ENTRYPOINT指令的差异，我们这里再列一下这两个指令的说明， 1234567891011121314CMD支持三种格式 CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] 使用 exec 执行，推荐方式； CMD command param1 param2 在 /bin/sh 中执行，提供给需要交互的应用； CMD [&quot;param1&quot;,&quot;param2&quot;] 提供给 ENTRYPOINT 的默认参数；指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条命令，只有最后一条会被执行。如果用户启动容器时候指定了运行的命令，则会覆盖掉 CMD 指定的命令。ENTRYPOINT两种格式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT command param1 param2（shell中执行）。配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。 从上面的说明，我们可以看到有两个共同点： 都可以指定shell或exec函数调用的方式执行命令 当存在多个CMD指令或ENTRYPOINT指令时，只有最后一个生效 而它们有如下差异： 差异1：CMD指令指定的容器启动时命令可以被docker run指定的命令覆盖，而ENTRYPOINT指令指定的命令不能被覆盖，而是将docker run指定的参数当做ENTRYPOINT指定命令的参数 差异2：CMD指令可以为ENTRYPOINT指令设置默认参数，而且可以被docker run指定的参数覆盖 编写高效的Dockerfile下面是一个常见企业门户网站架构，由一个Web Server和一个数据库组成，Web Server提供web服务，数据库保存用户数据。通常情况下，这样一个门户网站安装在一台服务器上。 如果把应用运行在一个Docker容器中，那么很可能写出下面这样的Dockerfile来。 12345678910111213FROM ubuntuADD . /appRUN apt-get update RUN apt-get upgrade -y RUN apt-get install -y nodejs ssh mysql RUN cd /app &amp;&amp; npm install# this should start three processes, mysql and ssh# in the background and node app in foreground# isn&apos;t it beautifully terrible? &lt;3CMD mysql &amp; sshd &amp; npm start 当然这样Dockerfile有很多问题，这里CMD命令是错误的，只是为了说明问题而写。 下面的内容中将展示对这个Dockerfile进行改造，说明如何写出更好的Dockerfile，共有如下几种处理方法。 一个容器只运行一个进程从技术角度讲，Docker容器中可以运行多个进程，您可以将数据库，前端，后端，ssh等都运行在同一个Docker容器中。但是，这样跟未使用容器前没有太大区别，且这样容器的构建时间非常长（一处修改就要构建全部）、镜像体积大、横向扩展时非常浪费资源（不同的应用需要运行的容器数并不相同）。 通常所说的容器化改造是对应用整体微服务架构改造，再容器化，这样做可以带来如下好处。 单独扩展：拆分为微服务后，可单独增加或缩减每个微服务的实例数量。 提升开发速度：各微服务之间解耦，某个微服务的代码开发不影响其他微服务。 通过隔离确保安全：整体应用中，若存在安全漏洞，会获得所有功能的权限。微服务架构中，若攻击了某个服务，只可获得该服务的访问权限，无法入侵其他服务。 隔离崩溃：如果其中一个微服务崩溃，其它微服务还可以持续正常运行。 如前面的示例可以改造成下面架构，Web应用和MySQL运行在不同容器中。 MySQL运行在单独的镜像中，下面的示例中删掉了MySQL，只安装node.js。 1234567891011FROM ubuntuADD . /appRUN apt-get update RUN apt-get upgrade -yRUN apt-get install -y nodejs RUN cd /app &amp;&amp; npm installCMD npm start 不要在构建中升级版本为了降低复杂性、减少依赖、减小文件大小、节约构建时间，你应该避免安装任何不必要的包。例如，不要在数据库镜像中包含一个文本编辑器。 apt-get upgrade会使得镜像构建非常不确定，在构建时不确定哪些包会被安装，此时可能会产生不一致的镜像。 如果基础镜像中的某个包过时了，你应该联系它的维护者。如果你确定某个特定的包，比如 foo，需要升级，使用 apt-get install -y foo 就行，该指令会自动升级foo包。 删掉apt-get upgrade后，Dockerfile如下： 12345678910FROM ubuntuADD . /appRUN apt-get updateRUN apt-get install -y nodejsRUN cd /app &amp;&amp; npm installCMD npm start 将变化频率一样的RUN指令合一Docker镜像是分层的，类似于洋葱，它们都有很多层，为了修改内层，则需要将外面的层都删掉。Docker镜像有如下特性： Dockerfile中的每个指令都会创建一个新的镜像层。 镜像层将被缓存和复用。 Dockerfile修改后，复制的文件变化了或者构建镜像时指定的变量不同了，对应的镜像层缓存就会失效。 某一层的镜像缓存失效之后，它之后的镜像层缓存都会失效。 镜像层是不可变的，如果我们再某一层中添加一个文件，然后在下一层中删除它，则镜像中依然会包含该文件，只是这个文件在Docker容器中不可见。 将变化频率一样的指令合并在一起，目的是为了更好的将镜像分层，避免带来不必要的成本。如本例中将node.js安装与npm模块安装放在一起的话，则每次修改源代码，都需要重新安装node.js，这显然不合适。 12345678910FROM ubuntuADD . /appRUN apt-get update \ &amp;&amp; apt-get install -y nodejs \ &amp;&amp; cd /app \ &amp;&amp; npm installCMD npm start 因此，正确的写法是这样的: 1234567FROM ubuntuRUN apt-get update &amp;&amp; apt-get install -y nodejs ADD . /appRUN cd /app &amp;&amp; npm installCMD npm start 使用特定的标签当镜像没有指定标签时，将默认使用latest 标签。因此， FROM ubuntu 指令等同于FROM ubuntu:latest。当镜像更新时，latest标签会指向不同的镜像，这时构建镜像有可能失败。 如下示例中使用16.04作为标签。 1234567FROM ubuntu:16.04RUN apt-get update &amp;&amp; apt-get install -y nodejs ADD . /app RUN cd /app &amp;&amp; npm installCMD npm start 删除多余文件假设我们更新了apt-get源，下载解压并安装了一些软件包，它们都保存在/var/lib/apt/lists/目录中。 但是，运行应用时Docker镜像中并不需要这些文件。因此最好将它们删除，因为它会使Docker镜像变大。 示例Dockerfile中，删除/var/lib/apt/lists/目录中的文件。 12345678910FROM ubuntu:16.04RUN apt-get update \ &amp;&amp; apt-get install -y nodejs \ &amp;&amp; rm -rf /var/lib/apt/lists/*ADD . /app RUN cd /app &amp;&amp; npm installCMD npm start 选择合适的基础镜像在示例中，我们选择了ubuntu作为基础镜像。但是我们只需要运行node程序，没有必要使用一个通用的基础镜像，node镜像应该是更好的选择。 更好的选择是alpine版本的node镜像。alpine是一个极小化的Linux发行版，只有4MB，这让它非常适合作为基础镜像。 123456FROM node:7-alpineADD . /app RUN cd /app &amp;&amp; npm installCMD npm start 设置WORKDIR和 CMDWORKDIR指令可以设置默认目录，也就是运行RUN / CMD / ENTRYPOINT指令的地方。 CMD指令可以设置容器创建时执行的默认命令。另外，您应该将命令写在一个数组中，数组中每个元素为命令的每个单词。 1234567FROM node:7-alpineWORKDIR /app ADD . /app RUN npm installCMD [&quot;npm&quot;, &quot;start&quot;] 使用ENTRYPOINT (可选)ENTRYPOINT指令并不是必须的，因为它会增加复杂度。ENTRYPOINT是一个脚本，它会默认执行，并且将指定的命令作为其参数。它通常用于构建可执行的Docker镜像。 12345678FROM node:7-alpineWORKDIR /app ADD . /app RUN npm installENTRYPOINT [&quot;./entrypoint.sh&quot;] CMD [&quot;start&quot;] 在entrypoint脚本中使用exec在前文的entrypoint脚本中，我使用了exec命令运行node应用。不使用exec的话，我们则不能顺利地关闭容器，因为SIGTERM信号会被bash脚本进程吞没。exec命令启动的进程可以取代脚本进程，因此所有的信号都会正常工作。 优先使用COPYCOPY指令非常简单，仅用于将文件拷贝到镜像中。ADD相对来讲复杂一些，可以用于下载远程文件以及解压压缩包。 123456789FROM node:7-alpineWORKDIR /appCOPY . /appRUN npm installENTRYPOINT [&quot;./entrypoint.sh&quot;] CMD [&quot;start&quot;] 合理调整COPY与RUN的顺序将变化最少的部分放在Dockerfile的前面，这样可以充分利用镜像缓存。 示例中，源代码会经常变化，则每次构建镜像时都需要重新安装NPM模块，这显然不是我们希望看到的。因此我们可以先拷贝package.json，然后安装NPM模块，最后才拷贝其余的源代码。这样的话，即使源代码变化，也不需要重新安装NPM模块。 12345678910FROM node:7-alpineWORKDIR /appCOPY package.json /app RUN npm install COPY . /appENTRYPOINT [&quot;./entrypoint.sh&quot;] CMD [&quot;start&quot;] 设置默认的环境变量，映射端口和数据卷运行Docker容器时很可能需要一些环境变量。在Dockerfile设置默认的环境变量是一种很好的方式。另外，我们应该在Dockerfile中设置映射端口和数据卷。示例如下: 123456789101112FROM node:7-alpineENV PROJECT_DIR=/appWORKDIR $PROJECT_DIRCOPY package.json $PROJECT_DIR RUN npm install COPY . $PROJECT_DIRENTRYPOINT [&quot;./entrypoint.sh&quot;] CMD [&quot;start&quot;] ENV指令指定的环境变量在容器中可以使用。如果你只是需要指定构建镜像时的变量，你可以使用ARG指令。 使用EXPOSE暴露端口EXPOSE指令用于指定容器将要监听的端口。因此，你应该为你的应用程序使用常见的端口。例如，提供Apache web服务的镜像应该使用EXPOSE 80，而提供MongoDB服务的镜像使用EXPOSE 27017。 对于外部访问，用户可以在执行 docker run 时使用一个标志来指示如何将指定的端口映射到所选择的端口。 123456789101112131415FROM node:7-alpineENV PROJECT_DIR=/appWORKDIR $PROJECT_DIRCOPY package.json $PROJECT_DIR RUN npm install COPY . $PROJECT_DIRENV APP_PORT=3000EXPOSE $APP_PORTENTRYPOINT [&quot;./entrypoint.sh&quot;] CMD [&quot;start&quot;] 使用VOLUME管理数据卷VOLUME 指令用于暴露任何数据库存储文件，配置文件，或容器创建的文件和目录。强烈建议使用 VOLUME来管理镜像中的可变部分和用户可以改变的部分。 下面示例中填写一个媒体目录。 123456789101112131415161718FROM node:7-alpineENV PROJECT_DIR=/appWORKDIR $PROJECT_DIRCOPY package.json $PROJECT_DIR RUN npm install COPY . $PROJECT_DIRENV MEDIA_DIR=/media \ APP_PORT=3000VOLUME $MEDIA_DIR EXPOSE $APP_PORTENTRYPOINT [&quot;./entrypoint.sh&quot;] CMD [&quot;start&quot;] 使用LABEL设置镜像元数据你可以给镜像添加标签来帮助组织镜像、记录许可信息、辅助自动化构建等。每个标签一行，由 LABEL 开头加上一个或多个标签对。 如果你的字符串中包含空格，必须将字符串放入引号中或者对空格使用转义。如果字符串内容本身就包含引号，必须对引号使用转义。 12FROM node:7-alpine LABEL com.example.version=&quot;0.0.1-beta&quot; 添加HEALTHCHECK运行容器时，可以指定–restart always选项。这样的话，容器崩溃时，docker daemon会重启容器。对于需要长时间运行的容器，这个选项非常有用。但是，如果容器的确在运行，但是不可用怎么办？使用HEALTHCHECK指令可以让Docker周期性的检查容器的健康状况。我们只需要指定一个命令，如果一切正常的话返回0，否则返回1。当请求失败时，curl –fail 命令返回非0状态。示例如下： 12345678910111213141516171819FROM node:7-alpine LABEL com.example.version=&quot;0.0.1-beta&quot;ENV PROJECT_DIR=/app WORKDIR $PROJECT_DIRCOPY package.json $PROJECT_DIR RUN npm install COPY . $PROJECT_DIRENV MEDIA_DIR=/media \ APP_PORT=3000VOLUME $MEDIA_DIR EXPOSE $APP_PORT HEALTHCHECK CMD curl --fail http://localhost:$APP_PORT || exit 1ENTRYPOINT [&quot;./entrypoint.sh&quot;] CMD [&quot;start&quot;] 编写.dockerignore文件.dockerignore的作用和语法类似于.gitignore，可以忽略一些不需要的文件，这样可以有效加快镜像构建时间，同时减少Docker镜像的大小。 构建镜像时，Docker需要先准备context ，将所有需要的文件收集到进程中。默认的context包含Dockerfile目录中的所有文件，但是实际上，我们并不需要.git目录等内容。 示例如下： 1.git/]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes部署nginx服务器]]></title>
    <url>%2FKubernetes%E9%83%A8%E7%BD%B2nginx%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[GitHub: https://github.com/kubernetes/examples/tree/master/staging/https-nginx https 自动检测配置文件更改 部署下面是我自己写的http服务器的yaml文件 nginx-deploy.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849---apiVersion: apps/v1beta1kind: Deploymentmetadata: name: nginx namespace: rollouts-validspec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.17 ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/nginx.conf readOnly: true subPath: nginx.conf name: nginx-conf - mountPath: /etc/nginx/conf.d name: ro-content-conf - mountPath: /var/log/nginx name: log resources: limits: cpu: 200m memory: 1Gi requests: cpu: 100m memory: 0.5Gi volumes: - name: nginx-conf configMap: name: nginx-conf # place ConfigMap `nginx-conf` on /etc/nginx items: - key: nginx.conf path: nginx.conf - name: ro-content-conf configMap: name: nginx-conf # place ConfigMap `nginx-conf` on /etc/nginx items: - key: ro-content.conf path: ro-content.conf - name: log emptyDir: &#123;&#125; nginx-config.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051---apiVersion: apps/v1beta1kind: Deploymentmetadata: name: nginx namespace: rollouts-validspec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: swr.cn-east-2.myhuaweicloud.com/bosch-test/ro-static-content-nginx:1.0 ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/nginx.conf readOnly: true subPath: nginx.conf name: nginx-conf - mountPath: /etc/nginx/conf.d name: ro-content-conf - mountPath: /var/log/nginx name: log resources: limits: cpu: 200m memory: 1Gi requests: cpu: 100m memory: 0.5Gi volumes: - name: nginx-conf configMap: name: nginx-conf # place ConfigMap `nginx-conf` on /etc/nginx items: - key: nginx.conf path: nginx.conf - name: ro-content-conf configMap: name: nginx-conf # place ConfigMap `nginx-conf` on /etc/nginx items: - key: ro-content.conf path: ro-content.conf - name: log emptyDir: &#123;&#125; imagePullSecrets: - name: default-secret nginx-svc.yml 12345678910111213---apiVersion: v1kind: Servicemetadata: name: nginx namespace: rollouts-validspec: type: NodePort ports: - port: 80 targetPort: 80 selector: app: nginx 用户认证配置ngx_http_auth_basic_module模块实现让访问着，只有输入正确的用户密码才允许访问web内容。web上的一些内容不想被其他人知道，但是又想让部分人看到。nginx的http auth模块以及Apache http auth都是很好的解决方案。 默认情况下nginx已经安装了ngx_http_auth_basic_module模块，如果不需要这个模块，可以加上 –without-http_auth_basic_module 。 nginx basic auth指令 语法: auth_basic string | off;默认值: auth_basic off;配置段: http, server, location, limit_except 默认表示不开启认证，后面如果跟上字符，这些字符会在弹窗中显示。 语法: auth_basic_user_file file;默认值: —配置段: http, server, location, limit_except 用户密码文件，文件内容类似如下： 12ttlsauser1:password1ttlsauser2:password2 nginx认证配置实例 123456789101112server&#123; server_name www.ttlsa.com ttlsa.com; index index.html index.php; root /data/site/www.ttlsa.com; location /&#123; auth_basic &quot;nginx bastic auth&quot;; auth_basic_user_file /etc/nginx/conf.d/htpasswd; autoindex on; &#125;&#125; 一定要注意auth_basic_user_file路径，否则会不厌其烦的出现403。 生成密码 可以使用htpasswd，或者使用openssl 1234#printf &quot;ttlsa:$(openssl passwd -crypt 123456)\n&quot; &gt;&gt;conf/htpasswd#cat conf/htpasswd ttlsa:xyJkVhXGAZ8tM 账号：ttlsa密码：123456 reload nginx 1nginx -s reload]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes技巧及常用组件了解]]></title>
    <url>%2FKubernetes%E6%8A%80%E5%B7%A7%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6%E4%BA%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[官方文档：https://kubernetes.io/ 中文文档: http://docs.kubernetes.org.cn/ GitHub: https://github.com/kubernetes/kubernetes [TOC] API集群外部:1.使用kubectl proxy 1kubectl proxy --port=8080 2.使用curl http方式访问 1curl http://localhost:8080/api/ 3.使用curl https方式访问,认证方式为token 12345APISERVER=$(kubectl config view | grep server | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d &apos; &apos;) | grep -E &apos;^token&apos; | cut -f2 -d&apos;:&apos; | tr -d &apos;\t&apos;)curl --header &quot;Authorization: Bearer $TOKEN&quot; --insecure $APISERVER/api/v1/nodes 4.使用kubectl https方式访问,认证方式为证书,双向认证 1234567891011121314151617181920#cd /etcd/kubernetes/ssl#openssl genrsa -out pwm.key 2048#openssl req -new -key pwm.key -out pwm.csr -subj &quot;/CN=pwm&quot;#openssl x509 -req -days 365 -in pwm.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out pwm.crt#创建用户pwm,已经想要的role和rolebongding# kubectl --server=https://192.168.61.100:6443 \--certificate-authority=ca.pem \--client-certificate=pwm.crt \--client-key=pwm.key \kubectl get nodes 集群内部:1.在使用了service account的pod内部 123curl -k -v -H &quot;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; https://kubernetes.default/api/v1/namespaces/curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.cem -v -H &quot;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; https://kubernetes.default/api/v1/namespaces/ ConfigMapConfigMap是用来存储配置文件的kubernetes资源对象，所有的配置内容都存储在etcd中。 官方示例：https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/ 创建ConfigMap创建ConfigMap的方式有4种： 通过直接在命令行中指定configmap参数创建，即--from-literal 通过指定文件创建，即将一个配置文件创建为一个ConfigMap--from-file=&lt;文件&gt; 通过指定目录创建，即将一个目录下的所有配置文件创建为一个ConfigMap，--from-file=&lt;目录&gt; 事先写好标准的configmap的yaml文件，然后kubectl create -f 创建 通过命令行参数--from-literal创建创建命令： 1kubectl create configmap test-config1 --from-literal=db.host=10.5.10.116 --from-listeral=db.port=&apos;3306&apos; 指定文件创建配置文件app.properties的内容： 123[mysqld]port=3306socket=...... 创建命令（可以有多个--from-file）： 1kubectl create configmap test-config2 --from-file=./app.properties1 假如不想configmap中的key为默认的文件名，还可以在创建时指定key名字： 1kubectl create configmap game-config-3 --from-file=&lt;my-key-name&gt;=&lt;path-to-file&gt; 指定目录创建configs 目录下的config-1和config-2内容如下所示： 12345678config-1:aaaaabbbbbcccccconfig-2:dddddeeeee 创建命令： 1kubectl create configmap test-config3 --from-file=./configs1 可以看到指定目录创建时configmap内容中的各个文件会创建一个key/value对，key是文件名，value是文件内容。 那假如目录中还包含子目录呢？继续做实验：在上一步的configs目录下创建子目录subconfigs，并在subconfigs下面创建两个配置文件，指定目录configs创建名为test-config4的configmap: 1kubectl create configmap test-config4 --from-file=./configs1 结果说明指定目录时只会识别其中的文件，忽略子目录 通过事先写好configmap的标准yaml文件创建yaml文件如图所示： 注意其中一个key的value有多行内容时的写法 12345678910111213141516apiVersion: v1kind: ConfigMapmetadata: name: test-cfg namespace: defaultdata: cache_host: memcached-gcxt cache_port: &quot;11211&quot; cache_prefix: gcxt my.cnf: | [mysqld] log-bin = mysql-bin app.properties: | property.1 = value-1 property.2 = value-2 property.3 = value-3 创建： 1kubectl create -f test-cfg.yml 使用ConfigMap使用ConfigMap有三种方式: 第一种是通过环境变量的方式，直接传递给pod 第二种是通过在pod的命令行下运行的方式(启动命令中) 第三种是作为volume的方式挂载到pod内 通过环境变量使用使用valueFrom.configMapKeyRef name、key指定要用的key: 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: LOG_LEVEL valueFrom: configMapKeyRef: name: env-config key: log_level restartPolicy: Never 还可以通过envFrom.configMapRef、name使得configmap中的所有key/value对都自动变成环境变量 12345678910111213apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] envFrom: - configMapRef: name: special-config restartPolicy: Never 在启动命令中引用在命令行下引用时，需要先设置为环境变量，之后可以通过$(VAR_NAME)设置容器启动命令的启动参数： 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never 作为volume挂载使用1234567891011121314151617181920212223apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-configmapspec: replicas: 1 template: metadata: labels: app: nginx-configmap spec: containers: - name: nginx-configmap image: nginx ports: - containerPort: 80 volumeMounts: - name: config-volume4 mountPath: /tmp/config4 volumes: - name: config-volume4 configMap: name: test-config 这样会在在/tmp/config4文件夹下以每一个key为文件名value为值创建了多个文件 注意：如果原有/tmp/config4目录下有文件或者文件夹，将会全部覆盖掉，只留有test-config中的key文件 假如不想以key名作为配置文件名可以引入items 字段，在其中逐个指定要用相对路径path替换的key： 123456789volumes: - name: config-volume4 configMap: name: test-config4 items: - key: my.cnf path: path/to/mysql-key #最终生成/tmp/config4/path/to/mysql-key文件 - key: cache_host path: cache-host 备注： 删除configmap后原pod不受影响；然后再删除pod后，重启的pod的events会报找不到cofigmap的volume； pod起来后再通过kubectl edit configmap …修改configmap，过一会pod内部的配置也会刷新。 在容器内部修改挂进去的配置文件后，过一会内容会再次被刷新为原始configmap内容 深度解析mountPath,subPath,key,path的关系和作用结论： kubernetes key (pod.spec.volums[0].configMap.items[0].key)用于指定configMap中的哪些条目可用于挂载 kubernetes path (pod.spec.volums[0].configMap.items[0].path)用于将key重命名 kubernetes suPath (pod.spec.containers[0].volumeMounts.subPath)决定容器中有无挂载（按名字从key，有path时以path为主，中比对是否存在要的条目） kubernetes mountPath (pod.spec.containers[0].volumeMounts.mountPath)决定容器中挂载的结果文件名 无subPath时： mountPath为文件夹，其下挂载ConfigMap中的文件，key为文件名，value为文件内容，并且会覆盖掉原有mountPath中的内容。如果此时指定了vloumes.configMap.items[0].path，则path条目用于将key重命名 有subPath时： subPath匹配为true时，mountPath为文件名 subPath匹配为false时，mountPath为文件夹名 mountPath结合subPath作用有subPath时且subPath推荐筛选结果为true，mountPath指定到文件名 1234567891011121314151617181920212223242526[root@k8s-master k8s-objs]# cat pod-configmap-testvolume.yamlapiVersion: v1kind: Podmetadata: labels: purpose: test-configmap-volume name: testvolumespec: containers: - name: test-configmap-volume image: tomcat:8 imagePullPolicy: IfNotPresent #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ] volumeMounts: - name: config-volume mountPath: /etc/config/app.properties #此处配合suPath使用时，app.properties为文件名，即pod容器中只生成了/etc/config目录，目录之下为文件，只有一个名为app.properties的文件（subPath筛选只挂载app.properties文件） subPath: app.properties volumes: - name: config-volume configMap: name: test-cfg items: - key: cache_host path: path/to/special-key-cache - key: app.properties path: app.properties 进入容器查看： 123456789[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bashroot@testvolume:/usr/local/tomcat# cd /etc/configroot@testvolume:/etc/config# ls -ltotal 4-rw-r--r-- 1 root root 63 Apr 12 01:59 app.propertiesroot@testvolume:/etc/config# cat app.properties property.1 = value-1property.2 = value-2property.3 = value-3 有subPath但筛选结果为false,容器中生成一个空目录/etc/config/app.properties，无文件 subPath筛选范围优先级为pod.spec.volums[0].configMap.items[0].path&gt;pod.spec.volums[0].configMap.items[0].key&gt;configMap.key，本例中为path,即在path指定的条目【“cache_host”,”app-properties “注意中间是横杠不是点】找是否有subPath项“app.properties”注意中间为点，查找结果为false,所以无文件挂载。容器将“/etc/config/app.properties”当成一个待创建的路径。 1234567891011121314151617181920212223242526[root@k8s-master k8s-objs]# vi pod-configmap-testvolume.yamlapiVersion: v1kind: Podmetadata: labels: purpose: test-configmap-volume name: testvolumespec: containers: - name: test-configmap-volume image: tomcat:8 imagePullPolicy: IfNotPresent #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ] volumeMounts: - name: config-volume mountPath: /etc/config/app.properties subPath: app.properties volumes: - name: config-volume configMap: name: test-cfg items: - key: cache_host path: path/to/special-key-cache - key: app.properties path: app-properties #此处path相当于更改文件名mv app.properties app-properties 进入容器查看： 123456789[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bashroot@testvolume:/usr/local/tomcat# cd /etc/configroot@testvolume:/etc/config# ls -ltotal 0drwxrwxrwx 2 root root 6 Apr 12 02:11 app.propertiesroot@testvolume:/etc/config# cat app.propertiescat: app.properties: Is a directoryroot@testvolume:/etc/config# cd app.propertiesroot@testvolume:/etc/config/app.properties# ls #此目录下为空 无 subPath,path相当于重命名1234567891011121314151617181920212223242526[root@k8s-master k8s-objs]# cat pod-configmap-testvolume.yamlapiVersion: v1kind: Podmetadata: labels: purpose: test-configmap-volume name: testvolumespec: containers: - name: test-configmap-volume image: tomcat:8 imagePullPolicy: IfNotPresent #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ] volumeMounts: - name: config-volume mountPath: /etc/config/app.properties ##此处app.properties为目录 #subPath: app.properties volumes: - name: config-volume configMap: name: test-cfg items: - key: cache_host path: path/to/special-key-cache - key: app.properties path: app-properties #此处path相当于重命名 进入容器内查看 1234567891011121314151617181920212223[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bashroot@testvolume:/usr/local/tomcat# cd /etc/configroot@testvolume:/etc/config# ls -ltotal 0drwxrwxrwx 3 root root 93 Apr 12 02:20 app.propertiesroot@testvolume:/etc/config# cd app.propertiesroot@testvolume:/etc/config/app.properties# ls -ltotal 0lrwxrwxrwx 1 root root 21 Apr 12 02:20 app-properties -&gt; ..data/app-propertieslrwxrwxrwx 1 root root 11 Apr 12 02:20 path -&gt; ..data/pathroot@testvolume:/etc/config/app.properties# cat app-properties property.1 = value-1property.2 = value-2property.3 = value-3root@testvolume:/etc/config/app.properties# cat pathcat: path: Is a directoryroot@testvolume:/etc/config/app.properties# cd pathroot@testvolume:/etc/config/app.properties/path# cd toroot@testvolume:/etc/config/app.properties/path/to# ls -ltotal 4-rw-r--r-- 1 root root 9 Apr 12 02:20 special-key-cacheroot@testvolume:/etc/config/app.properties/path/to# cat special-key-cache mysql-k8s 有subPath且筛选结果为true,mouthPath指定文件名，可以和subPath不一样subPath决定有无，mountPath决定文件名 1234567891011121314151617181920212223242526[root@k8s-master k8s-objs]# vi pod-configmap-testvolume.yamlapiVersion: v1kind: Podmetadata: labels: purpose: test-configmap-volume name: testvolumespec: containers: - name: test-configmap-volume image: tomcat:8 imagePullPolicy: IfNotPresent #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ] volumeMounts: - name: config-volume mountPath: /etc/config/z.txt #subPath决定有无，mountPath决定文件名为z.txt subPath: app-properties volumes: - name: config-volume configMap: name: test-cfg items: - key: cache_host path: path/to/special-key-cache - key: app.properties path: app-properties 进入容器查看： 12345678910[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bashroot@testvolume:/usr/local/tomcat# cd /etc/configroot@testvolume:/etc/config# lsz.txtroot@testvolume:/etc/config# pwd/etc/configroot@testvolume:/etc/config# cat z.txt property.1 = value-1property.2 = value-2property.3 = value-3 configmap的热更新研究更新 ConfigMap 后： 使用该 ConfigMap 挂载的 Env 不会同步更新 使用该 ConfigMap 挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新 ENV 是在容器启动的时候注入的，启动之后 kubernetes 就不会再改变环境变量的值，且同一个 namespace 中的 pod 的环境变量是不断累加的，参考 Kubernetes中的服务发现与docker容器间的环境变量传递源码探究。为了更新容器中使用 ConfigMap 挂载的配置，可以通过滚动更新 pod 的方式来强制重新挂载 ConfigMap，也可以在更新了 ConfigMap 后，先将副本数设置为 0，然后再扩容。 ServiceAccountService account是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的。它与User account不同 User account是为人设计的，而service account则是为Pod中的进程调用Kubernetes API而设计； User account是跨namespace的，而service account则是仅局限它所在的namespace； 每个namespace都会自动创建一个default service account Token controller检测service account的创建，并为它们创建secret 开启ServiceAccount Admission Controller后 每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout） 验证Pod引用的service account已经存在，否则拒绝创建 如果Pod没有指定ImagePullSecrets，则把service account的ImagePullSecrets加到Pod中 每个container启动后都会挂载该service account的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/ 1234$ kubectl exec nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccountca.crtnamespacetoken 创建Service Account1234567891011121314$ kubectl create serviceaccount jenkinsserviceaccount &quot;jenkins&quot; created$ kubectl get serviceaccounts jenkins -o yamlapiVersion: v1kind: ServiceAccountmetadata: creationTimestamp: 2017-05-27T14:32:25Z name: jenkins namespace: default resourceVersion: &quot;45559&quot; selfLink: /api/v1/namespaces/default/serviceaccounts/jenkins uid: 4d66eb4c-42e9-11e7-9860-ee7d8982865fsecrets:- name: jenkins-token-l9v7v 自动创建的secret： 123456789101112131415161718kubectl get secret jenkins-token-l9v7v -o yamlapiVersion: v1data: ca.crt: (APISERVER CA BASE64 ENCODED) namespace: ZGVmYXVsdA== token: (BEARER TOKEN BASE64 ENCODED)kind: Secretmetadata: annotations: kubernetes.io/service-account.name: jenkins kubernetes.io/service-account.uid: 4d66eb4c-42e9-11e7-9860-ee7d8982865f creationTimestamp: 2017-05-27T14:32:25Z name: jenkins-token-l9v7v namespace: default resourceVersion: &quot;45558&quot; selfLink: /api/v1/namespaces/default/secrets/jenkins-token-l9v7v uid: 4d697992-42e9-11e7-9860-ee7d8982865ftype: kubernetes.io/service-account-token 添加ImagePullSecrets 123456789101112apiVersion: v1kind: ServiceAccountmetadata: creationTimestamp: 2015-08-07T22:02:39Z name: default namespace: default selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6secrets:- name: default-token-uudgeimagePullSecrets:- name: myregistrykey 授权Service Account为服务提供了一种方便的认证机制，但它不关心授权的问题。可以配合RBAC来为Service Account鉴权： 配置–authorization-mode=RBAC和–runtime-config=rbac.authorization.k8s.io/v1alpha1 配置–authorization-rbac-super-user=admin 定义Role、ClusterRole、RoleBinding或ClusterRoleBinding 比如 12345678910111213141516171819202122232425# This role allows to read pods in the namespace &quot;default&quot;kind: RoleapiVersion: rbac.authorization.k8s.io/v1alpha1metadata: namespace: default name: pod-readerrules: - apiGroups: [&quot;&quot;] # The API group &quot;&quot; indicates the core API Group. resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] nonResourceURLs: []---# This role binding allows &quot;default&quot; to read pods in the namespace &quot;default&quot;kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1alpha1metadata: name: read-pods namespace: defaultsubjects: - kind: ServiceAccount # May be &quot;User&quot;, &quot;Group&quot; or &quot;ServiceAccount&quot; name: defaultroleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io 示例12345678#定义namespace：testcat &gt;&gt; test.yaml &lt;&lt; EOFapiVersion: v1kind: Namespacemetadata: name: test labels: name: test 12#创建namespace：testkubectl create -f ./test.yaml 1234567#查看命名空间test的sakubectl get sa -n testNAME SECRETS AGEdefault 1 3h##说明：（1）如果kubernetes开启了ServiceAccount（–admission_control=…,ServiceAccount,… ）那么会在每个namespace下面都会创建一个默认的default的sa。如上命令查看的default ！（2）ServiceAccount默认是开启的。 1234567891011121314151617#查看命名空间test生成的defaultkubectl get sa default -o yaml -n testapiVersion: v1kind: ServiceAccountmetadata: creationTimestamp: 2018-05-31T06:21:10Z name: default namespace: test resourceVersion: &quot;45560&quot; selfLink: /api/v1/namespaces/test/serviceaccounts/default uid: cf57c735-649a-11e8-adc5-000c290a7d06secrets:- name: default-token-ccf9m##说明：（1）当用户再该namespace下创建pod的时候都会默认使用这个sa；（2）每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout）；（3）每个container启动后都会挂载对应的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/。 12345678910111213141516171819#创建deploycat &gt;&gt; nginx_deploy.yaml &lt;&lt; EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-test namespace: testspec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 12345#查看生成的Podskubectl get po -n testNAME READY STATUS RESTARTS AGEnginx-test-75675f5897-7l5bc 1/1 Running 0 1hnginx-test-75675f5897-b7pcn 1/1 Running 0 1h 123456789101112131415161718#查看其中一个Pod的详细信息，如：nginx-test-75675f5897-7l5bckubectl describe po nginx-test-75675f5897-7l5bc -n test##其中default-token-ccf9m，请留意！Environment: &lt;none&gt;Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-ccf9m (ro)Conditions:Type StatusInitialized TrueReady TruePodScheduled TrueVolumes:default-token-ccf9m:Type: Secret (a volume populated by a Secret)SecretName: default-token-ccf9m##说明：（1）每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout）；（2）每个container启动后都会挂载对应的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/。 12345678910#进入其中一个Pod的容器内，如：nginx-test-75675f5897-7l5bckubectl exec -it nginx-test-75675f5897-7l5bc /bin/bash --namespace=test##在容器内执行：ls -l /var/run/secrets/kubernetes.io/serviceaccount/lrwxrwxrwx 1 root root 13 May 31 08:15 ca.crt -&gt; ..data/ca.crtlrwxrwxrwx 1 root root 16 May 31 08:15 namespace -&gt; ..data/namespacelrwxrwxrwx 1 root root 12 May 31 08:15 token -&gt; ..data/token##说明：可以看到已将ca.crt 、namespace和token放到容器内了，那么这个容器就可以通过https的请求访问apiserver了。 手工创建ServiceAccount 12345678#编辑heapster_test.yaml文件cat &gt;&gt; heapster_test.yaml &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: heapster namespace: testEOF 123#创建Service Account：heapsterkubectl create -f heapster_test.yamlserviceaccount &quot;heapster&quot; created 12345#查看Service Account：heapsterkubectl get sa -o yaml -n test##主要内容如下： secrets: - name: heapster-token-7xrlg Service概念 运行在Pod中的应用是向客户端提供服务的守护进程，比如，nginx、tomcat、etcd等等，它们都是受控于控制器的资源对象，存在生命周期，我们知道Pod资源对象在自愿或非自愿终端后，只能被重构的Pod对象所替代，属于不可再生类组件。而在动态和弹性的管理模式下，Service为该类Pod对象提供了一个固定、统一的访问接口和负载均衡能力。 其实，就是说Pod存在生命周期，有销毁，有重建，无法提供一个固定的访问接口给客户端。并且为了同类的Pod都能够实现工作负载的价值，由此Service资源出现了，可以为一类Pod资源对象提供一个固定的访问接口和负载均衡，类似于阿里云的负载均衡或者是LVS的功能。 但是要知道的是，Service和Pod对象的IP地址，一个是虚拟地址，一个是Pod IP地址，都仅仅在集群内部可以进行访问，无法接入集群外部流量。而为了解决该类问题的办法可以是在单一的节点上做端口暴露（hostPort）以及让Pod资源共享工作节点的网络名称空间（hostNetwork）以外，还可以使用NodePort或者是LoadBalancer类型的Service资源，或者是有7层负载均衡能力的Ingress资源。 Service是Kubernetes的核心资源类型之一，Service资源基于标签选择器将一组Pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求到组内的Pod对象，如下图所示，它向客户端隐藏了真是的，处理用户请求的Pod资源，使得从客户端上看，就像是由Service直接处理并响应一样，是不是很像负载均衡器呢！ Service对象的IP地址也称为Cluster IP，它位于为Kubernetes集群配置指定专用的IP地址范围之内，是一种虚拟的IP地址，它在Service对象创建之后保持不变，并且能够被同一集群中的Pod资源所访问。Service端口用于接受客户端请求，并将请求转发至后端的Pod应用的相应端口，这样的代理机制，也称为端口代理，它是基于TCP/IP 协议栈的传输层。 Service的实现模型 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 在 Kubernetes v1.0 版本，代理完全在 userspace。在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。 从 Kubernetes v1.2 起，默认就是 iptables 代理。在Kubernetes v1.8.0-beta.0中，添加了ipvs代理。在 Kubernetes v1.0 版本，Service 是 “4层”（TCP/UDP over IP）概念。 在 Kubernetes v1.1 版本，新增了 Ingress API（beta 版），用来表示 “7层”（HTTP）服务。 kube-proxy 这个组件始终监视着apiserver中有关service的变动信息，获取任何一个与service资源相关的变动状态，通过watch监视，一旦有service资源相关的变动和创建，kube-proxy都要转换为当前节点上的能够实现资源调度规则（例如：iptables、ipvs） userspace代理模式这种模式，当客户端Pod请求内核空间的service iptables后，把请求转到给用户空间监听的kube-proxy 的端口，由kube-proxy来处理后，再由kube-proxy将请求转给内核空间的 service ip，再由service iptalbes根据请求转给各节点中的的service pod。 由此可见这个模式有很大的问题，由客户端请求先进入内核空间的，又进去用户空间访问kube-proxy，由kube-proxy封装完成后再进去内核空间的iptables，再根据iptables的规则分发给各节点的用户空间的pod。这样流量从用户空间进出内核带来的性能损耗是不可接受的。在Kubernetes 1.1版本之前，userspace是默认的代理模型。 iptables代理模式 客户端IP请求时，直接请求本地内核service ip，根据iptables的规则直接将请求转发到到各pod上，因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。iptables代理模式由Kubernetes 1.1版本引入，自1.2版本开始成为默认类型。 ipvs代理模式Kubernetes自1.9-alpha版本引入了ipvs代理模式，自1.11版本开始成为默认设置。客户端IP请求时到达内核空间时，根据ipvs的规则直接分发到各pod上。kube-proxy会监视Kubernetes Service对象和Endpoints，调用netlink接口以相应地创建ipvs规则并定期与Kubernetes Service对象和Endpoints对象同步ipvs规则，以确保ipvs状态与期望一致。访问服务时，流量将被重定向到其中一个后端Pod。 与iptables类似，ipvs基于netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如： rr：轮询调度 lc：最小连接数 dh：目标哈希 sh：源哈希 sed：最短期望延迟 nq：不排队调度 注意： ipvs模式假定在运行kube-proxy之前在节点上都已经安装了IPVS内核模块。当kube-proxy以ipvs代理模式启动时，kube-proxy将验证节点上是否安装了IPVS模块，如果未安装，则kube-proxy将回退到iptables代理模式。 如果某个服务后端pod发生变化，标签选择器适应的pod有多一个，适应的信息会立即反映到apiserver上,而kube-proxy一定可以watch到etc中的信息变化，而将它立即转为ipvs或者iptables中的规则，这一切都是动态和实时的，删除一个pod也是同样的原理。如图： Service的定义Service字段含义1234567891011121314151617181920212223242526272829303132333435[root@k8s-master ~]# kubectl explain svcKIND: ServiceVERSION: v1DESCRIPTION: Service is a named abstraction of software service (for example, mysql) consisting of local port (for example 3306) that the proxy listens on, and the selector that determines which pods will answer requests sent through the proxy.FIELDS: apiVersion &lt;string&gt; APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources kind &lt;string&gt; Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds metadata &lt;Object&gt; Standard object&apos;s metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata spec &lt;Object&gt; Spec defines the behavior of a service. https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status status &lt;Object&gt; Most recently observed status of the service. Populated by the system. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status 其中重要的4个字段： 12345678apiVersion:kind:metadata:spec: clusterIP: 可以自定义，也可以动态分配 ports:（与后端容器端口关联） selector:（关联到哪些pod资源上） type：服务类型 service的类型对一些应用（如 Frontend）的某些部分，可能希望通过外部（Kubernetes 集群外部）IP 地址暴露 Service。 Kubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP 类型。 Type 的取值以及行为如下： ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 &lt;NodeIP&gt;:&lt;NodePort&gt;，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。 没有任何类型代理被创建，这只有 Kubernetes 1.7 或更高版本的 kube-dns 才支持。 ClusterIP的service类型演示1234567891011121314151617181920212223242526272829303132333435363738[root@k8s-master mainfests]# cat redis-svc.yaml apiVersion: v1kind: Servicemetadata: name: redis namespace: defaultspec: selector: #标签选择器，必须指定pod资源本身的标签 app: redis role: logstor type: ClusterIP #指定服务类型为ClusterIP ports: #指定端口 - port: 6379 #暴露给服务的端口 - targetPort: 6379 #容器的端口[root@k8s-master mainfests]# kubectl apply -f redis-svc.yaml service/redis created[root@k8s-master mainfests]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 36dredis ClusterIP 10.107.238.182 &lt;none&gt; 6379/TCP 1m[root@k8s-master mainfests]# kubectl describe svc redisName: redisNamespace: defaultLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;redis&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;ports&quot;:[&#123;&quot;port&quot;:6379,&quot;targetPort&quot;:6379&#125;...Selector: app=redis,role=logstorType: ClusterIPIP: 10.107.238.182 #service ipPort: &lt;unset&gt; 6379/TCPTargetPort: 6379/TCPEndpoints: 10.244.1.16:6379 #此处的ip+端口就是pod的ip+端口Session Affinity: NoneEvents: &lt;none&gt;[root@k8s-master mainfests]# kubectl get pod redis-5b5d6fbbbd-v82pw -o wideNAME READY STATUS RESTARTS AGE IP NODEredis-5b5d6fbbbd-v82pw 1/1 Running 0 20d 10.244.1.16 k8s-node01 从上演示可以总结出：service不会直接到pod，service是直接到endpoint资源，就是地址加端口，再由endpoint再关联到pod。 service只要创建完，就会在dns中添加一个资源记录进行解析，添加完成即可进行解析。资源记录的格式为：SVC_NAME.NS_NAME.DOMAIN.LTD.默认的集群service 的A记录：svc.cluster.local.redis服务创建的A记录：redis.default.svc.cluster.local. NodePort的service类型演示 NodePort即节点Port，通常在部署Kubernetes集群系统时会预留一个端口范围用于NodePort，其范围默认为：30000~32767之间的端口。定义NodePort类型的Service资源时，需要使用.spec.type进行明确指定。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@k8s-master mainfests]# kubectl get pods --show-labels |grep myapp-deploymyapp-deploy-69b47bc96d-4hxxw 1/1 Running 0 12m app=myapp,pod-template-hash=2560367528,release=canarymyapp-deploy-69b47bc96d-95bc4 1/1 Running 0 12m app=myapp,pod-template-hash=2560367528,release=canarymyapp-deploy-69b47bc96d-hwbzt 1/1 Running 0 12m app=myapp,pod-template-hash=2560367528,release=canarymyapp-deploy-69b47bc96d-pjv74 1/1 Running 0 12m app=myapp,pod-template-hash=2560367528,release=canarymyapp-deploy-69b47bc96d-rf7bs 1/1 Running 0 12m app=myapp,pod-template-hash=2560367528,release=canary[root@k8s-master mainfests]# cat myapp-svc.yaml #为myapp创建serviceapiVersion: v1kind: Servicemetadata: name: myapp namespace: defaultspec: selector: app: myapp release: canary type: NodePort ports: - port: 80 targetPort: 80 nodePort: 30080[root@k8s-master mainfests]# kubectl apply -f myapp-svc.yaml service/myapp created[root@k8s-master mainfests]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 36dmyapp NodePort 10.101.245.119 &lt;none&gt; 80:30080/TCP 5sredis ClusterIP 10.107.238.182 &lt;none&gt; 6379/TCP 28m[root@k8s-master mainfests]# while true;do curl http://192.168.56.11:30080/hostname.html;sleep 1;donemyapp-deploy-69b47bc96d-95bc4myapp-deploy-69b47bc96d-4hxxwmyapp-deploy-69b47bc96d-pjv74myapp-deploy-69b47bc96d-rf7bsmyapp-deploy-69b47bc96d-95bc4myapp-deploy-69b47bc96d-rf7bsmyapp-deploy-69b47bc96d-95bc4[root@k8s-master mainfests]# while true;do curl http://192.168.56.11:30080/;sleep 1;done Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; 从以上例子，可以看到通过NodePort方式已经实现了从集群外部端口进行访问，访问链接如下：http://192.168.56.11:30080/。实践中并不鼓励用户自定义使用节点的端口，因为容易和其他现存的Service冲突，建议留给系统自动配置。 Pod的会话保持 Service资源还支持Session affinity（粘性会话）机制，可以将来自同一个客户端的请求始终转发至同一个后端的Pod对象，这意味着它会影响调度算法的流量分发功用，进而降低其负载均衡的效果。因此，当客户端访问Pod中的应用程序时，如果有基于客户端身份保存某些私有信息，并基于这些私有信息追踪用户的活动等一类的需求时，那么应该启用session affinity机制。 Service affinity的效果仅仅在一段时间内生效，默认值为10800秒，超出时长，客户端再次访问会重新调度。该机制仅能基于客户端IP地址识别客户端身份，它会将经由同一个NAT服务器进行原地址转换的所有客户端识别为同一个客户端，由此可知，其调度的效果并不理想。Service 资源 通过. spec. sessionAffinity 和. spec. sessionAffinityConfig 两个字段配置粘性会话。 spec. sessionAffinity 字段用于定义要使用的粘性会话的类型，它仅支持使用“ None” 和“ ClientIP” 两种属性值。如下： 1234567891011[root@k8s-master mainfests]# kubectl explain svc.spec.sessionAffinityKIND: ServiceVERSION: v1FIELD: sessionAffinity &lt;string&gt;DESCRIPTION: Supports &quot;ClientIP&quot; and &quot;None&quot;. Used to maintain session affinity. Enable client IP based session affinity. Must be ClientIP or None. Defaults to None. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies sessionAffinity支持ClientIP和None 两种方式，默认是None（随机调度） ClientIP是来自于同一个客户端的请求调度到同一个pod中 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master mainfests]# vim myapp-svc.yaml apiVersion: v1kind: Servicemetadata: name: myapp namespace: defaultspec: selector: app: myapp release: canary sessionAffinity: ClientIP type: NodePort ports: - port: 80 targetPort: 80 nodePort: 30080[root@k8s-master mainfests]# kubectl apply -f myapp-svc.yaml service/myapp configured[root@k8s-master mainfests]# kubectl describe svc myappName: myappNamespace: defaultLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;myapp&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;ports&quot;:[&#123;&quot;nodePort&quot;:30080,&quot;port&quot;:80,&quot;ta...Selector: app=myapp,release=canaryType: NodePortIP: 10.101.245.119Port: &lt;unset&gt; 80/TCPTargetPort: 80/TCPNodePort: &lt;unset&gt; 30080/TCPEndpoints: 10.244.1.18:80,10.244.1.19:80,10.244.2.15:80 + 2 more...Session Affinity: ClientIPExternal Traffic Policy: ClusterEvents: &lt;none&gt;[root@k8s-master mainfests]# while true;do curl http://192.168.56.11:30080/hostname.html;sleep 1;donemyapp-deploy-69b47bc96d-hwbztmyapp-deploy-69b47bc96d-hwbztmyapp-deploy-69b47bc96d-hwbztmyapp-deploy-69b47bc96d-hwbztmyapp-deploy-69b47bc96d-hwbztmyapp-deploy-69b47bc96d-hwbztmyapp-deploy-69b47bc96d-hwbztmyapp-deploy-69b47bc96d-hwbzt 也可以使用打补丁的方式进行修改yaml内的内容，如下： 123kubectl patch svc myapp -p &apos;&#123;&quot;spec&quot;:&#123;&quot;sessionAffinity&quot;:&quot;ClusterIP&quot;&#125;&#125;&apos; #session保持，同一ip访问同一个podkubectl patch svc myapp -p &apos;&#123;&quot;spec&quot;:&#123;&quot;sessionAffinity&quot;:&quot;None&quot;&#125;&#125;&apos; #取消session Headless Service有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（spec.clusterIP）的值为 &quot;None&quot; 来创建 Headless Service。 这个选项允许开发人员自由寻找他们自己的方式，从而降低与 Kubernetes 系统的耦合性。 应用仍然可以使用一种自注册的模式和适配器，对其它需要发现机制的系统能够很容易地基于这个 API 来构建。 对这类 Service 并不会分配 Cluster IP，kube-proxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了 selector。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293（1）编写headless service配置清单[root@k8s-master mainfests]# cp myapp-svc.yaml myapp-svc-headless.yaml [root@k8s-master mainfests]# vim myapp-svc-headless.yamlapiVersion: v1kind: Servicemetadata: name: myapp-headless namespace: defaultspec: selector: app: myapp release: canary clusterIP: &quot;None&quot; #headless的clusterIP值为None ports: - port: 80 targetPort: 80（2）创建headless service [root@k8s-master mainfests]# kubectl apply -f myapp-svc-headless.yaml service/myapp-headless created[root@k8s-master mainfests]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 36dmyapp NodePort 10.101.245.119 &lt;none&gt; 80:30080/TCP 1hmyapp-headless ClusterIP None &lt;none&gt; 80/TCP 5sredis ClusterIP 10.107.238.182 &lt;none&gt; 6379/TCP 2h（3）使用coredns进行解析验证[root@k8s-master mainfests]# dig -t A myapp-headless.default.svc.cluster.local. @10.96.0.10; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A myapp-headless.default.svc.cluster.local. @10.96.0.10;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 62028;; flags: qr aa rd ra; QUERY: 1, ANSWER: 5, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;myapp-headless.default.svc.cluster.local. IN A;; ANSWER SECTION:myapp-headless.default.svc.cluster.local. 5 IN A 10.244.1.18myapp-headless.default.svc.cluster.local. 5 IN A 10.244.1.19myapp-headless.default.svc.cluster.local. 5 IN A 10.244.2.15myapp-headless.default.svc.cluster.local. 5 IN A 10.244.2.16myapp-headless.default.svc.cluster.local. 5 IN A 10.244.2.17;; Query time: 4 msec;; SERVER: 10.96.0.10#53(10.96.0.10);; WHEN: Thu Sep 27 04:27:15 EDT 2018;; MSG SIZE rcvd: 349[root@k8s-master mainfests]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 36d[root@k8s-master mainfests]# kubectl get pods -o wide -l app=myappNAME READY STATUS RESTARTS AGE IP NODEmyapp-deploy-69b47bc96d-4hxxw 1/1 Running 0 1h 10.244.1.18 k8s-node01myapp-deploy-69b47bc96d-95bc4 1/1 Running 0 1h 10.244.2.16 k8s-node02myapp-deploy-69b47bc96d-hwbzt 1/1 Running 0 1h 10.244.1.19 k8s-node01myapp-deploy-69b47bc96d-pjv74 1/1 Running 0 1h 10.244.2.15 k8s-node02myapp-deploy-69b47bc96d-rf7bs 1/1 Running 0 1h 10.244.2.17 k8s-node02（4）对比含有ClusterIP的service解析[root@k8s-master mainfests]# dig -t A myapp.default.svc.cluster.local. @10.96.0.10; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A myapp.default.svc.cluster.local. @10.96.0.10;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50445;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;myapp.default.svc.cluster.local. IN A;; ANSWER SECTION:myapp.default.svc.cluster.local. 5 IN A 10.101.245.119;; Query time: 1 msec;; SERVER: 10.96.0.10#53(10.96.0.10);; WHEN: Thu Sep 27 04:31:16 EDT 2018;; MSG SIZE rcvd: 107[root@k8s-master mainfests]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 36dmyapp NodePort 10.101.245.119 &lt;none&gt; 80:30080/TCP 1hmyapp-headless ClusterIP None &lt;none&gt; 80/TCP 11mredis ClusterIP 10.107.238.182 &lt;none&gt; 6379/TCP 2h 从以上的演示可以看到对比普通的service和headless service，headless service做dns解析是直接解析到pod的，而servcie是解析到ClusterIP的，那么headless有什么用呢？？？这将在statefulset中应用到，这里暂时仅仅做了解什么是headless service和创建方法。 Ingress相关组件关系通俗的讲: Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务 Ingress 是反向代理规则，用来规定 HTTP/S 请求应该被转发到哪个 Service 上，比如根据请求中不同的 Host 和 url 路径让请求落到不同的 Service 上 Ingress Controller 就是一个反向代理程序，它负责解析 Ingress 的反向代理规则，如果 Ingress 有增删改的变动，所有的 Ingress Controller 都会及时更新自己相应的转发规则，当 Ingress Controller 收到请求后就会根据这些规则将请求转发到对应的 Service。 Kubernetes 并没有自带 Ingress Controller，它只是一种标准，具体实现有多种，需要自己单独安装，常用的是 Nginx Ingress Controller 和 Traefik Ingress Controller。 所以 Ingress 是一种转发规则的抽象，Ingress Controller 的实现需要根据这些 Ingress 规则来将请求转发到对应的 Service，我画了个图方便大家理解： 从图中可以看出，Ingress Controller 收到请求，匹配 Ingress 转发规则，匹配到了就转发到后端 Service，而 Service 可能代表的后端 Pod 有多个，选出一个转发到那个 Pod，最终由那个 Pod 处理请求。 有同学可能会问，既然 Ingress Controller 要接受外面的请求，而 Ingress Controller 是部署在集群中的，怎么让 Ingress Controller 本身能够被外面访问到呢，有几种方式： Ingress Controller 用 Deployment 方式部署，给它添加一个 Service，类型为 LoadBalancer，这样会自动生成一个 IP 地址，通过这个 IP 就能访问到了，并且一般这个 IP 是高可用的（前提是集群支持 LoadBalancer，通常云服务提供商才支持，自建集群一般没有） 使用集群内部的某个或某些节点作为边缘节点，给 node 添加 label 来标识，Ingress Controller 用 DaemonSet 方式部署，使用 nodeSelector 绑定到边缘节点，保证每个边缘节点启动一个 Ingress Controller 实例，用 hostPort 直接在这些边缘节点宿主机暴露端口，然后我们可以访问边缘节点中 Ingress Controller 暴露的端口，这样外部就可以访问到 Ingress Controller 了 Ingress Controller 用 Deployment 方式部署，给它添加一个 Service，类型为 NodePort，部署完成后查看会给出一个端口，通过 kubectl get svc 我们可以查看到这个端口，这个端口在集群的每个节点都可以访问，通过访问集群节点的这个端口就可以访问 Ingress Controller 了。但是集群节点这么多，而且端口又不是 80和443，太不爽了，一般我们会在前面自己搭个负载均衡器，比如用 Nginx，将请求转发到集群各个节点的那个端口上，这样我们访问 Nginx 就相当于访问到 Ingress Controller 了 一般比较推荐的是前面两种方式。 概述向外网暴露集群内服务，以使客户端能够访问，有以下几种方法，本文重点描述Ingress。 LoadBalancer LoadBalancer一般由云服务供应商提供或者用户自定义，运行在集群之外。在创建service时为其配置LoadBalancer相关参数，当从外网访问集群内servcie时，用户直接连接到LoadBalancer服务器，LoadBalancer服务器再将流量转发到集群内service。Loadbalancer配置及使用方法与各云服务供应商有关，本文不详细描述。 NodePort 这种方式要求集群中部分节点有被外网访问的能力。Kubernetes为每个NodePort类型的服务在集群中的每个节点上分配至少一个主机网络端口号。客户通过能被外网访问的节点IP加上节点端口的方式访问服务。大多数情况下不会通过这种方式向集群外暴露服务，原因有四。 其一：大多情况下，为了安全起见，集群中的节点位于完全内网环境中，不应该有被外网直接访问的能力。一般外网访问集群中的节点都是通过边界服务器如网关、跳板等，而这种边界服务器需要通过各种方式进行安全加固。 其二：如果集群内节点可以从外网直接访问的话，则会将集群内节点地址、服务名称、端口号等信息直接暴露在外，非常不安全。 其三：服务端口号一般由系统自动分配，并非固定，而服务名称也可能发生变更，此时外部客户端需要跟踪变更并修改，属于重试耦合。 其四：这种方式，每个服务至少向外网暴露一个端口号，当服务很多时不易于管理。 Ingress Ingress不是某种产品、组件的名称，它应该是kubernetes向集群外暴露服务的一种思路、技术，用户完全可以根据这种思路提供自己的Ingress实现，当然kubernetes提供了默认Ingress实现还有其它第三方实现，一般无需自己开发。它的思路是这样的，首先在集群内运行一个服务或者pod也可以是容器，不管是什么它至少应该有一个外网可以访问的IP，至少向外网开放一个端口号，让它充当反向代理服务器。当外网想要访问集群内service时，只需访问这个反向代理服务器并指定相关参数，代理服务器根据请求参数并结合内部规则，将请求转发到service。这种思路与LoadBalancer的不同之处是它就位于集群内，而LoadBalancer位于集群外。与NodePort的不同之处是集群只向外暴露一个服务或者pod等，而NodePort是暴露全部service。 Kubernetes用nginx实现反向代理服务器，称为Ingress Controller，是pod类型资源。同时提供了Ingress类型对象，通过创建Ingress对象配置nginx反向代理服务器的转发规则。Nginx反向代理服务器收到来自外网的请求后，用请求的URL地址、请求头字段区别不同service，然后转发请求。 部署GitHub：https://github.com/kubernetes/ingress-nginx/tree/nginx-0.20.0/deploy mandatory.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327apiVersion: v1kind: Namespacemetadata: name: ingress-nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: default-http-backend labels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx namespace: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx spec: terminationGracePeriodSeconds: 60 containers: - name: default-http-backend # Any image is permissible as long as: # 1. It serves a 404 page at / # 2. It serves 200 on a /healthz endpoint image: k8s.gcr.io/defaultbackend-amd64:1.5 livenessProbe: httpGet: path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 5 ports: - containerPort: 8080 resources: limits: cpu: 10m memory: 20Mi requests: cpu: 10m memory: 20Mi---apiVersion: v1kind: Servicemetadata: name: default-http-backend namespace: ingress-nginx labels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginxspec: ports: - port: 80 targetPort: 8080 selector: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: v1kind: ServiceAccountmetadata: name: nginx-ingress-serviceaccount namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: nginx-ingress-clusterrole labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - &quot;extensions&quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - &quot;extensions&quot; resources: - ingresses/status verbs: - update---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: nginx-ingress-role namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps resourceNames: # Defaults to &quot;&lt;election-id&gt;-&lt;ingress-class&gt;&quot; # Here: &quot;&lt;ingress-controller-leader&gt;-&lt;nginx&gt;&quot; # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - &quot;ingress-controller-leader-nginx&quot; verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - endpoints verbs: - get---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: nginx-ingress-role-nisa-binding namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-rolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: nginx-ingress-clusterrole-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 args: - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1--- 更改暴露方式此刻问题来了通过yaml创建的deploy以及server来看好像并没有把nginx端口映射到宿主机上,那么我访问宿主机ip就不会有任何返回,这里可以通过hostport+DaemonSet来解决这个问题修改yaml文件1.修改nginx 部署方式为DaemonSet2.注释replicas: 13.增加 hostNetwork: true 在spec: 段内增加4.增加hostPort 在Ports段内增加 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: # replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true containers: - name: nginx-ingress-controller image: siriuszg/nginx-ingress-controller:0.20.0 args: - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 hostPort: 80 - name: https containerPort: 443 hostPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1--- 创建一个tomcat并用ingress7层代理转发创建 123456789101112131415161718192021222324252627282930313233343536373839cat tomcat-ingress.yaml apiVersion: v1kind: Servicemetadata: name: tomcat namespace: defaultspec: type: ClusterIP selector: app: tomcat release: canary ports: - name: http port: 8080 targetPort: 8080---apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-deployspec: replicas: 1 selector: matchLabels: app: tomcat release: canary template: metadata: labels: app: tomcat release: canary spec: containers: - name: tomcat image: tomcat:7-alpine ports: - name: httpd containerPort: 8080 查看 1234kubectl get pod | grep tomcattomcat-deploy-64b488b68-wk45q 1/1 Running 0 29mkubectl get svc | grep tomcattomcat ClusterIP 10.0.0.183 &lt;none&gt; 8080/TCP 29m 创建ingress 1234567891011121314151617cat ingress-tomcat.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-tomcat namespace: default annotations: kubernets.io/ingress.class: &quot;nginx&quot;spec: rules: - host: www.aa.com #用来解析的域名地址 http: paths: - path: backend: serviceName: tomcat #集群服务的名字 servicePort: 8080 #集群服务开放的端口 访问测试 12345678 curl -H &quot;host:www.aa.com&quot; http://10.167.130.206:80 #IP地址为运行ingress-nginx-controller的主机地址,因为只有运行了这个容器才会监听宿主的80端口。&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;Apache Tomcat/7.0.91&lt;/title&gt; 可用命令查看ingress列表 1234567891011121314kubectl get ingressNAME HOSTS ADDRESS PORTS AGEingress-tomcat www.aa.com 80 34mkubectl describe ingress ingress-tomcatName: ingress-tomcatNamespace: defaultAddress: Default backend: default-http-backend:80 (&lt;none&gt;)Rules: Host Path Backends ---- ---- -------- www.aa.com tomcat:8080 (&lt;none&gt;) 用ingress来代理4层请求创建mysql 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748cat mysql.yamlapiVersion: v1kind: Servicemetadata: name: mysql namespace: defaultspec: type: ClusterIP selector: app: mysql release: canary ports: - name: mysql port: 3306 targetPort: 3306---apiVersion: apps/v1kind: DaemonSet #每个node都运行一个pod，我就两个node正好用来测试负载效果metadata: name: mysql-daemonsetspec:# replicas: 1 selector: matchLabels: app: mysql release: canary template: metadata: labels: app: mysql release: canary spec: containers: - name: mysql image: mysql env: - name: MYSQL_ROOT_PASSWORD #mysql镜像必须的变量,不写这个变量mysql跑不起来 value: &quot;mysql&quot; ports: - name: mysql containerPort: 3306 kubectl apply -f mysql.yaml #部署mysql podkubectl get podmysql-daemonset-2xdr7 1/1 Running 0 63mmysql-daemonset-stvhf 1/1 Running 0 63m 修改configmap文件 123456789cat configmap.yaml kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginxdata: 3306: &quot;default/mysql:3306&quot; #我们的mysql是在默认命名空间里创建的这个自行查看更改 添加或增减configmap直接在这个configmap文件中新增或去除即可,用kubectl apply重新应用,尽量不要直接kubectl delete -f configmap.yaml 因为这样会把整个tcp-services都删掉,删掉后node节点检测不到数据就不会对规则更新,这个和7层代理不太一样,7层可以一个服务创建一个name,4层在创建ingress服务时候就指定tcp-services和udp-services两个文件了,定义位置可以看ingress.yaml的281-282行 另外有一点,因为创建完ingress时候Node节点就是监听80和443的,在配置这个mysql时排错过程中发现,3306端口并不会默认监听,只有ingress可以正常连接到mysql集群时,node才会去监听3306端口,有错误可以按照这个思路排错,配置过程中也遇到很多问题,排错思路,容器&gt;容器ip&gt;集群ip&gt;ingress。 1234567891011121314151617181920212223断开连接几次试试，应该是轮训算法，分别在两个pod的数据库里写了a和b用来测试负载效果mysql&gt; show databases; +--------------------+| Database |+--------------------+| a || information_schema || mysql || performance_schema || sys |+--------------------+mysql&gt; show databases; +--------------------+| Database |+--------------------+| b || information_schema || mysql || performance_schema || sys |+--------------------+ 其它示例Single Service Ingress12345678apiVersion: extensions/v1beta1kind: Ingressmetadata: name: test-ingressspec: backend: serviceName: testsvc servicePort: 80 创建对象： 123$ kubectl get ingNAME RULE BACKEND ADDRESStest-ingress - testsvc:80 107.178.254.228 以上配置中没有具体的rule，所以诸如http(s)://107.178.254.228/xxx之类的请求都转发到testsvc的80端口。 其于URL转发12foo.bar.com -&gt; 178.91.123.132 -&gt; / foo s1:80 / bar s2:80 12345678910111213141516171819apiVersion: extensions/v1beta1kind: Ingressmetadata: name: test annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: - host: foo.bar.com http: paths: - path: /foo backend: serviceName: s1 servicePort: 80 - path: /bar backend: serviceName: s2 servicePort: 80 创建对象： 123456$ kubectl get ingNAME RULE BACKEND ADDRESStest - foo.bar.com /foo s1:80 /bar s2:80 基于名称的虚拟主机实现如下目标： 123foo.bar.com --| |-&gt; foo.bar.com s1:80 | 178.91.123.132 |bar.foo.com --| |-&gt; bar.foo.com s2:8 这种方式的核心逻辑是用http请求中的host字段区分不同服务，而不是URL。如host: foo.bar.com的请求被转发到s1服务80端口，如host: bar.foo.com的请求被转发到s2服务80端口。 123456789101112131415161718apiVersion: extensions/v1beta1kind: Ingressmetadata: name: testspec: rules: - host: foo.bar.com http: paths: - backend: serviceName: s1 servicePort: 80 - host: bar.foo.com http: paths: - backend: serviceName: s2 servicePort: 80 TLS利用Secret类型对象为Ingress Controller提供私钥及证书，对通信链路加密。 Secret配置： 123456789apiVersion: v1data: tls.crt: base64 encoded cert tls.key: base64 encoded keykind: Secretmetadata: name: testsecret namespace: defaulttype: Secret 在Ingress对象中引用： 1234567891011121314151617apiVersion: extensions/v1beta1kind: Ingressmetadata: name: no-rules-mapspec: tls: - hosts: - foo.bar.com secretName: testsecret rules: - host: foo.bar.com http: paths: - path: / backend: serviceName: s1 servicePort: 80 StatefulSet概述在具有以下特点时使用StatefulSets： 稳定性，唯一的网络标识符。 稳定性，持久化存储。 有序的部署和扩展。 有序的删除和终止。 有序的自动滚动更新。 Pod调度运行时，如果应用不需要任何稳定的标示、有序的部署、删除和扩展，则应该使用一组无状态副本的控制器来部署应用，例如 Deployment 或 ReplicaSet更适合无状态服务需求。 RC、Deployment、DaemonSet都是面向无状态的服务，它们所管理的Pod的IP、名字，启停顺序等都是随机的，而StatefulSet是什么？顾名思义，有状态的集合，管理所有有状态的服务，比如MySQL、MongoDB集群等。StatefulSet本质上是Deployment的一种变体，在v1.9版本中已成为GA版本，它为了解决有状态服务的问题，它所管理的Pod拥有固定的Pod名称，启停顺序，在StatefulSet中，Pod名字称为网络标识(hostname)，还必须要用到共享存储。在Deployment中，与之对应的服务是service，而在StatefulSet中与之对应的headless service，headless service，即无头服务，与service的区别就是它没有Cluster IP，解析它的名称时将返回该Headless Service对应的全部Pod的Endpoint列表。除此之外，StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod副本创建了一个DNS域名，这个域名的格式为： 12$(podname).(headless server name) FQDN： $(podname).(headless server name).namespace.svc.cluster.local 示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: &quot;nginx&quot; #声明它属于哪个Headless Service. replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: #可看作pvc的模板 - metadata: name: www spec: accessModes: [ &quot;ReadWriteOnce&quot; ] storageClassName: &quot;gluster-heketi&quot; #存储类名，改为集群中已存在的 resources: requests: storage: 1Gi 通过该配置文件，可看出StatefulSet的三个组成部分： Headless Service：名为nginx，用来定义Pod网络标识( DNS domain)。 StatefulSet：定义具体应用，名为Nginx，有三个Pod副本，并为每个Pod定义了一个域名。 volumeClaimTemplates： 存储卷申请模板，创建PVC，指定pvc名称大小，将自动创建pvc，且pvc必须由存储类供应。 为什么需要 headless service 无头服务？在用Deployment时，每一个Pod名称是没有顺序的，是随机字符串，因此是Pod名称是无序的，但是在statefulset中要求必须是有序 ，每一个pod不能被随意取代，pod重建后pod名称还是一样的。而pod IP是变化的，所以是以Pod名称来识别。pod名称是pod唯一性的标识符，必须持久稳定有效。这时候要用到无头服务，它可以给每个Pod一个唯一的名称 。为什么需要volumeClaimTemplate？对于有状态的副本集都会用到持久存储，对于分布式系统来讲，它的最大特点是数据是不一样的，所以各个节点不能使用同一存储卷，每个节点有自已的专用存储，但是如果在Deployment中的Pod template里定义的存储卷，是所有副本集共用一个存储卷，数据是相同的，因为是基于模板来的 ，而statefulset中每个Pod都要自已的专有存储卷，所以statefulset的存储卷就不能再用Pod模板来创建了，于是statefulSet使用volumeClaimTemplate，称为卷申请模板，它会为每个Pod生成不同的pvc，并绑定pv， 从而实现各pod有专用存储。这就是为什么要用volumeClaimTemplate的原因。 123$ kubectl create -f nginx.yaml service &quot;nginx&quot; createdstatefulset &quot;web&quot; created 123456789101112131415161718192021#第一个是创建web-0$ kubectl get podweb-0 1/1 ContainerCreating 0 51s#待web-0 running且ready时，创建web-1$ kubectl get podweb-0 1/1 Running 0 51sweb-1 0/1 ContainerCreating 0 42s#待web-1 running且ready时，创建web-2$ kubectl get podweb-0 1/1 Running 0 1mweb-1 1/1 Running 0 45sweb-2 1/1 ContainerCreating 0 36s#最后三个Pod全部running且ready$ kubectl get podNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 4mweb-1 1/1 Running 0 3mweb-2 1/1 Running 0 1m 12345$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEwww-web-0 Bound pvc-ecf003f3-828d-11e8-8815-000c29774d39 2G RWO gluster-heketi 7mwww-web-1 Bound pvc-0615e33e-828e-11e8-8815-000c29774d39 2G RWO gluster-heketi 6mwww-web-2 Bound pvc-43a97acf-828e-11e8-8815-000c29774d39 2G RWO gluster-heketi 4m 如果集群中没有StorageClass的动态供应PVC的机制，也可以提前手动创建多个PV、PVC，手动创建的PVC名称必须符合之后创建的StatefulSet命名规则：(volumeClaimTemplates.name)-(pod_name) Statefulset名称为web 三个Pod副本: web-0，web-1,web-2，volumeClaimTemplates名称为：www，那么自动创建出来的PVC名称为www-web-[0-2]，为每个Pod创建一个PVC。 规律总结 匹配Pod name(网络标识)的模式为：$(statefulset名称)-$(序号)，比如上面的示例：web-0，web-1，web-2。 StatefulSet为每个Pod副本创建了一个DNS域名，这个域名的格式为：$(podname).(headless server name)，也就意味着服务间是通过Pod域名来通信而非Pod IP，因为当Pod所在Node发生故障时，Pod会被飘移到其它Node上，Pod IP会发生变化，但是Pod域名不会有变化。 StatefulSet使用Headless服务来控制Pod的域名，这个域名的FQDN为：$(service name).$(namespace).svc.cluster.local，其中，“cluster.local”指的是集群的域名。 根据volumeClaimTemplates，为每个Pod创建一个pvc，pvc的命名规则匹配模式：(volumeClaimTemplates.name)-(pod_name)，比如上面的volumeMounts.name=www， Pod name=web-[0-2]，因此创建出来的PVC是www-web-0、www-web-1、www-web-2。 删除Pod不会删除其pvc，手动删除pvc将自动释放pv。关于Cluster Domain、headless service名称、StatefulSet 名称如何影响StatefulSet的Pod的DNS域名的示例： Cluster Domain Service (ns/name) StatefulSet (ns/name) StatefulSet Domain Pod DNS Pod Hostname cluster.local default/nginx default/web nginx.default.svc.cluster.local web-{0..N-1}.nginx.default.svc.cluster.local web-{0..N-1} cluster.local foo/nginx foo/web nginx.foo.svc.cluster.local web-{0..N-1}.nginx.foo.svc.cluster.local web-{0..N-1} kube.local foo/nginx foo/web nginx.foo.svc.kube.local web-{0..N-1}.nginx.foo.svc.kube.local web-{0..N-1} Statefulset的启停顺序： 有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到N-1）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。 有序删除：当Pod被删除时，它们被终止的顺序是从N-1到0。 有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态。 Statefulset Pod管理策略：在v1.7以后，通过允许修改Pod排序策略，同时通过.spec.podManagementPolicy字段确保其身份的唯一性。 OrderedReady：上述的启停顺序，默认设置。 Parallel：告诉StatefulSet控制器并行启动或终止所有Pod，并且在启动或终止另一个Pod之前不等待前一个Pod变为Running and Ready或完全终止。 StatefulSet使用场景： 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现。 稳定的网络标识符，即Pod重新调度后其PodName和HostName不变。 有序部署，有序扩展，基于init containers来实现。 有序收缩。 更新策略 在Kubernetes 1.7及更高版本中，通过.spec.updateStrategy字段允许配置或禁用Pod、labels、source request/limits、annotations自动滚动更新功能。OnDelete：通过.spec.updateStrategy.type 字段设置为OnDelete，StatefulSet控制器不会自动更新StatefulSet中的Pod。用户必须手动删除Pod，以使控制器创建新的Pod。RollingUpdate：通过.spec.updateStrategy.type 字段设置为RollingUpdate，实现了Pod的自动滚动更新，如果.spec.updateStrategy未指定，则此为默认策略。StatefulSet控制器将删除并重新创建StatefulSet中的每个Pod。它将以Pod终止（从最大序数到最小序数）的顺序进行，一次更新每个Pod。在更新下一个Pod之前，必须等待这个Pod Running and Ready。Partitions：通过指定 .spec.updateStrategy.rollingUpdate.partition 来对 RollingUpdate 更新策略进行分区，如果指定了分区，则当 StatefulSet 的 .spec.template 更新时，具有大于或等于分区序数的所有 Pod 将被更新。具有小于分区的序数的所有 Pod 将不会被更新，即使删除它们也将被重新创建。如果 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 大于其 .spec.replicas，则其 .spec.template 的更新将不会传播到 Pod。在大多数情况下，不需要使用分区。 DNS官方网站：https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ 介绍Kubenetes以插件的形式提供DNS服务，一般是运行在kube-system名称空间下的service，拥有固定IP地址。插件运行起来后，配置各个节点上的kubelet，告诉它集群中DNS服务的IP地址，kebelet在启动容器时再将DNS服务器的地址告诉容器，容器再使用此DNS服务器进行域名解析。 能通过DNS名称得到什么？ 集群中的service在创建时会被分配DNS名称，包含DNS服务自己。默认情况下客户pod的DNS搜索列表包含pod本身的namespace与集群默认域名，以下示例说明。 假设有一个名为foo的服务，们于bar名称空间。运行在bar名称空间中的其它pod直接以foo做为关键字查询DNS记录，对于bar名称空间中的pod需要使用关键字foo.bar查询foo的DNS记录。 以下小节详细介绍kubernetes DNS支持的记录类型及层次布局。 SERVICEA records普通服务（非无头服务）的名称被指派一条DNS A类记录，如位于my-namespace名称空间下的my-svc服务，为其指派的A类DNS记录为”my-svc.my-namespace.svc.cluster.local”，这条记录会被解析成服务的集群虚拟IP地址。 如果my-svn为无头服务，同样为其分配”my-svc.my-namespace.svc.cluster.local”的Ａ类记录。与普通服务不同，如果无头服务包含标签选择器，则此Ａ类记录会被解析成所有标签选择器选中pod的pod网络地址，用户可以通过某种算法如循环使用返回的条目集合。 SRV records当普通或者是无头服务包含命名端口时，创建此类SRV条目，例如: “_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local”，有多个命名端口则创建多条记录。对于普通服务，此条记录被解析成my-port-name所对应的端口号与一条CNAME记录：”my-svc.my-namespace.svc.cluster.local”。对于包含标签选择器的无头服务，其解析结果为每个pod中的my-port-name对应的端口号及每个pod的CNAME记录：pod-name.my-svc.my-namespace.svc.cluster.local。 Pods本节提及之pod应该是指由用户直接创建，而非由ReplicaSet等副本控制器创建。 A records如果功能被开启，pod以如下格式被分配A类记录：”pod-ip-address.my-namespace.pod.cluster.local”。例如pod的ip为1.2.3.4，名称空间为default，则在DNS中的Ａ类记录为”1-2-3-4.default.pod.cluster.local，当查询时此此条记录被解析成pod名称。 Pod’s hostname and subdomain fields默认情况下，pod的hostname与pod名称相同。同时pod Spec有一个可选字段hostname，其值优先于pod名称被设置成hostname。另外，pod Spec还包含subdomain可选字段，可以为pod设置子域。假如为pod设置hostname为foo，subdomain设置为bar，其位于my-namespace名称空间下，则其有如下的全限定域名：”foo.bar.my-namespace.svc.cluster.local”。此条记录被解析成pod的IP地址。 大多数情况下，用户不直接创建pod，而是创建各种类型本控制器。用户直接创建pod的一种常见场景是创建包含选择器的无头服务，然后直接创建pod，让无头服务中的选择器选中自己创建的pod。如果打算为自己创建的pod创建A类记录，则必需在pod Spec中设置hostname字段。示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344apiVersion: v1kind: Servicemetadata: name: default-subdomainspec: selector: name: busybox clusterIP: None ports: - name: foo # Actually, no port is needed. port: 1234 targetPort: 1234---apiVersion: v1kind: Podmetadata: name: busybox1 labels: name: busyboxspec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox command: - sleep - &quot;3600&quot; name: busybox---apiVersion: v1kind: Podmetadata: name: busybox2 labels: name: busyboxspec: hostname: busybox-2 subdomain: default-subdomain containers: - image: busybox command: - sleep - &quot;3600&quot; name: busybox 上例的结果就是既会为无头服务default-subdomain创建A类解析条目”default-subdomain.my-namespace.svc.cluster.local”，也会单独为每个pod创建诸如”busybox-1.default-subdomain.my-namespace.svc.cluster.local”、”busybox-2.default-subdomain.my-namespace.svc.cluster.local”，分别被解析成pod的IP地址。前文讲过，如果没有为pod Spec指定hostname字段，则不创建后两条记录。 上述记录的生成过程大概是先选中pod，根据pod生成endpoint对象，根据生成的endpoint对象生成以上记录。如果无头服务没有标签选择器，则可以手动为其创建endpoint，如果打算为手动创建的endpoint单独添加记录，则必需在其Spec中设置hostname字段，其作用与在pod中设置相同。 Pod’s DNS Policy以上介绍的是kubernetes如何为service、pod创建DNS记录。那么如何定义pod内部解析域名时的规则呢？可以设置pod Spec中的dnsPolicy字段，有如下几种取值： “Default“:从节点继承DNS相关配置，对节点依赖性强。 “ClusterFirst“:如果DNS查询与配置好的默认集群域名前缀不匹配，则将查询请求转发到从节点继承而来，作为查询的上游服务器。 “ClusterFirstWithHostNet“:如果pod工作在主机网络，就将dnsPolicy设置成“ClusterFirstWithHostNet”，这样效率更高。 “None“:1.9版本引入的新特性(Beta in v1.10)。完全忽略kubernetes系统提供的DNS，以pod Spec中dnsConfig配置取而代之。 如果dnsPolicy字段未设置，默认策略是”ClusterFirst”。 以下示例使用”ClusterFirstWithHostNet”，因为pod工作在主机网络： 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: busybox namespace: defaultspec: containers: - image: busybox command: - sleep - &quot;3600&quot; imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always hostNetwork: true dnsPolicy: ClusterFirstWithHostNet Pod’s DNS ConfigDNS Config从1.9版本引入，1.10版本可用，新增加特性的目的是为增强用户对pod之DNS控制。首先在apiServer与kubelet中设置特性开关，如”–feature-gates=CustomPodDNS=true,…”，而后在pod Spec中将dnsPolicy设置成None，并新添加dnsConfig字段。 dnsConfig字段： nameservers:DNS服务器IP地址，最多三个。如果dnsPolicy为None则此字段至少包含一个IP地址，为其它值时可选。此字段之地址会与其它方式生成的地址合并去重。 searches：查询域名，可选。与其它策略生成的域名合并去重。 options:对象选项列给，每个对象必需有name属性，value属性可选。 示例： 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: namespace: default name: dns-examplespec: containers: - name: test image: nginx dnsPolicy: &quot;None&quot; dnsConfig: nameservers: - 1.2.3.4 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: &quot;2&quot; - name: edns0 创建pod后，其/etc/resolv.conf内容如下： 123nameserver 1.2.3.4search ns1.svc.cluster.local my.dns.search.suffixoptions ndots:2 edns0 调度之节点亲和性Affinity 翻译成中文是“亲和性”，它对应的是 Anti-Affinity，我们翻译成“互斥”。这两个词比较形象，可以把 pod 选择 node 的过程类比成磁铁的吸引和互斥，不同的是除了简单的正负极之外，pod 和 node 的吸引和互斥是可以灵活配置的。 Affinity的优点： 匹配有更多的逻辑组合，不只是字符串的完全相等 调度分成软策略(soft)和硬策略(hard)，在软策略下，如果没有满足调度条件的节点，pod会忽略这条规则，继续完成调度。 目前主要的node affinity： requiredDuringSchedulingIgnoredDuringExecution表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。 requiredDuringSchedulingRequiredDuringExecution表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中RequiredDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点。 preferredDuringSchedulingIgnoredDuringExecution表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。 preferredDuringSchedulingRequiredDuringExecution表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。 软策略和硬策略的区分是有用处的，硬策略适用于 pod 必须运行在某种节点，否则会出现问题的情况，比如集群中节点的架构不同，而运行的服务必须依赖某种架构提供的功能；软策略不同，它适用于满不满足条件都能工作，但是满足条件更好的情况，比如服务最好运行在某个区域，减少网络传输等。这种区分是用户的具体需求决定的，并没有绝对的技术依赖。 下面是一个官方的示例： 1234567891011121314151617181920212223242526apiVersion: v1kind: Podmetadata: name: with-node-affinityspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: gcr.io/google_containers/pause:2.0 这个 pod 同时定义了 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 两种 nodeAffinity。第一个要求 pod 运行在特定 AZ 的节点上，第二个希望节点最好有对应的 another-node-label-key:another-node-label-value 标签。 这里的匹配逻辑是label在某个列表中，可选的操作符有： In: label的值在某个列表中 NotIn：label的值不在某个列表中 Exists：某个label存在 DoesNotExist：某个label不存在 Gt：label的值大于某个值（字符串比较） Lt：label的值小于某个值（字符串比较） 如果nodeAffinity中nodeSelector有多个选项，节点满足任何一个条件即可；如果matchExpressions有多个选项，则节点必须同时满足这些选项才能运行pod 。 滚动升级服务升级 修改其中的image 1kubectl set image deployment/demoservice demoservice=lib/demoservicelib:1.1.0 --namespace=demospace 或者 1kubectl edit deployment demoservice -n demospace 查看deployments版本 1kubectl rollout history deployments demoservice -n demospace 查看deployments指定版本信息 1kubectl rollout history deployments demoservice -n demospace --revision=2 回滚 1kubectl rollout undo deployment/demoservice --namespace=demospace 回滚到指定版本： 1kubectl rollout undo deployment/demoservice --to-revision=2 --namespace=demospace 查看历史 1kubectl describe deployment/demoservice --namespace=demospace 设置配额配置Namespace资源限制中文文档：http://docs.kubernetes.org.cn/746.html 配置容器资源限制对于一个pod来说，资源最基础的2个的指标就是：CPU和内存。Kubernetes提供了个采用requests和limits 两种类型参数对资源进行预分配和使用限制。limit 会限制pod的资源利用： 当pod 内存超过limit时，会被oom。 当cpu超过limit时，不会被kill，但是会限制不超过limit值。 测试内存限制部署一个压测容器，压测时会分配250M内存，但实际pod的内存limit为100Mi 12345678910111213141516apiVersion: v1kind: Podmetadata: name: memory-demo namespace: examplespec: containers: - name: memory-demo-2-ctr image: polinux/stress resources: requests: memory: &quot;50Mi&quot; limits: memory: &quot;100Mi&quot; command: [&quot;stress&quot;] args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;250M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;] 部署后查看pod状态，可以看到pod被OOM， 123 kubectl -n example get poNAME READY STATUS RESTARTS AGEmemory-demo 0/1 OOMKilled 1 11s 测试CPU限制1234567891011121314151617apiVersion: v1kind: Podmetadata: name: cpu-demo namespace: examplespec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: &quot;1&quot; requests: cpu: &quot;0.5&quot; args: - -cpus - &quot;2&quot; 查看容器信息，可以看到pod 虽然不会被kill掉，但是实际使用cpu被限制只有1000m。 123 kubectl -n example top po cpu-demoNAME CPU(cores) MEMORY(bytes)cpu-demo 1000m 0Mi 容器服务质量（QoS）Kubernetes 提供服务质量管理，根据容器的资源配置，将pod 分为Guaranteed, Burstable, BestEffort 3个级别。当资源紧张时根据分级决定调度和驱逐策略，这三个分级分别代表： Guaranteed： pod中所有容器都设置了limit和request， 并且相等（设置limit后假如没有设置request会自动设置为limit值） Burstable： pod中有容器未设置limit， 或者limit和request不相等。这种类型的pod在调度节点时， 可能出现节点超频的情况。 BestEffort： pod中没有任何容器设置request和limit。 计算qos代码：https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/helper/qos/qos.go 不同QoS对容器影响oom： Kubernetes会根据QoS设置oom的评分调整参数oom_score_adj，oom_killer 根据 内存使用情况算出oom_score， 并且和oom_score_adj综合评价，进程的评分越高，当发生oom时越优先被kill。 QoS oom_score_adj Guaranteed -998 BestEffort 1000 Burstable min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999) 当节点内存不足时，QoS为Guaranteed 的pod 最后被kill。 而BestEffort 级别的pod优先被kill。 其次是Burstable，根据计算公式 oom_score_adj 值范围2到999，设置的request越大，oom_score_adj越低，oom时保护程度越高。 实践 123456节点信息：# kubectl describe no cn-beijing.i-2zeavb11mttnqnnicwj9 | grep -A 3 CapacityCapacity: cpu: 4 memory: 8010196Ki pods: 110 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: v1kind: Podmetadata: name: memory-demo-qos-1 namespace: examplespec: containers: - name: memory-demo-qos-1 image: polinux/stress resources: requests: memory: &quot;200Mi&quot; command: [&quot;stress&quot;] args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;50M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]---apiVersion: v1kind: Podmetadata: name: memory-demo-qos-2 namespace: examplespec: containers: - name: memory-demo-qos-2 image: polinux/stress resources: requests: memory: &quot;400Mi&quot; command: [&quot;stress&quot;] args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;50M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]---apiVersion: v1kind: Podmetadata: name: memory-demo-qos-3 namespace: examplespec: containers: - name: memory-demo-qos-3 image: polinux/stress resources: requests: memory: &quot;200Mi&quot; cpu: &quot;2&quot; limits: memory: &quot;200Mi&quot; cpu: &quot;2&quot; command: [&quot;stress&quot;] args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;50M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;] 单个节点可分配内存为8010196Ki， 大约7822.45Mi。根据Burstable 的计算方式: 123request 200Mi: (1000 - 1000*200/7822.45) 约为975request 400Mi: (1000 - 1000*400/7822.45) 约为950 我们分别查看这3个pod的oom参数 1234567891011// request 200Mi kubectl -n example exec memory-demo-qos-1 cat /proc/1/oom_score_adj975// request 400Miß kubectl -n example exec memory-demo-qos-2 cat /proc/1/oom_score_adj949// Guaranteed kubectl -n example exec memory-demo-qos-3 cat /proc/1/oom_score_adj-998 设置oom 规则代码：https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/qos/policy.go pod 驱逐： 当节点的内存和cpu资源不足，开始驱逐节点上的pod时。QoS同样会影响驱逐的优先级。顺序如下： kubelet 优先驱逐 BestEffort的pod 和 实际占用资源大于requests的Burstable pod。 接下来驱逐实际占用资源小于request的Burstable pod。 QoS为Guaranteed的pod最后驱逐， kubelet 会保证Guaranteed的pod 不会因为其他pod的资源消耗而被驱逐。 当QoS相同时，kubelet 根据Priority计算驱逐的优先级 ResourceQuotaKubernetes提供ResourceQuota对象，用于配置限制namespace内的每种类型的k8s对象数量和资源（cpu，内存）。 一个namespace中可以创建一个或多个ResourceQuota 如果namespace中配置了ResourceQuota， 部署时必须设置request和limit， 否则会拒绝创建请求。 可以通过这是limitRange配置每个pod默认的requests和limits避免上述问题 1.10以后支持扩展资源 详见：https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/ 123456789101112apiVersion: v1kind: ResourceQuotametadata: name: mem-cpu-demo namespace: examplespec: hard: requests.cpu: &quot;3&quot; requests.memory: 1Gi limits.cpu: &quot;5&quot; limits.memory: 2Gi pods: &quot;5&quot; LimitRangeLimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值，以及大小范围。 1234567891011121314151617181920212223apiVersion: v1kind: LimitRangemetadata: name: mem-limit-range namespace: examplespec: limits: - default: # default limit memory: 512Mi cpu: 2 defaultRequest: # default request memory: 256Mi cpu: 0.5 max: # max limit memory: 800Mi cpu: 3 min: # min request memory: 100Mi cpu: 0.3 maxLimitRequestRatio: # max value for limit / request memory: 2 cpu: 2 type: Container # limit type, support: Container / Pod / PersistentVolumeClaim limitRange支持的参数如下： default 代表默认的limit defaultRequest 代表默认的request max 代表limit的最大值 min 代表request的最小值 maxLimitRequestRatio 代表 limit / request的最大值。由于节点是根据pod request 调度资源，可以做到节点超卖，maxLimitRequestRatio 代表pod最大超卖比例。 总结 Kubernetes 提供request 和 limit 两种方式设置容器资源。 为了提高资源利用率，k8s调度时根据pod 的request值计算调度策略，从而实现节点资源超卖。 k8s根据limit限制pod使用资源，当内存超过limit时会触发oom。 且限制pod的cpu 不允许超过limit。 根据pod的 request和limit，k8s会为pod 计算服务质量，并分为Guaranteed, Burstable, BestEffort 这3级。当节点资源不足时，发生驱逐或者oom时， Guaranteed 级别的pod 优先保护， Burstable 节点次之（request越大，使用资源量越少 保护级别越高）， BestEffort 最先被驱逐。 Kubernetes提供了RequestQuota和LimitRange 用于设置namespace 内pod 的资源范围 和 规模总量。 RequestQuota 用于设置各种类型对象的数量， cpu和内存的总量。 LimitRange 用于设置pod或者容器 request和limit 的默认值，最大最小值， 以及超卖比例（limit / request）。 对于一些重要的线上应用，我们应该合理设置limit和request，limit和request 设置一致，资源不足时k8s会优先保证这些pod正常运行。 为了提高资源利用率。 对一些非核心，并且资源不长期占用的应用，可以适当减少pod的request，这样pod在调度时可以被分配到资源不是十分充裕的节点，提高使用率。但是当节点的资源不足时，也会优先被驱逐或被oom kill。 PV &amp; PVC本质上，Kubernetes Volume 是一个目录，这一点与 Docker Volume 类似。当 Volume 被 mount 到 Pod，Pod 中的所有容器都可以访问这个 Volume。Kubernetes Volume 也支持多种 backend 类型，包括 emptyDir、hostPath、GCE Persistent Disk、AWS Elastic Block Store、NFS、Ceph 等，完整列表可参考 https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes emptyDiremptyDir 是最基础的 Volume 类型。正如其名字所示，一个 emptyDir Volume 是 Host 上的一个空目录。 emptyDir Volume 对于容器来说是持久的，对于 Pod 则不是。当 Pod 从节点删除时，Volume 的内容也会被删除。但如果只是容器被销毁而 Pod 还在，则 Volume 不受影响。 也就是说：emptyDir Volume 的生命周期与 Pod 一致。 Pod 中的所有容器都可以共享 Volume，它们可以指定各自的 mount 路径。下面通过例子来实践 emptyDir，配置文件如下： 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - name: producer image: busybox volumeMounts: - name: shared-volume mountPath: /producer_dir args: - /bin/sh - -c - echo &quot;hello world&quot; &gt; /producer_dir/hello; sleep 30000 - name: consumer image: busybox volumeMounts: - name: shared-volume mountPath: /consumer_dir args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000 volumes: - name: shared-volume emptyDir: &#123;&#125; 这里我们模拟了一个 producer-consumer 场景。Pod 有两个容器 producer和 consumer，它们共享一个 Volume。producer 负责往 Volume 中写数据，consumer 则是从 Volume 读取数据。 文件最底部 volumes 定义了一个 emptyDir 类型的 Volume shared-volume。 producer 容器将 shared-volume mount 到 /producer_dir 目录。 producer 通过 echo 将数据写到文件 hello 里。 consumer 容器将 shared-volume mount 到 /consumer_dir 目录。 consumer 通过 cat 从文件 hello 读数据。 执行如下命令创建 Pod： 1234567[root@master ~]# kubectl apply -f emptydir.yaml pod/producer-consumer created[root@master ~]# kubectl get podsNAME READY STATUS RESTARTS AGEproducer-consumer 2/2 Running 0 8s[root@master ~]# kubectl logs producer-consumer consumerhello world kubectl logs 显示容器 consumer 成功读到了 producer 写入的数据，验证了两个容器共享 emptyDir Volume。 emptyDir 是 Host 上创建的临时目录，其优点是能够方便地为 Pod 中的容器提供共享存储，不需要额外的配置。但它不具备持久性，如果 Pod 不存在了，emptyDir 也就没有了。根据这个特性，emptyDir 特别适合 Pod 中的容器需要临时共享存储空间的场景，比如前面的生产者消费者用例。 hostPathhostPath Volume 的作用是将 Docker Host 文件系统中已经存在的目录 mount 给 Pod 的容器。大部分应用都不会使用 hostPath Volume，因为这实际上增加了 Pod 与节点的耦合，限制了 Pod 的使用。不过那些需要访问 Kubernetes 或 Docker 内部数据（配置文件和二进制库）的应用则需要使用 hostPath。 下面的例子，我们把主机上的目录/data/pod/v1挂载到 Pod 上容器的/usr/share/nginx/html/。 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-vol-hostPathspec: containers: - name: mytest image: wangzan18/mytest:v1 volumeMounts: - name: html mountPath: /usr/share/nginx/html/ volumes: - name: html hostPath: path: /data/pod/v1 type: DirectoryOrCreate 如果 Pod 被销毁了，hostPath 对应的目录也还会被保留，从这点看，hostPath 的持久性比 emptyDir 强。不过一旦 Host 崩溃，hostPath 也就没法访问了。 PV&amp;PVC介绍PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。 生命周期pv和pvc遵循以下生命周期： 供应准备。通过集群外的存储系统或者云平台来提供存储持久化支持。- 静态提供：管理员手动创建多个PV，供PVC使用。- 动态提供：动态创建PVC特定的PV，并绑定。 绑定。用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。 使用。用户可在pod中像volume一样使用pvc。 释放。用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。 回收(Reclaiming)。pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。- 保留策略：允许人工处理保留的数据。- 删除策略：将删除pv和外部关联的存储资源，需要插件支持。- 回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。 目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。 Provisioning两种方式提供的PV资源供给： static 通过集群管理者创建多个PV，为集群“使用者”提供存储能力而隐藏真实存储的细节。并且存在于kubenretes api中，可被直接使用。 dynamic 动态卷供给是kubernetes独有的功能，这一功能允许按需创建存储建。在此之前，集群管理员需要事先在集群外由存储提供者或者云提供商创建存储卷，成功之后再创建PersistentVolume对象，才能够在kubernetes中使用。动态卷供给能让集群管理员不必进行预先创建存储卷，而是随着用户需求进行创建。在1.5版本提高了动态卷的弹性和可用性。在此前1.4版本中加入了一个 新的 API 对象 StorageClass，可以定义多个 StorageClass 对象，并可以分别指定存储插件、设置参数，用于提供不同的存储卷。这样的设计让集群管理员能够在同一个集群内，定义和提供不同类型的、不同参数的卷（相同或者不同的存储系统）。这样的设计还确保了最终用户在无需了解太多的情况下，有能力选择不同的存储选项。 PV类型pv支持以下类型: GCEPersistentDisk AWSElasticBlockStore NFS iSCSI RBD (Ceph Block Device) Glusterfs AzureFile AzureDisk CephFS cinder FC FlexVolume Flocker PhotonPersistentDisk Quobyte VsphereVolume HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) PV属性: 访问模式,与pv的语义相同。在请求资源时使用特定模式。 accessModes 指定访问模式为 ReadWriteOnce，支持的访问模式有： ReadWriteOnce – PV 能以 read-write 模式 mount 到单个节点。 ReadOnlyMany – PV 能以 read-only 模式 mount 到多个节点。 ReadWriteMany – PV 能以 read-write 模式 mount 到多个节点。 资源,申请的存储资源数额。 PV卷阶段状态： Available – 资源尚未被claim使用 Bound – 卷已经被绑定到claim了 Released – claim被删除，卷处于释放状态，但未被集群回收。 Failed – 卷自动回收失败 示例创建pv 1234567891011121314apiVersion: v1kind: PersistentVolumemetadata: name: ebs-pv labels: type: amazonEBSspec: capacity: storage: 5Gi accessModes: - ReadWriteOnce awsElasticBlockStore: volumeID: vol-079c492115a7be6e1 fsType: ext4 创建pvc 123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: nginx-pvc labels: type: amazonEBSspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi 创建deployment 12345678910111213141516171819202122apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-with-pvcspec: replicas: 1 template: metadata: labels: service: nginx app: test spec: containers: - image: nginx name: nginx-with-pvc volumeMounts: - mountPath: /test-ebs name: my-pvc volumes: - name: my-pvc persistentVolumeClaim: claimName: nginx-pvc 回收策略PersistentVolumes 可以有多种回收策略，包括 “Retain”、”Recycle” 和 “Delete”。对于动态配置的 PersistentVolumes来说，默认回收策略为 “Delete”。这表示当用户删除对应的 PersistentVolumeClaim 时，动态配置的 volume 将被自动删除。如果 volume 包含重要数据时，这种自动行为可能是不合适的。那种情况下，更适合使用 “Retain” 策略。使用 “Retain” 时，如果用户删除 PersistentVolumeClaim，对应的 PersistentVolume 不会被删除。相反，它将变为 Released 状态，表示所有的数据可以被手动恢复。 示例： pvc.yml 1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-testspec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi deployment.yml 123456789101112131415161718192021222324apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-rbdspec: replicas: 1 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: ceph-rbd-volume mountPath: &quot;/usr/share/nginx/html&quot; volumes: - name: ceph-rbd-volume persistentVolumeClaim: claimName: pvc-test 新建pvc、deployment、写入数据并删除pvc操作过程： 123456789101112131415161718192021222324252627[root@lab1 test]# lltotal 8-rw-r--r-- 1 root root 533 Oct 24 17:54 nginx.yaml-rw-r--r-- 1 root root 187 Oct 24 17:55 pvc.yaml[root@lab1 test]# kubectl apply -f pvc.yaml persistentvolumeclaim/pvc-test created[root@lab1 test]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc-test Bound pvc-069c4486-d773-11e8-bd12-000c2931d938 1Gi RWO ceph-rbd 7s[root@lab1 test]# kubectl apply -f nginx.yaml deployment.extensions/nginx-rbd created[root@lab1 test]# kubectl get pod |grep nginx-rbdnginx-rbd-7c6449886-thv25 1/1 Running 0 33s[root@lab1 test]# kubectl exec -it nginx-rbd-7c6449886-thv25 -- /bin/bash -c &apos;echo ygqygq2 &gt; /usr/share/nginx/html/ygqygq2.html&apos; [root@lab1 test]# kubectl exec -it nginx-rbd-7c6449886-thv25 -- cat /usr/share/nginx/html/ygqygq2.htmlygqygq2[root@lab1 test]# kubectl delete -f nginx.yaml deployment.extensions &quot;nginx-rbd&quot; deleted[root@lab1 test]# kubectl get pvc pvc-test NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc-test Bound pvc-069c4486-d773-11e8-bd12-000c2931d938 1Gi RWO ceph-rbd 4m10s[root@lab1 test]# kubectl delete pvc pvc-test # 删除PVCpersistentvolumeclaim &quot;pvc-test&quot; deleted[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-069c4486-d773-11e8-bd12-000c2931d938 1Gi RWO Retain Released default/pvc-test ceph-rbd 4m33s[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938 -o yaml &gt; /tmp/pvc-069c4486-d773-11e8-bd12-000c2931d938.yaml # 保留备用 从上面可以看到，pvc删除后，pv变成Released状态。 再次创建同名PVC，查看是否分配原来PV操作过程： 12345678[root@lab1 test]# kubectl apply -f pvc.yaml persistentvolumeclaim/pvc-test created[root@lab1 test]# kubectl get pvc # 查看新建的PVC NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc-test Bound pvc-f2df48ea-d773-11e8-b6c8-000c29ea3e30 1Gi RWO ceph-rbd 19s[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938 # 查看原来的PVNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-069c4486-d773-11e8-bd12-000c2931d938 1Gi RWO Retain Released default/pvc-test ceph-rbd 7m18s 从上面可以看到，PVC分配的是新的PV，因为PV状态不是Available。 那怎么才能让PV状态变成Available呢？我们来查看之前的PV： 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@lab1 test]# cat /tmp/pvc-069c4486-d773-11e8-bd12-000c2931d938.yamlapiVersion: v1kind: PersistentVolumemetadata: annotations: pv.kubernetes.io/provisioned-by: ceph.com/rbd rbdProvisionerIdentity: ceph.com/rbd creationTimestamp: 2018-10-24T09:56:06Z finalizers: - kubernetes.io/pv-protection name: pvc-069c4486-d773-11e8-bd12-000c2931d938 resourceVersion: &quot;11752758&quot; selfLink: /api/v1/persistentvolumes/pvc-069c4486-d773-11e8-bd12-000c2931d938 uid: 06b57ef7-d773-11e8-bd12-000c2931d938spec: accessModes: - ReadWriteOnce capacity: storage: 1Gi claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: pvc-test namespace: default resourceVersion: &quot;11751559&quot; uid: 069c4486-d773-11e8-bd12-000c2931d938 persistentVolumeReclaimPolicy: Retain rbd: fsType: ext4 image: kubernetes-dynamic-pvc-06a25bd3-d773-11e8-8c3e-0a580af400d5 keyring: /etc/ceph/keyring monitors: - 192.168.105.92:6789 - 192.168.105.93:6789 - 192.168.105.94:6789 pool: kube secretRef: name: ceph-secret namespace: kube-system user: kube storageClassName: ceph-rbdstatus: phase: Released 从上面可以看到，spec.claimRef这段，仍保留之前的PVC信息。 我们大胆删除spec.claimRef这段。再次查看PV： 1kubectl edit pv pvc-069c4486-d773-11e8-bd12-000c2931d938 123[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-069c4486-d773-11e8-bd12-000c2931d938 1Gi RWO Retain Available ceph-rbd 10m 从上面可以看到，之前的PV pvc-069c4486-d773-11e8-bd12-000c2931d938已经变为Available。 小结当前版本Kubernetes PVC存储大小是唯一能被设置或请求的资源，因我们没有修改PVC的大小，在PV的Available状态下，有PVC请求分配相同大小时，PV会被分配出去并绑定成功。在PV变成Available过程中，最关键的是PV的spec.claimRef字段，该字段记录着原来PVC的绑定信息，删除绑定信息，即可重新释放PV从而达到Available。 Context在kubeconfig配置文件中设置一个环境项。 如果指定了一个已存在的名字，将合并新字段并覆盖旧字段。 1kubectl config set-context NAME [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace] 示例 12# 设置gce环境项中的user字段，不影响其他字段。$ kubectl config set-context gce --user=cluster-admin --namespace=test --cluster=test 选项 123--cluster=&quot;&quot;: 设置kuebconfig配置文件中环境选项中的集群。--namespace=&quot;&quot;: 设置kuebconfig配置文件中环境选项中的命名空间。--user=&quot;&quot;: 设置kuebconfig配置文件中环境选项中的用户。 StorageClass存储类介绍Kubernetes集群管理员通过提供不同的存储类，可以满足用户不同的服务质量级别、备份策略和任意策略要求的存储需求。动态存储卷供应使用StorageClass进行实现，其允许存储卷按需被创建。如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。 基于StorageClass的动态存储供应整体过程如下图所示： 1）集群管理员预先创建存储类（StorageClass）； 2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)； 3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)； 4）系统读取存储类的信息； 5）系统基于存储类的信息，在后台自动创建PVC需要的PV； 6）用户创建一个使用PVC的Pod； 7）Pod中的应用通过PVC进行数据的持久化； 8）而PVC使用PV进行数据的最终持久化处理。 定义存储类每一个存储类都包含provisioner、parameters和reclaimPolicy这三个参数域，当一个属于某个类的PersistentVolume需要被动态提供时，将会使用上述的参数域。 存储类对象的名称非常重要，用户通过名称类请求特定的存储类。管理员创建存储类对象时，会设置类的名称和其它的参数，存储类的对象一旦被创建，将不能被更新。管理员能够为PVC指定一个默认的存储类。 123456789101112kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: standard# 指定存储类的供应者provisioner: kubernetes.io/aws-ebsparameters: type: gp2# 指定回收策略reclaimPolicy: RetainmountOptions: - debug 供应者存储类有一个供应者的参数域，此参数域决定PV使用什么存储卷插件。参数必需进行设置： 存储卷 内置供应者 配置例子 AWSElasticBlockStore ✓ AWS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS – – Cinder ✓ OpenStack Cinder FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ GCE Glusterfs ✓ Glusterfs iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ Quobyte NFS – – RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local – Local Kubernetes的存储类并不局限于表中的“interneal”供应者，“interneal”供应者的名称带有“kubernetes.io”前缀；也可以允许和指定外部的供应者，外部供应者通过独立的程序进行实现。外部供应者的作者对代码在何处生存、如何供应、如何运行、使用什么卷插件（包括Flex）等有充分的判断权，kubernetes-incubator/external-storage仓库中存在编写外部提供者的类库。例如，NFS不是内部的供应者，但也是可以使用。在kubernetes-incubator/external-storage仓库中以列表的形式展示了一些外部的供应者，一些第三方供应商也提供了他们自己的外部供应者。 提供者的参数存储类存在很多描述存储卷的参数，依赖不同的提供者可能有不同的参数。例如，对于type参数，它的值可能为io1。当一个参数被省略，则使用默认的值。 回收策略通过存储类创建的持久化存储卷通过reclaimPolicy参数来指定，它的值可以是Delete或者Retain，默认为Delete。对于通过手工创建的，并使用存储类进行管理的持久化存储卷，将使用任何在创建时指定的存储卷。 挂接选项通过存储类动态创建的持久化存储卷，会存在一个通过mountOptions参数指定的挂接选择。如果存储卷插件不支持指定的挂接选项，这提供存储供应就会失败，在存储类或者PV中都不会对挂接选项进行验证，因此需要在设置时进行确认。 使用存储类动态存储卷供应基于StorageClass的API对象的来实现，集群管理员能够按需定义StorageClass对象，每一个StorageClass对象能够指定一个存储卷插件（即供应者）。集群管理员能够在一个集群中定义各种存储卷供应，用户不需要了解存储的细节和复杂性，就能够选择符合自己要求的存储。 启用动态供应为了启用动态供应，集群管理员需要预先为用户创建一个或者多个存储类对象。存储类对象定义了使用哪个供应者，以及供应者相关的参数。下面是存储类的一个示例，它创建一个名称为slow的存储类，使用gce供应者： 1234567apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: slowprovisioner: kubernetes.io/gce-pdparameters: type: pd-standard 下面创建了一个名为“fast”的存储类，其提供类似固态磁盘的存储卷磁盘： 1234567apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: fastprovisioner: kubernetes.io/gce-pdparameters: type: pd-ssd 使用动态供应用户通过在PersistentVolumeClaim中包含一个存储类，来请求动态供应存储。在Kubernetes v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储；在v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。 下面是请求fast存储类的持久化存储卷声明的YAML配置文件示例： 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: claim1spec: accessModes: - ReadWriteOnce# 指定所使用的存储类，此存储类将会自动创建符合要求的PV storageClassName: fast resources: requests: storage: 30Gi 此声明将使用类似于固态存储磁盘，当持久化存储卷声明被删除后，存储卷也将会被销毁。 默认行为如果Kubernetes的集群中没有指定存储类，集群管理员可以通过执行下面的设置，启用默认的存储类： 标记一个默认的StorageClass对象； 确定API server中DefaultStorage接入控制器已被启用 管理员能够通过添加storageclass.kubernetes.io/is-default-class注释，标记一个特定的StorageClass作为默认的存储类。在集群中，如果存在一个默认的StorageClass，系统将能够在不指定storageClassName 的情况下创建一个PersistentVolume，DefaultStorageClass接入控制器会自动将storageClassName指向默认的存储类。注意：在一个集群中，最多只能有一个默认的存储类，如果没有默认的存储类，那么如果在PersistentVolumeClaim中没有显示指定storageClassName，则将无法创建PersistentVolume。 NFS存储类示例部署nfs-provisioner为nfs-provisioner实例选择存储状态和数据的存储卷，并将存储卷挂接到容器的/export 123456789... volumeMounts: - name: export-volume mountPath: /exportvolumes: - name: export-volume hostPath: path: /tmp/nfs-provisioner... 为StorageClass选择一个供应者名称，并在deploy/kubernetes/deployment.yaml进行设置。 123args: - &quot;-provisioner=example.com/nfs&quot;... 完整的deployment.yaml文件内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374kind: ServiceapiVersion: v1metadata: name: nfs-provisioner labels: app: nfs-provisionerspec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-provisioner spec: containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.8 ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: # 定义提供者的名称，存储类通过此名称指定提供者 - &quot;-provisioner=nfs-provisioner&quot; env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: &quot;IfNotPresent&quot; volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 在设置好deploy/kubernetes/deployment.yaml文件后，通过kubectl create命令在Kubernetes集群中部署nfs-provisioner。 1$ kubectl create -f &#123;path&#125;/deployment.yaml 创建StorageClass下面是example-nfs的StorageClass配置文件，此配置文件定义了一个名称为nfs-storageclass的存储类，此存储类的提供者为nfs-provisioner。 12345apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-storageclass provisioner: nfs-provisioner 通过kubectl create -f命令使用上面的配置文件创建： 1$ kubectl create -f deploy/kubernetes/class.yaml storageclass “example-nfs” created 在存储类被正确创建后，就可以创建PersistenetVolumeClaim来请求StorageClass，而StorageClass将会为PersistenetVolumeClaim自动创建一个可用PersistentVolume。 创建PersistenetVolumeClaimPersistenetVolumeClaim是对PersistenetVolume的声明，即PersistenetVolume为存储的提供者，而PersistenetVolumeClaim为存储的消费者。下面是PersistentVolumeClaim的YAML配置文件，此配置文件通过spec.storageClassName字段指定所使用的存储储类。 在此配置文件中，使用nfs-storageclass存储类为PersistenetVolumeClaim创建PersistenetVolume，所要求的PersistenetVolume存储空间大小为1Mi，可以被多个容器进行读取和写入操作。 1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-pvcspec: accessModes: - ReadWriteMany storageClassName： nfs-storageclass resources: requests: storage: 1Mi 通过kubectl create命令创建上述的持久化存储卷声明： 1$ kubectl create -f &#123;path&#125;/claim.yaml 创建使用PersistenVolumeClaim的部署 在这里定义名为busybox-deployment的部署YAML配置文件，使用的镜像为busybox。基于busybox镜像的容器需要对/mnt目录下的数据进行持久化，在YAML文件指定使用名称为nfs的PersistenVolumeClaim对容器的数据进行持久化。 1234567891011121314151617181920212223242526272829303132# This mounts the nfs volume claim into /mnt and continuously# overwrites /mnt/index.html with the time and hostname of the pod. apiVersion: v1kind: Deploymentmetadata: name: busybox-deploymentspec: replicas: 2 selector: name: busybox-deployment template: metadata: labels: name: busybox-deployment spec: containers: - image: busybox command: - sh - -c - &apos;while true; do date &gt; /mnt/index.html; hostname &gt;&gt; /mnt/index.html; sleep $(($RANDOM % 5 + 5)); done&apos; imagePullPolicy: IfNotPresent name: busybox volumeMounts: # name must match the volume name below - name: nfs mountPath: &quot;/mnt&quot; # volumes: - name: nfs persistentVolumeClaim: claimName: nfs-pvc 通过kubectl create创建busy-deployment部署： 1$ kubectl create -f &#123;path&#125;/nfs-busybox-deployment.yaml liveness和readiness探针官方文档：https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ 当你使用kuberentes的时候，有没有遇到过Pod在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过kubernetes是如何检测pod是否还存活？虽然容器已经启动，但是kubernetes如何知道容器的进程是否准备好对外提供服务了呢？ Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去（谁的程序还没几个bug呢）。 Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。 livenessProbe定义 liveness命令许多长时间运行的应用程序最终会转换到broken状态，除非重新启动，否则无法恢复。Kubernetes提供了liveness probe来检测和补救这种情况。 在本次实验中，你将基于 gcr.io/google_containers/busybox镜像创建运行一个容器的Pod。以下是Pod的配置文件exec-liveness.yaml： 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: labels: test: liveness name: liveness-execspec: containers: - name: liveness args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 image: gcr.io/google_containers/busybox livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 该配置文件给Pod配置了一个容器。periodSeconds 规定kubelet要每隔5秒执行一次liveness probe。 initialDelaySeconds 告诉kubelet在第一次执行probe之前要的等待5秒钟。探针检测命令是在容器中执行 cat /tmp/healthy 命令。如果命令执行成功，将返回0，kubelet就会认为该容器是活着的并且很健康。如果返回非0值，kubelet就会杀掉这个容器并重启它。 容器启动时，执行该命令： 1/bin/sh -c &quot;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&quot; 在容器生命的最初30秒内有一个 /tmp/healthy 文件，在这30秒内 cat /tmp/healthy命令会返回一个成功的返回码。30秒后， cat /tmp/healthy 将返回失败的返回码。 创建Pod： 1kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/exec-liveness.yaml 在30秒内，查看Pod的event： 1kubectl describe pod liveness-exec 结果显示没有失败的liveness probe： 1234567FirstSeen LastSeen Count From SubobjectPath Type Reason Message--------- -------- ----- ---- ------------- -------- ------ -------24s 24s 1 &#123;default-scheduler &#125; Normal Scheduled Successfully assigned liveness-exec to worker023s 23s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Pulling pulling image &quot;gcr.io/google_containers/busybox&quot;23s 23s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Pulled Successfully pulled image &quot;gcr.io/google_containers/busybox&quot;23s 23s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Created Created container with docker id 86849c15382e; Security:[seccomp=unconfined]23s 23s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Started Started container with docker id 86849c15382e 启动35秒后，再次查看pod的event： 1kubectl describe pod liveness-exec 在最下面有一条信息显示liveness probe失败，容器被删掉并重新创建。 12345678FirstSeen LastSeen Count From SubobjectPath Type Reason Message--------- -------- ----- ---- ------------- -------- ------ -------37s 37s 1 &#123;default-scheduler &#125; Normal Scheduled Successfully assigned liveness-exec to worker036s 36s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Pulling pulling image &quot;gcr.io/google_containers/busybox&quot;36s 36s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Pulled Successfully pulled image &quot;gcr.io/google_containers/busybox&quot;36s 36s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Created Created container with docker id 86849c15382e; Security:[seccomp=unconfined]36s 36s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Normal Started Started container with docker id 86849c15382e2s 2s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Warning Unhealthy Liveness probe failed: cat: can&apos;t open &apos;/tmp/healthy&apos;: No such file or directory 再等30秒，确认容器已经重启： 1kubectl get pod liveness-exec 从输出结果来RESTARTS值加1了。 12NAME READY STATUS RESTARTS AGEliveness-exec 1/1 Running 1 1m 定义一个liveness HTTP请求我们还可以使用HTTP GET请求作为liveness probe。下面是一个基于gcr.io/google_containers/liveness镜像运行了一个容器的Pod的例子http-liveness.yaml： 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: labels: test: liveness name: liveness-httpspec: containers: - name: liveness args: - /server image: gcr.io/google_containers/liveness livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 该配置文件只定义了一个容器，livenessProbe 指定kubelete需要每隔3秒执行一次liveness probe。initialDelaySeconds 指定kubelet在该执行第一次探测之前需要等待3秒钟。该探针将向容器中的server的8080端口发送一个HTTP GET请求。如果server的/healthz路径的handler返回一个成功的返回码，kubelet就会认定该容器是活着的并且很健康。如果返回失败的返回码，kubelet将杀掉该容器并重启它。 任何大于200小于400的返回码都会认定是成功的返回码。其他返回码都会被认为是失败的返回码。 查看该server的源码：server.go. 最开始的10秒该容器是活着的， /healthz handler返回200的状态码。这之后将返回500的返回码。 12345678910http.HandleFunc(&quot;/healthz&quot;, func(w http.ResponseWriter, r *http.Request) &#123; duration := time.Now().Sub(started) if duration.Seconds() &gt; 10 &#123; w.WriteHeader(500) w.Write([]byte(fmt.Sprintf(&quot;error: %v&quot;, duration.Seconds()))) &#125; else &#123; w.WriteHeader(200) w.Write([]byte(&quot;ok&quot;)) &#125;&#125;) 容器启动3秒后，kubelet开始执行健康检查。第一次健康监测会成功，但是10秒后，健康检查将失败，kubelet将杀掉和重启容器。 创建一个Pod来测试一下HTTP liveness检测： 1kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/http-liveness.yaml After 10 seconds, view Pod events to verify that liveness probes have failed and the Container has been restarted: 10秒后，查看Pod的event，确认liveness probe失败并重启了容器。 1kubectl describe pod liveness-http 定义TCP liveness探针第三种liveness probe使用TCP Socket。 使用此配置，kubelet将尝试在指定端口上打开容器的套接字。 如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: goproxy labels: app: goproxyspec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 如您所见，TCP检查的配置与HTTP检查非常相似。 此示例同时使用了readiness和liveness probe。 容器启动后5秒钟，kubelet将发送第一个readiness probe。 这将尝试连接到端口8080上的goproxy容器。如果探测成功，则该pod将被标记为就绪。Kubelet将每隔10秒钟执行一次该检查。 除了readiness probe之外，该配置还包括liveness probe。 容器启动15秒后，kubelet将运行第一个liveness probe。 就像readiness probe一样，这将尝试连接到goproxy容器上的8080端口。如果liveness probe失败，容器将重新启动。 使用命名的端口 可以使用命名的ContainerPort作为HTTP或TCP liveness检查： 123456789ports:- name: liveness-port containerPort: 8080 hostPort: 8080livenessProbe: httpGet: path: /healthz port: liveness-port 定义readiness探针有时，应用程序暂时无法对外部流量提供服务。 例如，应用程序可能需要在启动期间加载大量数据或配置文件。 在这种情况下，你不想杀死应用程序，但你也不想发送请求。 Kubernetes提供了readiness probe来检测和减轻这些情况。 Pod中的容器可以报告自己还没有准备，不能处理Kubernetes服务发送过来的流量。 Readiness probe的配置跟liveness probe很像。唯一的不同是使用 readinessProbe而不是livenessProbe。 1234567readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Readiness probe的HTTP和TCP的探测器配置跟liveness probe一样。 Readiness和livenss probe可以并行用于同一容器。 使用两者可以确保流量无法到达未准备好的容器，并且容器在失败时重新启动。 配置ProbeProbe中有很多精确和详细的配置，通过它们你能准确的控制liveness和readiness检查： initialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒。 periodSeconds：执行探测的频率。默认是10秒，最小1秒。 timeoutSeconds：探测超时时间。默认1秒，最小1秒。 successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。 failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 HTTP probe中可以给 httpGet设置其他配置项： host：连接的主机名，默认连接到pod的IP。你可能想在http header中设置”Host”而不是使用IP。 scheme：连接使用的schema，默认HTTP。 path: 访问的HTTP server的path。 httpHeaders：自定义请求的header。HTTP运行重复的header。 port：访问的容器的端口名字或者端口号。端口号必须介于1和65525之间。 对于HTTP探测器，kubelet向指定的路径和端口发送HTTP请求以执行检查。 Kubelet将probe发送到容器的IP地址，除非地址被httpGet中的可选host字段覆盖。 在大多数情况下，你不想设置主机字段。 有一种情况下你可以设置它。 假设容器在127.0.0.1上侦听，并且Pod的hostNetwork字段为true。 然后，在httpGet下的host应该设置为127.0.0.1。 如果你的pod依赖于虚拟主机，这可能是更常见的情况，你不应该是用host，而是应该在httpHeaders中设置Host头。 初始化容器理解初始容器一个pod里可以运行多个容器,它也可以运行一个或者多个初始容器,初始容器先于应用容器运行,除了以下两点外,初始容器和普通容器没有什么两样: 它们总是run to completion 一个初始容器必须成功运行另一个才能运行 如果pod中的一个初始容器运行失败,则kubernetes会尝试重启pod直到初始容器成功运行,如果pod的重启策略设置为从不(never),则不会重启. 创建容器时,在podspec里添加initContainers字段,则指定容器即为初始容器,它们的返回状态作为数组保存在.status.initContainerStatuses里(与普通容器状态存储字段.status.containerStatuses类似) 初始容器和普通容器的不同:初始容器支持所有普通容器的特征,包括资源配额限制和存储卷以及安全设置.但是对资源申请和限制处理初始容器略有不同,下面会介绍.此外,初始容器不支持可用性探针(readiness probe),因为它在ready之前必须run to completion 如果在一个pod里指定了多个初始容器,则它们会依次启动起来(pod内的普通容器并行启动),并且只有上一个成功下一个才能启动.当所有的初始容器都启动了,kubernetes才开始启普通应用容器. 初始容器能做什么由于初始容器和普通应用容器是分开的镜像,因此他在做一些初始化工作很有优势: 它们可以包含并且运行一些出于安全考虑不适合和应用放在一块的小工具. 它们可以一些小工具和自定义代码来做些初始化工作,这样就不需要在普通应用容器里使用sed,awk,python或者dig来做初始化工作了 应用构建者和发布者可以独立工作,而不必再联合起来处理同一个pod 它们使用linux namespaces因此它们和普通应用pod拥有不同的文件系统视图.因此他们可以被赋予普通应用容器获取不到的secrets 它们在应用容器启动前运行,因此它们可以阻止或者延缓普通应用容器的初始化直到需要的条件满足 示例: 通过执行shell命令来等待一个服务创建完成,命令如下: 1for i in &#123;1..100&#125;; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1 通过downward API把当前pod注册到远程服务器,命令如下: 1curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &apos;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&apos; 在容器启动之前等待一定时间:例如sleep 60 克隆一个git仓库到存储目录 通过模板工具动态把一些值写入到主应用程序的配置文件里. 更多详细示例请查看pod应用环境布置指南 初始容器使用123456789101112131415161718apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: containers: - name: myapp-container image: busybox command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo The app is running! &amp;&amp; sleep 3600&apos;] initContainers: - name: init-myservice image: busybox command: [&apos;sh&apos;, &apos;-c&apos;, &apos;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&apos;] - name: init-mydb image: busybox command: [&apos;sh&apos;, &apos;-c&apos;, &apos;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&apos;] 以上pod定义包含两个初始容器,第一个等待myservice服务可用,第二个等待mydb服务可用,这两个pod执行完成,应用容器开始执行. 下面是myservice和mydb两个服务的yaml文件 12345678910111213141516171819kind: ServiceapiVersion: v1metadata: name: myservicespec: ports: - protocol: TCP port: 80 targetPort: 9376---kind: ServiceapiVersion: v1metadata: name: mydbspec: ports: - protocol: TCP port: 80 targetPort: 9377 上面定义的pod可以通过以下使用初始化和调试 12kubectl create -f myapp.yamlpod/myapp-pod created 1234kubectl get -f myapp.yamlNAME READY STATUS RESTARTS AGEmyapp-pod 0/1 Init:0/2 0 6m 1234567891011121314151617181920212223242526272829303132Name: myapp-podNamespace: default[...]Labels: app=myappStatus: Pending[...]Init Containers: init-myservice:[...] State: Running[...] init-mydb:[...] State: Waiting Reason: PodInitializing Ready: False[...]Containers: myapp-container:[...] State: Waiting Reason: PodInitializing Ready: False[...]Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 &#123;default-scheduler &#125; Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 &#123;kubelet 172.17.4.201&#125; spec.initContainers&#123;init-myservice&#125; Normal Pulling pulling image &quot;busybox&quot; 13s 13s 1 &#123;kubelet 172.17.4.201&#125; spec.initContainers&#123;init-myservice&#125; Normal Pulled Successfully pulled image &quot;busybox&quot; 13s 13s 1 &#123;kubelet 172.17.4.201&#125; spec.initContainers&#123;init-myservice&#125; Normal Created Created container with docker id 5ced34a04634; Security:[seccomp=unconfined] 13s 13s 1 &#123;kubelet 172.17.4.201&#125; spec.initContainers&#123;init-myservice&#125; Normal Started Started container with docker id 5ced34a04634 12kubectl logs myapp-pod -c init-myservice # Inspect the first init containerkubectl logs myapp-pod -c init-mydb # Inspect the second init container 当我们启动mydb和myservice两个服务后,我们可以看到初始容器完成并且myapp-pod pod被创建. 1234kubectl create -f services.yamlservice/myservice createdservice/mydb created 123kubectl get -f myapp.yamlNAME READY STATUS RESTARTS AGEmyapp-pod 1/1 Running 0 9m 这些示例非常简单但是应该能为你创建自己的初始容器提供一些灵感 行为细节 在启动pod的过程中,在存储卷和网络创建以后,初始容器依次创建.上一个容器必须返回成功下一个才能启动,如果由于运行时错误或者其它异常退出,它会依照restartPolicy来重试,然而,如果restartPolicy设置为Always,初始容器实际上使用的是OnFailure策略 如果pod重启了,则所有的初始容器要重新执行 对初始容器的spec的更改仅限于镜像(image)字段的修改,更改了初始容器的镜像字段相当于重启pod 由于初始容器可以被重启,重试和重新执行,因此它里面的代码应当是幂等的,尤其是写入文件到EmptyDirs的代码应当注意文件可能已经存在 容器中的所有初始容器和普通容器名称必须惟一. 资源基于初始容器的执行顺序,以下关于资源的规则适用: 对于特定资源,所有初始容器申请的最高的生效 对于pod,相同资源申请取以下两者较高的一个: 1) 所有普通应用容器申请的资源总和2) 初始容器申请的生效的资源(上面说到,初始容器申请资源取所有初始容器申请最大的一个) 调度基于生效的初始请求,这就意味着初始容器可以申请预留资源,即便在pod以后的整个生命周期都用不到 pod重启原因一个pod基于以下列出的原因,会重启,重新执行初始容器: 用户更新初始容器的PodSpec导致镜像发生改变.普通应用容器改变只会使应用容器重启 由于restartPolicy被设置为Always,导致所有容器均被中止,强制重启,由于垃圾回收初始容器的初始状态记录丢失 定义pod postStart或preStop123456789101112131415[root@k8s-master01 manifests]# cat poststart-pod.yaml apiVersion: v1kind: Podmetadata: name: poststart-podspec: containers: - name: buxybox-httpd image: busybox imagePullPolicy: IfNotPresent lifecycle: postStart: exec: command: [&quot;mkdir&quot;, &quot;-p&quot;,&quot; /data/web/html&quot;] command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 3600&quot;]]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes之helm详解]]></title>
    <url>%2Fkubernetes%E4%B9%8Bhelm%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Helm简介很多人都使用过Ubuntu下的ap-get或者CentOS下的yum, 这两者都是Linux系统下的包管理工具。采用apt-get/yum,应用开发者可以管理应用包之间的依赖关系，发布应用；用户则可以以简单的方式查找、安装、升级、卸载应用程序。 我们可以将Helm看作Kubernetes下的apt-get/yum。Helm是Deis (https://deis.com/) 开发的一个用于kubernetes的包管理器。每个包称为一个Chart，一个Chart是一个目录（一般情况下会将目录进行打包压缩，形成name-version.tgz格式的单一文件，方便传输和存储）。 对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。 除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。 官方网站：https://helm.sh/ GitHub：https://github.com/helm/helm Helm 组件及相关术语Helm Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。 Tiller Tiller 是 Helm 的服务端，部署在 Kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件（ Helm 称为 Release ），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。 Chart Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。 Repoistory Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。 Release 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。 注：需要注意的是：Helm 中提到的 Release 和我们通常概念中的版本有所不同，这里的 Release 可以理解为 Helm 使用 Chart 包部署的一个应用实例。 Helm工作原理 Chart Install 过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将指定的Chart结构和Values信息通过gRPC传递给Tiller Tiller根据Chart和Values生成一个Release Tiller将Release发送给Kubernetes用于生成Release Chart Update过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将要更新的Release的名称和Chart结构，Values信息传递给Tiller Tiller生成Release并更新指定名称的Release的History Tiller将Release发送给Kubernetes用于更新Release Chart Rollback过程： Helm将要回滚的Release的名称传递给Tiller Tiller根据Release的名称查找History Tiller从History中获取上一个Release Tiller将上一个Release发送给Kubernetes用于替换当前Release Helm部署Helm 客户端安装Helm 的安装方式很多，这里采用二进制的方式安装。更多安装方法可以参考 Helm 的官方帮助文档。 方式一：使用官方提供的脚本一键安装 123curl https://raw.githubusercontent.com/helm/helm/master/scripts/get &gt; get_helm.sh$ chmod 700 get_helm.sh$ ./get_helm.sh 方式二：手动下载安装(推荐) 12345#从官网下载最新版本的二进制安装包到本地：https://github.com/kubernetes/helm/releasestar -zxvf helm-2.9.0.tar.gz # 解压压缩包# 把 helm 指令放到bin目录下mv helm-2.9.0/helm /usr/local/bin/helmhelm help # 验证 Helm 服务端安装Tiller注意：先在 K8S 集群上每个节点安装 socat 软件(yum install -y socat )，不然会报如下错误： 12E0522 22:22:15.492436 24409 portforward.go:331] an error occurred forwarding 38398 -&gt; 44134: error forwarding port 44134 to pod dc6da4ab99ad9c497c0cef1776b9dd18e0a612d507e2746ed63d36ef40f30174, uid : unable to do port forwarding: socat not found.Error: cannot connect to Tiller Tiller 是以 Deployment 方式部署在 Kubernetes 集群中的，只需使用以下指令便可简单的完成安装。 1$ helm init 由于 Helm 默认会去 storage.googleapis.com 拉取镜像，如果你当前执行的机器不能访问该域名的话可以使用以下命令来安装： 12345helm init --client-only --stable-repo-url https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts/helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/helm repo update 123456# 创建服务端helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts# 创建TLS认证服务端，参考地址：https://github.com/gjmzj/kubeasz/blob/master/docs/guide/helm.mdhelm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --tiller-tls-cert /etc/kubernetes/ssl/tiller001.pem --tiller-tls-key /etc/kubernetes/ssl/tiller001-key.pem --tls-ca-cert /etc/kubernetes/ssl/ca.pem --tiller-namespace kube-system --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 在 Kubernetes 中安装 Tiller 服务，因为官方的镜像因为某些原因无法拉取，使用-i指定自己的镜像，可选镜像：registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1（阿里云），该镜像的版本与helm客户端的版本相同，使用helm version可查看helm客户端版本。 如果在用helm init安装tiller server时一直部署不成功,检查deployment，根据描述解决问题。 给 Tiller 授权因为 Helm 的服务端 Tiller 是一个部署在 Kubernetes 中 Kube-System Namespace 下 的 Deployment，它会去连接 Kube-Api 在 Kubernetes 里创建和删除应用。 而从 Kubernetes 1.6 版本开始，API Server 启用了 RBAC 授权。目前的 Tiller 部署时默认没有定义授权的 ServiceAccount，这会导致访问 API Server 时被拒绝。所以我们需要明确为 Tiller 部署添加授权。 创建 Kubernetes 的服务帐号和绑定角色 12$ kubectl create serviceaccount --namespace kube-system tiller$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller 为 Tiller 设置帐号 123# 使用 kubectl patch 更新 API 对象$ kubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos;deployment.extensions &quot;tiller-deploy&quot; patched 查看是否授权成功 123$ kubectl get deploy --namespace kube-system tiller-deploy --output yaml|grep serviceAccountserviceAccount: tillerserviceAccountName: tiller 验证 Tiller 是否安装成功12345$ kubectl -n kube-system get pods|grep tillertiller-deploy-6d68f5c78f-nql2z 1/1 Running 0 5m$ helm versionClient: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125;Server: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125; 卸载 Helm 服务器端 Tiller如果你需要在 Kubernetes 中卸载已部署的 Tiller，可使用以下命令完成卸载。 12$ helm reset 或$helm reset --force Helm ChartChart 目录结构12345678910wordpress/ Chart.yaml # A YAML file containing information about the chart LICENSE # OPTIONAL: A plain text file containing the license for the chart README.md # OPTIONAL: A human-readable README file requirements.yaml # OPTIONAL: A YAML file listing dependencies for the chart values.yaml # The default configuration values for this chart charts/ # A directory containing any charts upon which this chart depends. templates/ # A directory of templates that, when combined with values, # will generate valid Kubernetes manifest files. templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes Chart.yaml 文件Chart.yaml文件是chart所必需的。它包含以下字段： 12345678910111213141516171819apiVersion: The chart API version, always &quot;v1&quot; (required)name: The name of the chart (required)version: A SemVer 2 version (required)kubeVersion: A SemVer range of compatible Kubernetes versions (optional)description: A single-sentence description of this project (optional)keywords: - A list of keywords about this project (optional)home: The URL of this project&apos;s home page (optional)sources: - A list of URLs to source code for this project (optional)maintainers: # (optional) - name: The maintainer&apos;s name (required for each maintainer) email: The maintainer&apos;s email (optional for each maintainer) url: A URL for the maintainer (optional for each maintainer)engine: gotpl # The name of the template engine (optional, defaults to gotpl)icon: A URL to an SVG or PNG image to be used as an icon (optional).appVersion: The version of the app that this contains (optional). This needn&apos;t be SemVer.deprecated: Whether this chart is deprecated (optional, boolean)tillerVersion: The version of Tiller that this chart requires. This should be expressed as a SemVer range: &quot;&gt;2.0.0&quot; (optional) Charts和版本控制每个chart都必须有一个版本号。版本必须遵循SemVer 2标准。与Helm Class 格式不同，Kubernetes Helm使用版本号作为发布标记。存储库中的软件包由名称加版本识别。 例如，nginx version字段设置为1.2.3将被命名为： 1nginx-1.2.3.tgz 更复杂的SemVer 2命名也是支持的，例如 version: 1.2.3-alpha.1+ef365。但非SemVer命名是明确禁止的。 注意：虽然Helm Classic和Deployment Manager在chart方面都非常适合GitHub，但Kubernetes Helm并不依赖或需要GitHub甚至Git。因此，它不使用Git SHA进行版本控制。 许多Helm工具都使用Chart.yaml的version字段，其中包括CLI和Tiller服务。在生成包时，helm package命令将使用它在Chart.yaml中的版本名作为包名。系统假定chart包名称中的版本号与Chart.yaml中的版本号相匹配。不符合这个情况会导致错误。 appVersion字段 请注意，appVersion字段与version字段无关。这是一种指定应用程序版本的方法。例如，drupal chart可能有一个appVersion: 8.2.1，表示chart中包含的Drupal版本（默认情况下）是8.2.1。该字段是信息标识，对chart版本没有影响。 Chart许可证文件，自述文件和说明文件 chart还可以包含描述chart的安装，配置，使用和许可证的文件。chart的自述文件应Markdown（README.md）语法格式化，并且通常应包含： chart提供的应用程序或服务的描述 运行chart的任何前提条件或要求 选项values.yaml和默认值的说明 任何其他可能与安装或配置chart相关的信息 chart还可以包含一个简短的纯文本templates/NOTES.txt文件，在安装后以及查看版本状态时将打印出来。此文件将作为模板template进行评估 ，并可用于显示使用说明，后续步骤或任何其他与发布chart相关的信息。例如，可以提供用于连接到数据库或访问Web UI的指令。由于运行时，该文件被打印到标准输出 helm install或helm status，建议保持内容简短并把更多细节指向自述文件。 Chart依赖关系在Helm中，一个chart可能依赖于任何数量的其他chart。这些依赖关系可以通过requirements.yaml 文件动态链接或引入charts/目录并手动管理。 Chart支持两种方式表示依赖关系，可以使用requirements.yaml或者直接将依赖的Chart放置到charts目录中。 虽然有一些团队需要手动管理依赖关系的优势，但声明依赖关系的首选方法是使用 chart 内部的requirements.yaml文件。 注意： 传统Helm 的Chart.yaml dependencies:部分字段已被完全删除弃用。 用requirements.yaml来管理依赖关系 requirements.yaml文件是列出chart的依赖关系的简单文件 1234567dependencies: - name: apache version: 1.2.3 repository: http://example.com/charts - name: mysql version: 3.2.1 repository: http://another.example.com/charts 该name字段是chart的名称。 version字段是chart的版本。 repository字段是chart repo的完整URL。请注意，还必须使用helm repo add添加该repo到本地才能使用。 有了依赖关系文件，你可以通过运行helm dependency update ，它会使用你的依赖关系文件将所有指定的chart下载到你的charts/目录中。 12345678910$ helm dep up foochartHang tight while we grab the latest from your chart repositories......Successfully got an update from the &quot;local&quot; chart repository...Successfully got an update from the &quot;stable&quot; chart repository...Successfully got an update from the &quot;example&quot; chart repository...Successfully got an update from the &quot;another&quot; chart repositoryUpdate Complete. Happy Helming!Saving 2 chartsDownloading apache from repo http://example.com/chartsDownloading mysql from repo http://another.example.com/charts 当helm dependency update检索chart时，它会将它们作为chart存档存储在charts/目录中。因此，对于上面的示例，可以在chart目录中看到以下文件： 123charts/ apache-1.2.3.tgz mysql-3.2.1.tgz 通过requirements.yaml管理chart是一种轻松更新chart的好方法，还可以在整个团队中共享requirements信息。 requirements.yaml中的alias字段 除上述其他字段外，每个requirement条目可能包含可选字段alias。 为依赖的chart添加别名会将chart放入依赖关系中，并使用别名作为新依赖关系的名称。 如果需要使用其他名称访问chart，可以使用alias。 12345678910111213# parentchart/requirements.yamldependencies: - name: subchart repository: http://localhost:10191 version: 0.1.0 alias: new-subchart-1 - name: subchart repository: http://localhost:10191 version: 0.1.0 alias: new-subchart-2 - name: subchart repository: http://localhost:10191 version: 0.1.0 在上面的例子中，我们将得到parentchart的3个依赖关系 123subchartnew-subchart-1new-subchart-2 实现这一目的的手动方法是charts/中用不同名称多次复制/粘贴目录中的同一chart 。 requirements.yaml中的tags和condition字段 除上述其他字段外，每个需求条目可能包含可选字段tags和condition。 所有charts都会默认加载。如果存在tags或condition字段，将对它们进行评估并用于控制应用的chart的加载。 Condition – condition 字段包含一个或多个YAML路径（用逗号分隔）。如果此路径存在于顶级父级的值中并且解析为布尔值，则将根据该布尔值启用或禁用chart。只有在列表中找到的第一个有效路径才被评估，如果没有路径存在，那么该条件不起作用。 Tags – 标签字段是与此chart关联的YAML标签列表。在顶级父级的值中，可以通过指定标签和布尔值来启用或禁用所有带有标签的chart。 1234567891011121314151617# parentchart/requirements.yamldependencies: - name: subchart1 repository: http://localhost:10191 version: 0.1.0 condition: subchart1.enabled, global.subchart1.enabled tags: - front-end - subchart1 - name: subchart2 repository: http://localhost:10191 version: 0.1.0 condition: subchart2.enabled,global.subchart2.enabled tags: - back-end - subchart2 123456# parentchart/values.yamlsubchart1: enabled: truetags: front-end: false back-end: true 在上面的示例中，所有带有标签的front-end的charts都将被禁用，但由于 subchart1.enabled的值在父项值中为“真”，因此条件将覆盖该 front-end标签，subchart1会启用。 由于subchart2被标记back-end和标签的计算结果为true，subchart2将被启用。还要注意的是，虽然subchart2有一个在requirements.yaml中指定的条件，但父项的值中没有对应的路径和值，因此条件无效。 使用命令行时带有tag和conditions --set参数可使用来更改tag和conditions值。 1helm install --set tags.front-end=true --set subchart2.enabled=false tags和conditions解析 Conditions (设置 values) 会覆盖tags配置.。第一个存在的condition路径生效，后续该chart的condition路径将被忽略。 如果chart的某tag的任一tag的值为true，那么该tag的值为true，并启用这个chart。 Tags 和 conditions值必须在顶级父级的值中进行设置。 tags:值中的关键字必须是顶级关键字。目前不支持全局和嵌套tags:表格。 通过requirements.yaml导入子值 在某些情况下，希望允许子chart的值传到父chart并作为通用默认值共享。使用这种exports格式的另一个好处是它可以使未来的工具能够考虑用户可设置的值。 要导入的值的键可以在父chart文件中requirements.yaml使用YAML list指定。list中的每个项目都是从子chart exports字段导入的key。 要导入不包含在exports key中的值，请使用子父级child-parent格式。下面描述了两种格式的例子。 使用exports格式 如果子chart的values.yaml文件exports在根目录中包含一个字段，则可以通过指定要导入的关键字将其内容直接导入到父项的值中，如下例所示： 1234# parent&apos;s requirements.yaml file ... import-values: - data 12345# child&apos;s values.yaml file...exports: data: myint: 99 由于我们在导入列表中指定了data键，因此Helm会在exports子图的字段中查找data键并导入其内容。 最终的父值将包含我们的导出字段： 123# parent&apos;s values file...myint: 99 请注意，父键data不包含在父chart的最终值中。如果需要指定父键，请使用’child-parent’格式。 使用child-parent格式 要访问未包含在子chart键值exports的中的值，需要指定要导入的值的源键（child）和父chart值（parent）中的目标路径。 下面的例子中的import-values告诉Helm去拿在child:路径发现的任何值，并将其复制到父值parent:指定的路径 123456789# parent&apos;s requirements.yaml filedependencies: - name: subchart1 repository: http://localhost:10191 version: 0.1.0 ... import-values: - child: default.data parent: myimports 在上面的例子中，在subchart1default.data的值中找到的值将被导入到父chart值中myimports的键值，详细如下： 123456# parent&apos;s values.yaml filemyimports: myint: 0 mybool: false mystring: &quot;helm rocks!&quot; 123456# subchart1&apos;s values.yaml filedefault: data: myint: 999 mybool: true 父chart的结果值为： 123456# parent&apos;s final valuesmyimports: myint: 999 mybool: true mystring: &quot;helm rocks!&quot; 父chart的最终值现在包含从subchart1导入的myint和mybool字段。 通过charts/目录手动管理依赖性 如果需要更多的控制依赖关系，可以通过将依赖的charts复制到charts/目录中来明确表达这些依赖关系 。 依赖关系可以是chart归档（foo-1.2.3.tgz）或解压缩的chart目录。但它的名字不能从_或.开始。这些文件被chart加载器忽略。 例如，如果WordPress chart依赖于Apache chart，则在WordPress chart的charts/目录中提供（正确版本的）Apache chart： 1234567891011wordpress: Chart.yaml requirements.yaml # ... charts/ apache/ Chart.yaml # ... mysql/ Chart.yaml # ... 上面的示例显示了WordPress chart如何通过在其`charts/“目录中包含这些charts来表示它对Apache和MySQL的依赖关系。 提示： 将依赖项放入charts/目录，请使用helm fetch命令 使用依赖关系的操作方面影响 上面的部分解释了如何指定chart依赖关系，但是这会如何影响使用helm install和helm upgrade的chart安装？ 假设名为“A”的chart创建以下Kubernetes对象 namespace “A-Namespace” statefulset “A-StatefulSet” service “A-Service” 此外，A依赖于创建对象的chart B. namespace “B-Namespace” replicaset “B-ReplicaSet” service “B-Service” 安装/升级chart A后，会创建/修改单个Helm版本。该版本将按以下顺序创建/更新所有上述Kubernetes对象： A-Namespace B-Namespace A-StatefulSet B-ReplicaSet A-Service B-Service 这是因为当Helm安装/升级charts时，charts中的Kubernetes对象及其所有依赖项都是如下 聚合成一个单一的集合; 然后 按类型排序，然后按名称排序; 接着 按该顺序创建/更新。 因此，单个release是使用charts及其依赖关系创建的所有对象。 Kubernetes类型的安装顺序由kind_sorter.go中的枚举InstallOrder给出（the Helm source file)）。 模板Templates和值ValuesHelm chart模板是用Go模板语言Go template language编写的 ，其中添加了来自Sprig库from the Sprig library的50个左右的附加模板函数以及一些其他专用函数specialized functions。 所有模板文件都存储在chart的templates/文件夹中。当Helm渲染charts时，它将通过模板引擎传递该目录中的每个文件。 模板的值有两种提供方法： chart开发人员可能会在chart内部提供一个values.yaml文件。该文件可以包含默认值。 chart用户可能会提供一个包含值的YAML文件。这可以通过命令行提供helm install -f。 当用户提供自定义值时，这些值将覆盖chart中values.yaml文件中的值。 模板文件模板文件遵循用于编写Go模板的标准约定（请参阅文the text/template Go package documentation 以了解详细信息）。示例模板文件可能如下所示： 123456789101112131415161718192021222324252627# db.yamlapiVersion: v1kind: ReplicationControllermetadata: name: deis-database namespace: deis labels: heritage: deisspec: replicas: 1 selector: app: deis-database template: metadata: labels: app: deis-database spec: serviceAccount: deis-database containers: - name: deis-database image: &#123;&#123;.Values.imageRegistry&#125;&#125;/postgres:&#123;&#123;.Values.dockerTag&#125;&#125; imagePullPolicy: &#123;&#123;.Values.pullPolicy&#125;&#125; ports: - containerPort: 5432 env: - name: DATABASE_STORAGE value: &#123;&#123;default &quot;minio&quot; .Values.storage&#125;&#125; 上面的示例基于此网址，是Kubernetes replication controller的模板。它可以使用以下四个模板值（通常在values.yaml文件中定义 ）： imageRegistry：Docker镜像的源。 dockerTag：docker镜像的标签。 pullPolicy：Kubernetes镜像拉取策略。 storage：存储后端，其默认设置为 &quot;minio&quot; 所有这些值都由模板作者定义。Helm不需要或指定参数。 要查更多charts，请查看Kubernetes charts项目。https://github.com/helm/charts 模板语法模版语法扩展了 golang/text/template的语法： 1234567# 这种方式定义的模版，会去除test模版尾部所有的空行&#123;&#123;- define &quot;test&quot;&#125;&#125;模版内容&#123;&#123;- end&#125;&#125;# 去除test模版头部的第一个空行&#123;&#123;- template &quot;test&quot; &#125;&#125; 用于yaml文件前置空格的语法： 1234567# 这种方式定义的模版，会去除test模版头部和尾部所有的空行&#123;&#123;- define &quot;test&quot; -&#125;&#125;模版内容&#123;&#123;- end -&#125;&#125;# 可以在test模版每一行的头部增加4个空格，用于yaml文件的对齐&#123;&#123; include &quot;test&quot; | indent 4&#125;&#125; 提供的一些声明和使用命名模板段的操作： 123define在模板中声明一个新的命名模板template导入一个命名模板block 声明了一种特殊的可填写模板区域 首先，模板名称是全局的。如果声明两个具有相同名称的模板，则最后加载一个模板是起作用的模板。由于子chart中的模板与顶级模板一起编译，因此注意小心地使用特定chart的名称来命名模板。 通用的命名方式是，以chart名称作为前缀， 1eg: &#123;&#123; define &quot;mychart.labels&quot; &#125;&#125; 用define和template声明,使用模板示例： 12345678910111213141516&#123;&#123;/* Generate basic labels */&#125;&#125;&#123;&#123;- define &quot;mychart.labels&quot; &#125;&#125; labels: generator: helm date: &#123;&#123; now | htmlDate &#125;&#125;&#123;&#123;- end &#125;&#125;apiVersion: v1kind: ConfigMapmetadata: name: &#123;&#123; .Release.Name &#125;&#125;-configmap &#123;&#123;- template &quot;mychart.labels&quot; &#125;&#125;data: myvalue: &quot;Hello World&quot; &#123;&#123;- range $key, $val := .Values.favorite &#125;&#125; &#123;&#123; $key &#125;&#125;: &#123;&#123; $val | quote &#125;&#125; &#123;&#123;- end &#125;&#125; 当模板引擎读取该文件时，它将存储引用mychart.labels直到template “mychart.labels”被调用。然后它将在文件内渲染该模板。结果： 123456789101112# Source: mychart/templates/configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: running-panda-configmap labels: generator: helm date: 2016-11-02data: myvalue: &quot;Hello World&quot; drink: &quot;coffee&quot; food: &quot;pizza&quot; 按照惯例，define函数会有一个简单的文档块如下来描述自己 1（&#123;&#123;/* ... */&#125;&#125;） Helm chart通常将这些模板放入partials文件中，通常是_helpers.tpl。 定义和引用的方式不变。 如果这样定义一个模板 123456&#123;&#123;/* Generate basic labels */&#125;&#125;&#123;&#123;- define &quot;mychart.labels&quot; &#125;&#125; labels: generator: helm version: &#123;&#123; .Chart.Version &#125;&#125;&#123;&#123;- end &#125;&#125; 还是这样引用： 1&#123;&#123;- template &quot;mychart.labels&quot; &#125;&#125; 则 version 的值为空，因为模板需要一个上下文： 123&#123;&#123;- template &quot;mychart.labels&quot; . &#125;&#125;把顶层对象&apos;.&apos;传递给模板，即可引用.Release,.Chaert 等。 include这是一个引用模板的函数： 假设我们定义了一个这样的模板： 1234&#123;&#123;- define &quot;mychart.app&quot; -&#125;&#125;app_name: &#123;&#123; .Chart.Name &#125;&#125;app_version: &quot;&#123;&#123; .Chart.Version &#125;&#125;+&#123;&#123; .Release.Time.Seconds &#125;&#125;&quot;&#123;&#123;- end -&#125;&#125; 并且正常引用： 123456789101112apiVersion: v1kind: ConfigMapmetadata: name: &#123;&#123; .Release.Name &#125;&#125;-configmap labels: &#123;&#123; template &quot;mychart.app&quot; .&#125;&#125;data: myvalue: &quot;Hello World&quot; &#123;&#123;- range $key, $val := .Values.favorite &#125;&#125; &#123;&#123; $key &#125;&#125;: &#123;&#123; $val | quote &#125;&#125; &#123;&#123;- end &#125;&#125;&#123;&#123; template &quot;mychart.app&quot; . &#125;&#125; 结果会有缩进错误： 1234567891011121314# Source: mychart/templates/configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: measly-whippet-configmap labels: app_name: mychartapp_version: &quot;0.1.0+1478129847&quot; ＃缩进错误data: myvalue: &quot;Hello World&quot; drink: &quot;coffee&quot; food: &quot;pizza&quot; app_name: mychartapp_version: &quot;0.1.0+1478129847&quot; ＃缩进错误 因为template的数据只是内嵌插入，是一个‘动作’，而非‘函数’，即不能通过管道传递给其他函数，来进行格式化。 include 配合 indent 可以解决这个问题。 12&#123;&#123; include &quot;mychart.app&quot; . | indent 4 &#125;&#125;引用 mychart.app 且每一行缩进４字符 上面的示例可以改为 123456789101112apiVersion: v1kind: ConfigMapmetadata: name: &#123;&#123; .Release.Name &#125;&#125;-configmap labels:&#123;&#123; include &quot;mychart.app&quot; . | indent 4 &#125;&#125;data: myvalue: &quot;Hello World&quot; &#123;&#123;- range $key, $val := .Values.favorite &#125;&#125; &#123;&#123; $key &#125;&#125;: &#123;&#123; $val | quote &#125;&#125; &#123;&#123;- end &#125;&#125;&#123;&#123; include &quot;mychart.app&quot; . | indent 2 &#125;&#125; 文件访问Helm通过.Files对象提供对文件的访问，例如Files.Get是一个按名称获取文件的函数（.Files.Get config.ini）。下面是几个要注意的点： 12345向Helm chart添加额外的文件是可以的。这些文件将被捆绑并发送给Tiller。不过要注意，由于Kubernetes对象的存储限制，chart必须小于1M。通常出于安全原因，某些文件不能通过.Files对象访问。templates/下的文件。使用.helmignore排除的文件不能被访问。 示例： 首先创建三个文件 12345678config1.toml: message = Hello from config 1 config2.toml: message = This is config 2config3.toml: message = Goodbye from config 3 然后，用一个range函数来遍历它们并将它们的内容注入到ConfigMap中。 12345678910apiVersion: v1kind: ConfigMapmetadata: name: &#123;&#123; .Release.Name &#125;&#125;-configmapdata: &#123;&#123;- $files := .Files &#125;&#125; &#123;&#123;- range tuple &quot;config1.toml&quot; &quot;config2.toml&quot; &quot;config3.toml&quot; &#125;&#125; &#123;&#123; . &#125;&#125;: |- &#123;&#123; $files.Get . &#125;&#125; &#123;&#123;- end &#125;&#125; 其结果如下： 1234567891011121314# Source: mychart/templates/configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: quieting-giraf-configmapdata: config1.toml: |- message = Hello from config 1 config2.toml: |- message = This is config 2 config3.toml: |- message = Goodbye from config 3 Glob 函数Glob是一个可以一次获取多个文件的函数。 123456789假设这样的一个目录结构：foo/: foo.txt foo.yamlbar/: bar.go bar.conf baz.yaml 则可以这样引用文件： 12345&#123;&#123; $root := . &#125;&#125;&#123;&#123; range $path, $bytes := .Files.Glob &quot;**.yaml&quot; &#125;&#125;&#123;&#123; $path &#125;&#125;: |-&#123;&#123; $root.Files.Get $path &#125;&#125;&#123;&#123; end &#125;&#125; 或者这样： 123&#123;&#123; range $path, $bytes := .Files.Glob &quot;foo/*&quot; &#125;&#125;&#123;&#123; $path.base &#125;&#125;: &apos;&#123;&#123; $root.Files.Get $path | b64enc &#125;&#125;&apos; ＃ ｂ64enc 是base64编码函数。&#123;&#123; end &#125;&#125; 有时候想要将文件内容放到configmap里，则可以用Glob ,ConfigMap和Secrets配合实现： 1234567891011121314apiVersion: v1kind: ConfigMapmetadata: name: confdata:&#123;&#123; (.Files.Glob &quot;foo/*&quot;).AsConfig | indent 2 &#125;&#125;---apiVersion: v1kind: Secretmetadata: name: very-secrettype: Opaquedata:&#123;&#123; (.Files.Glob &quot;bar/*&quot;).AsSecrets | indent 2 &#125;&#125; 按行获取文件： 123data: some-file.txt: &#123;&#123; range .Files.Lines &quot;foo/bar.txt&quot; &#125;&#125; &#123;&#123; . &#125;&#125;&#123;&#123; end &#125;&#125; 预定义值通过values.yaml文件（或通过--set 标志）提供的值可以从.Values模板中的对象访问。可以在模板中访问其他预定义的数据片段。 以下值是预定义的，可用于每个模板，并且不能被覆盖。与所有值一样，名称区分大小写。 Release.Name：release的名称（不是chart的） Release.Time：chart版本上次更新的时间。这将匹配Last Released发布对象上的时间。 Release.Namespace：chart release发布的namespace。 Release.Service：处理release的服务。通常是Tiller。 Release.IsUpgrade：如果当前操作是升级或回滚，则设置为true。 Release.IsInstall：如果当前操作是安装，则设置为true。 Release.Revision：版本号。它从1开始，并随着每个helm upgrade增加。 Chart：Chart.yaml的内容。chart版本可以从Chart.Version和维护人员 Chart.Maintainers一起获得。 Files：包含chart中所有非特殊文件的map-like对象。不会允许你访问模板，但会让你访问存在的其他文件（除非它们被排除使用.helmignore）。可以使用index .Files “file.name”或使用.Files.Get name或 .Files.GetString name功能来访问文件。也可以使用.Files.GetBytes访问该文件的内容[byte] Capabilities：包含有关Kubernetes版本信息的map-like对象（.Capabilities.KubeVersion)，Tiller（.Capabilities.TillerVersion)和支持的Kubernetes API版本（.Capabilities.APIVersions.Has “batch/v1″） 注意: 任何未知的Chart.yaml字段将被删除。它们不会在chart对象内部被访问。因此，Chart.yaml不能用于将任意结构化的数据传递到模板中。values文件可以用于传递。 值values文件考虑到上一节中的模板values.yaml，提供了如下必要值的信息： 1234imageRegistry: &quot;quay.io/deis&quot;dockerTag: &quot;latest&quot;pullPolicy: &quot;Always&quot;storage: &quot;s3&quot; values文件是YAML格式的。chart可能包含一个默认 values.yaml文件。Helm install命令允许用户通过提供额外的YAML值来覆盖值： 1$ helm install --values=myvals.yaml wordpress 当以这种方式传递值时，它们将被合并到默认values文件中。例如，考虑一个如下所示的myvals.yaml文件： 1storage: &quot;gcs&quot; 当它与chart中values.yaml的内容合并时，生成的内容将为： 1234imageRegistry: &quot;quay.io/deis&quot;dockerTag: &quot;latest&quot;pullPolicy: &quot;Always&quot;storage: &quot;gcs&quot; 注意只有最后一个字段被覆盖了，其他的不变。 注：包含在chart内的默认values文件必须命名 values.yaml。但是在命令行上指定的文件可以被命名为任何名称。 注：如果在helm install或helm upgrade使用--set，则这些值仅在客户端转换为YAML。 注意：如果values文件中存在任何必需的条目，则可以使用’required’函数在chart模板中声明它们 然后可以在模板内部访问任何这些.Values对象值 ： 1234567891011121314151617181920212223242526apiVersion: v1kind: ReplicationControllermetadata: name: deis-database namespace: deis labels: heritage: deisspec: replicas: 1 selector: app: deis-database template: metadata: labels: app: deis-database spec: serviceAccount: deis-database containers: - name: deis-database image: &#123;&#123;.Values.imageRegistry&#125;&#125;/postgres:&#123;&#123;.Values.dockerTag&#125;&#125; imagePullPolicy: &#123;&#123;.Values.pullPolicy&#125;&#125; ports: - containerPort: 5432 env: - name: DATABASE_STORAGE value: &#123;&#123;default &quot;minio&quot; .Values.storage&#125;&#125; 范围Scope，依赖Dependenciesvalues文件可以声明顶级chart的值，也可以为chart的charts/目录中包含的任何chart声明值。或者，用不同的方式来描述它，values文件可以为chart及其任何依赖项提供值。例如，上面的演示WordPresschart具有mysql和apache依赖性。values文件可以为所有这些组件提供值： 12345678title: &quot;My WordPress Site&quot; # Sent to the WordPress templatemysql: max_connections: 100 # Sent to MySQL password: &quot;secret&quot;apache: port: 8080 # Passed to Apache 更高级别的chart可以访问下面定义的所有变量。所以WordPresschart可以访问MySQL密码 .Values.mysql.password。但较低级别的chart无法访问父chart中的内容，因此MySQL将无法访问该title属性。同样的，也不能访问apache.port。 值是命名空间限制的，但命名空间已被修剪。因此对于WordPresschart来说，它可以访问MySQL密码字段.Values.mysql.password。但是对于MySQL chart来说，这些值的范围已经减小了，并且删除了名namespace前缀，所以它会将密码字段简单地视为 .Values.password。 全局值 从2.0.0-Alpha.2开始，Helm支持特殊的“全局”值。考虑前面例子的这个修改版本： 1234567891011title: &quot;My WordPress Site&quot; # Sent to the WordPress templateglobal: app: MyWordPressmysql: max_connections: 100 # Sent to MySQL password: &quot;secret&quot;apache: port: 8080 # Passed to Apache 上面添加了一个global区块，值app: MyWordPress。此值可供所有chart使用.Values.global.app。 比如，该mysql模板可以访问app如.Values.global.app，apache chart也同样的。上面的values文件是这样高效重新生成的： 123456789101112131415title: &quot;My WordPress Site&quot; # Sent to the WordPress templateglobal: app: MyWordPressmysql: global: app: MyWordPress max_connections: 100 # Sent to MySQL password: &quot;secret&quot;apache: global: app: MyWordPress port: 8080 # Passed to Apache 这提供了一种与所有子chart共享一个顶级变量的方法，这对设置metadata中像标签这样的属性很有用。 如果子chart声明了一个全局变量，则该全局将向下传递 （到子chart的子chart），但不向上传递到父chart。子chart无法影响父chart的值。 此外，父chart的全局变量优先于子chart中的全局变量。 参考当涉及到编写模板和values文件时，有几个标准参考可以帮助你。 Go templates Extra template functions The YAML format 使用Helm管理chart该helm工具有几个用于处理chart的命令。 它可以为你创建一个新的chart： 12$ helm create mychartCreated mychart/ 编辑完chart后，helm可以将其打包到chart压缩包中： 12$ helm package mychartArchived mychart-0.1.-.tgz 可以用helm来帮助查找chart格式或信息的问题： 123$ helm lint mychartNo issues foundor $ helm test . (TBD) 删除release 1$ helm delete mychart 导出helm的template模板yaml文件 1helm template --name mychart --namespace test --output-dir ./test . Chart repo库chart repo库是容纳一个或多个封装的chart的HTTP服务器。虽然helm可用于管理本地chart目录，但在共享chart时，首选机制是chart repo库。 任何可以提供YAML文件和tar文件并可以回答GET请求的HTTP服务器都可以用作repo库服务器。 Helm附带用于开发人员测试的内置服务器（helm serve）。Helm团队测试了其他服务器，包括启用了网站模式的Google Cloud Storage以及启用了网站模式的S3。 repo库的主要特征是存在一个名为的特殊文件index.yaml，它具有repo库提供的所有软件包的列表以及允许检索和验证这些软件包的元数据。 在客户端，repo库使用helm repo命令进行管理。但是，Helm不提供将chart上传到远程存储服务器的工具。这是因为这样做会增加部署服务器的需求，从而增加配置repo库的难度。 本地仓库假设我们已经打包了 Chart 并发布到了 Helm 的本地目录中，但通过 helm search 命令查找，并不能找不到刚才生成的 mychart包。 12$ helm search mychartNo results found 这是因为 Repository 目录中的 Chart 包还没有被 Helm 管理。通过 helm repo list 命令可以看到目前 Helm 中已配置的 Repository 的信息。 123$ helm repo listNAME URLstable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 注：新版本中执行 helm init 命令后默认会配置一个名为 local 的本地仓库。 我们可以在本地启动一个 Repository Server，并将其加入到 Helm Repo 列表中。Helm Repository 必须以 Web 服务的方式提供，这里我们就使用 helm serve 命令启动一个 Repository Server，该 Server 缺省使用 $HOME/.helm/repository/local 目录作为 Chart 存储，并在 8879 端口上提供服务。 12$ helm serve &amp;Now serving you on 127.0.0.1:8879 默认情况下该服务只监听 127.0.0.1，如果你要绑定到其它网络接口，可使用以下命令： 1$ helm serve --address 192.168.100.211:8879 &amp; 如果你想使用指定目录来做为 Helm Repository 的存储目录，可以加上 --repo-path 参数： 1$ helm serve --address 192.168.100.211:8879 --repo-path /data/helm/repository/ --url http://192.168.100.211:8879/charts/ 通过 helm repo index 命令将 Chart 的 Metadata 记录更新在 index.yaml 文件中: 123# 更新 Helm Repository 的索引文件$ cd /home/k8s/.helm/repository/local$ helm repo index --url=http://192.168.100.211:8879 . 完成启动本地 Helm Repository Server 后，就可以将本地 Repository 加入 Helm 的 Repo 列表。 12$ helm repo add local http://127.0.0.1:8879&quot;local&quot; has been added to your repositories 现在再次查找 mychart 包，就可以搜索到了。 1234$ helm repo update$ helm search mychartNAME CHART VERSION APP VERSION DESCRIPTIONlocal/mychart 0.1.0 1.0 A Helm chart for Kubernetes 外部仓库随着 Helm 越来越普及，除了使用预置官方存储库，三方仓库也越来越多了（前提是网络是可达的）。你可以使用如下命令格式添加三方 Chart 存储库。 12$ helm repo add 存储库名 存储库URL$ helm repo update 一些三方存储库资源: 123456789101112# Prometheus Operatorhttps://github.com/coreos/prometheus-operator/tree/master/helm# Bitnami Library for Kuberneteshttps://github.com/bitnami/charts# Openstack-Helmhttps://github.com/att-comdev/openstack-helmhttps://github.com/sapcc/openstack-helm# Tick-Chartshttps://github.com/jackzampolin/tick-charts 创建自己的Chart我们创建一个名为mongodb的chart，看一看chart的文件结构。 12345678910111213$ helm create mongodb$ tree mongodbmongodb├── Chart.yaml #Chart本身的版本和配置信息├── charts #依赖的chart├── templates #配置模板目录│ ├── NOTES.txt #helm提示信息│ ├── _helpers.tpl #用于修改kubernetes objcet配置的模板│ ├── deployment.yaml #kubernetes Deployment object│ └── service.yaml #kubernetes Serivce└── values.yaml #kubernetes object configuration 2 directories, 6 files 模板我们查看下deployment.yaml文件的内容。 1234567891011121314151617181920212223242526272829apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: &#123;&#123; template &quot;fullname&quot; . &#125;&#125; labels: chart: &quot;&#123;&#123; .Chart.Name &#125;&#125;-&#123;&#123; .Chart.Version | replace &quot;+&quot; &quot;_&quot; &#125;&#125;&quot;spec: replicas: &#123;&#123; .Values.replicaCount &#125;&#125; template: metadata: labels: app: &#123;&#123; template &quot;fullname&quot; . &#125;&#125; spec: containers: - name: &#123;&#123; .Chart.Name &#125;&#125; image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125;&quot; imagePullPolicy: &#123;&#123; .Values.image.pullPolicy &#125;&#125; ports: - containerPort: &#123;&#123; .Values.service.internalPort &#125;&#125; livenessProbe: httpGet: path: / port: &#123;&#123; .Values.service.internalPort &#125;&#125; readinessProbe: httpGet: path: / port: &#123;&#123; .Values.service.internalPort &#125;&#125; resources:&#123;&#123; toyaml .Values.resources | indent 12 &#125;&#125; 这是该应用的Deployment的yaml配置文件，其中的双大括号包扩起来的部分是Go template，其中的Values是在values.yaml文件中定义的： 1234567891011121314151617181920# Default values for mychart.# This is a yaml-formatted file.# Declare variables to be passed into your templates.replicaCount: 1image: repository: nginx tag: stable pullPolicy: IfNotPresentservice: name: nginx type: ClusterIP externalPort: 80 internalPort: 80resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi 比如在Deployment.yaml中定义的容器镜像 1image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125;&quot; 其中的： .Values.image.repository就是nginx .Values.image.tag就是stable 以上两个变量值是在create chart的时候自动生成的默认值。 我们将默认的镜像地址和tag改成我们自己的镜像harbor-001.jimmysong.io/library/nginx:1.9。 检查配置和模板是否有效12$ helm lint .No issues found 当使用kubernetes部署应用的时候实际上讲templates渲染成最终的kubernetes能够识别的yaml格式。 使用helm install --dry-run --debug &lt;chart_dir&gt;命令来验证chart配置。该输出中包含了模板的变量配置与最终渲染的yaml文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788$ helm install --dry-run --debug mychartCreated tunnel using local port: &apos;58406&apos;SERVER: &quot;localhost:58406&quot;CHART PATH: /Users/jimmy/Workspace/github/bitnami/charts/incubator/mean/charts/mychartNAME: filled-seahorseREVISION: 1RELEASED: Tue Oct 24 18:57:13 2017CHART: mychart-0.1.0USER-SUPPLIED VALUES:&#123;&#125; COMPUTED VALUES:image: pullPolicy: IfNotPresent repository: harbor-001.jimmysong.io/library/nginx tag: 1.9replicaCount: 1resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Miservice: externalPort: 80 internalPort: 80 name: nginx type: ClusterIP HOOKS:MANIFEST: ---# Source: mychart/templates/service.yamlapiVersion: v1kind: Servicemetadata: name: filled-seahorse-mychart labels: chart: &quot;mychart-0.1.0&quot;spec: type: ClusterIP ports: - port: 80 targetPort: 80 protocol: TCP name: nginx selector: app: filled-seahorse-mychart ---# Source: mychart/templates/deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: filled-seahorse-mychart labels: chart: &quot;mychart-0.1.0&quot;spec: replicas: 1 template: metadata: labels: app: filled-seahorse-mychart spec: containers: - name: mychart image: &quot;harbor-001.jimmysong.io/library/nginx:1.9&quot; imagePullPolicy: IfNotPresent ports: - containerPort: 80 livenessProbe: httpGet: path: / port: 80 readinessProbe: httpGet: path: / port: 80 resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi 我们可以看到Deployment和Service的名字前半截由两个随机的单词组成，最后才是我们在values.yaml中配置的值。 部署到kubernetes在mychart目录下执行下面的命令将nginx部署到kubernetes集群上。 123456789101112131415161718192021helm install .NAME: eating-houndLAST DEPLOYED: Wed Oct 25 14:58:15 2017NAMESPACE: defaultSTATUS: DEPLOYED RESOURCES:==&gt; v1/ServiceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEeating-hound-mychart 10.254.135.68 &lt;none&gt; 80/TCP 0s ==&gt; extensions/v1beta1/DeploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEeating-hound-mychart 1 1 1 0 0s NOTES:1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=eating-hound-mychart&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;) echo &quot;Visit http://127.0.0.1:8080 to use your application&quot; kubectl port-forward $POD_NAME 8080:80 现在nginx已经部署到kubernetes集群上，本地执行提示中的命令在本地主机上访问到nginx实例。 123export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=eating-hound-mychart&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;kubectl port-forward $POD_NAME 8080:80 在本地访问http://127.0.0.1:8080即可访问到nginx。 查看部署的relaese123$ helm listNAME REVISION UPDATED STATUS CHART NAMESPACEeating-hound 1 Wed Oct 25 14:58:15 2017 DEPLOYED mychart-0.1.0 default 删除部署的release12$ helm delete eating-houndrelease &quot;eating-hound&quot; deleted 打包分享我们可以修改Chart.yaml中的helm chart配置信息，然后使用下列命令将chart打包成一个压缩文件。 1helm package . 打包出mychart-0.1.0.tgz文件。 Helm升级和回退一个应用从上面 helm list 输出的结果中我们可以看到有一个 Revision（更改历史）字段，该字段用于表示某一个 Release 被更新的次数，我们可以用该特性对已部署的 Release 进行回滚。 修改 Chart.yaml 文件 将版本号从 0.1.0 修改为 0.2.0, 然后使用 helm package 命令打包并发布到本地仓库。 12345678910$ cat mychart/Chart.yamlapiVersion: v1appVersion: &quot;1.0&quot;description: A Helm chart for Kubernetesname: mychartversion: 0.2.0 $ helm package mychartSuccessfully packaged chart and saved it to: /home/k8s/mychart-0.2.0.tgz 查询本地仓库中的 Chart 信息 我们可以看到在本地仓库中 mychart 有两个版本。 1234$ helm search mychart -lNAME CHART VERSION APP VERSION DESCRIPTIONlocal/mychart 0.2.0 1.0 A Helm chart for Kuberneteslocal/mychart 0.1.0 1.0 A Helm chart for Kubernetes 升级一个应用现在用 helm upgrade 命令将已部署的 mike-test 升级到新版本。你可以通过 --version 参数指定需要升级的版本号，如果没有指定版本号，则缺省使用最新版本。 123456789101112131415161718192021222324$ helm upgrade mike-test local/mychartRelease &quot;mike-test&quot; has been upgraded. Happy Helming!LAST DEPLOYED: Mon Jul 23 10:50:25 2018NAMESPACE: defaultSTATUS: DEPLOYED RESOURCES:==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEmike-test-mychart-6d56f8c8c9-d685v 1/1 Running 0 9m ==&gt; v1/ServiceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmike-test-mychart ClusterIP 10.254.120.177 &lt;none&gt; 80/TCP 9m ==&gt; v1beta2/DeploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEmike-test-mychart 1 1 1 1 9m NOTES:1. Get the application URL by running these commands:export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=mychart,release=mike-test&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;kubectl port-forward $POD_NAME 8080:80 完成后，可以看到已部署的 mike-test 被升级到 0.2.0 版本。 123$ helm listNAME REVISION UPDATED STATUS CHART NAMESPACEmike-test 2 Mon Jul 23 10:50:25 2018 DEPLOYED mychart-0.2.0 default 回退一个应用如果更新后的程序由于某些原因运行有问题，需要回退到旧版本的应用。首先我们可以使用 helm history 命令查看一个 Release 的所有变更记录。 1234$ helm history mike-testREVISION UPDATED STATUS CHART DESCRIPTION1 Mon Jul 23 10:41:20 2018 SUPERSEDED mychart-0.1.0 Install complete2 Mon Jul 23 10:50:25 2018 DEPLOYED mychart-0.2.0 Upgrade complete 其次，我们可以使用下面的命令对指定的应用进行回退。 12$ helm rollback mike-test 1Rollback was a success! Happy Helming! 注：其中的参数 1 是 helm history 查看到 Release 的历史记录中 REVISION 对应的值。 最后，我们使用 helm list 和 helm history 命令都可以看到 mychart 的版本已经回退到 0.1.0 版本。 123456789$ helm listNAME REVISION UPDATED STATUS CHART NAMESPACEmike-test 3 Mon Jul 23 10:53:42 2018 DEPLOYED mychart-0.1.0 default $ helm history mike-testREVISION UPDATED STATUS CHART DESCRIPTION1 Mon Jul 23 10:41:20 2018 SUPERSEDED mychart-0.1.0 Install complete2 Mon Jul 23 10:50:25 2018 SUPERSEDED mychart-0.2.0 Upgrade complete3 Mon Jul 23 10:53:42 2018 DEPLOYED mychart-0.1.0 Rollback to 1 删除一个应用如果需要删除一个已部署的 Release，可以利用 helm delete 命令来完成删除。 12$ helm delete mike-testrelease &quot;mike-test&quot; deleted 确认应用是否删除，该应用已被标记为 DELETED 状态。 123$ helm ls -a mike-testNAME REVISION UPDATED STATUS CHART NAMESPACEmike-test 3 Mon Jul 23 10:53:42 2018 DELETED mychart-0.1.0 default 也可以使用 --deleted 参数来列出已经删除的 Release 123$ helm ls --deletedNAME REVISION UPDATED STATUS CHART NAMESPACEmike-test 3 Mon Jul 23 10:53:42 2018 DELETED mychart-0.1.0 default 从上面的结果也可以看出，默认情况下已经删除的 Release 只是将状态标识为 DELETED 了 ，但该 Release 的历史信息还是继续被保存的。 12345$ helm hist mike-testREVISION UPDATED STATUS CHART DESCRIPTION1 Mon Jul 23 10:41:20 2018 SUPERSEDED mychart-0.1.0 Install complete2 Mon Jul 23 10:50:25 2018 SUPERSEDED mychart-0.2.0 Upgrade complete3 Mon Jul 23 10:53:42 2018 DELETED mychart-0.1.0 Deletion complete 如果要移除指定 Release 所有相关的 Kubernetes 资源和 Release 的历史记录，可以用如下命令： 12$ helm delete --purge mike-testrelease &quot;mike-test&quot; deleted 再次查看已删除的 Release，已经无法找到相关信息。 123456$ helm hist mike-testError: release: &quot;mike-test&quot; not found # helm ls 命令也已均无查询记录。$ helm ls --deleted$ helm ls -a mike-test Helm 其它使用技巧 如何设置 helm 命令自动补全？ 为了方便 helm 命令的使用，Helm 提供了自动补全功能，如果使用 ZSH 请执行： 1$ source &lt;(helm completion zsh) 如果使用 BASH 请执行： 1$ source &lt;(helm completion bash) Helm 如何结合 CI/CD ？ 采用 Helm 可以把零散的 Kubernetes 应用配置文件作为一个 Chart 管理，Chart 源码可以和源代码一起放到 Git 库中管理。通过把 Chart 参数化，可以在测试环境和生产环境采用不同的 Chart 参数配置。 下图是采用了 Helm 的一个 CI/CD 流程 Helm 如何管理多环境下 (Test、Staging、Production) 的业务配置？ Chart 是支持参数替换的，可以把业务配置相关的参数设置为模板变量。使用 helm install 命令部署的时候指定一个参数值文件，这样就可以把业务参数从 Chart 中剥离了。例如： helm install --values=values-production.yaml wordpress。 Helm 如何解决服务依赖？ 在 Chart 里可以通过 requirements.yaml 声明对其它 Chart 的依赖关系。如下面声明表明 Chart 依赖 Apache 和 MySQL 这两个第三方 Chart。 12345678910dependencies:- name: mariadbversion: 2.1.1repository: https://kubernetes-charts.storage.googleapis.com/condition: mariadb.enabledtags:- wordpress-database- name: apacheversion: 1.4.0repository: https://kubernetes-charts.storage.googleapis.com/ 如何让 Helm 连接到指定 Kubernetes 集群？ Helm 默认使用和 kubectl 命令相同的配置访问 Kubernetes 集群，其配置默认在 ~/.kube/config 中。 如何在部署时指定命名空间？ helm install 默认情况下是部署在 default 这个命名空间的。如果想部署到指定的命令空间，可以加上 --namespace 参数，比如： 1$ helm install local/mychart --name mike-test --namespace mynamespace 如何查看已部署应用的详细信息？ 1$ helm get wordpress-test 默认情况下会显示最新的版本的相关信息，如果想要查看指定发布版本的信息可加上 --revision 参数。 1$ helm get --revision 1 wordpress-test Helm优秀实例GitHub: https://github.com/helm/charts Harbor-helm: https://github.com/gongzhao1/harbor-helm Redis-helm: https://www.kubernetes.org.cn/3974.html elk-helm: https://github.com/gongzhao1/elk（这是我个人写的部署elk集群的helm）]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比对两个数据库的差异]]></title>
    <url>%2F%E6%AF%94%E5%AF%B9%E4%B8%A4%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[有时候，需要对比一下测试环境和生产环境中，数据库的表结构是否有所差异。有三个常用的工具。 Navicat比对两个数据库的差异参考： https://blog.csdn.net/qq_31156277/article/details/80410120 https://jingyan.baidu.com/article/fdffd1f870a570f3e98ca1a7.html AmpNmp.DatabaseCompareGUI 界面，支持多种数据库（MySQL、SQL Server、SQLite）简单高效，推荐。 官方下载地址及文档 mysqldiff官方文档官方下载地址 下载安装 mysqldiff 是命令行工具，其优点是可以直接根据差异生成 SQL 语句。 一般直接在本地开发环境安装，如果是 Windows 环境，需要提前安装 Visual C++ Redistributable Packages for Visual Studio 2013。 语法 1mysqldiff --server1=user:pass@host:port:socket --server2=user:pass@host:port:socket db1.object1:db2.object1 db3:db41 mysqldiff 可以对比两个数据库，或只对比表： db1:db2：如果只是指定数据库，那么就将两个数据库中互相缺少的对象显示出来，而对象里面的差异不进行对比；包括表、存储过程、函数、触发器等。 db1.object1:db2.object1：如果指定了具体表对象，那么就会详细对比两个表的差异，包括表名、字段名、备注、索引、大小写等都有的表相关的对象。 参数： --server1：配置server1的连接 --character-set：配置连接时用的字符集，如果不显示配置默认使用“character_set_client” --width：配置显示的宽度 --skip-table-options：这个选项的意思是保持表的选项不变，即对比的差异里面不包括表名、AUTO_INCREMENT,ENGINE, CHARSET等差异。 -d DIFFTYPE,--difftype：差异的信息显示的方式，有[unified|context|differ|sql](default: unified),如果使用sql那么就直接生成差异的SQL这样非常方便。 --changes-for=：例如–changes-for=server2,那么对比以sever1为主，生成的差异的修改也是针对server2的对象的修改。 --show-reverse：这个字面意思是显示相反的意思，其实是生成的差异修改里面同时会包含server2和server1的修改。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式REGEX]]></title>
    <url>%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8FREGEX%2F</url>
    <content type="text"><![CDATA[参考 手册：http://tool.oschina.net/uploads/apidocs/jquery/regexp.html 正则工具：http://tool.oschina.net/regex/ [TOC] 常用规则 匹配到第一个出现的字符, 如匹配除了分号之外的所有内容 1234[^;] 除了;以外的任何字符[^;]* 0个或多个除了;以外的任何字符^[^;]* 从左向右匹配非;的0个或多个字符串^[^;]*; 从左向右匹配第一个包含;的字符串 匹配包括换行符在内的任意字符 1[\s\S]* 正则?&lt;=和？=用法(exp) 匹配exp,并捕获文本到自动命名的组里(?exp) 匹配exp,并捕获文本到名称为name的组里，也可以写成(?’name’exp)(?:exp) 匹配exp,不捕获匹配的文本位置指定(?=exp) 匹配exp前面的位置(?&lt;=exp) 匹配exp后面的位置(?!exp) 匹配后面跟的不是exp的位置(?&lt;!exp) 匹配前面不是exp的位置 -———————————————————————————————————————————- 例子： 测试1： 假如有下面一串经纬度str，小数点后长度&lt;16，长度不一。 1&#123;&quot;lat&quot;:&quot;30.769432950801377&quot;,&quot;lng&quot;:&quot;103.9813772899&quot;&#125;,&#123;&#123;&quot;lat&quot;:&quot;30.6697164&quot;,&quot;lng&quot;:&quot;103.9816054999999947&quot;&#125; 测试1：匹配经纬度末尾4位(引号前4位)，以便干掉。 12\d&#123;4&#125;(?=\&quot;)----------------------------------- 共找到 4 处匹配： 1377 2899 7164 9947 1 测试2： 1&#123;&quot;lat&quot;:&quot;30.769432950801377&quot;,&quot;lng&quot;:&quot;103.9813772899&quot;&#125;,&#123;&#123;&quot;lat&quot;:&quot;30.6697164&quot;,&quot;lng&quot;:&quot;103.9816054999999947&quot;&#125; 经纬度保留小数点后6位，匹配出多余的数字以便干掉：（小数点后第七位到末尾的数字） 表达式为： 12(?&lt;=\.\d&#123;6&#125;)\d&#123;0,10&#125;(?=\&quot;)--------------------------------- 共找到 4 处匹配： 950801377 2899 4 4999999947 (?:pattern) 非获取匹配，匹配pattern但不获取匹配结果，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分是很有用。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。 (?=pattern) 非获取匹配，正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如，“Windows(?=95|98|NT|2000)”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?!pattern) 非获取匹配，正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如“Windows(?!95|98|NT|2000)”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。 (?&lt;=pattern) 非获取匹配，反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“(?&lt;=95|98|NT|2000)Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。 (?&lt;!pattern) 非获取匹配，反向否定预查，与正向否定预查类似，只是方向相反。例如“(?&lt;!95|98|NT|2000)Windows”能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”。这个地方不正确，有问题 概念 【分组】我们已经提到了怎么重复单个字符（直接在字符后面加上限定符就行了）；但如果想要重复一个字符串又该怎么办？你可以用小括号来指定子表达式(也叫做分组)，然后你就可以指定这个子表达式的重复次数了，你也可以对子表达式进行其它一些操作(后面会有介绍)。(\d{1,3}.){3}\d{1,3}是一个简单的IP地址匹配表达式。要理解这个表达式，请按下列顺序分析它： \d{1,3}匹配1到3位的数字，(\d{1,3}.}{3}匹配三位数字加上一个英文句号(这个整体也就是这个分组)重复3次，最后再加上一个一到三位的数字(\d{1,3})。 不幸的是，它也将匹配256.300.888.999这种不可能存在的IP地址(IP地址中每个数字都不能大于255)。如果能使用算术比较的话，或许能简单地解决这个问题，但是正则表达式中并不提供关于数学的任何功能，所以只能使用冗长的分组，选择，字符类来描述一个正确的IP地址：((2[0-4]\d|25[0-5]|[01]?\d\d?).){3}(2[0-4]\d|25[0-5]|[01]?\d\d?)。 理解这个表达式的关键是理解2[0-4]\d|25[0-5]|[01]?\d\d?，这里我就不细说了，你自己应该能分析得出来它的意义。 【后向引用】使用小括号指定一个子表达式后，匹配这个子表达式的文本可以在表达式或其它程序中作进一步的处理。默认情况下，每个分组会自动拥有一个组号，规则是：从左向右，以分组的左括号为标志，第一个出现的分组的组号为1，第二个为2，以此类推。 后向引用用于重复搜索前面某个分组匹配的文本。例如，\1代表分组1匹配的文本。难以理解？请看示例： \b(\w+)\b\s+\1\b可以用来匹配重复的单词，像go go, kitty kitty。首先是一个单词，也就是单词开始处和结束处之间的多于一个的字母或数字(\b(\w+)\b)，然后是1个或几个空白符(\s+)，最后是前面匹配的那个单词(\1)。 你也可以自己指定子表达式的组名。要指定一个子表达式的组名，请使用这样的语法：(?\w+)(或者把尖括号换成’也行：(?’Word’\w+)),这样就把\w+的组名指定为Word了。要反向引用这个分组捕获的内容，你可以使用\k,所以上一个例子也可以写成这样：\b(?\w+)\b\s+\k\b。 使用小括号的时候，还有很多特定用途的语法。下面列出了最常用的一些： 分组语法 捕获(exp) 匹配exp,并捕获文本到自动命名的组里(?exp) 匹配exp,并捕获文本到名称为name的组里，也可以写成(?’name’exp)(?:exp) 匹配exp,不捕获匹配的文本位置指定(?=exp) 匹配exp前面的位置(?&lt;=exp) 匹配exp后面的位置(?!exp) 匹配后面跟的不是exp的位置(?&lt;!exp) 匹配前面不是exp的位置注释(?#comment) 这种类型的组不对正则表达式的处理产生任何影响，只是为了提供让人阅读注释 我们已经讨论了前两种语法。第三个(?:exp)不会改变正则表达式的处理方式，只是这样的组匹配的内容不会像前两种那样被捕获到某个组里面。 位置指定接下来的四个用于查找在某些内容(但并不包括这些内容)之前或之后的东西，也就是说它们用于指定一个位置，就像\b,^,$那样，因此它们也被称为零宽断言。最好还是拿例子来说明吧： (?=exp)也叫零宽先行断言，它匹配文本中的某些位置，这些位置的后面能匹配给定的后缀exp。比如\b\w+(?=ing\b)，匹配以ing结尾的单词的前面部分(除了ing以外的部分)，如果在查找I’m singing while you’re dancing.时，它会匹配sing和danc。 (?&lt;=exp)也叫零宽后行断言，它匹配文本中的某些位置，这些位置的前面能给定的前缀匹配exp。比如(?&lt;=\bre)\w+\b会匹配以re开头的单词的后半部分(除了re以外的部分)，例如在查找reading a book时，它匹配ading。 假如你想要给一个很长的数字中每三位间加一个逗号(当然是从右边加起了)，你可以这样查找需要在前面和里面添加逗号的部分：((?&lt;=\d)\d{3})*\b。请仔细分析这个表达式，它可能不像你第一眼看出来的那么简单。 下面这个例子同时使用了前缀和后缀：(?&lt;=\s)\d+(?=\s)匹配以空白符间隔的数字(再次强调，不包括这些空白符)。 负向位置指定前面我们提到过怎么查找不是某个字符或不在某个字符类里的字符的方法(反义)。但是如果我们只是想要确保某个字符没有出现，但并不想去匹配它时怎么办？例如，如果我们想查找这样的单词–它里面出现了字母q,但是q后面跟的不是字母u,我们可以尝试这样： \b\wq[^u]\w\b匹配包含后面不是字母u的字母q的单词。但是如果多做测试(或者你思维足够敏锐，直接就观察出来了)，你会发现，如果q出现在单词的结尾的话，像Iraq,Benq，这个表达式就会出错。这是因为[^u]总是匹配一个字符，所以如果q是单词的最后一个字符的话，后面的[^u]将会匹配q后面的单词分隔符(可能是空格，或者是句号或其它的什么)，后面的\w\b将会匹配下一个单词，于是\b\wq[^u]\w\b就能匹配整个Iraq fighting。负向位置指定能解决这样的问题，因为它只匹配一个位置，并不消费任何字符。现在，我们可以这样来解决这个问题：\b\wq(?!u)\w*\b。 零宽负向先行断言(?!exp)，只会匹配后缀exp不存在的位置。\d{3}(?!\d)匹配三位数字，而且这三位数字的后面不能是数字。 同理，我们可以用(?&lt;!exp),零宽负向后行断言来查找前缀exp不存在的位置：(?&lt;![a-z])\d{7}匹配前面不是小写字母的七位数字(实验时发现错误？注意你的“区分大小写”先项是否选中)。 一个更复杂的例子：(?&lt;=&lt;(\w+)&gt;).(?=&lt;/\1&gt;)匹配不包含属性的简单HTML标签内里的内容。(&lt;?(\w+)&gt;)指定了这样的前缀：被尖括号括起来的单词(比如可能是)，然后是.(任意的字符串),最后是一个后缀(?=&lt;/\1&gt;)。注意后缀里的/，它用到了前面提过的字符转义；\1则是一个反向引用，引用的正是捕获的第一组，前面的(\w+)匹配的内容，这样如果前缀实际上是的话，后缀就是了。整个表达式匹配的是和之间的内容(再次提醒，不包括前缀和后缀本身)。 正则表达式中需要转义的字符$ 匹配输入字符串的结尾位置。如果设置了 RegExp 对象的 Multiline 属性，则 $ 也匹配 \n 或 \r。要匹配 $ 字符本身，请使用 \$。 ( ) 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。要匹配这些字符，请使用\() 。 * 匹配前面的子表达式零次或多次。要匹配 * 字符，请使用 \*。 + 匹配前面的子表达式一次或多次。要匹配 + 字符，请使用 \+。 . 匹配除换行符 \n之外的任何单字符。要匹配 .，请使用\.。 [ ] 标记一个中括号表达式的开始。要匹配 [，请使用 \[。 ? 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符。要匹配 ? 字符，请使用 \?。 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如， ‘n’ 匹配字符 ‘n’。\n 匹配换行符。序列 \\匹配 \，而 \( 则匹配 (。 ^ 匹配输入字符串的开始位置，除非在方括号表达式中使用，此时它表示不接受该字符集合。要匹配 ^ 字符本身，请使用\^。 { } 标记限定符表达式的开始。要匹配 {，请使用\{。 | 指明两项之间的一个选择。要匹配 |，请使用\|。]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>REGEX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群日志收集 Filebeat+ELK]]></title>
    <url>%2FKubernetes%E9%9B%86%E7%BE%A4%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86-Filebeat-ELK%2F</url>
    <content type="text"><![CDATA[前言我们首先介绍一下传统的日志监控方案。其中，ELK Stack 是我们最熟悉不过的架构。所谓ELK，分别指Elastic公司的Elasticsearch、Logstash、Kibana。在比较旧的ELK架构中，Logstash身兼日志的采集、过滤两职。但由于Logstash基于JVM，性能有一定限制，因此，目前业界更推荐使用Go语言开发FIiebeat代替Logstash的采集功能，Logstash只作为了日志过滤的中间件。 最常见的ELK架构如下： 如上图所示，各角色功能如下： 多个Filebeat在各个业务端进行日志采集，然后上传至Logstash 多个Logstash节点并行（负载均衡，不作为集群），对日志记录进行过滤处理，然后上传至Elasticsearch集群 多个Elasticsearch构成集群服务，提供日志的索引和存储能力 Kibana负责对Elasticsearch中的日志数据进行检索、分析 当然，在该架构中，根据业务特点，还可以加入某些中间件，如Redis、Kafak等： 如上图所示，使用Redis或Kafka集群作为消息缓冲队列，可以降低大量FIlebeat对Logstash的并发访问压力。 日志采集方式官方文档：https://kubernetes.io/docs/concepts/cluster-administration/logging/ 方式1： Node级别日志代理在每个节点（即宿主机）上可以独立运行一个Node级日志代理，通常的实现方式为DaemonSet。用户应用只需要将日志写到标准输出，Docker 的日志驱动会将每个容器的标准输出收集并写入到主机文件系统，这样Node级日志代理就可以将日志统一收集并上传。另外，可以使用K8S的logrotate或Docker 的log-opt 选项负责日志的轮转。 Docker默认的日志驱动（LogDriver）是json-driver，其会将日志以JSON文件的方式存储。所有容器输出到控制台的日志，都会以*-json.log的命名方式保存在/var/lib/docker/containers/目录下。对于Docker日志驱动的具体介绍，请参考官方文档。另外，除了收集Docker容器日志，一般建议同时收集K8S自身的日志以及宿主机的所有系统日志，其位置都在var/log下。 所以，简单来说，本方式就是在每个node上各运行一个日志代理容器，对本节点/var/log和 /var/lib/docker/containers/两个目录下的日志进行采集，然后汇总到elasticsearch集群，最后通过kibana展示。 方式2：伴生容器（sidecar container）作为日志代理创建一个伴生容器（也可称作日志容器），与应用程序容器在处于同一个Pod中。同时伴生容器内部运行一个独立的、专门为收集应用日志的代理，常见的有Logstash、Fluentd 、Filebeat等。日志容器通过共享卷可以获得应用容器的日志，然后进行上传。 方式3：应用直接上传日志应用程序容器直接通过网络连接上传日志到后端，这是最简单的方式。 对比 其中，相对来说，方式1在业界使用更为广泛，并且官方也更为推荐。因此，最终我们采用ELK+Filebeat架构，并基于方式1. 日志采集部署filebeatGitHub: https://github.com/elastic/beats 官方文档：https://www.elastic.co/guide/en/beats/filebeat/current/index.html 有任何问题请优先查找官方文档！！！ filebeat处理多行日志123456789101112131415161718192021222324252627282930313233343536373839404142434445filebeat.inputs:- type: log enabled: True paths: - /var/log/mysql-slow-*#这个地方是关键，我们给上边日志加上了tags，方便在logstash里边通过这个tags 过滤并格式化自己想要的内容； tags: [&quot;mysql_slow_logs&quot;]#有的时候日志不是一行输出的，如果不用multiline的话，会导致一条日志被分割成多条收集过来，形成不完整日志，这样的日志对于我们来说是没有用的！通过正则匹配语句开头，这样multiline 会在匹配开头之后，一直到下一个这样开通的语句合并成一条语句。#pattern：多行日志开始的那一行匹配的pattern#negate：是否需要对pattern条件转置使用，不翻转设为true，反转设置为false#match：匹配pattern后，与前面（before）还是后面（after）的内容合并为一条日志#max_lines：合并的最多行数（包含匹配pattern的那一行 默认值是500行#timeout：到了timeout之后，即使没有匹配一个新的pattern（发生一个新的事件），也把已经匹配的日志事件发送出去 multiline.pattern: &apos;^\d&#123;4&#125;/\d&#123;2&#125;/\d&#123;2&#125;&apos; (2018\05\01 我的匹配是已这样的日期开头的） multiline.negate: true multiline.match: after multiline.Max_lines:20 multiline.timeout: 10s#在输入中排除符合正则表达式列表的那些行 exclude_lines: [&quot;^DBG&quot;]#包含输入中符合正则表达式列表的那些行默认包含所有行include_lines执行完毕之后会执行exclude_lines。 include_lines: [&quot;^ERR&quot;, &quot;^WARN&quot;] #向输出的每一条日志添加额外的信息比如“level:debug”方便后续对日志进行分组统计。默认情况下会在输出信息的fields子目录下以指定的新增fields建立子目录例如fields.level。 fields: level: debug review: 1#如果该选项设置为true则新增fields成为顶级目录而不是将其放在fields目录下。自定义的field会覆盖filebeat默认的field。 fields_under_root: false- input_type: log paths: - /var/log/mysql-sql-* tags: [&quot;mysql_sql_logs&quot;] multiline.pattern: &apos;^\d&#123;4&#125;/\d&#123;2&#125;/\d&#123;2&#125;&apos; multiline.negate: true multiline.match: after multiline.timeout: 10s encoding: utf-8 document_type: mysql-proxy scan_frequency: 20s harverster_buffer_size: 16384 max_bytes: 10485760 tail_files: true #tail_files：如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送，而不是从文件开始处重新发送所有内容。默认是false； filebeat测试配置文件是否正确 1filebeat test config Yaml Filefilebeat-kubernetes.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: filebeat namespace: elk-testsubjects:- kind: ServiceAccount name: filebeat namespace: elk-testroleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: filebeat namespace: elk-test labels: elastic-app: filebeatrules:- apiGroups: [""] # "" indicates the core API group resources: - namespaces - pods verbs: - get - watch - list---apiVersion: v1kind: ServiceAccountmetadata: name: filebeat namespace: elk-test labels: elastic-app: filebeat---apiVersion: v1kind: ConfigMapmetadata: name: filebeat-config namespace: elk-test labels: elastic-app: filebeatdata: filebeat.yml: |- filebeat.config: inputs: # Mounted `filebeat-inputs` configmap: path: $&#123;path.config&#125;/inputs.d/*.yml # Reload inputs configs as they change: reload.enabled: true modules: path: $&#123;path.config&#125;/modules.d/*.yml # Reload module configs as they change: reload.enabled: true # To enable hints based autodiscover, remove `filebeat.config.inputs` configuration and uncomment this: #filebeat.autodiscover: # providers: # - type: kubernetes # hints.enabled: true processors: - add_cloud_metadata: cloud.id: $&#123;ELASTIC_CLOUD_ID&#125; cloud.auth: $&#123;ELASTIC_CLOUD_AUTH&#125; #output.elasticsearch: # hosts: ['$&#123;ELASTICSEARCH_HOST:elasticsearch-service&#125;:$&#123;ELASTICSEARCH_PORT:9200&#125;'] #username: $&#123;ELASTICSEARCH_USERNAME&#125; #password: $&#123;ELASTICSEARCH_PASSWORD&#125; output.redis: hosts: ["192.168.0.95:6379"] db: 3 timeout: 10 key: "k8s"---apiVersion: v1kind: ConfigMapmetadata: name: filebeat-inputs namespace: elk-test labels: elastic-app: filebeatdata: kubernetes.yml: |- - type: docker containers.ids: - "*" processors: - add_kubernetes_metadata: in_cluster: true multiline: pattern: '(^((0[1-9])|([12][0-9])|(3[01])|[1-9])-(\w+)-(\d\d)&#123;1,2&#125;)|(^(\d\d)&#123;1,2&#125;-(0?[1-9]|1[0-2])-((0[1-9])|([12][0-9])|(3[01])|[1-9]))' negate: true match: after---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: filebeat namespace: elk-test labels: elastic-app: filebeatspec: template: metadata: labels: elastic-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: docker.elastic.co/beats/filebeat:6.8.1 args: [ "-c", "/etc/filebeat.yml", "-e", ] env: - name: ELASTICSEARCH_HOST value: elasticsearch-service - name: ELASTICSEARCH_PORT value: "9200" - name: ELASTIC_CLOUD_ID value: - name: ELASTIC_CLOUD_AUTH value: securityContext: runAsUser: 0 # If using Red Hat OpenShift uncomment this: #privileged: true resources: limits: memory: 200Mi cpu: 200m requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: inputs configMap: defaultMode: 0600 name: filebeat-inputs # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat type: DirectoryOrCreate 我们先重点关注一下DaemonSet的volumeMounts和volumes，以了解ConfigMap的挂载方式： configfilebeat-config这个Configmap会生成一个filebeat.yml文件，其会被挂载为Filebeat的配置文件/etc/filebeat.yml inputsinputs这个Configmap会生成一个filebeat-inputs.yml文件，其会被挂载到路径/usr/share/filebeat/inputs.d下，并被filebeat.yml引用 dataFilebeat自身的数据挂载为hostPath: /var/lib/filebeat varlibdockercontainersK8S集群的日志都存储在/var/lib/docker/containers，Filebeat将从该路径进行收集日志 部署1kubectl apply -f filebeat-kubernetes.yaml Redis/Kafka这里缓存选择Redis，直接采用云服务商提供的了，大家可以也可以自己搭建。 只需要将Redis的数据库以及IP地址写到filebeat以及logstash中即可，具体可以参照filebeat以及logstash的配置文件。 LogstashGitHub:https://github.com/elastic/logstash 官方文档：https://www.elastic.co/guide/en/logstash/current/index.html Logstash配置文件详解/logstash/config/logstash.yml：主要用于控制logstash运行时的状态/logstash/config/startup.options：logstash 运行相关参数 logstash.yml 参数 用途 默认值 node.name 节点名称 主机名称 path.data /数据存储路径 LOGSTASH_HOME/data/ pipeline.workers 输出通道的工作workers数据量（提升输出效率） cpu核数 pipeline.output.workers 每个输出插件的工作wokers数量 1 pipeline.batch.size 每次input数量 125 path.config 过滤配置文件目录 config.reload.automatic 自动重新加载被修改配置 false or true config.reload.interval 配置文件检查时间 path.logs 日志输出路径 http.host 绑定主机地址，用户指标收集 “127.0.0.1” http.port 绑定端口 5000-9700 log.level 日志输出级别,如果config.debug开启，这里一定要是debug日志 info log.format 日志格式 * plain* path.plugins 自定义插件目录 startup.options 参数 用途 JAVACMD=/usr/bin/java 本地jdk LS_HOME=/opt/logstash logstash所在目录 LS_SETTINGS_DIR=”${LS_HOME}/config” 默认logstash配置文件目录 LS_OPTS=”–path.settings ${LS_SETTINGS_DIR}” logstash启动命令参数 指定配置文件目录 LS_JAVA_OPTS=”” 指定jdk目录 LS_PIDFILE=/var/run/logstash.pid logstash.pid所在目录 LS_USER=logstash logstash启动用户 LS_GROUP=logstash logstash启动组 LS_GC_LOG_FILE=/var/log/logstash/gc.log logstash jvm gc日志路径 LS_OPEN_FILES=65534 logstash最多打开监控文件数量 测试配置文件是否正确，可以用下面这个方法测试 1$/usr/share/logstash/bin/logstash -t -f /etc/logstash/conf.d/nginx.conf 配置参数input plugin 让logstash可以读取特定的事件源。 事件源可以是从stdin屏幕输入读取，可以从file指定的文件，也可以从es，filebeat，kafka，redis等读取 stdin 标准输入 file 从文件读取数据 123456789file&#123; path =&gt; [&apos;/var/log/nginx/access.log&apos;] #要输入的文件路径 type =&gt; &apos;nginx_access_log&apos; start_position =&gt; &quot;beginning&quot;&#125;# path 可以用/var/log/*.log,/var/log/**/*.log，如果是/var/log则是/var/log/*.log# type 通用选项. 用于激活过滤器# start_position 选择logstash开始读取文件的位置，begining或者end。还有一些常用的例如：discover_interval，exclude，sincedb_path,sincedb_write_interval等可以参考官网 syslog 通过网络将系统日志消息读取为事件 12345678910syslog&#123; port =&gt;&quot;514&quot; type =&gt; &quot;syslog&quot;&#125;# port 指定监听端口(同时建立TCP/UDP的514端口的监听)#从syslogs读取需要实现配置rsyslog：# cat /etc/rsyslog.conf 加入一行*.* @172.17.128.200:514 #指定日志输入到这个端口，然后logstash监听这个端口，如果有新日志输入则读取# service rsyslog restart #重启日志服务 beats 从Elastic beats接收事件 12345678910beats &#123; port =&gt; 5044 #要监听的端口&#125;# 还有host等选项# 从beat读取需要先配置beat端，从beat输出到logstash。# vim /etc/filebeat/filebeat.yml ..........output.logstash:hosts: [&quot;localhost:5044&quot;] kafka 将 kafka topic 中的数据读取为事件 123456kafka&#123; bootstrap_servers=&gt; &quot;kafka01:9092,kafka02:9092,kafka03:9092&quot; topics =&gt; [&quot;access_log&quot;] group_id =&gt; &quot;logstash-file&quot; codec =&gt; &quot;json&quot;&#125; 12345kafka&#123; bootstrap_servers=&gt; &quot;kafka01:9092,kafka02:9092,kafka03:9092&quot; topics =&gt; [&quot;weixin_log&quot;,&quot;user_log&quot;] codec =&gt; &quot;json&quot;&#125; 1234# bootstrap_servers 用于建立群集初始连接的Kafka实例的URL列表。# topics 要订阅的主题列表，kafka topics# group_id 消费者所属组的标识符，默认为logstash。kafka中一个主题的消息将通过相同的方式分发到Logstash的group_id# codec 通用选项，用于输入数据的编解码器。 还有很多的input插件类型，可以参考官方文档来配置。 filter plugin 过滤器插件，对事件执行中间处理 grok 解析文本并构造 。把非结构化日志数据通过正则解析成结构化和可查询化 123456 grok &#123; match =&gt; &#123;&quot;message&quot;=&gt;&quot;^%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;$&quot;&#125; &#125;匹配nginx日志# 203.202.254.16 - - [22/Jun/2018:16:12:54 +0800] &quot;GET / HTTP/1.1&quot; 200 3700 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7&quot;#220.181.18.96 - - [13/Jun/2015:21:14:28 +0000] &quot;GET /blog/geekery/xvfb-firefox.html HTTP/1.1&quot; 200 10975 &quot;-&quot; &quot;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&quot; 注意这里grok 可以有多个match匹配规则，如果前面的匹配失败可以使用后面的继续匹配。例如 1234grok &#123; match =&gt; [&quot;message&quot;, &quot;%&#123;IP:clientip&#125; - %&#123;USER:user&#125; \[%&#123;HTTPDATE:raw_datetime&#125;\] \&quot;(?:%&#123;WORD:verb&#125; %&#123;URIPATHPARAM:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;)\&quot; (?:\&quot;%&#123;DATA:body&#125;\&quot; )?(?:\&quot;%&#123;DATA:cookie&#125;\&quot; )?%&#123;NUMBER:response&#125; (?:%&#123;NUMBER:bytes:int&#125;|-) \&quot;%&#123;DATA:referrer&#125;\&quot; \&quot;%&#123;DATA:agent&#125;\&quot; (?:(%&#123;IP:proxy&#125;,? ?)*|-|unknown) (?:%&#123;DATA:upstream_addr&#125; |)%&#123;NUMBER:request_time:float&#125; (?:%&#123;NUMBER:upstream_time:float&#125;|-)&quot;] match =&gt; [&quot;message&quot;, &quot;%&#123;IP:clientip&#125; - %&#123;USER:user&#125; \[%&#123;HTTPDATE:raw_datetime&#125;\] \&quot;(?:%&#123;WORD:verb&#125; %&#123;URI:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;)\&quot; (?:\&quot;%&#123;DATA:body&#125;\&quot; )?(?:\&quot;%&#123;DATA:cookie&#125;\&quot; )?%&#123;NUMBER:response&#125; (?:%&#123;NUMBER:bytes:int&#125;|-) \&quot;%&#123;DATA:referrer&#125;\&quot; \&quot;%&#123;DATA:agent&#125;\&quot; (?:(%&#123;IP:proxy&#125;,? ?)*|-|unknown) (?:%&#123;DATA:upstream_addr&#125; |)%&#123;NUMBER:request_time:float&#125; (?:%&#123;NUMBER:upstream_time:float&#125;|-)&quot;] &#125; grok 语法：%{SYNTAX:SEMANTIC} 即 %{正则:自定义字段名} 官方提供了很多正则的grok pattern可以直接使用 :[https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns](https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns) grok debug工具： http://grokdebug.herokuapp.com 正则表达式调试工具： https://www.debuggex.com/ 需要用到较多的正则知识，参考文档有：https://www.jb51.net/tools/zhengze.html ​ 自定义模式： (?&lt;字段名&gt;the pattern) ​ 例如： 匹配 2018/06/27 14:00:54 ​ (?\d\d\d\d/\d\d/\d\d \d\d:\d\d:\d\d) ​ 得到结果： “datetime”: “2018/06/27 14:00:54” date 日期解析 解析字段中的日期，然后转存到@timestamp date插件对于排序事件和回填旧数据尤其重要，它可以用来转换日志记录中的时间字段，变成Logstash timestamp对象，然后转存到@timestamp字段里面。 为什么要使用这个插件呢？ 1、一方面由于Logstash会给收集到的每条日志自动打上时间戳（即@timestamp），但是这个时间戳记录的是input接收数据的时间，而不是日志生成的时间（因为日志生成时间与input接收的时间肯定不同），这样就可能导致搜索数据时产生混乱。 2、另一方面，在上面那段rubydebug编码格式的输出中，@timestamp字段虽然已经获取了timestamp字段的时间，但是仍然比北京时间晚了8个小时，这是因为在Elasticsearch内部，对时间类型字段都是统一采用UTC时间，而日志统一采用UTC时间存储，是国际安全、运维界的一个共识。其实这并不影响什么，因为ELK已经给出了解决方案，那就是在Kibana平台上，程序会自动读取浏览器的当前时区，然后在web页面自动将UTC时间转换为当前时区的时间。 12345filter&#123; date&#123; match =&gt; [&quot;timestamp&quot;,&quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSS&quot;, &apos;ISO8601&apos;] &#125;&#125; 12345678filter&#123; grok&#123; match =&gt; &#123;&quot;message&quot; =&gt; &quot;\ -\ -\ \[%&#123;HTTPDATE:timestamp&#125;\]&quot;&#125; &#125; date&#123; match =&gt; [&quot;timestamp&quot;,&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;] &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041#读input &#123; file &#123; path =&gt; &quot;文件全路径&quot; type =&gt; &quot;任意名字最好有意义&quot;#自定义日志区分类型 start_position =&gt; &quot;beginning&quot; #从文件开始处读写 &#125;&#125;#过滤filter &#123; grok &#123; #切割后日期名字叫logdate match =&gt; [&quot;message&quot;, &quot;%&#123;TIMESTAMP_ISO8601:logdate&#125;&quot;] &#125; date &#123; #logdate 从上面过滤后取到的字段名，yyyy-MM-dd HH:mm:ss.SSS 日期格式条件 match =&gt; [&quot;logdate&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;] #match =&gt; [&quot;logdate&quot;, &quot;yyyyMMdd&quot;,&quot;yyyy-MM-dd&quot;] #赋值给那个key target =&gt; &quot;@timestamp&quot; #删除不需要的字段 remove_field =&gt; [&quot;logdate&quot;] &#125; #合并错误日志 multiline &#123; pattern =&gt; &quot;^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;\s\d&#123;1,2&#125;:\d&#123;1,2&#125;:\d&#123;1,2&#125;&quot; negate =&gt; true what =&gt; &quot;previous&quot; &#125;&#125;#输出output&#123; #输出到ES elasticsearch&#123; hosts=&gt;[&quot;127.0.0.1:9200&quot;] #es的index名字，默认就是这个，可以更改 index =&gt; &quot;logstash-%&#123;+YYYY.MM.dd&#125;&quot; &#125; #输出到控制台 stdout&#123;codec =&gt; rubydebug&#125;&#125; 123456789101112131415[2018-07-04 17:43:35,503]grok&#123; match =&gt; &#123;&quot;message&quot;=&gt;&quot;%&#123;DATA:raw_datetime&#125;&quot;&#125;&#125;date&#123; match =&gt; [&quot;raw_datetime&quot;,&quot;YYYY-MM-dd HH:mm:ss,SSS&quot;] remove_field =&gt;[&quot;raw_datetime&quot;]&#125;#将raw_datetime存到@timestamp 然后删除raw_datetime#24/Jul/2018:18:15:05 +0800date &#123; match =&gt; [&quot;timestamp&quot;,&quot;dd/MMM/YYYY:HH:mm:ss Z]&#125; mutate 对字段做处理 重命名、删除、替换和修改字段。 covert 类型转换。类型包括：integer，float，integer_eu，float_eu，string和boolean 12345678910111213filter&#123; mutate&#123;# covert =&gt; [&quot;response&quot;,&quot;integer&quot;,&quot;bytes&quot;,&quot;float&quot;] #数组的类型转换 convert =&gt; &#123;&quot;message&quot;=&gt;&quot;integer&quot;&#125; &#125;&#125;#测试-------&gt;&#123; &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;message&quot; =&gt; 123, #没带“”,int类型 &quot;@timestamp&quot; =&gt; 2018-06-26T02:51:08.651Z, &quot;@version&quot; =&gt; &quot;1&quot;&#125; split 使用分隔符把字符串分割成数组 12345678910111213141516171819202122232425mutate&#123; split =&gt; &#123;&quot;message&quot;=&gt;&quot;,&quot;&#125;&#125;#----------&gt;aaa,bbb&#123; &quot;@timestamp&quot; =&gt; 2018-06-26T02:40:19.678Z, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;message&quot; =&gt; [ [0] &quot;aaa&quot;, [1] &quot;bbb&quot; ]&#125;192,128,1,100&#123; &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;message&quot; =&gt; [ [0] &quot;192&quot;, [1] &quot;128&quot;, [2] &quot;1&quot;, [3] &quot;100&quot; ], &quot;@timestamp&quot; =&gt; 2018-06-26T02:45:17.877Z, &quot;@version&quot; =&gt; &quot;1&quot;&#125; merge 合并字段 。数组和字符串 ，字符串和字符串 123456789101112131415161718192021222324252627282930313233343536filter&#123; mutate&#123; add_field =&gt; &#123;&quot;field1&quot;=&gt;&quot;value1&quot;&#125; &#125; mutate&#123; split =&gt; &#123;&quot;message&quot;=&gt;&quot;.&quot;&#125; #把message字段按照.分割 &#125; mutate&#123; merge =&gt; &#123;&quot;message&quot;=&gt;&quot;field1&quot;&#125; #将filed1字段加入到message字段 &#125;&#125;#---------------&gt;abc&#123; &quot;message&quot; =&gt; [ [0] &quot;abc,&quot; [1] &quot;value1&quot; ], &quot;@timestamp&quot; =&gt; 2018-06-26T03:38:57.114Z, &quot;field1&quot; =&gt; &quot;value1&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;&#125;abc,.123&#123; &quot;message&quot; =&gt; [ [0] &quot;abc,&quot;, [1] &quot;123&quot;, [2] &quot;value1&quot; ], &quot;@timestamp&quot; =&gt; 2018-06-26T03:38:57.114Z, &quot;field1&quot; =&gt; &quot;value1&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;&#125; rename 对字段重命名 12345678910111213filter&#123; mutate&#123; rename =&gt; &#123;&quot;message&quot;=&gt;&quot;info&quot;&#125; &#125;&#125;#--------&gt;123&#123; &quot;@timestamp&quot; =&gt; 2018-06-26T02:56:00.189Z, &quot;info&quot; =&gt; &quot;123&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;&#125; remove_field 移除字段 123mutate &#123; remove_field =&gt; [&quot;message&quot;,&quot;datetime&quot;]&#125; join 用分隔符连接数组，如果不是数组则不做处理 123456789101112131415161718192021mutate&#123; split =&gt; &#123;&quot;message&quot;=&gt;&quot;:&quot;&#125;&#125;mutate&#123; join =&gt; &#123;&quot;message&quot;=&gt;&quot;,&quot;&#125;&#125;------&gt;abc:123&#123; &quot;@timestamp&quot; =&gt; 2018-06-26T03:55:41.426Z, &quot;message&quot; =&gt; &quot;abc,123&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;@version&quot; =&gt; &quot;1&quot;&#125;aa:cc&#123; &quot;@timestamp&quot; =&gt; 2018-06-26T03:55:47.501Z, &quot;message&quot; =&gt; &quot;aa,cc&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;@version&quot; =&gt; &quot;1&quot;&#125; gsub 用正则或者字符串替换字段值。仅对字符串有效 123456789101112 mutate&#123; gsub =&gt; [&quot;message&quot;,&quot;/&quot;,&quot;_&quot;] #用_替换/ &#125;------&gt;a/b/c/&#123; &quot;@version&quot; =&gt; &quot;1&quot;, &quot;message&quot; =&gt; &quot;a_b_c_&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;@timestamp&quot; =&gt; 2018-06-26T06:20:10.811Z&#125; update 更新字段。如果字段不存在，则不做处理 123456789101112131415 mutate&#123; add_field =&gt; &#123;&quot;field1&quot;=&gt;&quot;value1&quot;&#125; &#125; mutate&#123; update =&gt; &#123;&quot;field1&quot;=&gt;&quot;v1&quot;&#125; update =&gt; &#123;&quot;field2&quot;=&gt;&quot;v2&quot;&#125; #field2不存在 不做处理 &#125;----------------&gt;&#123; &quot;@timestamp&quot; =&gt; 2018-06-26T06:26:28.870Z, &quot;field1&quot; =&gt; &quot;v1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;message&quot; =&gt; &quot;a&quot;&#125; replace 更新字段。如果字段不存在，则创建 12345678910111213141516 mutate&#123; add_field =&gt; &#123;&quot;field1&quot;=&gt;&quot;value1&quot;&#125; &#125; mutate&#123; replace =&gt; &#123;&quot;field1&quot;=&gt;&quot;v1&quot;&#125; replace =&gt; &#123;&quot;field2&quot;=&gt;&quot;v2&quot;&#125; &#125;----------------------&gt;&#123; &quot;message&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;@timestamp&quot; =&gt; 2018-06-26T06:28:09.915Z, &quot;field2&quot; =&gt; &quot;v2&quot;, #field2不存在，则新建 &quot;@version&quot; =&gt; &quot;1&quot;, &quot;field1&quot; =&gt; &quot;v1&quot;&#125; geoip 根据来自Maxmind GeoLite2数据库的数据添加有关IP地址的地理位置的信息 1234geoip &#123; source =&gt; &quot;clientip&quot; database =&gt;&quot;/tmp/GeoLiteCity.dat&quot;&#125; ruby ruby插件可以执行任意Ruby代码 123456789101112131415161718192021222324252627282930313233343536373839404142filter&#123; urldecode&#123; field =&gt; &quot;message&quot; &#125; ruby &#123; init =&gt; &quot;@kname = [&apos;url_path&apos;,&apos;url_arg&apos;]&quot; code =&gt; &quot; new_event = LogStash::Event.new(Hash[@kname.zip(event.get(&apos;message&apos;).split(&apos;?&apos;))]) event.append(new_event)&quot; &#125; if [url_arg]&#123; kv&#123; source =&gt; &quot;url_arg&quot; field_split =&gt; &quot;&amp;&quot; target =&gt; &quot;url_args&quot; remove_field =&gt; [&quot;url_arg&quot;,&quot;message&quot;] &#125; &#125;&#125;# ruby插件# 以？为分隔符，将request字段分成url_path和url_arg--------------------&gt;www.test.com?test&#123; &quot;url_arg&quot; =&gt; &quot;test&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;url_path&quot; =&gt; &quot;www.test.com&quot;, &quot;message&quot; =&gt; &quot;www.test.com?test&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; 2018-06-26T07:31:04.887Z&#125;www.test.com?title=elk&amp;content=学习elk&#123; &quot;url_args&quot; =&gt; &#123; &quot;title&quot; =&gt; &quot;elk&quot;, &quot;content&quot; =&gt; &quot;学习elk&quot; &#125;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;url_path&quot; =&gt; &quot;www.test.com&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; 2018-06-26T07:33:54.507Z&#125; urldecode 用于解码被编码的字段,可以解决URL中 中文乱码的问题 123456 urldecode&#123; field =&gt; &quot;message&quot; &#125;# field :指定urldecode过滤器要转码的字段,默认值是&quot;message&quot;# charset(缺省): 指定过滤器使用的编码.默认UTF-8 kv 通过指定分隔符将字符串分割成key/value 123456789101112131415161718kv&#123; prefix =&gt; &quot;url_&quot; #给分割后的key加前缀 target =&gt; &quot;url_ags&quot; #将分割后的key-value放入指定字段 source =&gt; &quot;message&quot; #要分割的字段 field_split =&gt; &quot;&amp;&quot; #指定分隔符 remove_field =&gt; &quot;message&quot; &#125;--------------------------&gt;a=1&amp;b=2&amp;c=3&#123; &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;url_ags&quot; =&gt; &#123; &quot;url_c&quot; =&gt; &quot;3&quot;, &quot;url_a&quot; =&gt; &quot;1&quot;, &quot;url_b&quot; =&gt; &quot;2&quot; &#125;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; 2018-06-26T07:07:24.557Z useragent 添加有关用户代理(如系列,操作系统,版本和设备)的信息 12345678910if [agent] != &quot;-&quot; &#123; useragent &#123; source =&gt; &quot;agent&quot; target =&gt; &quot;ua&quot; remove_field =&gt; &quot;agent&quot; &#125;&#125;# if语句，只有在agent字段不为空时才会使用该插件#source 为必填设置,目标字段#target 将useragent信息配置到ua字段中。如果不指定将存储在根目录中 output plugin 输出插件，将事件发送到特定目标 stdout 标准输出。将事件输出到屏幕上 12345output&#123; stdout&#123; codec =&gt; &quot;rubydebug&quot; &#125;&#125; file 将事件写入文件 1234file &#123; path =&gt; &quot;/data/logstash/%&#123;host&#125;/&#123;application&#125; codec =&gt; line &#123; format =&gt; &quot;%&#123;message&#125;&quot;&#125; &#125;&#125; kafka 将事件发送到kafka 1234kafka&#123; bootstrap_servers =&gt; &quot;localhost:9092&quot; topic_id =&gt; &quot;test_topic&quot; #必需的设置。生成消息的主题&#125; elasticseach 在es中存储日志 12345 elasticsearch &#123; hosts =&gt; &quot;localhost:9200&quot; index =&gt; &quot;nginx-access-log-%&#123;+YYYY.MM.dd&#125;&quot; &#125;#index 事件写入的索引。可以按照日志来创建索引，以便于删旧数据和按时间来搜索日志 补充一个codec plugin 编解码器插件 codec 本质上是流过滤器，可以作为input 或output 插件的一部分运行。例如上面output的stdout插件里有用到。 multiline codec plugin 多行合并, 处理堆栈日志或者其他带有换行符日志需要用到 12345678910111213141516171819202122232425262728input &#123; stdin &#123; codec =&gt; multiline &#123; pattern =&gt; &quot;pattern, a regexp&quot; #正则匹配规则，匹配到的内容按照下面两个参数处理 negate =&gt; &quot;true&quot; or &quot;false&quot; # 默认为false。处理匹配符合正则规则的行。如果为true，处理不匹配符合正则规则的行。 what =&gt; &quot;previous&quot; or &quot;next&quot; #指定上下文。将指定的行是合并到上一行或者下一行。 &#125; &#125;&#125;codec =&gt; multiline &#123; pattern =&gt; &quot;^\s&quot; what =&gt; &quot;previous&quot; &#125;# 以空格开头的行都合并到上一行codec =&gt; multiline &#123; # Grok pattern names are valid! :) pattern =&gt; &quot;^%&#123;TIMESTAMP_ISO8601&#125; &quot; negate =&gt; true what =&gt; &quot;previous&quot;&#125;# 任何不以这个时间戳格式开头的行都与上一行合并codec =&gt; multiline &#123; pattern =&gt; &quot;\\$&quot; what =&gt; &quot;next&quot;&#125;# 以反斜杠结尾的行都与下一行合并 logstash配置语法中的条件判断 Logstash中的条件查看和行为与编程语言中的条件相同。 条件语支持if，else if和else语句并且可以嵌套。 条件语法如下： 1234567if EXPRESSION &#123; ...&#125; else if EXPRESSION &#123; ...&#125; else &#123; ...&#125; 比较操作： 相等: ==, !=, &lt;, &gt;, &lt;=, &gt;= 正则: =~(匹配正则), !~(不匹配正则) 包含: in(包含), not in(不包含) 布尔操作： - and(与), or(或), nand(非与), xor(非或) 一元运算符： 表达式可能很长且很复杂。表达式可以包含其他表达式，您可以使用！来取消表达式，并且可以使用括号（…）对它们进行分组。 - !(取反) - ()(复合表达式), !()(对复合表达式结果取反) 如若action是login则mutate filter删除secret字段： 12345filter &#123; if [action] == &quot;login&quot; &#123; mutate &#123; remove_field =&gt; &quot;secret&quot; &#125; &#125;&#125; 若是正则匹配，成功后添加自定义字段： 1234567filter &#123; if [message] =~ /\w+\s+\/\w+(\/learner\/course\/)/ &#123; mutate &#123; add_field =&gt; &#123; &quot;learner_type&quot; =&gt; &quot;course&quot; &#125; &#125; &#125;&#125; 在一个条件里指定多个表达式： 12345678output &#123; # Send production errors to pagerduty if [loglevel] == &quot;ERROR&quot; and [deployment] == &quot;production&quot; &#123; pagerduty &#123; ... &#125; &#125;&#125; 在in条件，可以比较字段值： 1234567891011121314151617181920filter &#123; if [foo] in [foobar] &#123; mutate &#123; add_tag =&gt; &quot;field in field&quot; &#125; &#125; if [foo] in &quot;foo&quot; &#123; mutate &#123; add_tag =&gt; &quot;field in string&quot; &#125; &#125; if &quot;hello&quot; in [greeting] &#123; mutate &#123; add_tag =&gt; &quot;string in field&quot; &#125; &#125; if [foo] in [&quot;hello&quot;, &quot;world&quot;, &quot;foo&quot;] &#123; mutate &#123; add_tag =&gt; &quot;field in list&quot; &#125; &#125; if [missing] in [alsomissing] &#123; mutate &#123; add_tag =&gt; &quot;shouldnotexist&quot; &#125; &#125; if !(&quot;foo&quot; in [&quot;hello&quot;, &quot;world&quot;]) &#123; mutate &#123; add_tag =&gt; &quot;shouldexist&quot; &#125; &#125;&#125; not in示例： 12345output &#123; if &quot;_grokparsefailure&quot; not in [tags] &#123; elasticsearch &#123; ... &#125; &#125;&#125; @metadata field 在logstash1.5版本开始，有一个特殊的字段，叫做@metadata。@metadata包含的内容不会作为事件的一部分输出。 12345678910111213input &#123; stdin &#123; &#125; &#125;filter &#123; mutate &#123; add_field =&gt; &#123; &quot;show&quot; =&gt; &quot;This data will be in the output&quot; &#125; &#125; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][test]&quot; =&gt; &quot;Hello&quot; &#125; &#125; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][no_show]&quot; =&gt; &quot;This data will not be in the output&quot; &#125; &#125;&#125;output &#123; if [@metadata][test] == &quot;Hello&quot; &#123; stdout &#123; codec =&gt; rubydebug &#125; &#125;&#125; 输出结果 12345678910$ bin/logstash -f ../test.confPipeline main startedasdf&#123; &quot;@timestamp&quot; =&gt; 2016-06-30T02:42:51.496Z, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;windcoder.com&quot;, &quot;show&quot; =&gt; &quot;This data will be in the output&quot;, &quot;message&quot; =&gt; &quot;asdf&quot;&#125; “asdf”变成message字段内容。条件与@metadata内嵌的test字段内容判断成功，但是输出并没有展示@metadata字段和其内容。 不过，如果指定了metadata =&gt; true，rubydebug codec允许显示@metadata字段的内容。 1stdout &#123; codec =&gt; rubydebug &#123; metadata =&gt; true &#125; &#125; 输出结果 1234567891011121314$ bin/logstash -f ../test.confPipeline main startedasdf&#123; &quot;@timestamp&quot; =&gt; 2016-06-30T02:46:48.565Z, &quot;@metadata字段及其子字段内容。&quot; =&gt; &#123; &quot;test&quot; =&gt; &quot;Hello&quot;, &quot;no_show&quot; =&gt; &quot;This data will not be in the output&quot; &#125;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;windcoder.com&quot;, &quot;show&quot; =&gt; &quot;This data will be in the output&quot;, &quot;message&quot; =&gt; &quot;asdf&quot;&#125; 现在就可以见到@metadata字段及其子字段内容。 只有rubydebug codec允许显示@metadata字段的内容。 只要您需要临时字段但不希望它在最终输出中，就可以使用@metadata字段。 最常见的情景是filter的时间字段，需要一临时的时间戳。如： 12345678910input &#123; stdin &#123; &#125; &#125;filter &#123; grok &#123; match =&gt; [ &quot;message&quot;, &quot;%&#123;HTTPDATE:[@metadata][timestamp]&#125;&quot; ] &#125; date &#123; match =&gt; [ &quot;[@metadata][timestamp]&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ] &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 输出结果 123456789$ bin/logstash -f ../test.confPipeline main started02/Mar/2014:15:36:43 +0100&#123; &quot;@timestamp&quot; =&gt; 2014-03-02T14:36:43.000Z, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;windcoder.com&quot;, &quot;message&quot; =&gt; &quot;02/Mar/2014:15:36:43 +0100&quot;&#125; 处理匹配失败的日志忽略过滤器 123456filter &#123; grok &#123; match =&gt; [ &quot;message&quot;, &quot;something&quot; ] &#125; if &quot;_grokparsefailure&quot; in [tags] &#123; drop&#123; &#125; &#125;&#125; Grok插件了解原文链接：https://www.elastic.co/cn/blog/do-you-grok-grok 在日志处理的过程中，有一项非常常见的任务就是把原始的单行日志转换成结构化的日志。如果你使用了ELK，那么你可以利用ES对数据进行聚合，使用Kibana来进行数据可视化从日志中来发现一些有价值的信息。 在LogStash中，这项工作是由logstash-filter-grok来完成的，它有超过200个可用的，大家都认为是比较有用的Grok模式，例如IPv6地址、UNIX路径等等。 下面是一个示例日志 12016-09-19T18:19:00 [8.8.8.8:prd] DEBUG this is an example log message 使用Grok库，我们可以很容易的就完成日志格式化提取的任务 1%&#123;TIMESTAMP_ISO8601:timestamp&#125; \[%&#123;IPV4:ip&#125;;%&#123;WORD:environment&#125;\] %&#123;LOGLEVEL:log_level&#125; %&#123;GREEDYDATA:message&#125; 提取后的数据格式如下 1234567&#123; &quot;timestamp&quot;: &quot;2016-09-19T18:19:00&quot;, &quot;ip&quot;: &quot;8.8.8.8&quot;, &quot;environment&quot;: &quot;prd&quot;, &quot;log_level&quot;: &quot;DEBUG&quot;, &quot;message&quot;: &quot;this is an example log message&quot;&#125; 看起来这是一件非常简单的事情，好吧。。那这篇文章就这样写完了么，当然不是。。 为什么我的Grok使用起来非常的慢 这是一个非常常见的问题。性能这个问题通常都是要被拿出来讨论的，用户通常会发现使用了Grok表达式之后，LogStash处理日志的速度变得很慢。就像前面所说的一样，Grok模式是基于正则表达式的，所以这个插件在性能上已经对正则做了非常多的性能优化的了。接下来的章节，我们会讨论在使用Grok模式中需要注意的点 多做些性能测试 在设计Grok表达式的时候，我们需要一些方法来测试究竟哪种写法性能表现更好。出于这个原因，我些了个很小的jruby脚步用于测试Grok插件处理我所写的Grok模式的性能，你可以在这里获取到这个脚本 留意grok匹配失败的时候对性能的影响 尽管Grok匹配的性能是非常重要的，但是匹配失败的时候对性能的影响也是我们需要留意的。当grok匹配失败的时候，插件会为这个事件打个tag，默认是_grokparsefailure。LogStash允许你把这些处理失败的事件路由到其他地方做后续的处理，例如 1234567891011121314input &#123; # ... &#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \[%&#123;IPV4:ip&#125;;%&#123;WORD:environment&#125;\] %&#123;LOGLEVEL:log_level&#125; %&#123;GREEDYDATA:message&#125;&quot; &#125; &#125;&#125;output &#123; if &quot;_grokparsefailure&quot; in [tags] &#123; # write events that didn&apos;t match to a file file &#123; &quot;path&quot; =&gt; &quot;/tmp/grok_failures.txt&quot; &#125; &#125; else &#123; elasticsearch &#123; &#125; &#125;&#125; 这样的话我们就可以对这些处理失败的事件做性能基准测试了。 现在，我们要开始对Apache的日志进行格式化处理了 1220.181.108.96 - - [13/Jun/2015:21:14:28 +0000] &quot;GET /blog/geekery/xvfb-firefox.html HTTP/1.1&quot; 200 10975 &quot;-&quot; &quot;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&quot; 然后我们使用下面的Grok模式去进行格式化提取 1%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125; 然后我们使用三种示例日志去测试这个Grok的性能，和Grok不匹配的日志分别出现在开始，中间和结束的位置 12345678# beginning mismatch - doesn&apos;t start with an IPORHOST&apos;tash-scale11x/css/fonts/Roboto-Regular.ttf HTTP/1.1&quot; 200 41820 &quot;http://semicomplete.com/presentations/logs&apos;# middle mismatch - instead of an HTTP verb like GET or PUT there&apos;s the number 111&apos;220.181.108.96 - - [13/Jun/2015:21:14:28 +0000] &quot;111 /blog/geekery/xvfb-firefox.html HTTP/1.1&quot; 200 10975 &quot;-&quot; &quot;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&quot;&apos;# end mismatch - the last element isn&apos;t a quoted string, but a number&apos;220.181.108.96 - - [13/Jun/2015:21:14:28 +0000] &quot;GET /blog/geekery/xvfb-firefox.html HTTP/1.1&quot; 200 10975 &quot;-&quot; 1&apos; 下面是性能测试的结果 Paste_Image.png 基于上面这个测试结果，我们可以发现，Grok的性能和不匹配的日志所出现的位置有关，最快与最慢的性能差了差不多6倍。这就能解释为什么有用户提出当Grok匹配日志失败的时候CPU会被吃满的原因了，例如这个issueshttps://github.com/logstash-plugins/logstash-filter-grok/issues/37. 我们能做些什么呢 快速失败，设置锚点 我们已经知道了处理失败对grok的性能影响是非常大的，所以我们需要解决这个问题。对于正则引擎来说，你需要做的最合适的事情就是减少正则表达式所需要的猜测。这就是为什么贪婪匹配最好少用的原因，那回到这个问题，有没一种更好的方法来调整这个Grok模式呢，让我们重新来看看这行Apache的日志 1220.181.108.96 - - [13/Jun/2015:21:14:28 +0000] &quot;GET /blog/geekery/xvfb-firefox.html HTTP/1.1&quot; 200 10975 &quot;-&quot; &quot;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&quot; 刚才我们使用的Grok模式是这样的 1%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125; 由于用户以为Grok表达式只会从开头匹配到结束，所以导致了在一些普通的场景下也会出现性能问题。但是实际上，Grok只是被告知“在这段文本中寻找匹配的内容”，这就意味着下面这种示例也会被Grok所匹配。。。 1OMG OMG OMG EXTRA INFORMATION 220.181.108.96 - - [13/Jun/2015:21:14:28 +0000] &quot;GET /blog/geekery/xvfb-firefox.html HTTP/1.1&quot; 200 10975 &quot;-&quot; &quot;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&quot; OH LOOK EVEN MORE STUFF 呃。。这都行，不过解决这个问题还是很简单的，我们加一些锚点就搞定了。锚点可以让你再一个指定的位置处理字符串。加入了开始和结束的锚点之后(^和$)，Grok就会从开头处理日志到结束了。这对处理那些不能匹配的日志有非常重要的作用。假如我们没有假如锚点，当正则无法匹配这行日志的时候，它就会开始从子字符串中进行匹配，然后性能就会下降，接下来我们把锚点加上，然后再做一次测试 1^%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;$ Paste_Image.png 可以看到性能有了很大的提升，在一开始就匹配失败的场景中，性能提升了将近10倍 留意匹配了两次的表达式 你可能会说，“好吧，我的日志都是能匹配通过的，没有上面的问题”，但是事情可能并不是这样的 我们看到过非常多的grok模式在处理同一个网关发出的多种应用日志时候所出现的问题，例如syslog。想象一下这样一个场景，我们使用了“common_header: payload“这种日志格式来记录了三种应用日志 123Application 1: &apos;8.8.8.8 process-name[666]: a b 1 2 a lot of text at the end&apos;Application 2: &apos;8.8.8.8 process-name[667]: a 1 2 3 a lot of text near the end;4&apos;Application 3: &apos;8.8.8.8 process-name[421]: a completely different format | 1111&apos; 通常我们会在一个Grok里面就把三种日志都处理掉 1234567grok &#123; &quot;match&quot; =&gt; &#123; &quot;message =&gt; [ &apos;%&#123;IPORHOST:clientip&#125; %&#123;DATA:process_name&#125;\[%&#123;NUMBER:process_id&#125;\]: %&#123;WORD:word_1&#125; %&#123;WORD:word_2&#125; %&#123;NUMBER:number_1&#125; %&#123;NUMBER:number_2&#125; %&#123;DATA:data&#125;&apos;, &apos;%&#123;IPORHOST:clientip&#125; %&#123;DATA:process_name&#125;\[%&#123;NUMBER:process_id&#125;\]: %&#123;WORD:word_1&#125; %&#123;NUMBER:number_1&#125; %&#123;NUMBER:number_2&#125; %&#123;NUMBER:number_3&#125; %&#123;DATA:data&#125;;%&#123;NUMBER:number_4&#125;&apos;, &apos;%&#123;IPORHOST:clientip&#125; %&#123;DATA:process_name&#125;\[%&#123;NUMBER:process_id&#125;\]: %&#123;DATA:data&#125; | %&#123;NUMBER:number&#125;&apos; ] &#125;&#125; 值得留意的是即使你的日志是能正常匹配的，Grok还是会按照顺序许匹配送进来的日志，当碰到第一个匹配成功的日志就break掉这个循环。这就要我们自己去判断一下，怎么放是最合适的了，不然的话会一个一个往下进行尝试，毕竟是多种不同的格式。一种常用的优化方案是使用分层匹配来对这个Grok进行优化 12345678910111213filter &#123; grok &#123; &quot;match&quot; =&gt; &#123; &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;DATA:process_name&#125;\[%&#123;NUMBER:process_id&#125;\]: %&#123;GREEDYDATA:message&#125;&apos; &#125;, &quot;overwrite&quot; =&gt; &quot;message&quot; &#125; grok &#123; &quot;match&quot; =&gt; &#123; &quot;message&quot; =&gt; [ &apos;%&#123;WORD:word_1&#125; %&#123;WORD:word_2&#125; %&#123;NUMBER:number_1&#125; %&#123;NUMBER:number_2&#125; %&#123;GREEDYDATA:data&#125;&apos;, &apos;%&#123;WORD:word_1&#125; %&#123;NUMBER:number_1&#125; %&#123;NUMBER:number_2&#125; %&#123;NUMBER:number_3&#125; %&#123;DATA:data&#125;;%&#123;NUMBER:number_4&#125;&apos;, &apos;%&#123;DATA:data&#125; | %&#123;NUMBER:number&#125;&apos; ] &#125; &#125;) 这是两种匹配方案的性能测试结果 Paste_Image.png 看起来真有意思。。使用锚点的话，无论哪种方案性能都是一样的。不用锚点的情况下分层Grok的方案比不分层的又快很多 那我们怎么知道我们所创建的Grok是合适的 我们已经得出了对_grokparsefaiure进行处理的必要性了，那么我们还能做什么呢？从3.2.0这个Grok插件开始，它有一些参数可以帮助你了解为什么一个事件会被处理那么久了。使用timeout_millis和tag_on_timeout可以设置Grok匹配的最大处理时长。如果超时了，这个事件会被打上_groktimeout的tag，然后我们就可以把他们送到一个Grok处理失败的ES索引里面去做后续的分析了 另外一个很棒的方法是LogStash5.0带了插件性能统计的功能，我们可以通过API来查看插件处理日志的性能了 12345678910111213141516171819202122232425$ curl localhost:9600/_node/stats/pipeline?pretty | jq &quot;.pipeline.plugins.filters&quot;[ &#123; &quot;id&quot;: &quot;grok_b61938f3833f9f89360b5fba6472be0ad51c3606-2&quot;, &quot;events&quot;: &#123; &quot;duration_in_millis&quot;: 7, &quot;in&quot;: 24, &quot;out&quot;: 24 &#125;, &quot;failures&quot;: 24, &quot;patterns_per_field&quot;: &#123; &quot;message&quot;: 1 &#125;, &quot;name&quot;: &quot;grok&quot; &#125;, &#123; &quot;id&quot;: &quot;kv_b61938f3833f9f89360b5fba6472be0ad51c3606-3&quot;, &quot;events&quot;: &#123; &quot;duration_in_millis&quot;: 2, &quot;in&quot;: 24, &quot;out&quot;: 24 &#125;, &quot;name&quot;: &quot;kv&quot; &#125;] 然后我们就可以通过duration_in_millis来判断一个插件的性能了 总结 希望这篇文章能帮你了解为什么Grok的性能会变得慢和如何去提升他的性能。下面是对这篇文字的总结： Grok在匹配失败的时候性能可能并不那么好 多留意_grokparsefailures出现的频率和出现时候的性能 写正则的时候记得打锚点 不使用锚点的时候分层Grok处理的性能会比不分层的性能好，不过打了锚点的话两个都一样 多使用LogStash的性能监控功能，后续还可以拿来分析用 Yaml Filelogstash-kubernetes.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182---apiVersion: v1kind: ConfigMapmetadata: name: logstash-config namespace: elk-test labels: elastic-app: logstashdata: logstash.conf: |- input &#123; beats &#123; port =&gt; 5044 &#125; redis &#123; host =&gt; "192.168.0.95" port =&gt; 6379 db =&gt; "2" data_type =&gt; "list" batch_count =&gt; 1 key =&gt; "ecs" &#125; redis &#123; host =&gt; "192.168.0.95" port =&gt; 6379 db =&gt; "3" data_type =&gt; "list" batch_count =&gt; 1 key =&gt; "k8s" &#125; &#125; filter &#123; #server01 Logs if "server01_logs" in [tags]&#123; grok &#123; match =&gt; &#123; "message" =&gt; "^\[%&#123;TIMESTAMP_ISO8601:body_date&#125;\s\w&#123;3&#125;\]\s%&#123;DATA:level&#125;\s%&#123;DATA:bundle&#125;\s%&#123;DATA:device&#125;\s%&#123;GREEDYDATA:body&#125;$"&#125; &#125; date &#123; match =&gt; ["body_date","yyyy-MM-dd HH:mm:ss.SSS"] target =&gt; "@timestamp" &#125; mutate &#123; remove_field =&gt; ["message"] &#125; &#125; #server02 Logs if [kubernetes][namespace] == "server02-test" &#123; #admin-ui #if [kubernetes][container][name] == "admin-ui" or [kubernetes][container][name] == "monitoring" &#123; #&#125; grok &#123; match =&gt; &#123; "message" =&gt; [ "^(?&lt;body_date&gt;%&#123;MONTHDAY&#125;-%&#123;MONTH&#125;-%&#123;YEAR&#125;\s%&#123;TIME&#125;)\s*(?&lt;level&gt;\w+)\s*(?&lt;thread&gt;\[\S+\])\s*%&#123;GREEDYDATA:body&#125;$", "^%&#123;TIMESTAMP_ISO8601:body_date&#125;\s*(?&lt;thread&gt;\[\S+\])\s*(?&lt;level&gt;\w+)\s*%&#123;GREEDYDATA:body&#125;$" ] &#125; &#125; #drop failuer if "_grokparsefailure" in [tags] &#123; drop &#123; &#125; &#125; date &#123; match =&gt; ["body_date","yyyy-MM-dd'T'HH:mm:ss,SSSZ","dd-MMM-yyyy HH:mm:ss.SSS"] target =&gt; "@timestamp" &#125; mutate &#123; remove_field =&gt; ["message","MONTHDAY","MONTHNUM","MONTH","YEAR","TIME","HOUR","MINUTE","SECOND","ISO8601_TIMEZONE"] &#125; &#125; &#125; output &#123; #server01 if "server01_logs" in [tags]&#123; elasticsearch &#123; #hosts =&gt; ["192.168.0.92:31200"] hosts =&gt; ["elasticsearch-service:9200"] index =&gt; "server01_logs-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; #server02 if [kubernetes][namespace] == "server02-test" &#123; elasticsearch &#123; hosts =&gt; ["elasticsearch-service:9200"] index =&gt; "server02-test-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; &#125; ---apiVersion: v1kind: ConfigMapmetadata: name: logstash-yml namespace: elk-test labels: elastic-app: logstashdata: logstash.yml: |- path.config: /usr/share/logstash/config/config.d config.reload.automatic: true config.reload.interval: 10s---apiVersion: v1kind: Servicemetadata: namespace: elk-test name: logstash labels: elastic-app: logstashspec: type: NodePort ports: - port: 5044 name: filebeat - port: 9600 name: logstash selector: elastic-app: logstashapiVersion: apps/v1kind: Deploymentmetadata: namespace: elk-test name: logstash labels: elastic-app: logstashspec: replicas: 2 selector: matchLabels: elastic-app: logstash template: metadata: labels: elastic-app: logstash spec: containers: - name: logstash image: docker.elastic.co/logstash/logstash:6.8.1 imagePullPolicy: IfNotPresent resources: limits: cpu: "1000m" memory: "2Gi" requests: cpu: "500m" memory: "1Gi" env: - name: REQUESTS_MEMORY valueFrom: resourceFieldRef: resource: requests.memory divisor: 1Mi - name: LS_JAVA_OPTS value: "-Xms$(REQUESTS_MEMORY)m -Xmx$(REQUESTS_MEMORY)m" ports: - containerPort: 5044 name: filebeat protocol: TCP - containerPort: 9600 name: logstash protocol: TCP volumeMounts: - mountPath: /usr/share/logstash/config/logstash.yml subPath: logstash.yml name: logstash-yml - mountPath: /usr/share/logstash/config/config.d name: logstash-config volumes: - name: logstash-config configMap: name: logstash-config - name: logstash-yml configMap: name: logstash-yml 部署1kubectl apply -f logstash-kubernetes.yaml Elasticsearch官方网站：https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html GitHub：https://github.com/elastic/elasticsearch elasticsearch-head：https://github.com/mobz/elasticsearch-head 入门elastocsearchhttp://www.ruanyifeng.com/blog/2017/08/elasticsearch.html 介绍为何要搭建 Elasticsearch 集群凡事都要讲究个为什么。在搭建集群之前，我们首先先问一句，为什么我们需要搭建集群？它有什么优势呢？ 高可用性Elasticsearch 作为一个搜索引擎，我们对它的基本要求就是存储海量数据并且可以在非常短的时间内查询到我们想要的信息。所以第一步我们需要保证的就是 Elasticsearch 的高可用性，什么是高可用性呢？它通常是指，通过设计减少系统不能提供服务的时间。假设系统一直能够提供服务，我们说系统的可用性是 100%。如果系统在某个时刻宕掉了，比如某个网站在某个时间挂掉了，那么就可以它临时是不可用的。所以，为了保证 Elasticsearch 的高可用性，我们就应该尽量减少 Elasticsearch 的不可用时间。 那么怎样提高 Elasticsearch 的高可用性呢？这时集群的作用就体现出来了。假如 Elasticsearch 只放在一台服务器上，即单机运行，假如这台主机突然断网了或者被攻击了，那么整个 Elasticsearch 的服务就不可用了。但如果改成 Elasticsearch 集群的话，有一台主机宕机了，还有其他的主机可以支撑，这样就仍然可以保证服务是可用的。 那可能有的小伙伴就会说了，那假如一台主机宕机了，那么不就无法访问这台主机的数据了吗？那假如我要访问的数据正好存在这台主机上，那不就获取不到了吗？难道其他的主机里面也存了一份一模一样的数据？那这岂不是很浪费吗？ 为了解答这个问题，这里就引出了 Elasticsearch 的信息存储机制了。首先解答上面的问题，一台主机宕机了，这台主机里面存的数据依然是可以被访问到的，因为在其他的主机上也有备份，但备份的时候也不是整台主机备份，是分片备份的，那这里就又引出了一个概念——分片。 分片，英文叫做 Shard，顾名思义，分片就是对数据切分成了多个部分。我们知道 Elasticsearch 中一个索引（Index）相当于是一个数据库，如存某网站的用户信息，我们就建一个名为 user 的索引。但索引存储的时候并不是整个存一起的，它是被分片存储的，Elasticsearch 默认会把一个索引分成五个分片，当然这个数字是可以自定义的。分片是数据的容器，数据保存在分片内，分片又被分配到集群内的各个节点里。当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里，所以相当于一份数据被分成了多份并保存在不同的主机上。 那这还是没解决问题啊，如果一台主机挂掉了，那么这个分片里面的数据不就无法访问了？别的主机都是存储的其他的分片。其实是可以访问的，因为其他主机存储了这个分片的备份，叫做副本，这里就引出了另外一个概念——副本。 副本，英文叫做 Replica，同样顾名思义，副本就是对原分片的复制，和原分片的内容是一样的，Elasticsearch 默认会生成一份副本，所以相当于是五个原分片和五个分片副本，相当于一份数据存了两份，并分了十个分片，当然副本的数量也是可以自定义的。这时我们只需要将某个分片的副本存在另外一台主机上，这样当某台主机宕机了，我们依然还可以从另外一台主机的副本中找到对应的数据。所以从外部来看，数据结果是没有任何区别的。 一般来说，Elasticsearch 会尽量把一个索引的不同分片存储在不同的主机上，分片的副本也尽可能存在不同的主机上，这样可以提高容错率，从而提高高可用性。 但这时假如你只有一台主机，那不就没办法了吗？分片和副本其实是没意义的，一台主机挂掉了，就全挂掉了。 健康状态针对一个索引，Elasticsearch 中其实有专门的衡量索引健康状况的标志，分为三个等级： green，绿色。这代表所有的主分片和副本分片都已分配。你的集群是 100% 可用的。 yellow，黄色。所有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。不过，你的高可用性在某种程度上被弱化。如果更多的分片消失，你就会丢数据了。所以可把 yellow 想象成一个需要及时调查的警告。 red，红色。至少一个主分片以及它的全部副本都在缺失中。这意味着你在缺少数据：搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。 如果你只有一台主机的话，其实索引的健康状况也是 yellow，因为一台主机，集群没有其他的主机可以防止副本，所以说，这就是一个不健康的状态，因此集群也是十分有必要的。 存储空间另外，既然是群集，那么存储空间肯定也是联合起来的，假如一台主机的存储空间是固定的，那么集群它相对于单个主机也有更多的存储空间，可存储的数据量也更大。 所以综上所述，我们需要一个集群！ 详细了解 Elasticsearch 集群接下来我们再来了解下集群的结构是怎样的。 首先我们应该清楚多台主机构成了一个集群，每台主机称作一个节点（Node）。 如图就是一个三节点的集群： 在图中，每个 Node 都有三个分片，其中 P 开头的代表 Primary 分片，即主分片，R 开头的代表 Replica 分片，即副本分片。所以图中主分片 1、2，副本分片 0 储存在 1 号节点，副本分片 0、1、2 储存在 2 号节点，主分片 0 和副本分片 1、2 储存在 3 号节点，一共是 3 个主分片和 6 个副本分片。同时我们还注意到 1 号节点还有个 MASTER 的标识，这代表它是一个主节点，它相比其他的节点更加特殊，它有权限控制整个集群，比如资源的分配、节点的修改等等。 这里就引出了一个概念就是节点的类型，我们可以将节点分为这么四个类型： 主节点：即 Master 节点。主节点的主要职责是和集群操作相关的内容，如创建或删除索引，跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点。稳定的主节点对集群的健康是非常重要的。默认情况下任何一个集群中的节点都有可能被选为主节点。索引数据和搜索查询等操作会占用大量的cpu，内存，io资源，为了确保一个集群的稳定，分离主节点和数据节点是一个比较好的选择。虽然主节点也可以协调节点，路由搜索和从客户端新增数据到数据节点，但最好不要使用这些专用的主节点。一个重要的原则是，尽可能做尽量少的工作。 数据节点：即 Data 节点。数据节点主要是存储索引数据的节点，主要对文档进行增删改查操作，聚合操作等。数据节点对 CPU、内存、IO 要求较高，在优化的时候需要监控数据节点的状态，当资源不够的时候，需要在集群中添加新的节点。 负载均衡节点：也称作 Client 节点，也称作客户端节点。当一个节点既不配置为主节点，也不配置为数据节点时，该节点只能处理路由请求，处理搜索，分发索引操作等，从本质上来说该客户节点表现为智能负载平衡器。独立的客户端节点在一个比较大的集群中是非常有用的，他协调主节点和数据节点，客户端节点加入集群可以得到集群的状态，根据集群的状态可以直接路由请求。 预处理节点：也称作 Ingest 节点，在索引数据之前可以先对数据做预处理操作，所有节点其实默认都是支持 Ingest 操作的，也可以专门将某个节点配置为 Ingest 节点。 以上就是节点几种类型，一个节点其实可以对应不同的类型，如一个节点可以同时成为主节点和数据节点和预处理节点，但如果一个节点既不是主节点也不是数据节点，那么它就是负载均衡节点。具体的类型可以通过具体的配置文件来设置。 Elasticsearch配置文件详解elasticsearch的config文件夹里面有两个配置文 件：elasticsearch.yml和logging.yml，第一个是es的基本配置文件，第二个是日志配置文件，es也是使用log4j来记录日志的，所以logging.yml里的设置按普通log4j配置文件来设置就行了。下面主要讲解下elasticsearch.yml这个文件中可配置的东西。 只是挑些重要的配置选项进行注释,别的可以参考官方文档！！！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276 ################################### Cluster ################################### # 代表一个集群,集群中有多个节点,其中有一个为主节点,这个主节点是可以通过选举产生的,主从节点是对于集群内部来说的. # es的一个概念就是去中心化,字面上理解就是无中心节点,这是对于集群外部来说的,# 因为从外部来看es集群,在逻辑上是个整体,你与任何一个节点的通信和与整个es集群通信是等价的cluster.name: elasticsearch配置es的集群名称，默认是elasticsearch，es会自动发现在同一网段下的es，如果在同一网段下有多个集群，就可以用这个属性来区分不同的集群，cluster.name就成为同一个集群的标识。node.name: &quot;Franz Kafka&quot;节点名，默认随机指定一个name列表中名字，该列表在es的jar包中config文件夹里name.txt文件中，其中有很多作者添加的有趣名字，节点名称同理,可自动生成也可手动配置。node.master: true指定该节点是否有资格被选举成为node，默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master。node.data: true指定该节点是否存储索引数据，默认为true。1. # 配置文件中给出了三种配置高性能集群拓扑结构的模式,如下： 2. # 1. 如果你想让节点从不选举为主节点,只用来存储数据,可作为负载器 3. # node.master: **false** 4. # node.data: **true** 5. # node.ingest: **false** 6. 7. # 2. 如果想让节点成为主节点,且不存储任何数据,并保有空闲资源,可作为协调器 8. # node.master: **true** 9. # node.data: **false** 10. # node.ingest: **false** 11. 12. # 3. 如果想让节点既不称为主节点,又不成为数据节点,那么可将他作为搜索器,从节点中获取数据,生成搜索结果等 13. # node.master: **false** 14. # node.data: **false** 15. # node.ingest: **true** (可不指定默认开启) 16. 17. # 4. 仅作为协调器 18. # node.master: **false** 19. # node.data: **false** 20. # node.ingest: **false** index.number_of_shards: 5设置默认索引分片个数，默认为5片。index.number_of_replicas: 1设置默认索引副本个数，默认为1个副本。1. # 配置文件中提到的最佳实践是,如果服务器够多,可以将分片提高,尽量将数据平均分布到大集群中去 2. # 同时,如果增加副本数量可以有效的提高搜索性能 3. # 需要注意的是,&quot;number_of_shards&quot; 是索引创建后一次生成的,后续不可更改设置 4. # &quot;number_of_replicas&quot; 是可以通过API去实时修改设置的 path.conf: /path/to/conf设置配置文件的存储路径，默认是es根目录下的config文件夹。path.data: /path/to/data设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：path.data: /path/to/data1,/path/to/data2path.work: /path/to/work设置临时文件的存储路径，默认是es根目录下的work文件夹。path.logs: /path/to/logs设置日志文件的存储路径，默认是es根目录下的logs文件夹path.plugins: /path/to/plugins设置插件的存放路径，默认是es根目录下的plugins文件夹bootstrap.memory_lock：服务器发生系统swapping的时候ES节点的性能会非常差，也会影响节点的稳定性。所以要不惜一切代价来避免swapping。swapping会导致Java GC的周期延迟从毫秒级恶化到分钟，更严重的是会引起节点响应延迟甚至脱离集群。这个参数的目的是当你无法关闭系统的swap的时候，建议把这个参数设为true。防止在内存不够用的时候，elasticsearch的内存被交换至交换区，导致性能骤降。bootstrap.mlockall: true设置为true来锁住内存。因为当jvm开始swapping时es的效率会降低，所以要保证它不swap，可以把ES_MIN_MEM和 ES_MAX_MEM两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。同时也要允许elasticsearch的进程可以锁住内存，linux下可以通过`ulimit -l unlimited`命令。network.bind_host: 192.168.0.1设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0。network.publish_host: 192.168.0.1设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。network.host: 192.168.0.1这个参数是用来同时设置bind_host和publish_host上面两个参数。transport.tcp.port: 9300设置节点间交互的tcp端口，默认是9300。transport.tcp.compress: true设置是否压缩tcp传输时的数据，默认为false，不压缩。http.port: 9200设置对外服务的http端口，默认为9200。http.max_content_length: 100mb设置内容的最大容量，默认100mbhttp.enabled: false是否使用http协议对外提供服务，默认为true，开启。1. ###################### 使用head等插件监控集群信息，需要打开以下配置项 ########### 2. # http.cors.enabled: **true** 3. # http.cors.allow-origin: &quot;*&quot; 4. # http.cors.allow-credentials: **true** 1. # 下面的配置控制怎样以及何时启动一整个集群重启的初始化恢复过程 2. # (当使用shard gateway时,是为了尽可能的重用local data(本地数据)) gateway.type: localgateway的类型，默认为local即为本地文件系统，可以设置为本地文件系统，分布式文件系统，Hadoop的HDFS，和amazon的s3服务器。gateway.recover_after_nodes: 1集群中N个节点启动后进行数据恢复，默认为1。gateway.recover_after_time: 5m设置初始化恢复过程的超时时间,超时时间从上一个配置中配置的N个节点启动后算起，默认是5分钟。gateway.expected_nodes: 2设置这个集群中节点的数量，默认为2，一旦这N个节点启动，就会立即进行数据恢复。1. # 下面这些配置允许在初始化恢复,副本分配,再平衡,或者添加和删除节点时控制节点间的分片分配 2. # 设置一个节点的并行恢复数 cluster.routing.allocation.node_initial_primaries_recoveries: 4初始化数据恢复时，并发恢复线程的个数，默认为4。cluster.routing.allocation.node_concurrent_recoveries: 2添加删除节点或负载均衡时并发恢复线程的个数，默认为4。indices.recovery.max_size_per_sec: 0设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制。indices.recovery.concurrent_streams: 5设置这个参数来限制从其它分片恢复数据时最大同时打开并发流的个数，默认为5。discovery.zen.minimum_master_nodes: 1设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4）discovery.zen.ping.timeout: 3s设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错。discovery.zen.ping.multicast.enabled: false设置是否打开多播发现节点，默认是true。discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;, &quot;host3[portX-portY]&quot;]设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。###节点ping设置，更多等待时间设置discovery.zen.fd.ping_interval：该属性默认为1s（1秒钟），指定了节点互相ping的时间间隔。discovery.zen.fd.ping_timeout：该属性默认为30s（30秒钟），指定了节点发送ping信息后等待响应的时间，超过此时间则认为对方节点无响应。discovery.zen.fd.ping_retries：该属性默认为3，指定了重试次数，超过此次数则认为对方节点已停止工作。1. # 官方插件 相关设置请查看此处 2. # https://www.elastic.co/guide/en/x-pack/current/xpack-settings.html下面是一些查询时的慢日志参数设置index.search.slowlog.level: TRACEindex.search.slowlog.threshold.query.warn: 10sindex.search.slowlog.threshold.query.info: 5sindex.search.slowlog.threshold.query.debug: 2sindex.search.slowlog.threshold.query.trace: 500msindex.search.slowlog.threshold.fetch.warn: 1sindex.search.slowlog.threshold.fetch.info: 800msindex.search.slowlog.threshold.fetch.debug:500msindex.search.slowlog.threshold.fetch.trace: 200ms jvm配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113##JVM configuration################################################################## IMPORTANT: JVM heap size#################################################################### You should always set the min and max JVM heap## size to the same value. For example, to set## the heap to 4 GB, set:#### -Xms4g## -Xmx4g#### See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html## for more information################################################################### Xms represents the initial size of total heap space# Xmx represents the maximum size of total heap space-Xms32g-Xmx32g#-Xms4g#-Xmx4g################################################################## Expert settings#################################################################### All settings below this section are considered## expert settings. Don&apos;t tamper with them unless## you understand what you are doing#################################################################### GC configuration-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=75-XX:+UseCMSInitiatingOccupancyOnly## optimizations# pre-touch memory pages used by the JVM during initialization-XX:+AlwaysPreTouch## basic# force the server VM (remove on 32-bit client JVMs)-server# explicitly set the stack size (reduce to 320k on 32-bit client JVMs)-Xss1m# set to headless, just in case-Djava.awt.headless=true# ensure UTF-8 encoding by default (e.g. filenames)-Dfile.encoding=UTF-8# use our provided JNA always versus the system one-Djna.nosys=true# use old-style file permissions on JDK9-Djdk.io.permissionsUseCanonicalPath=true# flags to configure Netty-Dio.netty.noUnsafe=true-Dio.netty.noKeySetOptimization=true-Dio.netty.recycler.maxCapacityPerThread=0# log4j 2-Dlog4j.shutdownHookEnabled=false-Dlog4j2.disable.jmx=true-Dlog4j.skipJansi=true## heap dumps# generate a heap dump when an allocation from the Java heap fails# heap dumps are created in the working directory of the JVM-XX:+HeapDumpOnOutOfMemoryError# specify an alternative path for heap dumps# ensure the directory exists and has sufficient space#-XX:HeapDumpPath=$&#123;heap.dump.path&#125;## GC logging#-XX:+PrintGCDetails#-XX:+PrintGCTimeStamps#-XX:+PrintGCDateStamps#-XX:+PrintClassHistogram#-XX:+PrintTenuringDistribution#-XX:+PrintGCApplicationStoppedTime# log GC status to a file with time stamps# ensure the directory exists#-Xloggc:$&#123;loggc&#125;# By default, the GC log file will not rotate.# By uncommenting the lines below, the GC log file# will be rotated every 128MB at most 32 times.#-XX:+UseGCLogFileRotation#-XX:NumberOfGCLogFiles=32#-XX:GCLogFileSize=128M# Elasticsearch 5.0.0 will throw an exception on unquoted field names in JSON.# If documents were already indexed with unquoted fields in a previous version# of Elasticsearch, some operations may throw errors.## WARNING: This option will be removed in Elasticsearch 6.0.0 and is provided# only for migration purposes.#-Delasticsearch.json.allow_unquoted_field_names=true Elasticsearch最佳优化角色划分 es分为三种角色： master、client、data，三种角色根据elasticsearch.yml配置中node.master、node.data区分，分别为true false、false false、true true master： 该节点不和应用创建连接，主要用于元数据(metadata)的处理，比如索引的新增、删除、分片分配等，master节点不占用io和cpu，内存使用量一般 client： 该节点和检索应用创建连接、接受检索请求，但其本身不负责存储数据，可当成负载均衡节点，client节点不占用io、cpu、内存 data： 该节点和索引应用创建连接、接受索引请求，该节点真正存储数据，es集群的性能取决于该节点个数（每个节点最优配置情况下），data节点会占用大量的cpu、io、内存 各节点间关系： master节点具备主节点的选举权，主节点控制整个集群元数据。client节点接受检索请求后将请求转发到与查询条件相关的的data节点的分片上，data节点的分片执行查询语句获得查询结果后将结果反馈至client，在client对数据进行聚合、排序等操作将最终结果返回给上层请求 资源规划 master节点： 只需部署三个节点，每个节点jvm分配2-10G，根据集群大小决定 client节点： 增加client节点可增加检索并发,但检索的速度还是取决于查询所命中的分片个数以及分片中的数据量。如果不清楚检索并发，初始节点数可设置和data节点数一致，每个节点jvm分配2-10 data节点： ①单个索引在一个data节点上分片数保持在3个以内；②每1GB堆内存对应集群的分片保持在20个以内；③每个分片不要超过30G。 data节点经验： 如果单索引每个节点可支撑90G数据，依此可计算出所需data节点数 。 如果是多索引按照单个data节点jvm内存最大30G来计算，一个节点的分片保持在600个以内，存储保持在18T以内。 主机的cpu、io固定，建议一台主机只部署一个data节点，不同角色节点独立部署，方便扩容 每条数据保持在2k以下索引性能大约3000-5000条/s/data节点，增加data节点数可大幅度增加索引速率，节点数与索引效率的增长关系呈抛物线形状 优秀的插件与工具 ik分词器： es默认分词器只支持英文分词，ik分词器支持中文分词 head数据查询工具： 类似于mysql的Navicat logstash： 数据处理管。采样各种样式、大小的数据来源，实时解析和转换数据，选择众多输出目标导出数据 x-pack性能监控： 获取进程运行时资源与状态信息并存储至es中。可通过kibana查看es、logstash性能指标，试用版包括集群状态、延迟、索引速率、检索速率、内存、cpu、io、磁盘、文件量等还可以看到集群数据负载均衡时的情况。商用版还支持安全、告警等功能 kibana可视化工具： es的可视化工具可制作各种图表，可在该工具上执行dsl语句灵活操作es es-sql： 用sql查询elasticsearch的工具，将封装复杂的dsl语句封装成sql beats： 轻量级的数据采集工具，可监控网络流量、日志数据、进程信息（负载、内存、磁盘等），支持docker镜像的file采集 repository-hdfs： 该插件支持将es中离线数据转存至hdfs中长期存储 ​ Elasticsearch优化经验 参数调优 开启内存锁，禁止swapping 执行linux命令(临时生效) 1ulimit -l unlimited 修改主机配置：/etc/security/limits.conf 12* soft memlock unlimited* hard memlock unlimited 修改es配置：config/elasticsearch.yml 1bootstrap.memory_lock : true 调大文件描述符数量 执行linux命令(临时生效) 1ulimit -n 65535 修改linux配置文件：/etc/security/limits.conf 12* soft nofile 65536* hard nofile 65536 调大最大映射数 执行linux命令(临时生效) 1sysctl -w vm.max_map_count=262144 修改linux配置文件：/etc/sysctl.conf 1vm.max_map_count=262144 索引配置 settings:{efresh_interval}：数据写入刷新间隔，默认1s，调整增加该值可以减少写入压力、增加写入速度，如设为60 12345&#123;&quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;60s&quot;&#125;&#125; mappings:{dynamic}： 禁止es自动创建字段，仅允许预先设定好的字段存入es，防止索引结构混乱 1234567&#123; &quot;mappings&quot;: &#123; &quot;mytype&quot;: &#123; &quot;dynamic&quot;: false &#125; &#125;&#125; _all：建议禁用 12345&#123;&quot;_all&quot;: &#123; &quot;enable&quot;: false &#125;&#125; keyword字段属性: ingore_above超过多少字符不写入，keyword一般用于精确查询，不能写入太长。 123456&#123;&quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ingore_above&quot;: 1000 &#125; &#125; index属性：将 不作为查询字段的index值设为false 12345678&#123; &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: &quot;false&quot; &#125; &#125;&#125; JVM内存溢出处理 123防止es节点内存溢出后处于僵死状态且无法恢复，影响整个集群，在进程出现OOM时让进程宕掉，退出ES集群并引发告警，然后重启。在config/jvm.options中增加JVM启动参数： 1-XX:+ExitOnOutOfMemoryError 该参数在jdk 1.8.0_92版本上线 数据生命周期 es中的开启状态的索引都会占用堆内存来存储倒排索引，过多的索引会导致集群整体内存使用率多大，甚至引起内存溢出。所以需要根据自身业务管理历史数据的生命周期，如近3个月的数据开启用于快速查询；过去3-6月的数据索引关闭以释放内存，需要时再开启；超过6个月的可以生成快照保存至hdfs并删除索引，需要的时候从hdfs选择需要的索引恢复至集群中进行查询 生产上常常使用logstash+索引模板的方式按照一定时间创建新的索引，例如按天创建索引，索引的命名可能是index-yyyy-mm-dd，每天生产不同的索引，清除历史数据时可直接关闭或删除 需要注意的是：如何按照logstash默认的时间分割索引会有8个小时的误差，所以需要在logstash中将真实数据中的时间字段作为分割条件，保障按照业务时间分割索引 路由查询 在将数据写入es时，指定一个字段作为路由字段，es会将该字段进行hash计算写入到对应的分片上；查询时根据查询条件中的路由值，直接查找所在的分片，大幅度提高查询速度。 需要注意的是：路由字段必须是随机分布，否则会导致分片数据不平均引发的主机存储使用不平均，可以作为路由字段的：如业务流水、省份、系统编码等。 过滤器 ES中的查询操作分为2种：查询（query）和过滤（filter），查询默认会计算每个返回文档的得分，然后根据得分排序；而过滤（filter）只会筛选出符合的文档，并不计算得分，且它可以缓存文档。单从性能考虑，过滤比查询更快而且更节省io资源。过滤适合在大范围筛选数据，而查询则适合精确匹配数据。开发时应先使用过滤操作过滤数据，然后使用查询匹配数据 查询限制 限制是为了保证es集群的稳定性。限制的内容包括：查询范围、单次查询数量等，过大的查询范围不仅会导致查询效率低，而且会是es集群资源耗费急剧增加，甚至引起es集群崩溃；单次查询数量限制是为了保证内存不会被查询内存大量占用，就是分页原理，es默认可以查询10000条数据 批量导入 如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0关闭副本。把每个索引的 index.refresh_interval 改到 -1关闭刷新。导入完毕后再开启副本和刷新 YAMLelasticsearch cluster(2client+3master+2data) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249---apiVersion: v1kind: ServiceAccountmetadata: labels: elastic-app: elasticsearch name: elasticsearch-admin namespace: elk-test---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: elasticsearch-admin labels: elastic-app: elasticsearchroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: elasticsearch-admin namespace: elk-test---kind: ServiceapiVersion: v1metadata: labels: elastic-app: elasticsearch name: elasticsearch-discovery namespace: elk-testspec: ports: - port: 9300 targetPort: 9300 selector: elastic-app: elasticsearch role: master---kind: ServiceapiVersion: v1metadata: labels: elastic-app: elasticsearch-service name: elasticsearch-service namespace: elk-testspec: ports: - port: 9200 targetPort: 9200 selector: elastic-app: elasticsearch role: client type: NodePort---kind: DeploymentapiVersion: apps/v1metadata: labels: elastic-app: elasticsearch role: client name: elasticsearch-client namespace: elk-testspec: replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: elasticsearch role: client template: metadata: labels: elastic-app: elasticsearch role: client spec: containers: - name: elasticsearch-client image: docker.elastic.co/elasticsearch/elasticsearch:6.8.1 lifecycle: postStart: exec: command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;sysctl -w vm.max_map_count=262144; ulimit -l unlimited;&quot;] ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: &quot;node.name&quot; value: &quot;$&#123;HOSTNAME&#125;&quot; - name: &quot;cluster.name&quot; value: &quot;elasticsearch-cluster&quot; - name: &quot;discovery.zen.ping.unicast.hosts&quot; value: &quot;elasticsearch-discovery&quot; - name: &quot;node.master&quot; value: &quot;false&quot; - name: &quot;node.data&quot; value: &quot;false&quot; - name: &quot;ES_JAVA_OPTS&quot; value: &quot;-Xms512m -Xmx512m&quot; - name: &quot;http.cors.enabled&quot; value: &quot;true&quot; - name: &quot;http.cors.allow-origin&quot; value: &quot;*&quot; securityContext: privileged: true resources: requests: memory: 0.3Gi cpu: 0.2 limits: memory: 1.0Gi cpu: 0.5 serviceAccountName: elasticsearch-admin---kind: DeploymentapiVersion: apps/v1metadata: labels: elastic-app: elasticsearch role: master name: elasticsearch-master namespace: elk-testspec: replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: elasticsearch role: master template: metadata: labels: elastic-app: elasticsearch role: master spec: containers: - name: elasticsearch-master image: docker.elastic.co/elasticsearch/elasticsearch:6.8.1 lifecycle: postStart: exec: command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;sysctl -w vm.max_map_count=262144; ulimit -l unlimited;&quot;] ports: - containerPort: 9300 protocol: TCP env: - name: &quot;node.name&quot; value: &quot;$&#123;HOSTNAME&#125;&quot; - name: &quot;cluster.name&quot; value: &quot;elasticsearch-cluster&quot; - name: &quot;discovery.zen.ping.unicast.hosts&quot; value: &quot;elasticsearch-discovery&quot; - name: &quot;discovery.zen.minimum_master_nodes&quot; value: &quot;2&quot; - name: &quot;discovery.zen.ping_timeout&quot; value: &quot;5s&quot; - name: &quot;node.master&quot; value: &quot;true&quot; - name: &quot;node.data&quot; value: &quot;false&quot; - name: &quot;node.ingest&quot; value: &quot;false&quot; - name: &quot;ES_JAVA_OPTS&quot; value: &quot;-Xms512m -Xmx512m&quot; securityContext: privileged: true resources: requests: memory: 0.3Gi cpu: 0.3 limits: memory: 1.0Gi cpu: 0.5 serviceAccountName: elasticsearch-admin---kind: StatefulSetapiVersion: apps/v1metadata: labels: elastic-app: elasticsearch role: data name: elasticsearch-data namespace: elk-testspec: replicas: 2 selector: matchLabels: elastic-app: elasticsearch role: data template: metadata: labels: elastic-app: elasticsearch role: data spec: containers: - name: elasticsearch-data image: docker.elastic.co/elasticsearch/elasticsearch:6.8.1 lifecycle: postStart: exec: command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;sysctl -w vm.max_map_count=262144; ulimit -l unlimited;&quot;] ports: - containerPort: 9300 protocol: TCP volumeMounts: - name: es-data mountPath: /usr/share/elasticsearch/data env: - name: &quot;node.name&quot; value: &quot;$&#123;HOSTNAME&#125;&quot; - name: &quot;cluster.name&quot; value: &quot;elasticsearch-cluster&quot; - name: &quot;discovery.zen.ping.unicast.hosts&quot; value: &quot;elasticsearch-discovery&quot; - name: &quot;node.master&quot; value: &quot;false&quot; - name: &quot;node.data&quot; value: &quot;true&quot; - name: &quot;ES_JAVA_OPTS&quot; value: &quot;-Xms1024m -Xmx1024m&quot; securityContext: privileged: true resources: requests: memory: 1.0Gi cpu: 1 limits: memory: 2.0Gi cpu: 2 serviceAccountName: elasticsearch-admin serviceName: elasticsearch-data volumeClaimTemplates: - metadata: name: es-data creationTimestamp: null annotations: volume.beta.kubernetes.io/storage-class: nfs-rw volume.beta.kubernetes.io/storage-provisioner: flexvolume-huawei.com/fuxinfs enable: true spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi elasticsearch(3data node statefulset) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127apiVersion: apps/v1kind: StatefulSetmetadata: name: es-cluster namespace: elasticsearchspec: serviceName: elasticsearch replicas: 3 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: &quot;app&quot; operator: In values: - elasticsearch topologyKey: &quot;kubernetes.io/hostname&quot; imagePullSecrets: - name: registry-secret containers: - name: elasticsearch image: registry.cn-beijing.aliyuncs.com/rj-bai/elasticsearch:6.5.4 imagePullPolicy: IfNotPresent livenessProbe: tcpSocket: port: 9200 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 readinessProbe: tcpSocket: port: 9200 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 resources: limits: cpu: &quot;1000m&quot; memory: &quot;2Gi&quot; requests: cpu: &quot;500m&quot; memory: &quot;1Gi&quot; ports: - containerPort: 9200 name: es-http protocol: TCP - containerPort: 9300 name: cluster-port protocol: TCP volumeMounts: - mountPath: /usr/share/elasticsearch/data name: data-elasticsearch env: - name: REQUESTS_MEMORY valueFrom: resourceFieldRef: resource: requests.memory divisor: 1Mi - name: cluster.name value: es-cluster - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: http.cors.enabled value: &quot;true&quot; - name: http.cors.allow-origin value: &quot;*&quot; - name: discovery.zen.minimum_master_nodes value: &quot;2&quot; - name: discovery.zen.fd.ping_timeout value: &quot;120s&quot; - name: discovery.zen.fd.ping_retries value: &quot;6&quot; - name: discovery.zen.fd.ping_interval value: &quot;30s&quot; - name: ES_JAVA_OPTS value: &quot;-Xms$(REQUESTS_MEMORY)m -Xmx$(REQUESTS_MEMORY)m&quot; - name: discovery.zen.ping.unicast.hosts value: &quot;es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch&quot; initContainers: - name: vm-max-map image: busybox imagePullPolicy: IfNotPresent command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;] securityContext: privileged: true - name: fd-ulimit image: busybox imagePullPolicy: IfNotPresent command: [&quot;sh&quot;, &quot;-c&quot;, &quot;ulimit -n 65535&quot;] securityContext: privileged: true volumeClaimTemplates: - metadata: name: data-elasticsearch spec: accessModes: [ &quot;ReadWriteOnce&quot; ] resources: requests: storage: 5Gi---kind: ServiceapiVersion: v1metadata: name: elasticsearch namespace: elasticsearch labels: app: elasticsearchspec: type: NodePort selector: app: elasticsearch ports: - port: 9200 name: es-http nodePort: 39200 crobjob定期清理历史数据 123456789101112131415161718192021222324252627282930apiVersion: batch/v1beta1kind: CronJobmetadata: name: elasticsearch labels: app.kubernetes.io/name: elasticsearch helm.sh/chart: elasticsearch-0.1.0 app.kubernetes.io/instance: elasticsearch app.kubernetes.io/managed-by: Tillerspec: schedule: &quot;0 1 * * *&quot; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: hello image: &quot;192.168.101.88:5000/busybox:1.29.3&quot; imagePullPolicy: IfNotPresent command: - &quot;sh&quot; - &quot;-c&quot; - &gt; history=$(date -D &apos;%s&apos; +&quot;%Y.%m.%d&quot; -d &quot;$(( `date +%s`-60*60*24*6 ))&quot;); echo $&#123;history&#125;; hostname=elasticsearch.ns-monitor; echo $&#123;hostname&#125;; echo -ne &quot;DELETE /*$&#123;history&#125;* HTTP/1.1\r\nHost: $&#123;hostname&#125;\r\n\r\n&quot; | nc -v -i 1 $&#123;hostname&#125; 9200 restartPolicy: OnFailure 脚本定期清理历史数据 1234567891011121314151617#!/bin/bashset -e#set -xLAST_DATE=`date -d &quot;-7 days&quot; &quot;+%Y-%m-%d&quot;`LAST_TIMESTAMP=`date -d &quot;$LAST_DATE&quot; +%s`indexs=`curl http://192.168.0.250:31191/_cat/indices|awk &apos;$3~/server01|server02/ &#123;print $3&#125;&apos;`for index in $indexs;do index_time=`echo $index|awk -F - &apos;&#123;print $NF&#125;&apos;|tr &apos;.&apos; &apos;-&apos;` index_timestamp=`date -d &quot;$index_time&quot; +%s` if [ $index_timestamp -lt $LAST_TIMESTAMP ];then curl -XDELETE &quot;http://192.168.0.250:31191/$index&quot; &amp;&gt; /dev/null fidone 部署1kubectl apply -f es-kubernetes.yaml Kibana官方文档：https://www.elastic.co/guide/en/kibana/current/ GitHub：https://github.com/elastic/kibana YAML12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152---kind: ServiceapiVersion: v1metadata: labels: elastic-app: kibana name: kibana-service namespace: elk-testspec: ports: - port: 5601 targetPort: 5601 selector: elastic-app: kibana type: NodePort---kind: DeploymentapiVersion: apps/v1metadata: name: kibana namespace: elk-test labels: elastic-app: kibanaspec: replicas: 1 selector: matchLabels: elastic-app: kibana template: metadata: name: kibana labels: elastic-app: kibana spec: containers: - name: kibana image: &apos;docker.elastic.co/kibana/kibana:6.8.1&apos; ports: - name: http containerPort: 5601 protocol: TCP env: - name: ELASTICSEARCH_URL value: &apos;http://elasticsearch-service:9200&apos; resources: limits: cpu: 500m memory: 1Gi requests: cpu: 100m memory: 256Mi imagePullPolicy: IfNotPresent 部署1kubectl apply -f kibana-kubernetes.yaml 参考资料 https://cloud.tencent.com/developer/article/1189282 https://blog.csdn.net/chenleiking/article/details/79453460 https://blog.csdn.net/miss1181248983/article/details/89492918 https://blog.rj-bai.com/post/157.html https://www.jianshu.com/p/b264b6cf9340]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[terraform创建华为云、AWS资源]]></title>
    <url>%2Fterraform%E5%88%9B%E5%BB%BA%E5%8D%8E%E4%B8%BA%E4%BA%91%20AWS%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[前言目前IT设备对于大多数企业公司来说是必不可少的基础设施，由于云计算的按需付费，便捷配置计算资源（资源包括网络，服务器，存储，应用软件，服务）的特点很受企业的欢迎，因此大多数企业都从传统的IDC迁移到云平台，或者直接到云平台上搭建自己的业务。然而云计算平台很多家，国外有云计算鼻祖亚马AWS，还有微软的Azure，Google的GCP，国内而言比较有名的，阿里巴巴的Alicloud，腾讯的TencentCloud，华为的HuaweiCloud。 在运维领域中，针对这些云平台的IT资源管理也是重中之重。尤其是最近几逐渐流行的DevOps理念，Infrastructure as Code（基础架构即代码）文化，所以在社区的贡献下，Terraform这个工具应用而生，Terraform是来自HashiCorp家族，因此采用了 HashiCorp 配置语言 (HCL)，Terraform的意义在于，通过同一套规则和命令来操作不同的云平台（包括私有云）。详情可参考官方文档https://www.terraform.io。 参考： https://bbs.huaweicloud.com/blogs/86505eb5eefa11e8bd5a7ca23e93a891 https://www.jianshu.com/p/d0035b7c3801 https://www.linode.com/docs/applications/configuration-management/create-terraform-module/ 安装terraform以下是官方安装教程https://www.terraform.io/intro/getting-started/install.html，下载解压，配置环境变量即可。以下是我的环境 12345678910111213141516[centos@ip-10-20-6-165 ~]$ uname -r3.10.0-514.26.2.el7.x86_64[centos@ip-10-20-6-165 ~]$ cat /etc/redhat-releaseCentOS Linux release 7.3.1611 (Core)[centos@ip-10-20-6-165 ~]$ ll /opt/terraform*-rwxrwxr-x 1 centos centos 69122624 Apr 10 2018 /opt/terraform-rw-r--r-- 1 root root 16490308 Apr 10 2018 /opt/terraform_0.11.7_linux_amd64.zip[centos@ip-10-20-6-165 ~]$ grep opt /etc/profileexport PATH=$PATH:/opt[centos@ip-10-20-6-165 ~]$ which terraform/opt/terraform[centos@ip-10-20-6-165 ~]$ terraform --versionTerraform v0.11.7Your version of Terraform is out of date! The latest versionis 0.11.9. You can update by downloading from www.terraform.io/downloads.html 编写配置目录配置12345678910111213141516terraform_scripts/├── main.tf #主执行文件├── outputs.tf #定义输出变量├── secrets.tfvars #.....├── terraform #.....├── terraform.tfvars #直接定义变量文件（变量需要在variables.tf事先定义）├── variables.tf #变量文件└── modules/ #模块 ├── network/ │ ├── main.tf │ ├── variables.tf │ └── outputs.tf └── instance/ ├── main.tf ├── variables.tf └── outputs.tf 模块化脚本（华为云）network模块network/variables.tf 123456789101112131415161718variable &quot;vpc_name&quot; &#123; &#125;variable &quot;vpc_cidr&quot; &#123; default = &quot;192.168.0.0/16&quot;&#125;variable &quot;subnet_name&quot; &#123; &#125;variable &quot;subnet_dir&quot; &#123; default = &quot;192.168.2.0/24&quot;&#125;variable &quot;subnet_gateway_ip&quot; &#123; default = &quot;192.168.2.1&quot;&#125;variable &quot;secgroup_name&quot; &#123; &#125; network/outputs.tf 123456output &quot;vpc_id&quot; &#123; value = &quot;$&#123;huaweicloud_vpc_v1.vpc.id&#125;&quot;&#125;output &quot;secgroup_1_name&quot; &#123; value = &quot;$&#123;huaweicloud_networking_secgroup_v2.secgroup_1.name&#125;&quot;&#125; network/main.tf 123456789101112131415161718#-----VPCresource &quot;huaweicloud_vpc_v1&quot; &quot;vpc&quot; &#123; name = &quot;$&#123;var.vpc_name&#125;&quot; cidr = &quot;$&#123;var.vpc_cidr&#125;&quot;&#125;#----subnet-----resource &quot;huaweicloud_vpc_subnet_v1&quot; &quot;subnet&quot; &#123; name = &quot;$&#123;var.subnet_name&#125;&quot; cidr = &quot;$&#123;var.subnet_cidr&#125;&quot; gateway_ip = &quot;$&#123;var.subnet_gateway_ip&#125;&quot; vpc_id = &quot;$&#123;huaweicloud_vpc_v1.vpc.id&#125;&quot;&#125;#----secgroupresource &quot;huaweicloud_networking_secgroup_v2&quot; &quot;secgroup_1&quot; &#123; name = &quot;$&#123;var.secgroup_name&#125;&quot;&#125; instance模块instance/variables.tf 123456789101112131415161718192021variable &quot;instance_sg&quot; &#123; &#125;variable &quot;instance_name&quot; &#123; &#125;variable &quot;instance_az&quot; &#123; default = &quot;cn-east-2b&quot;&#125;vaiable &quot;instance_image_id&quot; &#123; default = &quot;9526f9b7-423c-4fdc-92ad-f1630a524652&quot; description = &quot;CentOS7.6 64bit&quot;&#125;variable &quot;instance_flavor_name&quot; &#123; default = &quot;s3.small.1&quot;&#125;variable &quot;instance_network_id&quot; &#123; &#125;variable &quot;instance_admin_pass&quot; &#123;&#125; instance/outputs.tf 12 instance/main.tf 123456789101112#-----ECSresources &quot;huaweicloud_compute_instance_v2&quot; &quot;instance&quot; &#123; availability_zone = &quot;$&#123;var.instance_az&#125;&quot; name = &quot;$&#123;var.instance_name&#125;&quot; image_id = &quot;$&#123;var.instance_image_id&#125;&quot; flavor_name = &quot;$&#123;var.instance_flavor_name&#125;&quot; security_groups = &quot;$&#123;var.instance_sg&#125;&quot; admin_pass = &quot;$&#123;var.instance_admin_pass&#125;&quot; network &#123; name = &quot;$&#123;var.instance_network_id&#125;&quot; #not sure, maybe should be uuid &#125;&#125; 主执行文件variables.tf 12345678910111213variable &quot;secret_key&quot; &#123; &#125;variable &quot;access_key&quot; &#123; &#125;variable &quot;region&quot; &#123; default = &quot;cn-east-2&quot; description = &quot;string&quot;&#125;variable &quot;env&quot; &#123; &#125; terraform.tfvars 123secret_key = &quot;string&quot;access_key = &quot;string&quot;env = &quot;test&quot; main.tf 12345678910111213141516171819202122#---providerprovider &quot;huaweicloud&quot; &#123; access_key = &quot;$&#123;var.access_key&#125;&quot; secret_key = &quot;$&#123;var.secret_key&#125;&quot; tenant_name = &quot;tenant&quot; region = &quot;$&#123;var.region&#125;&quot; auth_url = &quot;https://iam.myhwclouds.com:443/v3&quot;&#125;module &quot;network&quot; &#123; source = &quot;./modules/network&quot; vpc_name = &quot;vpc-$&#123;var.env&#125;&quot; subnet_name = &quot;subnet-$&#123;var.env&#125;&quot; secgroup_name = &quot;sg-$&#123;var.env&#125;&quot;&#125;module &quot;instance&quot; &#123; source = &quot;./modules/instance&quot; instance_name = &quot;test&quot; instance_flavor_name = &quot;s3.small.1&quot; instance_security_group = &quot;$&#123;modules.network.secgroup_name&#125;&quot; instance_network_id = &quot;$&#123;modules.network.vpc_id&#125;&quot; instance_admin_pass &quot;$&#123;var.admin_pass&#125;&quot;&#125; AWS基本使用脚本（非模块化）目录结构123456789[centos@ip-10-20-6-165 stg_cn_rubin]$ lltotal 32-rw-rw-r-- 1 centos centos 285 Sep 21 07:24 backend.tf-rw-rw-r-- 1 centos centos 1723 Sep 25 07:54 bastion.tf-rw-rw-r-- 1 centos centos 977 Sep 25 08:10 common.tf-rw-rw-r-- 1 centos centos 593 Sep 25 09:27 outputs.tf-rw-rw-r-- 1 centos centos 173 Sep 21 07:24 terraform.tfvars-rw-rw-r-- 1 centos centos 901 Sep 25 09:48 variables.tf-rw-rw-r-- 1 centos centos 6676 Oct 20 03:09 vpc.tf backend.tf这个文件是定义provider和远程存储terraform.tfstate的s3存储桶，provider是Terraform定制的一套接口，阿里云、AWS、私有云等如果想接入进来被Terraform编排和管理就要实现一套Provider，官网https://www.terraform.io/docs/providers/index.html，我使用的AWS的云平台，所以使用的AWS的provider。 关于terraform.tfstate，我在前言中说过，状态文件是记录当前资源的状态。每次运行terraform apply时，都会把最新的配置和当前状态文件中的内容进行比较后，再做更改。所以这个文件不能乱改，团队协作时可以加个锁，因此手动通过AWS console添加了一个存储桶，然后再加了一个dynamodb_table的锁。 123456789101112131415[centos@ip-10-20-6-165 stg_cn_rubin]$ cat backend.tfprovider &quot;aws&quot; &#123; region = &quot;$&#123;var.aws_region&#125;&quot; # 引用了变量，变量值在variables.tf中&#125;terraform &#123; required_version = &quot;&gt;= 0.11.7&quot; backend &quot;s3&quot; &#123; encrypt = &quot;true&quot; bucket = &quot;rubin-cn-stg-terraform-state&quot; region = &quot;cn-north-1&quot; key = &quot;vpc/stg_cn_rubin/terraform.tfstate&quot; dynamodb_table = &quot;terraform-lock&quot; &#125;&#125; variables.tf这个文件相当于申明了variable，可以在此加一个默认的值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[centos@ip-10-20-6-165 stg_cn_rubin]$ cat variables.tfvariable &quot;aws_region&quot; &#123; default = &quot;cn-north-1&quot;&#125;variable &quot;vpc_name&quot; &#123; description = &quot;The name of the VPC&quot;&#125;variable &quot;cidr_numeral&quot; &#123; description = &quot;The VPC CIDR numeral (10.x.0.0/16)&quot;&#125;variable &quot;cidr_numeral_public&quot; &#123; default = &#123; &quot;0&quot; = &quot;0&quot; &quot;1&quot; = &quot;1&quot; &quot;2&quot; = &quot;2&quot; &#125;&#125;variable &quot;cidr_numeral_private&quot; &#123; default = &#123; &quot;0&quot; = &quot;3&quot; &quot;1&quot; = &quot;4&quot; &quot;2&quot; = &quot;5&quot; &#125;&#125;variable &quot;cidr_numeral_private_db&quot; &#123; default = &#123; &quot;0&quot; = &quot;6&quot; &quot;1&quot; = &quot;7&quot; &quot;2&quot; = &quot;8&quot; &#125;&#125;variable &quot;cidr_numeral_private_emr&quot; &#123; default = &#123; &quot;0&quot; = &quot;9&quot; &quot;1&quot; = &quot;10&quot; &quot;2&quot; = &quot;11&quot; &#125;&#125;variable &quot;ssh_key_name&quot; &#123; description = &quot;A master ssh key&quot;&#125;variable &quot;bastion_image&quot; &#123; default = &#123; cn-north-1 = &quot;ami-7866b115&quot; &#125;&#125;variable &quot;env&quot; &#123; description = &quot;The AWS env tag&quot;&#125;variable &quot;availability_zones&quot; &#123; description = &quot;A comma-delimited list of availability zones for the VPC.&quot;&#125; terraform.tfvars此文件是在文件申明的变量赋值，赋值可以供当前目录的其他的文件引用 1234567[centos@ip-10-20-6-165 stg_cn_rubin]$ cat terraform.tfvarsaws_region = &quot;cn-north-1&quot;cidr_numeral = &quot;101&quot;vpc_name = &quot;rubin_stg_cn&quot;ssh_key_name = &quot;rubin-stg-cn-master&quot;env = &quot;staging&quot;availability_zones = &quot;cn-north-1a,cn-north-1b&quot; vpc.tf这个文件就是主文件，用来建立VPC，subnet，gateway，natgateway，路由表，路由表关联等一系列操作。其中有些是Terraform的内部函数，关于内部函数可参考https://www.terraform.io/docs/configuration/interpolation.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205[centos@ip-10-20-6-165 stg_cn_rubin]$ cat vpc.tf # VPC DESIGNresource &quot;aws_vpc&quot; &quot;default&quot; &#123; cidr_block = &quot;10.$&#123;var.cidr_numeral&#125;.0.0/16&quot; enable_dns_hostnames = true tags &#123; Name = &quot;vpc-$&#123;var.vpc_name&#125;&quot; &#125;&#125;resource &quot;aws_internet_gateway&quot; &quot;default&quot; &#123; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; tags &#123; Name = &quot;igw-$&#123;var.vpc_name&#125;&quot; &#125;&#125;resource &quot;aws_eip&quot; &quot;nat&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc = true tags &#123; Name = &quot;ip-NAT-$&#123;var.vpc_name&#125;&quot; &#125;&#125;resource &quot;aws_nat_gateway&quot; &quot;nat&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; allocation_id = &quot;$&#123;element(aws_eip.nat.*.id, count.index)&#125;&quot; subnet_id = &quot;$&#123;element(aws_subnet.public.*.id, count.index)&#125;&quot; tags &#123; Name = &quot;gw-NAT-$&#123;var.vpc_name&#125;&quot; &#125;&#125;# PUBLIC SUBNETS# The public subnet is where the bastion, NATs and ELBs reside. In most cases,# there should not be any servers in the public subnet.resource &quot;aws_subnet&quot; &quot;public&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; cidr_block = &quot;10.$&#123;var.cidr_numeral&#125;.$&#123;lookup(var.cidr_numeral_public, count.index)&#125;.0/24&quot; availability_zone = &quot;$&#123;element(split(&quot;,&quot;, var.availability_zones), count.index)&#125;&quot; map_public_ip_on_launch = true tags &#123; Name = &quot;public$&#123;count.index&#125;-$&#123;var.vpc_name&#125;&quot; immutable_metadata = &quot;&#123; \&quot;purpose\&quot;: \&quot;external_$&#123;var.vpc_name&#125;\&quot;, \&quot;target\&quot;: null &#125;&quot; &#125;&#125;# PUBLIC SUBNETS - Default route#resource &quot;aws_route_table&quot; &quot;public&quot; &#123; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; route &#123; cidr_block = &quot;0.0.0.0/0&quot; gateway_id = &quot;$&#123;aws_internet_gateway.default.id&#125;&quot; &#125; tags &#123; Name = &quot;publicrt-$&#123;var.vpc_name&#125;&quot; &#125;&#125;# PUBLIC SUBNETS - Route associations#resource &quot;aws_route_table_association&quot; &quot;public&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; subnet_id = &quot;$&#123;element(aws_subnet.public.*.id, count.index)&#125;&quot; route_table_id = &quot;$&#123;aws_route_table.public.id&#125;&quot;&#125;# PRIVATE SUBNETS## Route Tables in a private subnet will not have Route resources created# statically for them as the NAT instances are responsible for dynamically# managing them on a per-AZ level using the Network=Private tag.resource &quot;aws_subnet&quot; &quot;private&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; cidr_block = &quot;10.$&#123;var.cidr_numeral&#125;.$&#123;lookup(var.cidr_numeral_private, count.index)&#125;.0/24&quot; availability_zone = &quot;$&#123;element(split(&quot;,&quot;, var.availability_zones), count.index)&#125;&quot; tags &#123; Name = &quot;private$&#123;count.index&#125;-$&#123;var.vpc_name&#125;&quot; immutable_metadata = &quot;&#123; \&quot;purpose\&quot;: \&quot;internal_$&#123;var.vpc_name&#125;\&quot;, \&quot;target\&quot;: null &#125;&quot; Network = &quot;Private&quot; &#125;&#125;resource &quot;aws_route_table&quot; &quot;private&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; route &#123; cidr_block = &quot;0.0.0.0/0&quot; nat_gateway_id = &quot;$&#123;element(aws_nat_gateway.nat.*.id, count.index)&#125;&quot; &#125; tags &#123; Name = &quot;private$&#123;count.index&#125;rt-$&#123;var.vpc_name&#125;&quot; Network = &quot;Private&quot; &#125;&#125;resource &quot;aws_route_table_association&quot; &quot;private&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; subnet_id = &quot;$&#123;element(aws_subnet.private.*.id, count.index)&#125;&quot; route_table_id = &quot;$&#123;element(aws_route_table.private.*.id, count.index)&#125;&quot;&#125;# PRIVATE SUBNETS (DB)## Route Tables in a private subnet will not have Route resources created# statically for them as the NAT instances are responsible for dynamically# managing them on a per-AZ level using the Network=Private tag.resource &quot;aws_subnet&quot; &quot;private_db&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; cidr_block = &quot;10.$&#123;var.cidr_numeral&#125;.$&#123;lookup(var.cidr_numeral_private_db, count.index)&#125;.0/24&quot; availability_zone = &quot;$&#123;element(split(&quot;,&quot;, var.availability_zones), count.index)&#125;&quot; tags &#123; Name = &quot;db-private$&#123;count.index&#125;-$&#123;var.vpc_name&#125;&quot; immutable_metadata = &quot;&#123; \&quot;purpose\&quot;: \&quot;internal_$&#123;var.vpc_name&#125;\&quot;, \&quot;target\&quot;: null &#125;&quot; Network = &quot;Private&quot; &#125;&#125;resource &quot;aws_route_table&quot; &quot;private_db&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; route &#123; cidr_block = &quot;0.0.0.0/0&quot; nat_gateway_id = &quot;$&#123;element(aws_nat_gateway.nat.*.id, count.index)&#125;&quot; &#125; tags &#123; Name = &quot;privatedb$&#123;count.index&#125;rt-$&#123;var.vpc_name&#125;&quot; Network = &quot;Private&quot; &#125;&#125;resource &quot;aws_route_table_association&quot; &quot;private_db&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; subnet_id = &quot;$&#123;element(aws_subnet.private_db.*.id, count.index)&#125;&quot; route_table_id = &quot;$&#123;element(aws_route_table.private_db.*.id, count.index)&#125;&quot;&#125;# PRIVATE SUBNETS## Route Tables in a private subnet will not have Route resources created# statically for them as the NAT instances are responsible for dynamically# managing them on a per-AZ level using the Network=Private tag.resource &quot;aws_subnet&quot; &quot;private_emr&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; cidr_block = &quot;10.$&#123;var.cidr_numeral&#125;.$&#123;lookup(var.cidr_numeral_private_emr, count.index)&#125;.0/24&quot; availability_zone = &quot;$&#123;element(split(&quot;,&quot;, var.availability_zones), count.index)&#125;&quot; tags &#123; Name = &quot;private_emr$&#123;count.index&#125;-$&#123;var.vpc_name&#125;&quot; immutable_metadata = &quot;&#123; \&quot;purpose\&quot;: \&quot;internal_$&#123;var.vpc_name&#125;\&quot;, \&quot;target\&quot;: null &#125;&quot; Network = &quot;Private&quot; &#125;&#125;resource &quot;aws_route_table&quot; &quot;private_emr&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; route &#123; cidr_block = &quot;0.0.0.0/0&quot; nat_gateway_id = &quot;$&#123;element(aws_nat_gateway.nat.*.id, count.index)&#125;&quot; &#125; tags &#123; Name = &quot;private_emr$&#123;count.index&#125;rt-$&#123;var.vpc_name&#125;&quot; Network = &quot;Private&quot; &#125;&#125;resource &quot;aws_route_table_association&quot; &quot;private_emr&quot; &#123; count = &quot;$&#123;length(split(&quot;,&quot;, &quot;$&#123;var.availability_zones&#125;&quot;))&#125;&quot; subnet_id = &quot;$&#123;element(aws_subnet.private_emr.*.id, count.index)&#125;&quot; route_table_id = &quot;$&#123;element(aws_route_table.private_emr.*.id, count.index)&#125;&quot;&#125; outputs.tf当我们创建的资源后，经常需要知道这些资源的ID，因此定义一个output，将我们想要的资源ID显示出来或者输出到文件，从而避免我们在去控制台上查询获取这些信息。Terraform的出参就像是存过的产出，开发人员可以在编排时定义output出参来指定自己关心的内容，该内容会在任务执行的日志中高亮显示，而且在任务执行完毕后我们可以通过terrafomr output var_name的方式查看参数结果。 1234567891011121314151617181920212223242526272829303132[centos@ip-10-20-6-165 stg_cn_rubin]$ cat outputs.tf output &quot;vpc_id&quot; &#123; value = &quot;$&#123;aws_vpc.default.id&#125;&quot;&#125;output &quot;cidr_block&quot; &#123; value = &quot;$&#123;aws_vpc.default.cidr_block&#125;&quot;&#125;output &quot;private_subnets&quot; &#123; value = &quot;$&#123;join(&quot;,&quot;, aws_subnet.private.*.id)&#125;&quot;&#125;output &quot;public_subnets&quot; &#123; value = &quot;$&#123;join(&quot;,&quot;, aws_subnet.public.*.id)&#125;&quot;&#125;output &quot;private_db_subnets&quot; &#123; value = &quot;$&#123;join(&quot;,&quot;, aws_subnet.private_db.*.id)&#125;&quot;&#125;output &quot;private_emr_subnets&quot; &#123; value = &quot;$&#123;join(&quot;,&quot;, aws_subnet.private_emr.*.id)&#125;&quot;&#125;output &quot;nat_eip&quot; &#123; value = &quot;$&#123;join(&quot;,&quot;, aws_eip.nat.*.public_ip)&#125;&quot;&#125;output &quot;bastion_eip&quot; &#123; value = &quot;$&#123;aws_eip.bastion.public_ip&#125;&quot;&#125; bastion.tf当VPC建立好的时候，我们只需要通过一台有公网IP的跳板机，去访问私网的机器，这个文件就是创建一个跳板机的安全组，能通过IP(1...*/28 )访问跳板机的22，2022端口，80，443，53，123端口流量能出，通过安全组限制网络进出流量。安全起见，我对其中IP加密显示，可以自行更改。更严格的还可以通过VPC配置的Network ACLs去控制。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[centos@ip-10-20-6-165 stg_cn_rubin]$ cat bastion.tf resource &quot;aws_security_group&quot; &quot;bastion&quot; &#123; name = &quot;bastion-$&#123;var.vpc_name&#125;&quot; description = &quot;Allows SSH access to the bastion server&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; ingress &#123; from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;1.*.*.*/28&quot;] description = &quot;Office IP&quot; &#125; ingress &#123; from_port = 2022 to_port = 2022 protocol = &quot;tcp&quot; cidr_blocks = [&quot;1.*.*.*/28&quot;] description = &quot;Office IP&quot; &#125; egress &#123; from_port = 80 to_port = 80 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; from_port = 443 to_port = 443 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; from_port = 53 to_port = 53 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; from_port = 53 to_port = 53 protocol = &quot;udp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; from_port = 123 to_port = 123 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; from_port = 123 to_port = 123 protocol = &quot;udp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; from_port = 0 to_port = 0 protocol = &quot;-1&quot; cidr_blocks = [&quot;10.0.0.0/8&quot;] &#125; tags &#123; Name = &quot;bastion-$&#123;var.vpc_name&#125;&quot; &#125;&#125;# EIP for bastionresource &quot;aws_eip&quot; &quot;bastion&quot; &#123; vpc = true&#125; common.tf这个文件是创建了一个共有的安全组给私网地址的机器使用。能接受当前VPC下私网10.0.0.0/8网段的任何流量，流量能从80，443，53，123的端口出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[centos@ip-10-20-6-165 stg_cn_rubin]$ cat common.tf resource &quot;aws_security_group&quot; &quot;internal_common&quot; &#123; name = &quot;internal_common-$&#123;var.vpc_name&#125;&quot; description = &quot;commonly used security group for ocf instances&quot; vpc_id = &quot;$&#123;aws_vpc.default.id&#125;&quot; ingress &#123; from_port = 0 to_port = 0 protocol = &quot;-1&quot; cidr_blocks = [&quot;10.0.0.0/8&quot;] &#125; egress &#123; from_port = 80 to_port = 80 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; from_port = 443 to_port = 443 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; # dns from_port = 53 to_port = 53 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; # dns from_port = 53 to_port = 53 protocol = &quot;udp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; # ntp from_port = 123 to_port = 123 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125; egress &#123; # ntp from_port = 123 to_port = 123 protocol = &quot;udp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] &#125;&#125; AWS key创建Access key 参考官网https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/id_credentials_access-keys.html配置Access key 参考官网https://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-getting-started.html 12345678910111213[centos@ip-10-20-6-165 ~]$ aws configure --profile rubin-stgAWS Access Key ID [****************M67J]: AWS Secret Access Key [****************mgNp]: Default region name [cn-north-1]: Default output format [json]: [centos@ip-10-20-6-165 ~]$ sed -n &apos;24,26p&apos; /home/centos/.aws/config [profile rubin-stg]output = jsonregion = cn-north-1[centos@ip-10-20-6-165 ~]$ sed -n &apos;25,27p&apos; /home/centos/.aws/credentials [rubin-stg]aws_access_key_id = ****************M67Jaws_secret_access_key = ****************mgNp 创建资源Terrform get &amp; init执行terraform init命令，就像git init一样，对当前目录做初始化，下载tf中的provider，并为后续的操作准备必要的环境条件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[centos@ip-10-20-6-165 ~]$ cd rubin/terraform/vpc/stg_cn_rubin/[centos@ip-10-20-6-165 stg_cn_rubin]$ pwd/home/centos/rubin/terraform/vpc/stg_cn_rubin[centos@ip-10-20-6-165 stg_cn_rubin]$ export AWS_DEFAULT_PROFILE=rubin-stg &amp;&amp; export AWS_PROFILE=rubin-stg[centos@ip-10-20-6-165 stg_cn_rubin]$ terraform --helpUsage: terraform [--version] [--help] &lt;command&gt; [args]The available commands for execution are listed below.The most common, useful commands are shown first, followed byless common or more advanced commands. If you&apos;re just gettingstarted with Terraform, stick with the common commands. For theother commands, please read the help and docs before usage.Common commands: apply Builds or changes infrastructure console Interactive console for Terraform interpolations destroy Destroy Terraform-managed infrastructure env Workspace management fmt Rewrites config files to canonical format get Download and install modules for the configuration graph Create a visual graph of Terraform resources import Import existing infrastructure into Terraform init Initialize a Terraform working directory output Read an output from a state file plan Generate and show an execution plan providers Prints a tree of the providers used in the configuration push Upload this Terraform module to Atlas to run refresh Update local state file against real resources show Inspect Terraform state or plan taint Manually mark a resource for recreation untaint Manually unmark a resource as tainted validate Validates the Terraform files version Prints the Terraform version workspace Workspace managementAll other commands: debug Debug output management (experimental) force-unlock Manually unlock the terraform state state Advanced state management[centos@ip-10-20-6-165 stg_cn_rubin]$ terraform get #编写配置文件中没有导入module，所以执行没有结果，如果配置文件有导入module应先执行[centos@ip-10-20-6-165 stg_cn_rubin]$ terraform initInitializing the backend...Initializing provider plugins...- Checking for available provider plugins on https://releases.hashicorp.com...- Downloading plugin for provider &quot;aws&quot; (1.41.0)...The following providers do not have any version constraints in configuration,so the latest version was installed.To prevent automatic upgrades to new major versions that may contain breakingchanges, it is recommended to add version = &quot;...&quot; constraints to thecorresponding provider blocks in configuration, with the constraint stringssuggested below.* provider.aws: version = &quot;~&gt; 1.41&quot;Terraform has been successfully initialized!You may now begin working with Terraform. Try running &quot;terraform plan&quot; to seeany changes that are required for your infrastructure. All Terraform commandsshould now work.If you ever set or change modules or backend configuration for Terraform,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary. Terraform plan预览执行计划，不是必须，可以预览要创建的资源,终端日志太长，只粘贴一部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[centos@ip-10-20-6-165 stg_cn_rubin]$ terraform planRefreshing Terraform state in-memory prior to plan...The refreshed state will be used to calculate this plan, but will not bepersisted to local or remote state storage.------------------------------------------------------------------------An execution plan has been generated and is shown below.Resource actions are indicated with the following symbols: + createTerraform will perform the following actions: + aws_eip.bastion id: &lt;computed&gt; allocation_id: &lt;computed&gt; association_id: &lt;computed&gt; domain: &lt;computed&gt; instance: &lt;computed&gt; network_interface: &lt;computed&gt; private_ip: &lt;computed&gt; public_ip: &lt;computed&gt; vpc: &quot;true&quot; + aws_eip.nat[0] id: &lt;computed&gt; allocation_id: &lt;computed&gt; association_id: &lt;computed&gt; domain: &lt;computed&gt; instance: &lt;computed&gt; network_interface: &lt;computed&gt; private_ip: &lt;computed&gt; public_ip: &lt;computed&gt; tags.%: &quot;1&quot; tags.Name: &quot;ip-NAT-rubin_stg_cn&quot; vpc: &quot;true&quot; + aws_eip.nat[1] id: &lt;computed&gt; allocation_id: &lt;computed&gt; association_id: &lt;computed&gt; domain: &lt;computed&gt;... + aws_subnet.public[1] id: &lt;computed&gt; arn: &lt;computed&gt; assign_ipv6_address_on_creation: &quot;false&quot; availability_zone: &quot;cn-north-1b&quot; cidr_block: &quot;10.101.1.0/24&quot; ipv6_cidr_block: &lt;computed&gt; ipv6_cidr_block_association_id: &lt;computed&gt; map_public_ip_on_launch: &quot;true&quot; tags.%: &quot;2&quot; tags.Name: &quot;public1-rubin_stg_cn&quot; tags.immutable_metadata: &quot;&#123; \&quot;purpose\&quot;: \&quot;external_rubin_stg_cn\&quot;, \&quot;target\&quot;: null &#125;&quot; vpc_id: &quot;$&#123;aws_vpc.default.id&#125;&quot; + aws_vpc.default id: &lt;computed&gt; arn: &lt;computed&gt; assign_generated_ipv6_cidr_block: &quot;false&quot; cidr_block: &quot;10.101.0.0/16&quot; default_network_acl_id: &lt;computed&gt; default_route_table_id: &lt;computed&gt; default_security_group_id: &lt;computed&gt; dhcp_options_id: &lt;computed&gt; enable_classiclink: &lt;computed&gt; enable_classiclink_dns_support: &lt;computed&gt; enable_dns_hostnames: &quot;true&quot; enable_dns_support: &quot;true&quot; instance_tenancy: &quot;default&quot; ipv6_association_id: &lt;computed&gt; ipv6_cidr_block: &lt;computed&gt; main_route_table_id: &lt;computed&gt; tags.%: &quot;1&quot; tags.Name: &quot;vpc-rubin_stg_cn&quot;Plan: 32 to add, 0 to change, 0 to destroy.------------------------------------------------------------------------Note: You didn&apos;t specify an &quot;-out&quot; parameter to save this plan, so Terraformcan&apos;t guarantee that exactly these actions will be performed if&quot;terraform apply&quot; is subsequently run. Terraform apply真正执行编排计划,创建资源的终端日志太长,只粘贴一部分，几分钟后整个资源全被创建完成，最后output把当前资源的ID显示出来了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[centos@ip-10-20-6-165 stg_cn_rubin]$ terraform applyAn execution plan has been generated and is shown below.Resource actions are indicated with the following symbols: + createTerraform will perform the following actions: + aws_eip.bastion id: &lt;computed&gt; allocation_id: &lt;computed&gt; association_id: &lt;computed&gt;... tags.Name: &quot;vpc-rubin_stg_cn&quot;Plan: 32 to add, 0 to change, 0 to destroy.Do you want to perform these actions? Terraform will perform the actions described above. Only &apos;yes&apos; will be accepted to approve. Enter a value: yesaws_vpc.default: Creating... arn: &quot;&quot; =&gt; &quot;&lt;computed&gt;&quot; assign_generated_ipv6_cidr_block: &quot;&quot; =&gt; &quot;false&quot; cidr_block: &quot;&quot; =&gt; &quot;10.101.0.0/16&quot; default_network_acl_id: &quot;&quot; =&gt; &quot;&lt;computed&gt;&quot; default_route_table_id: &quot;&quot; =&gt; &quot;&lt;computed&gt;&quot;...aws_route_table_association.private[0]: Creation complete after 0s (ID: rtbassoc-0dec0c6caa85081aa)aws_route_table_association.private[1]: Creation complete after 0s (ID: rtbassoc-00541f1193c1f006a)aws_route_table_association.private_db[0]: Creation complete after 0s (ID: rtbassoc-0707dc991b6f0b502)aws_route_table_association.private_db[1]: Creation complete after 0s (ID: rtbassoc-0e1c00b46b31e13a6)aws_route_table_association.private_emr[1]: Creation complete after 0s (ID: rtbassoc-0dcfc3ec369382fa4)aws_route_table_association.private_emr[0]: Creation complete after 0s (ID: rtbassoc-0ed91fcc9955b3674)Apply complete! Resources: 32 added, 0 changed, 0 destroyed.Outputs:bastion_eip = 54.223.216.43cidr_block = 10.101.0.0/16nat_eip = 54.222.176.3,54.222.249.234private_db_subnets = subnet-095f9065be2e9bb6d,subnet-0d9fdec0b255f17c1private_emr_subnets = subnet-090653a4e725b8d90,subnet-0d0fd140f10721f60private_subnets = subnet-0f34c39ec475c0384,subnet-095341650c1b43695public_subnets = subnet-06bb6bea3d757253b,subnet-03307ab8f793fd944vpc_id = vpc-02d9520415468c7f0]]></content>
      <categories>
        <category>linux工具</category>
      </categories>
      <tags>
        <tag>terraform</tag>
        <tag>华为云</tag>
        <tag>AWS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Systemd服务介绍]]></title>
    <url>%2FSystemd%E6%9C%8D%E5%8A%A1%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[转载：http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html 一、开机启动对于那些支持 Systemd 的软件，安装的时候，会自动在/usr/lib/systemd/system目录添加一个配置文件。 如果你想让该软件开机启动，就执行下面的命令（以httpd.service为例）。 12&gt; $ sudo systemctl enable httpd&gt; 上面的命令相当于在/etc/systemd/system目录添加一个符号链接，指向/usr/lib/systemd/system里面的httpd.service文件。 这是因为开机时，Systemd只执行/etc/systemd/system目录里面的配置文件。这也意味着，如果把修改后的配置文件放在该目录，就可以达到覆盖原始配置的效果。 二、启动服务设置开机启动以后，软件并不会立即启动，必须等到下一次开机。如果想现在就运行该软件，那么要执行systemctl start命令。 12&gt; $ sudo systemctl start httpd&gt; 执行上面的命令以后，有可能启动失败，因此要用systemctl status命令查看一下该服务的状态。 12345678910111213141516171819&gt; $ sudo systemctl status httpd&gt;&gt; httpd.service - The Apache HTTP Server&gt; Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled)&gt; Active: active (running) since 金 2014-12-05 12:18:22 JST; 7min ago&gt; Main PID: 4349 (httpd)&gt; Status: &quot;Total requests: 1; Current requests/sec: 0; Current traffic: 0 B/sec&quot;&gt; CGroup: /system.slice/httpd.service&gt; ├─4349 /usr/sbin/httpd -DFOREGROUND&gt; ├─4350 /usr/sbin/httpd -DFOREGROUND&gt; ├─4351 /usr/sbin/httpd -DFOREGROUND&gt; ├─4352 /usr/sbin/httpd -DFOREGROUND&gt; ├─4353 /usr/sbin/httpd -DFOREGROUND&gt; └─4354 /usr/sbin/httpd -DFOREGROUND&gt;&gt; 12月 05 12:18:22 localhost.localdomain systemd[1]: Starting The Apache HTTP Server...&gt; 12月 05 12:18:22 localhost.localdomain systemd[1]: Started The Apache HTTP Server.&gt; 12月 05 12:22:40 localhost.localdomain systemd[1]: Started The Apache HTTP Server.&gt; 上面的输出结果含义如下。 Loaded行：配置文件的位置，是否设为开机启动 Active行：表示正在运行 Main PID行：主进程ID Status行：由应用本身（这里是 httpd ）提供的软件当前状态 CGroup块：应用的所有子进程 日志块：应用的日志 三、停止服务终止正在运行的服务，需要执行systemctl stop命令。 12&gt; $ sudo systemctl stop httpd.service&gt; 有时候，该命令可能没有响应，服务停不下来。这时候就不得不”杀进程”了，向正在运行的进程发出kill信号。 12&gt; $ sudo systemctl kill httpd.service&gt; 此外，重启服务要执行systemctl restart命令。 12&gt; $ sudo systemctl restart httpd.service&gt; 四、读懂配置文件一个服务怎么启动，完全由它的配置文件决定。下面就来看，配置文件有些什么内容。 前面说过，配置文件主要放在/usr/lib/systemd/system目录，也可能在/etc/systemd/system目录。找到配置文件以后，使用文本编辑器打开即可。 systemctl cat命令可以用来查看配置文件，下面以sshd.service文件为例，它的作用是启动一个 SSH 服务器，供其他用户以 SSH 方式登录。 1234567891011121314151617181920&gt; $ systemctl cat sshd.service&gt;&gt; [Unit]&gt; Description=OpenSSH server daemon&gt; Documentation=man:sshd(8) man:sshd_config(5)&gt; After=network.target sshd-keygen.service&gt; Wants=sshd-keygen.service&gt;&gt; [Service]&gt; EnvironmentFile=/etc/sysconfig/sshd&gt; ExecStart=/usr/sbin/sshd -D $OPTIONS&gt; ExecReload=/bin/kill -HUP $MAINPID&gt; Type=simple&gt; KillMode=process&gt; Restart=on-failure&gt; RestartSec=42s&gt;&gt; [Install]&gt; WantedBy=multi-user.target&gt; 可以看到，配置文件分成几个区块，每个区块包含若干条键值对。 下面依次解释每个区块的内容。 五、 [Unit] 区块：启动顺序与依赖关系。Unit区块的Description字段给出当前服务的简单描述，Documentation字段给出文档位置。 接下来的设置是启动顺序和依赖关系，这个比较重要。 After字段：表示如果network.target或sshd-keygen.service需要启动，那么sshd.service应该在它们之后启动。 相应地，还有一个Before字段，定义sshd.service应该在哪些服务之前启动。 注意，After和Before字段只涉及启动顺序，不涉及依赖关系。 举例来说，某 Web 应用需要 postgresql 数据库储存数据。在配置文件中，它只定义要在 postgresql 之后启动，而没有定义依赖 postgresql 。上线后，由于某种原因，postgresql 需要重新启动，在停止服务期间，该 Web 应用就会无法建立数据库连接。 设置依赖关系，需要使用Wants字段和Requires字段。 Wants字段：表示sshd.service与sshd-keygen.service之间存在”弱依赖”关系，即如果”sshd-keygen.service”启动失败或停止运行，不影响sshd.service继续执行。 Requires字段则表示”强依赖”关系，即如果该服务启动失败或异常退出，那么sshd.service也必须退出。 注意，Wants字段与Requires字段只涉及依赖关系，与启动顺序无关，默认情况下是同时启动的。 六、[Service] 区块：启动行为Service区块定义如何启动当前服务。 6.1 启动命令许多软件都有自己的环境参数文件，该文件可以用EnvironmentFile字段读取。 EnvironmentFile字段：指定当前服务的环境参数文件。该文件内部的key=value键值对，可以用$key的形式，在当前配置文件中获取。 上面的例子中，sshd 的环境参数文件是/etc/sysconfig/sshd。 配置文件里面最重要的字段是ExecStart。 ExecStart字段：定义启动进程时执行的命令。 上面的例子中，启动sshd，执行的命令是/usr/sbin/sshd -D $OPTIONS，其中的变量$OPTIONS就来自EnvironmentFile字段指定的环境参数文件。 与之作用相似的，还有如下这些字段。 ExecReload字段：重启服务时执行的命令 ExecStop字段：停止服务时执行的命令 ExecStartPre字段：启动服务之前执行的命令 ExecStartPost字段：启动服务之后执行的命令 ExecStopPost字段：停止服务之后执行的命令 请看下面的例子。 1234567&gt; [Service]&gt; ExecStart=/bin/echo execstart1&gt; ExecStart=&gt; ExecStart=/bin/echo execstart2&gt; ExecStartPost=/bin/echo post1&gt; ExecStartPost=/bin/echo post2&gt; 上面这个配置文件，第二行ExecStart设为空值，等于取消了第一行的设置，运行结果如下。 1234&gt; execstart2&gt; post1&gt; post2&gt; 所有的启动设置之前，都可以加上一个连词号（-），表示”抑制错误”，即发生错误的时候，不影响其他命令的执行。比如，EnvironmentFile=-/etc/sysconfig/sshd（注意等号后面的那个连词号），就表示即使/etc/sysconfig/sshd文件不存在，也不会抛出错误。 6.2 启动类型Type字段定义启动类型。它可以设置的值如下。 simple（默认值）：ExecStart字段启动的进程为主进程 forking：ExecStart字段将以fork()方式启动，此时父进程将会退出，子进程将成为主进程 oneshot：类似于simple，但只执行一次，Systemd 会等它执行完，才启动其他服务 dbus：类似于simple，但会等待 D-Bus 信号后启动 notify：类似于simple，启动结束后会发出通知信号，然后 Systemd 再启动其他服务 idle：类似于simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合 下面是一个oneshot的例子，笔记本电脑启动时，要把触摸板关掉，配置文件可以这样写。 12345678910&gt; [Unit]&gt; Description=Switch-off Touchpad&gt;&gt; [Service]&gt; Type=oneshot&gt; ExecStart=/usr/bin/touchpad-off&gt;&gt; [Install]&gt; WantedBy=multi-user.target&gt; 上面的配置文件，启动类型设为oneshot，就表明这个服务只要运行一次就够了，不需要长期运行。 如果关闭以后，将来某个时候还想打开，配置文件修改如下。 123456789101112&gt; [Unit]&gt; Description=Switch-off Touchpad&gt;&gt; [Service]&gt; Type=oneshot&gt; ExecStart=/usr/bin/touchpad-off start&gt; ExecStop=/usr/bin/touchpad-off stop&gt; RemainAfterExit=yes&gt;&gt; [Install]&gt; WantedBy=multi-user.target&gt; 上面配置文件中，RemainAfterExit字段设为yes，表示进程退出以后，服务仍然保持执行。这样的话，一旦使用systemctl stop命令停止服务，ExecStop指定的命令就会执行，从而重新开启触摸板。 6.3 重启行为Service区块有一些字段，定义了重启行为。 KillMode字段：定义 Systemd 如何停止 sshd 服务。 上面这个例子中，将KillMode设为process，表示只停止主进程，不停止任何sshd 子进程，即子进程打开的 SSH session 仍然保持连接。这个设置不太常见，但对 sshd 很重要，否则你停止服务的时候，会连自己打开的 SSH session 一起杀掉。 KillMode字段可以设置的值如下。 control-group（默认值）：当前控制组里面的所有子进程，都会被杀掉 process：只杀主进程 mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号 none：没有进程会被杀掉，只是执行服务的 stop 命令。 接下来是Restart字段。 Restart字段：定义了 sshd 退出后，Systemd 的重启方式。 上面的例子中，Restart设为on-failure，表示任何意外的失败，就将重启sshd。如果 sshd 正常停止（比如执行systemctl stop命令），它就不会重启。 Restart字段可以设置的值如下。 no（默认值）：退出后不会重启 on-success：只有正常退出时（退出状态码为0），才会重启 on-failure：非正常退出时（退出状态码非0），包括被信号终止和超时，才会重启 on-abnormal：只有被信号终止和超时，才会重启 on-abort：只有在收到没有捕捉到的信号终止时，才会重启 on-watchdog：超时退出，才会重启 always：不管是什么退出原因，总是重启 对于守护进程，推荐设为on-failure。对于那些允许发生错误退出的服务，可以设为on-abnormal。 最后是RestartSec字段。 RestartSec字段：表示 Systemd 重启服务之前，需要等待的秒数。上面的例子设为等待42秒。 七、[Install] 区块Install区块，定义如何安装这个配置文件，即怎样做到开机启动。 WantedBy字段：表示该服务所在的 Target。 Target的含义是服务组，表示一组服务。WantedBy=multi-user.target指的是，sshd 所在的 Target 是multi-user.target。 这个设置非常重要，因为执行systemctl enable sshd.service命令时，sshd.service的一个符号链接，就会放在/etc/systemd/system目录下面的multi-user.target.wants子目录之中。 Systemd 有默认的启动 Target。 123&gt; $ systemctl get-default&gt; multi-user.target&gt; 上面的结果表示，默认的启动 Target 是multi-user.target。在这个组里的所有服务，都将开机启动。这就是为什么systemctl enable命令能设置开机启动的原因。 使用 Target 的时候，systemctl list-dependencies命令和systemctl isolate命令也很有用。 1234567&gt; # 查看 multi-user.target 包含的所有服务&gt; $ systemctl list-dependencies multi-user.target&gt;&gt; # 切换到另一个 target&gt; # shutdown.target 就是关机状态&gt; $ sudo systemctl isolate shutdown.target&gt; 一般来说，常用的 Target 有两个：一个是multi-user.target，表示多用户命令行状态；另一个是graphical.target，表示图形用户状态，它依赖于multi-user.target。官方文档有一张非常清晰的 Target 依赖关系图。 八、Target 的配置文件Target 也有自己的配置文件。 12345678910&gt; $ systemctl cat multi-user.target&gt;&gt; [Unit]&gt; Description=Multi-User System&gt; Documentation=man:systemd.special(7)&gt; Requires=basic.target&gt; Conflicts=rescue.service rescue.target&gt; After=basic.target rescue.service rescue.target&gt; AllowIsolate=yes&gt; 注意，Target 配置文件里面没有启动命令。 上面输出结果中，主要字段含义如下。 Requires字段：要求basic.target一起运行。 Conflicts字段：冲突字段。如果rescue.service或rescue.target正在运行，multi-user.target就不能运行，反之亦然。 After：表示multi-user.target在basic.target 、 rescue.service、 rescue.target之后启动，如果它们有启动的话。 AllowIsolate：允许使用systemctl isolate命令切换到multi-user.target。 九、修改配置文件后重启修改配置文件以后，需要重新加载配置文件，然后重新启动相关服务。 123456&gt; # 重新加载配置文件&gt; $ sudo systemctl daemon-reload&gt;&gt; # 重启相关服务&gt; $ sudo systemctl restart foobar&gt; （完）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>systemd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux三剑客之awk,sed,grep]]></title>
    <url>%2FLinux%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bawk-sed-grep%2F</url>
    <content type="text"><![CDATA[正则表达式全集：http://tool.oschina.net/uploads/apidocs/jquery/regexp.html [TOC] Sed参考：https://www.cnblogs.com/ctaixw/p/5860221.html sed提取文件中的指定内容假如有这样一个文件 12345678910111213141516[root@rac1 tmp]# cat 123[green]192.168.1.1[bule]192.168.1.123192.168.1.156[red]192.168.1.14192.168.1.231192.168.14.27[yellow]192.168.2.55192.168.13.23 想要提取red组的IP 1sed -n &apos;/red/,/^$/p&apos; | sed -e &apos;s/\[red\]//&apos; -e &apos;/^$/d&apos; sed递归替换1find . -type f|xargs -0 sed -i &quot;s/x/y/g&quot; sed文件的最后一行和第一行后添加内容123sed -i &apos;$a\hahahaha&apos; file.txtsed -i &apos;1a\hahahahah&apos; file.txt sed在某行的前一行或者后一行添加内容在某行的前一行或后一行添加内容 具休操作如下： 1234#匹配行前加sed -i &apos;/allow 361way.com/iallow www.361way.com&apos; the.conf.file#匹配行前后sed -i &apos;/allow 361way.com/aallow www.361way.com&apos; the.conf.file 而在书写的时候为便与区分，往往会在i和a前面加一个反加一个反斜扛 。代码就变成了： 12sed -i &apos;/2222222222/a\3333333333&apos; test.txtsed -i &apos;/2222222222/i\3333333333&apos; test.txt 这就就可以很方便的看出要在某一行前或某一行后加入什么内容 。不过经常我记不住a 、i 那个是前那个是后。我的记法是a = after ，i = in front 。这样就知道 i 是前，a 是后了。不过官方的man文件里不是这样解释的，man文件里是这样解释的： 1234atext Append text, which has each embedded newline preceded by a backslash.itext Insert text, which has each embedded newline preceded by a backslash. 而且其可以配合find查找的内容处理，如下： 12find . -name server.xml|xargs sed -i &apos;/directory/i &lt;!--&apos;find . -name server.xml|xargs sed -i &apos;/pattern=&quot;%h/a --&gt;&apos; 在某行（指具体行号）前或后加一行内容这里指定的行号是第四行 12sed -i &apos;N;4addpdf&apos; a.txtsed -i &apos;N;4ieepdf&apos; a.txt sed在指定字符前后添加内容假设文档内容如下： 123456789[root@localhost ~]# cat /tmp/input.txtnull000011112222 test 要求：在1111之前添加AAA,方法如下： sed -i ‘s/指定的字符/要插入的字符&amp;/‘ 文件 1234567891011[root@localhost ~]# sed -i &apos;s/1111/AAA&amp;/&apos; /tmp/input.txt [root@localhost ~]# cat /tmp/input.txt null0000AAA11112222 test 要求：在1111之后添加BBB，方法如下： sed -i ‘s/指定的字符/&amp;要插入的字符/‘ 文件 1234567891011[root@localhost ~]# sed -i &apos;s/1111/&amp;BBB/&apos; /tmp/input.txt [root@localhost ~]# cat /tmp/input.txt null0000AAA1111BBB2222 test 要求：(1) 删除所有空行；(2) 一行中，如果包含”1111”，则在”1111”前面插入”AAA”，在”11111”后面插入”BBB” 1234567[root@localhost ~]# sed &apos;/^$/d;s/1111/AAA&amp;/;s/1111/&amp;BBB/&apos; /tmp/input.txt null0000BBB1111AAA2222test 要求：在每行的头添加字符，比如”HEAD”，命令如下： 1234567891011[root@localhost ~]# sed -i &apos;s/^/HEAD&amp;/&apos; /tmp/input.txt [root@localhost ~]# cat /tmp/input.txtHEADnullHEAD000011112222HEADHEADtest 要求：在每行的尾部添加字符，比如”tail”，命令如下： 1234567891011[root@localhost ~]# sed -i &apos;s/$/&amp;tail/&apos; /tmp/input.txt [root@localhost ~]# cat /tmp/input.txt HEADnulltailHEAD000011112222tailHEADtailHEADtesttail 说明：1.”^”代表行首，”$”代表行尾2.’s/$/&amp;tail/g’中的字符g代表每行出现的字符全部替换，如果想在特定字符处添加，g就有用了，否则只会替换每行第一个，而不继续往后找。]]></content>
      <categories>
        <category>shell脚本</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
        <tag>sed</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysqldump数据库备份还原]]></title>
    <url>%2Fmysqldump%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E8%BF%98%E5%8E%9F%2F</url>
    <content type="text"><![CDATA[mysqldump备份常见用法备份 1、导出整个数据库(包括数据库中的数据） mysqldump -u username -p “password” dbname &gt; dbname.sql 2、导出数据库结构（不含数据） mysqldump -u username -p “password” -d dbname &gt; dbname.sql 3、导出数据库中的某张数据表（包含数据） mysqldump -u username -p “password” dbname tablename &gt; tablename.sq mysqldump -uroot -p “password” -B dbname –table tablename &gt; tablename.sql 4、导出数据库中的某张数据表的表结构（不含数据） mysqldump -u username -p “password” -d dbname tablename &gt; tablename.sql 5、备份数据库时使用参数（–ignore-table）排除某个表或多个表 mysqldump -uroot -p“password” –ignore-table=dbname.table1 –ignore-table=dbname.table2 dbname &gt; /opt/backup.sql 6、备份所有数据库 mysqldump -u username -p “password” –all-databases &gt; all.sql sql文件还原方法 1、mysql -uroot -p”db_password” -f dbname &lt; backup.sql （-f 参数表示在导出过程中忽略出现的SQL错误） nohup mysql -uroot -p”db_password” -f dbname &lt; backup.sql &gt; /dev/null 2&gt;&amp;1 &amp; （在导入较大的sql文件时可以放到后台执行） 2、mysql -uroot -p 先登录到数据库，use dbname; 切换到需要导入的库中，然后执行 source /opt/backup.sql; 这样就可以导入了 mysqldump 参数详解 –host 指定要备份数据库的服务器 –port MySQL服务器的端口号 –user 连接MySQL服务器的用户名 –password 连接MySQL服务器的密码 –add-locks 备份数据库表时锁定数据库表 –databases, -B 导出多少数据库，参数后面所有参量都被看作数据库名 –all-databases 备份MySQL服务器上的所有数据库 –all-tablespaces , -Y导出全部表空间 -y不导出任何表空间信息 –add-drop-table 在每个创建数据库表语句前添加删除数据库表的语句 –no-create-db 禁止生成创建数据库语句 –no-create-info 禁止生成创建数据库库表语句 –complete-insert, -c使用完整的insert语句(包含列名称)，这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败 –no-data, -d 不导出任何数据，只导出数据库表结构 –no-create-info, -t 只导出数据，而不添加CREATE TABLE 语句 –no-create-db, -n 只导出数据，而不添加CREATE DATABASE 语句 –ignore-table 导出数据库时忽略某个表 –force，-f 在导出过程中忽略出现的SQL错误，当出现错误时仍然继续后面的操作 –add-drop-database 每个数据库创建之前添加drop数据库语句 –add-drop-table 每个数据表创建之前添加drop数据表语句，默认为打开状态，使用–skip-add-drop-table取消选项 –add-locks在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE，默认为打开状态，使用–skip-add-locks取消选项 –default-character-set 设置默认字符集，默认值为utf8 –comments 附加注释信息，默认为打开，可以–skip-comments取消 –compact导出更少的输出信息(用于调试)，去掉注释和头尾等结构，（可以使用选项 –skip-add-drop-table –skip-add-locks –skip-comments –skip-disable-keys） –events, -E 导出事件 –flush-privileges 在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句 –flush-logs 开始导出之前刷新日志，请注意，假如一次导出多个数据库(使用选项–databases或者–all-databases)，将会逐个数据库刷新日志 除使用–lock-all-tables或者–master-data外，在这种情况下，日志将会被刷新一次，相应的表同时被锁定，因此，如果打算同时导出和刷新日志应该使用–lock-all-tables 或者–master-data 和–flush-logs –delayed-insert 采用延时插入方式（INSERT DELAYED） –comments 添加注释信息 –compact 压缩模式，产生更少的输出 –complete-insert 输出完成的插入语句 –default-character-set 指定默认字符集 –lock-tables 备份前，锁定所有数据库表 –lock-all-tables, -x提交请求锁定所有数据库中的所有表，以保证数据的一致性，这是一个全局读锁，并且自动关闭–single-transaction 和–lock-tables 选项 –debug 输出debug信息，用于调试（默认值为d:t:o,/tmp/mysqldump.trace）例 mysqldump -uroot -p –all-databases –debug=” d:t:o,/tmp/debug.trace”]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysqldump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[[TOC] date1、命令：date 2、命令功能：date 可以用来显示或设定系统的日期与时间。 3、命令参数 12345-d&lt;字符串&gt;：显示字符串所指的日期与时间。字符串前后必须加上双引号； -s&lt;字符串&gt;：根据字符串来设置日期与时间。字符串前后必须加上双引号； -u：显示GMT； --help：在线帮助； --version：显示版本信息。 4、日期格式字符串列表 123456789101112131415161718192021222324252627282930313233%H 小时(以00-23来表示)。 %I 小时(以01-12来表示)。 %K 小时(以0-23来表示)。 %l 小时(以0-12来表示)。 %M 分钟(以00-59来表示)。 %P AM或PM。 %r 时间(含时分秒，小时以12小时AM/PM来表示)。 %s 总秒数。起算时间为1970-01-01 00:00:00 UTC。 %S 秒(以本地的惯用法来表示)。 %T 时间(含时分秒，小时以24小时制来表示)。 %X 时间(以本地的惯用法来表示)。 %Z 市区。 %a 星期的缩写。 %A 星期的完整名称。 %b 月份英文名的缩写。 %B 月份的完整英文名称。 %c 日期与时间。只输入date指令也会显示同样的结果。 %d 日期(以01-31来表示)。 %D 日期(含年月日)。 %j 该年中的第几天。 %m 月份(以01-12来表示)。 %U 该年中的周数。 %w 该周的天数，0代表周日，1代表周一，异词类推。 %x 日期(以本地的惯用法来表示)。 %y 年份(以00-99来表示)。 %Y 年份(以四位数来表示)。 %n 在显示时，插入新的一行。 %t 在显示时，插入tab。 MM 月份(必要) DD 日期(必要) hh 小时(必要) mm 分钟(必要)ss 秒(选择性) 5、实例 格式化输出： 12date +&quot;%Y-%m-%d&quot; 2015-12-07 输出昨天日期： 12date -d &quot;1 day ago&quot; +&quot;%Y-%m-%d&quot;2015-11-19 2秒后输出： 12date -d &quot;2 second&quot; +&quot;%Y-%m-%d %H:%M.%S&quot;2015-11-20 14:21.31 传说中的 1234567890 秒： 12date -d &quot;1970-01-01 1234567890 seconds&quot; +&quot;%Y-%m-%d %H:%m:%S&quot;2009-02-13 23:02:30 普通转格式： 12date -d &quot;2009-12-12&quot; +&quot;%Y/%m/%d %H:%M.%S&quot;2009/12/12 00:00.00 apache格式转换： 12date -d &quot;Dec 5, 2009 12:00:37 AM&quot; +&quot;%Y-%m-%d %H:%M.%S&quot;2009-12-05 00:00.37 格式转换后时间： 12date -d &quot;Dec 5, 2009 12:00:37 AM 2 year ago&quot; +&quot;%Y-%m-%d %H:%M.%S&quot;2007-12-05 00:00.37 加减操作： 1234567date +%Y%m%d #显示前天年月日 date -d &quot;+1 day&quot; +%Y%m%d #显示前一天的日期 date -d &quot;-1 day&quot; +%Y%m%d #显示后一天的日期 date -d &quot;-1 month&quot; +%Y%m%d #显示上一月的日期 date -d &quot;+1 month&quot; +%Y%m%d #显示下一月的日期 date -d &quot;-1 year&quot; +%Y%m%d #显示前一年的日期 date -d &quot;+1 year&quot; +%Y%m%d #显示下一年的日期 设定时间： 1234567date -s #设置当前时间，只有root权限才能设置，其他只能查看 date -s 20120523 #设置成20120523，这样会把具体时间设置成空00:00:00 date -s 01:01:01 #设置具体时间，不会对日期做更改 date -s &quot;01:01:01 2012-05-23&quot; #这样可以设置全部时间 date -s &quot;01:01:01 20120523&quot; #这样可以设置全部时间 date -s &quot;2012-05-23 01:01:01&quot; #这样可以设置全部时间 date -s &quot;20120523 01:01:01&quot; #这样可以设置全部时间 检查一组命令花费的时间： 123456#!/bin/bash start=$(date +%s) nmap man.linuxde.net &amp;&gt; /dev/null end=$(date +%s) difference=$(( end - start )) echo $difference seconds. trtr命令可以对来自标准输入的字符进行替换、压缩和删除。它可以将一组字符变成另一组字符，经常用来编写优美的单行命令，作用很强大。 语法 1tr(选项)(参数) 选项 1234-c或——complerment：取代所有不属于第一字符集的字符；-d或——delete：删除所有属于第一字符集的字符；-s或--squeeze-repeats：把连续重复的字符以单独一个字符表示；-t或--truncate-set1：先删除第一字符集较第二字符集多出的字符。 参数 字符集1：指定要转换或删除的原字符集。当执行转换操作时，必须使用参数“字符集2”指定转换的目标字符集。但执行删除操作时，不需要参数“字符集2”； 字符集2：指定要转换成的目标字符集。 实例 将输入字符由大写转换为小写： 12echo &quot;HELLO WORLD&quot; | tr &apos;A-Z&apos; &apos;a-z&apos;hello world ‘A-Z’ 和 ‘a-z’都是集合，集合是可以自己制定的，例如：’ABD-}’、’bB.,’、’a-de-h’、’a-c0-9’都属于集合，集合里可以使用’\n’、’\t’，可以可以使用其他ASCII字符。 使用tr删除字符： 12echo &quot;hello 123 world 456&quot; | tr -d &apos;0-9&apos;hello world 将制表符转换为空格： 1cat text | tr &apos;\t&apos; &apos; &apos; 字符集补集，从输入文本中将不在补集中的所有字符删除： 12echo aa.,a 1 b#$bb 2 c*/cc 3 ddd 4 | tr -d -c &apos;0-9 \n&apos; 1 2 3 4 此例中，补集中包含了数字0~9、空格和换行符\n，所以没有被删除，其他字符全部被删除了。 用tr压缩字符，可以压缩输入中重复的字符： 12echo &quot;thissss is a text linnnnnnne.&quot; | tr -s &apos; sn&apos;this is a text line. 巧妙使用tr做数字相加操作： 1echo 1 2 3 4 5 6 7 8 9 | xargs -n1 | echo $[ $(tr &apos;\n&apos; &apos;+&apos;) 0 ] 删除Windows文件“造成”的’^M’字符： 123cat file | tr -s &quot;\r&quot; &quot;\n&quot; &gt; new_file或cat file | tr -d &quot;\r&quot; &gt; new_file tr可以使用的字符类： 1234567891011[:alnum:]：字母和数字[:alpha:]：字母[:cntrl:]：控制（非打印）字符[:digit:]：数字[:graph:]：图形字符[:lower:]：小写字母[:print:]：可打印字符[:punct:]：标点符号[:space:]：空白字符[:upper:]：大写字母[:xdigit:]：十六进制字符 使用方式： 1234tr &apos;[:lower:]&apos; &apos;[:upper:]&apos;生成固定长度的随机密码head /dev/urandom | tr -dc A-Za-z0-9 | head -c 20 cutcut 命令是用来从文本文件中移除“某些列”的经典工具。在本文中的“一列”可以被定义为按照一行中位置区分的一系列字符串或者字节，或者是以某个分隔符为间隔的某些域。 选项 1234567891011121314151617-b：仅显示行中指定直接范围的内容；-c：仅显示行中指定范围的字符；-d：指定字段的分隔符，默认的字段分隔符为“TAB”；-f：显示指定字段的内容；-n：与“-b”选项连用，不分割多字节字符；--complement：补足被选择的字节、字符或字段；--out-delimiter=&lt;字段分隔符&gt;：指定输出内容是的字段分割符；--help：显示指令的帮助信息；--version：显示指令的版本信息。 实例 123456cat test.txtNo Name Mark Percent01 tom 69 9102 jack 71 8703 alex 68 98 使用 -f 选项提取指定字段： 123456cut -f 1 test.txt结果：No010203 123456cut -f2,3 test.txt结果：Name Marktom 69jack 71alex 68 –complement 选项提取指定字段之外的列（打印除了第二列之外的列）： 123456cut -f2 --complement test.txt结果：No Mark Percent01 69 9102 71 8703 68 98 使用 -d 选项指定字段分隔符： 12345678910111213cat test2.txtNo;Name;Mark;Percent01;tom;69;9102;jack;71;8703;alex;68;98cut -f2 -d&quot;;&quot; test2.txt结果：Nametomjackalex 3.指定字段的字符或者字节范围 123456789101112131415cut命令可以将一串字符作为列来显示，字符字段的记法：N-：从第N个字节、字符、字段到结尾；N-M：从第N个字节、字符、字段到第M个（包括M在内）字节、字符、字段；-M：从第1个字节、字符、字段到第M个（包括M在内）字节、字符、字段。上面是记法，结合下面选项将摸个范围的字节、字符指定为字段：-b 表示字节；-c 表示字符；-f 表示定义字段。 1234567cat test.txtabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz 打印第1个到第3个字符： 1234567cut -c1-3 test.txt结果：abcabcabcabcabc 打印前2个字符： 1234567cut -c-2 test.txt结果：ababababab 打印从第5个字符开始到结尾： 1234567cut -c5- test.txt结果：efghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyz jqjq命令允许直接在命令行下对JSON进行操作包括分片、过滤、转换等JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。基于javascript(Standard ECMA-262 3rd Edition - December 1999)的一个子集JSON 数据的书写格式是： 名称/对象。JSON的结构基于两点 “名称/值”的集合 ,在不同的语言中,它可以被理解为对象(object)，记录(record)，结构(struct)，字典(dictionary)，哈希表(hash table)，键列表(keyed list)等 .值的有序列表 多数语言中被理解为数组(array) JSON的基础结构说明 对象是属性、值的集合。一个对象开始与”{” ,结束于”}”。每一个属性名和值间用”:”分隔。每个属性间用”，”分隔。值可以是字符串，数字，逻辑值，数组，对象，null。数字：整数或浮点数字符串：在双引号中逻辑符：true和false数组：在方括号中对象：在花括号中null： 代表空 jq命令的格式 12345678910111213jq [options] filter [files]**options：**--version：输出jq的版本信息并退出--slurp/-s：读入整个输入流到一个数组。--raw-input/-R：不作为JSON解析，将每一行的文本作为字符串输出到屏幕。--null-input/ -n：不读取任何输入，过滤器运行使用null作为输入。一般用作从头构建JSON数据。--compact-output /-c：使输出紧凑，而不是把每一个JSON对象输出在一行。--colour-output / -C：打开颜色显示--monochrome-output / -M：关闭颜色显示--ascii-output /-a：指定输出格式为ASCII-raw-output /-r ：如果过滤的结果是一个字符串，那么直接写到标准输出（去掉字符串的引号） filter： 123456. : 默认输出.foo: 输出指定属性，foo代表属性。.[foo] ：输出指定数组元素。foo代表数组下标。.[]：输出指定数组中全部元素， ：指定多个属性作为过滤条件时，用逗号分隔| ： 将指定的数组元素中的某个属性作为过滤条件 示例 用作示例的JOSN文件。 1[&#123;&quot;hostCompany&quot;:&quot;Beijing Autelan Technology&quot;,&quot;hostModel&quot;:&quot;CS-VIC-2000-C&quot;,&quot;hostsn&quot;:&quot;01010730b12014A00477&quot;,&quot;mac&quot;:&quot;00:1F:64:CE:F3:8E&quot;,&quot;cpuModel&quot;:&quot;MIPS 74Kc V4.12&quot;,&quot;cpuSN&quot;:&quot;000000&quot;,&quot;memoryModel&quot;:&quot;abcdefg&quot;,&quot;memorySN&quot;:&quot;000000&quot;,&quot;boardSN&quot;:&quot;01010730b12014A00477&quot;,&quot;networkCardMac&quot;:&quot;00:1F:64:CE:F3:8F&quot;,&quot;lowFreModel&quot;:&quot;AR9344&quot;,&quot;lowFreSN&quot;:&quot;000000&quot;,&quot;hignFreModel&quot;:&quot;AR9582&quot;,&quot;hignFreSN&quot;:&quot;000000&quot;,&quot;gpsModel&quot;:&quot;abcdefg&quot;,&quot;gpsSN&quot;:&quot;000000&quot;,&quot;MEID_3g&quot;:&quot;A000004E123ABD2&quot;,&quot;Company_3g&quot;:&quot;ZTEMT INCORPORATED&quot;,&quot;modelOf3g&quot;:&quot;MC271X&quot;,&quot;snOf3g&quot;:&quot;A000004E123ABD2&quot;,&quot;iccid&quot;:&quot;89860314400200885980&quot;,&quot;Operators&quot;:&quot;CTCC&quot;,&quot;hardVersion&quot;:&quot;1.20&quot;,&quot;firmwareVersion&quot;:&quot;1.0.6.29&quot;&#125;] 1234567891011121314151617181920212223242526272829$ jq . apinfo.json [ &#123; &quot;firmwareVersion&quot;: &quot;1.0.6.29&quot;, &quot;hardVersion&quot;: &quot;1.20&quot;, &quot;Operators&quot;: &quot;CTCC&quot;, &quot;iccid&quot;: &quot;DATA DATA&quot;, &quot;snOf3g&quot;: &quot;A000004E123ABD2&quot;, &quot;modelOf3g&quot;: &quot;MC271X&quot;, &quot;Company_3g&quot;: &quot;ZTEMT INCORPORATED&quot;, &quot;MEID_3g&quot;: &quot;A000004E123ABD2&quot;, &quot;memorySN&quot;: &quot;000000&quot;, &quot;memoryModel&quot;: &quot;abcdefg&quot;, &quot;cpuSN&quot;: &quot;000000&quot;, &quot;cpuModel&quot;: &quot;MIPS 74Kc V4.12&quot;, &quot;mac&quot;: &quot;00:1F:64:CE:F3:8E&quot;, &quot;hostsn&quot;: &quot;01010730b12014A00477&quot;, &quot;hostModel&quot;: &quot;CS-VIC-1999-C&quot;, &quot;stCompany&quot;: &quot;Beijing Autelan Technology&quot;, &quot;boardSN&quot;: &quot;01010730b12014A00477&quot;, &quot;networkCardMac&quot;: &quot;00:1F:64:CE:F3:8F&quot;, &quot;lowFreModel&quot;: &quot;AR9344&quot;, &quot;lowFreSN&quot;: &quot;000000&quot;, &quot;hignFreModel&quot;: &quot;AR9582&quot;, &quot;hignFreSN&quot;: &quot;000000&quot;, &quot;gpsModel&quot;: &quot;abcdefg&quot;, &quot;gpsSN&quot;: &quot;000000&quot; &#125;] 123456789101112131415161718192021222324252627jq &apos;.[]&apos; apinfo.json &#123; &quot;firmwareVersion&quot;: &quot;1.0.6.29&quot;, &quot;hardVersion&quot;: &quot;1.20&quot;, &quot;Operators&quot;: &quot;CTCC&quot;, &quot;iccid&quot;: &quot;DATA DATA&quot;, &quot;snOf3g&quot;: &quot;A000004E123ABD2&quot;, &quot;modelOf3g&quot;: &quot;MC271X&quot;, &quot;Company_3g&quot;: &quot;ZTEMT INCORPORATED&quot;, &quot;MEID_3g&quot;: &quot;A000004E123ABD2&quot;, &quot;memorySN&quot;: &quot;000000&quot;, &quot;memoryModel&quot;: &quot;abcdefg&quot;, &quot;cpuSN&quot;: &quot;000000&quot;, &quot;cpuModel&quot;: &quot;MIPS 74Kc V4.12&quot;, &quot;mac&quot;: &quot;00:1F:64:CE:F3:8E&quot;, &quot;hostsn&quot;: &quot;01010730b12014A00477&quot;, &quot;hostModel&quot;: &quot;CS-VIC-1999-C&quot;, &quot;stCompany&quot;: &quot;Beijing Autelan Technology&quot;, &quot;boardSN&quot;: &quot;01010730b12014A00477&quot;, &quot;networkCardMac&quot;: &quot;00:1F:64:CE:F3:8F&quot;, &quot;lowFreModel&quot;: &quot;AR9344&quot;, &quot;lowFreSN&quot;: &quot;000000&quot;, &quot;hignFreModel&quot;: &quot;AR9582&quot;, &quot;hignFreSN&quot;: &quot;000000&quot;, &quot;gpsModel&quot;: &quot;abcdefg&quot;, &quot;gpsSN&quot;: &quot;000000&quot;&#125; 12345678910111213$ jq -r .[].mac apinfo.json 00:1F:64:CE:F3:8E$ jq -r &apos;.[] | .mac&apos; apinfo.json 00:1F:64:CE:F3:8E$ jq -r &apos;.[] |.mac, .gpsSN&apos; apinfo.json 00:1F:64:CE:F3:8E000000jq -r &apos;.[].mac, .[].gpsSN&apos; apinfo.json 00:1F:64:CE:F3:8E000000 xargsxargs与find经常结合来进行文件操作，平时删日志的时候只是习惯的去删除，比如 # find . -type f -name “*.log” | xargs rm -rf * 就将以log结尾的文件删除了，如果我想去移动或者复制就需要使用参数来代替了。 1[root@jstuz6zw4s2vwp tmp]# find . -type f -name &quot;*.log&quot; | xargs -i cp &#123;&#125; /tmp/k/ 1[root@jstuz6zw4s2vwp tmp]# find . -type f -name &quot;*.log&quot; | xargs -I &#123;&#125; cp &#123;&#125; /tmp/n/ 结论： 加-i 参数直接用 {}就能代替管道之前的标准输出的内容；加 -I 参数 需要事先指定替换字符。 其它示例 12echo --help | xargs cat #将echo输出的信息作为cat命令的参数使用，xargs传递参数，将前一个命令的标准输出作为后一个命令的参数使用 12ls *.txt | xargs -i mv &#123;&#125; /mnt#查看当前目录下所有txt文件，xargs的-i参数是将前面的标准输出作为参数传递给&#123;&#125; 12echo &quot;ni|shi|shui&quot; | xargs -d&quot;|&quot; -n2#xargs的-d参数指定分隔符，-n表示每行显示的列数 1234567891011121314151617181920212223[root@b test]# find . -name &quot;*.txt&quot; -exec tar -cf a.tar &#123;&#125; \;[root@b test]# tar -tf a.tar./m.txtfind命令的-exec参数将前面find查找到的内容交给后面的tar命令打包，由于find每次查找一个就执行一次exec，所以tar最后打包的文件全部覆盖只剩下最后一个文件。[root@b test]# find . -name &quot;*.txt&quot; | xargs -i tar cf b.tar &#123;&#125;[root@b test]# tar -tf b.tar./m.txt[root@b test]# find . -name &quot;*.txt&quot; -print | xargs -i tar cf b.tar &#123;&#125;[root@b test]# tar -tf b.tar./m.txt#写法相同，xargs的-i参数表示将前面find的信息传递到后面&#123;&#125;进行打包，每查找到一个文件就进行打包一次，所以会重复覆盖。[root@b test]# find . -name &quot;*.txt&quot; -print0 | xargs -0 tar cf b.tar[root@b test]# tar -tf b.tar./a.txt./b.txt./m.txt#print0表示将find查找的内容在同一行输出，xargs的-0参数指定以null为分隔符来进行打包。 ulimitulimit 是一个计算机命令，用于shell启动进程所占用的资源，可用于修改系统资源限制 常用参数 12345678910111213141516-H 设置硬资源限制.-S 设置软资源限制.-a 显示当前所有的资源限制.-c size:设置core文件的最大值.单位:blocks-d size:设置数据段的最大值.单位:kbytes-f size:设置创建文件的最大值.单位:blocks-l size:设置在内存中锁定进程的最大值.单位:kbytes-m size:设置可以使用的常驻内存的最大值.单位:kbytes-n size:设置内核可以同时打开的文件描述符的最大值.单位:n-p size:设置管道缓冲区的最大值.单位:kbytes-s size:设置堆栈的最大值.单位:kbytes-t size:设置CPU使用时间的最大上限.单位:seconds-v size:设置虚拟内存的最大值.单位:kbytes-u &lt;程序数目&gt; 用户最多可开启的程序数目文件： /etc/security/limits.conf 常用命令 1.查看最大的标示符 1ulimit -n 2.调整文件描述符 1ulimit -SHn 65535 临时修改 S:soft limit H:Hard limit 3.推荐控制文件描述符大小 123456更改： &lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt;echo &apos;* - nofile 65535&apos; &gt;&gt; /etc/security/limits.conf 更改后查看：tail -1 /etc/security/limits.conf 退出重新登陆后生效确认查看：ulimit -n]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell脚本技巧]]></title>
    <url>%2FShell%E8%84%9A%E6%9C%AC%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[参考文章http://www.ruanyifeng.com/blog/2017/11/bash-set.html [TOC] shell语法检查与执行verbose模式读取脚本时，显示读到的每一行 1$ bash -v script.sh 语法检查调试模式shell读取脚本时检查语法，一旦发现语法错误即在终端输出，无语法错误不输出 1$ bash -n script.sh 分步执行1$ bash -x script.sh shell 中 &amp;&amp; || () {} 用法&amp;&amp; 运算符:格式 1command1 &amp;&amp; command2 &amp;&amp;左边的命令（命令1）返回真(即返回0，成功被执行）后，&amp;&amp;右边的命令（命令2）才能够被执行；换句话说，“如果这个命令执行成功&amp;&amp;那么执行这个命令”。语法格式如下： 1command1 &amp;&amp; command2 &amp;&amp; command3 ... 命令之间使用 &amp;&amp; 连接，实现逻辑与的功能。 只有在 &amp;&amp; 左边的命令返回真（命令返回值 $? == 0），&amp;&amp; 右边的命令才会被执行。 只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。 || 运算符:格式 1command1 || command2 ||则与&amp;&amp;相反。如果||左边的命令（command1）未执行成功，那么就执行||右边的命令（command2）；或者换句话说，“如果这个命令执行失败了||那么就执行这个命令。 命令之间使用 || 连接，实现逻辑或的功能。 只有在 || 左边的命令返回假（命令返回值 $? == 1），|| 右边的命令才会被执行。这和 c 语言中的逻辑或语法功能相同，即实现短路逻辑或操作。 只要有一个命令返回真（命令返回值 $? == 0），后面的命令就不会被执行。 ||与&amp;&amp;1command1 &amp;&amp; command2 || command3 将||与&amp;&amp;组合使用则表达，如果command1执行成功，则执行command2，否则执行command3 下面是一个shell脚本中常用的||组合示例 1echo $BASH |grep -q &apos;bash&apos; || &#123; exec bash &quot;$0&quot; &quot;$@&quot; || exit 1; &#125; 系统调用exec是以新的进程去代替原来的进程，但进程的PID保持不变。因此，可以这样认为，exec系统调用并没有创建新的进程，只是替换了原来进程上下文的内容。原进程的代码段，数据段，堆栈段被新的进程所代替。 () 运算符:如果希望把几个命令合在一起执行，shell提供了两种方法。既可以在当前shell也可以在子shell中执行一组命令。格式: 1(command1;command2;command3....) 多个命令之间用;分隔 一条命令需要独占一个物理行，如果需要将多条命令放在同一行，命令之间使用命令分隔符（;）分隔。执行的效果等同于多个独立的命令单独执行的效果。 () 表示在当前 shell 中将多个命令作为一个整体执行。需要注意的是，使用 () 括起来的命令在执行前面都不会切换当前工作目录，也就是说命令组合都是在当前工作目录下被执行的，尽管命令中有切换目录的命令。 命令组合常和命令执行控制结合起来使用。 {} 运算符:如果使用{}来代替()，那么相应的命令将在子shell而不是当前shell中作为一个整体被执行，只有在{}中所有命令的输出作为一个整体被重定向时，其中的命令才被放到子shell中执行，否则在当前shell执行。它的一般形式为： 1&#123; command1;command2;command3… &#125; 注意：在使用&#123;&#125;时，&#123;&#125;与命令之间必须使用一个空格 shell脚本获取当前目录和文件夹名1234#!/bin/bashproject_path = $(cd `dirname $0`;pwd)project_name = &quot;$&#123;project_path##*/&#125;&quot; 可以用${ }分别替换得到不同的值：file=/dir1/dir2/dir3/my.file.txt${file#/}：删掉第一个 / 及其左边的字符串：dir1/dir2/dir3/my.file.txt${file##/}：删掉最后一个 / 及其左边的字符串：my.file.txt${file#.}：删掉第一个 . 及其左边的字符串：file.txt${file##.}：删掉最后一个 . 及其左边的字符串：txt${file%/}：删掉最后一个 / 及其右边的字符串：/dir1/dir2/dir3${file%%/}：删掉第一个 / 及其右边的字符串：(空值)${file%.}：删掉最后一个 . 及其右边的字符串：/dir1/dir2/dir3/my.file${file%%.}：删掉第一个 . 及其右边的字符串：/dir1/dir2/dir3/my Shell脚本输出颜色介绍1234567echo -e &quot;\033[43;35m david use echo say Hello World \033[0m \n&quot; printf &quot;\033[44;36m david use printf say Hello World \033[0m \n&quot; echo -e &quot;\033[47;30;5m david use echo say \033[0m Hello World \n&quot;echo -e &quot;\033[字背景颜色;字体颜色m 字符串 \033[0m&quot; 或者printf &quot;\033[字背景颜色;字体颜色m 字符串 \033[0m&quot; 或者echo -e &quot;\033[字背景颜色;字体颜色m;ascii码m 字符串 \033[0m 字符串(can null) \n&quot; 1234567echo -e &quot;\033[47;30;5m david use echo say \033[0m Hello World \n&quot; 作用: 42-&gt;背景色为白色， 30-&gt;字体为黑色， 5-&gt;字体闪烁， 0-&gt;关闭所有属性 输出字符 “david use echo say”，然后重新设置屏幕到缺省设置，输出字符 “Hello World”后颜色回复正常 1echo -e &quot;\033[20;1H\033[1;4;32m david use echo say \033[0m Hello World \n&quot; 这行命令首先\033[20;1H将光标移动到终端第20行第1列，之后的\033[1;4;32m将文本属性设置为高亮、带下划线且颜色为绿色，然后输出Hello,world；最后\033[0m将终端属性设为缺省，这样就不会看到连命令完成后的命令提示符也变了样儿了。我们可以通过各种命令的组合可以实现对终端输出地复杂控制。 123456789101112131415161718192021222324252627282930313233343536373839404142434445460 重新设置属性到缺省设置1 设置粗体2 设置一半亮度(模拟彩色显示器的颜色)4 设置下划线(模拟彩色显示器的颜色)5 设置闪烁7 设置反向图象8 消隐 22 设置一般密度24 关闭下划线25 关闭闪烁27 关闭反向图象// 字体颜范围(前景颜色):30~3930:黑 31:红 32:绿 33:黄 34:蓝色 35:紫色 36:深绿 37:白色 38:在缺省的前景颜色上设置下划线39:在缺省的前景颜色上关闭下划线// 字背景颜色范围(背景颜色):40~49 40:黑 41:深红 42:绿 43:黄色 44:蓝色 45:紫色 46:深绿 47:白色 nA 光标上移n行 nB 光标下移n行 nC 光标右移n行 nD 光标左移n行 y;xH设置光标位置 2J 清屏 K 清除从光标到行尾的内容 s 保存光标位置 u 恢复光标位置 ?25l 隐藏光标 ?25h 显示光标 脚本颜色变量123456789RED_COLOR=&apos;\E[1;31m&apos; YELOW_COLOR=&apos;\E[1;33m&apos; BLUE_COLOR=&apos;\E[1;34m&apos; RESET=&apos;\E[0m&apos;#需要使用echo -eecho -e &quot;$&#123;RED_COLOR&#125;===david say red color===$&#123;RESET&#125;&quot;echo -e &quot;$&#123;YELOW_COLOR&#125;===david say yelow color===$&#123;RESET&#125;&quot;echo -e &quot;$&#123;BLUE_COLOR&#125;===david say green color===$&#123;RESET&#125;&quot; set命令 set -u 遇到不存在的变量，Bash报错并停止执行。还有另外一中写法-u还有另一种写法-o nounset，两者是等价的。 set -x 默认情况下，脚本执行后，屏幕只显示运行结果，没有其他内容。如果多个命令连续执行，它们的运行结果就会连续输出。有时会分不清，某一段内容是什么命令产生的。set -x用来在运行结果之前，先输出执行的那一行命令。-x还有另一种写法-o xtrace。 set -e 它使得脚本只要发生错误，就终止执行。set -e根据返回值来判断，一个命令是否运行失败。但是，某些命令的非零返回值可能不表示失败，或者开发者希望在命令失败的情况下，脚本继续执行下去。这时可以暂时关闭set -e，该命令执行结束后，再重新打开set -e set +e表示关闭-e选项，set -e表示重新打开-e选项。 -e还有另一种写法-o errexit set -o pipefail set -e有一个例外情况，就是不适用于管道命令。所谓管道命令，就是多个子命令通过管道运算符（|）组合成为一个大的命令。Bash 会把最后一个子命令的返回值，作为整个命令的返回值。也就是说，只要最后一个子命令不失败，管道命令总是会执行成功，因此它后面命令依然会执行，set -e就失效了。 总结set命令的上面这四个参数，一般都放在一起使用 123456# 写法一set -euxo pipefail# 写法二set -euxset -o pipefail 这两种写法建议放在所有 Bash 脚本的头部。 另一种办法是在执行 Bash 脚本的时候，从命令行传入这些参数。 1$ bash -euxo pipefail script.sh Bash的错误处理如果脚本里面有运行失败的命令（返回值非0），Bash 默认会继续执行后面的命令。 12345&gt; #!/usr/bin/env bash&gt;&gt; foo&gt; echo bar&gt; 上面脚本中，foo是一个不存在的命令，执行时会报错。但是，Bash 会忽略这个错误，继续往下执行。 1234&gt; $ bash script.sh&gt; script.sh:行3: foo: 未找到命令&gt; bar&gt; 可以看到，Bash 只是显示有错误，并没有终止执行。 这种行为很不利于脚本安全和除错。实际开发中，如果某个命令失败，往往需要脚本停止执行，防止错误累积。这时，一般采用下面的写法。 12&gt; command || exit 1&gt; 上面的写法表示只要command有非零返回值，脚本就会停止执行。 如果停止执行之前需要完成多个操作，就要采用下面三种写法。 12345678910&gt; # 写法一&gt; command || &#123; echo &quot;command failed&quot;; exit 1; &#125;&gt;&gt; # 写法二&gt; if ! command; then echo &quot;command failed&quot;; exit 1; fi&gt;&gt; # 写法三&gt; command&gt; if [ &quot;$?&quot; -ne 0 ]; then echo &quot;command failed&quot;; exit 1; fi&gt; 另外，除了停止执行，还有一种情况。如果两个命令有继承关系，只有第一个命令成功了，才能继续执行第二个命令，那么就要采用下面的写法。 12&gt; command1 &amp;&amp; command2&gt; command || true，使得该命令即使执行失败，脚本也不会终止执行。 123456&gt; #!/bin/bash&gt; set -e&gt;&gt; foo || true&gt; echo bar&gt; 上面代码中，true使得这一行语句总是会执行成功，后面的echo bar会执行。 常用表达式文件表达式 -e filename 如果 filename存在，则为真 -d filename 如果 filename为目录，则为真 -f filename 如果 filename为常规文件，则为真 -L filename 如果 filename为符号链接，则为真 -r filename 如果 filename可读，则为真 -w filename 如果 filename可写，则为真 -x filename 如果 filename可执行，则为真 -s filename 如果文件长度不为0，则为真 -h filename 如果文件是软链接，则为真 filename1 -nt filename2 如果 filename1比 filename2新，则为真。 filename1 -ot filename2 如果 filename1比 filename2旧，则为真。 整数变量表达式 -eq 等于 -ne 不等于 -gt 大于 -ge 大于等于 -lt 小于 -le 小于等于 字符串变量表达式 If [ $a = $b ] 如果string1等于string2，则为真，字符串允许使用赋值号做等号 if [ $string1 != $string2 ] 如果string1不等于string2，则为真 if [ -n $string ] 如果string 非空(非0），返回0(true) if [ -z $string ] 如果string 为空，则为真 if [ $sting ] 如果string 非空，返回0 (和-n类似) if [ ! -d $num ] if [ 表达式1 –a 表达式2 ] if [ 表达式1 –o 表达式2 ] Shell脚本中调用另外一个脚本的方法在Linux平台上开发，经常会在console(控制台)上执行另外一个脚本文件，经常用的方法有：./my.sh 或 source my.sh 或 . my.sh；这三种方法有什么不同呢？我们先来了解一下在一个shell脚本中如何调用另外一个shell脚本，其方法有 fork exec source。 fork ( /directory/script.sh) ： 如果shell中包含执行命令，那么子命令并不影响父级的命令，在子命令执行完后再执行父级命令。子级的环境变量不会影响到父级。 fork是最普通的, 就是直接在脚本里面用/directory/script.sh来调用script.sh这个脚本. 运行的时候开一个sub-shell执行调用的脚本，sub-shell执行的时候,parent-shell还在。 sub-shell执行完毕后返回parent-shell. sub-shell从parent-shell继承环境变量.但是sub-shell中的环境变量不会带回parent-shell exec (exec /directory/script.sh)： 执行子级的命令后，不再执行父级命令。 exec与fork不同，不需要新开一个sub-shell来执行被调用的脚本. 被调用的脚本与父脚本在同一个shell内执行。但是使用exec调用一个新脚本以后, 父脚本中exec行之后的内容就不会再执行了。这是exec和source的区别 source (source /directory/script.sh)： 执行子级命令后继续执行父级命令，同时子级设置的环境变量会影响到父级的环境变量。 与fork的区别是不新开一个sub-shell来执行被调用的脚本，而是在同一个shell中执行. 所以被调用的脚本中声明的变量和环境变量, 都可以在主脚本中得到和使用. 以上三种就是调用shell脚本的不同方法，./my.sh即是fork的方法，source my.sh和. my.sh（点加空格加脚本文件）既是source的方法。 在linux系统上，搭建嵌入式开发平台，在交叉编译代码之前，都需要执行脚本设置环境变量，切记需要使用sourc 或 点的方式执行shell脚本，原因如上。 Shell脚本执行mysql命令 mysql -hhostname -uuser -ppasswd -e “mysql_cmd” mysql -hhostname -uuser -ppasswd &lt;&lt; EOF mysql_cmd EOF Shell for循环多个变量需求：需要输出以下2开头的端口号和其对应的文件 1port and port_k8s_xxx.conf 1234567891011#! /bash/shell#以value_name=(value1 value2 value3)的形式定义数组a=(`ls |grep -v ^1|grep -v 22281_k8s_qkd_http.conf|grep -v for.sh| awk -F &apos;_&apos; &apos;&#123;print $1&#125;&apos;`)b=(`ls |grep -v ^1|grep -v 22281_k8s_qkd_http.conf|grep -v for.sh`)#以 $&#123;a[number]&#125; 的形式调用数组的第 number 个变量for (( i=0; i&lt;16; i++ ))do echo $&#123;a[$i]&#125; and $&#123;b[$i]&#125;done Shell中去除字符串前后空格的方法12345678910111213141516171819202122232425262728[root@localhost ~]# echo &apos; A B C &apos; | awk &apos;&#123;gsub(/^\s+|\s+$/, &quot;&quot;);print&#125;&apos;^\s+ 匹配行首一个或多个空格\s+$ 匹配行末一个或多个空格^\s+|\s+$ 同时匹配行首或者行末的空格[root@local ~]# echo &quot; A BC &quot; A BC[root@local ~]# eval echo &quot; A BC &quot;A BC[root@linux ~]# echo &apos; A BC &apos; | python -c &quot;s=raw_input();print(s.strip())&quot;A BC[root@linux ~]# s=`echo &quot; A BC &quot;`[root@linux ~]# echo $sA BC[root@linux ~]# echo &apos; A BC &apos; | sed -e &apos;s/^[ ]*//g&apos; | sed -e &apos;s/[ ]*$//g&apos;A BC[root@linux ~]# echo &quot; A BC &quot; | awk &apos;$1=$1&apos;A BC[root@linux ~]# echo &quot; A BC &quot; | sed -r &apos;s/^[ \t]+(.*)[ \t]+$//g&apos;A BC[root@linux ~]# echo &apos; A BC &apos; | awk &apos;&#123;sub(/^ */, &quot;&quot;);sub(/ *$/, &quot;&quot;)&#125;1&apos;A BC Shell脚本操作mysql数据库mysql -hhostname -Pport -uusername -ppassword -e 相关mysql的sql语句，不用在mysql的提示符下运行mysql，即可以在shell中操作mysql的方法。 123456789#!/bin/bashHOSTNAME=&quot;192.168.111.84&quot; #数据库信息PORT=&quot;3306&quot;USERNAME=&quot;root&quot;PASSWORD=&quot;&quot;DBNAME=&quot;test_db_test&quot; #数据库名称TABLENAME=&quot;test_table_test&quot; #数据库中表的名称 12345678910111213141516171819202122232425262728293031#创建数据库create_db_sql=&quot;create database IF NOT EXISTS $&#123;DBNAME&#125;&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; -e &quot;$&#123;create_db_sql&#125;&quot;#创建表create_table_sql=&quot;create table IF NOT EXISTS $&#123;TABLENAME&#125; ( name varchar(20), id int(11) default 0 )&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; $&#123;DBNAME&#125; -e &quot;$&#123;create_table_sql&#125;&quot;#插入数据insert_sql=&quot;insert into $&#123;TABLENAME&#125; values(&apos;billchen&apos;,2)&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; $&#123;DBNAME&#125; -e &quot;$&#123;insert_sql&#125;&quot;#查询select_sql=&quot;select * from $&#123;TABLENAME&#125;&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; $&#123;DBNAME&#125; -e &quot;$&#123;select_sql&#125;&quot;#更新数据update_sql=&quot;update $&#123;TABLENAME&#125; set id=3&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; $&#123;DBNAME&#125; -e &quot;$&#123;update_sql&#125;&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; $&#123;DBNAME&#125; -e &quot;$&#123;select_sql&#125;&quot;#删除数据delete_sql=&quot;delete from $&#123;TABLENAME&#125;&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; $&#123;DBNAME&#125; -e &quot;$&#123;delete_sql&#125;&quot;mysql -h$&#123;HOSTNAME&#125; -P$&#123;PORT&#125; -u$&#123;USERNAME&#125; -p$&#123;PASSWORD&#125; $&#123;DBNAME&#125; -e &quot;$&#123;select_sql&#125;&quot;show processlist如何过滤的问题我终于知道如何解决了mysql -uroot -e -p password &apos;show processlist\G&apos;;mysql -uroot -e &apos;show processlist\G&apos;|grep &apos;Info&apos;|grep -v &quot;NULL&quot;|awk -F &quot;:&quot; &apos;&#123;print $2&#125;&apos;|sort|uniq -c|sort -rn;(查看正在执行的语句有哪些,并做好归并排序:) 123mysql -uroot -h$host -p$password &lt;&lt; EOFcreate database if not exists $&#123;database&#125;;EOF]]></content>
      <categories>
        <category>shell脚本</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab定时执行任务]]></title>
    <url>%2Fcrontab%E5%AE%9A%E6%97%B6%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Crontab介绍 crond 是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务 工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 Cron服务 cron是一个linux下 的定时执行工具，可以在无需人工干预的情况下运行作业。 service crond start //启动服务 service crond stop //关闭服务 service crond restart //重启服务 service crond reload //重新载入配置 service crond status //查看服务状态 定时文件 /var/spool/cron/ 这个目录下存放的是每个用户包括root的crontab任务，每个任务以创建者的名字命名，比如tom建的crontab任务对应的文件就是/var/spool/cron/tom。一般一个用户最多只有一个crontab文件。（使用crontab -e创建的文件就存放在/var/spool/cron下） /etc/crontab 这个文件负责安排由系统管理员制定的维护系统以及其他任务的crontab 123456789101112131415SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=rootHOME=/# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed MAILTO=root：是说，当 /etc/crontab 这个档案中的例行性命令发生错误时，会将错误讯息或者是屏幕显示的讯息传给谁？由于 root 并无法再用户端收信，因此，我通常都將这个 e-mail 改成自己的账号，好让我随时了解系统的状态！ 01 * * * * root run-parts /etc/cron.hourly：在 #run-parts 这一行以后的命令，我们可以发现，五个数字后面接的是 root ，这一行代表的是『执行的级别为root身份』当然，你也可以将这一行改为成其他的身份！而 run-parts代表后面接的 /etc/cron.hourly 是『一个目录内（/etc/cron.hourly）的所有可执行文件』，也就是说，每个小时的01分，系统会以root身份去/etc/cron.hourly这个目录下执行所有可执行的文件！后面三行也是类似的意思！你可以到 /etc/ 底下去看看，系统本来就预设了这4个目录！你可以将每天需要执行的命令直接写到/etc/cron.daily即可，还不需要使用到crontab -e的程式！ /etc/cron.d/ 这个目录用来存放任何要执行的crontab文件或脚本 权限crontab权限问题到/var/adm/cron/下一看，文件cron.allow和cron.deny是否存在用法如下：1、如果两个文件都不存在，则只有root用户才能使用crontab命令。2、如果cron.allow存在但cron.deny不存在，则只有列在cron.allow文件里的用户才能使用crontab命令，如果root用户也不在里面，则root用户也不能使用crontab。3、如果cron.allow不存在, cron.deny存在，则只有列在cron.deny文件里面的用户不能使用crontab命令，其它用户都能使用。4、如果两个文件都存在，则列在cron.allow文件中而且没有列在cron.deny中的用户可以使用crontab，如果两个文件中都有同一个用户，以cron.allow文件里面是否有该用户为准，如果cron.allow中有该用户，则可以使用crontab命令。 AIX 中 普通用户默认都有 crontab 权限，如果要限制用户使用 crontab ,就需要编辑/var/adm/cron/cron.denyHP-UNIX 中默认普通用户没得crontab 权限 ，要想放开普通用户的crontab 权限可以编 创建cron脚本12345678#创建脚本，每隔15分钟执行一次命令cat &gt; crontest.cron &lt;&lt; EOF15,30,45,59 * * * * echo &quot;xgmtest.....&quot; &gt;&gt; xgmtest.txtEOF#添加定时任务crontab crontest.cron#查看是否添加成功crontab -l 注意：这操作是直接替换该用户下的crontab，而不是新增 Crontab用法 -l 在标准输出上显示当前的crontab。 -r 删除当前的crontab文件。 -e 使用VISUAL或者EDITOR环境变量所指的编辑器编辑当前的crontab文件。当结束编辑离开时，编辑后的文件将自动安装。 crontab命令用于安装、删除或者列出用于驱动cron后台进程的表格。用户把需要执行的命令序列放到crontab文件中以获得执行。 每个用户都可以有自己的crontab文件。/var/spool/cron下的crontab文件不可以直接创建或者直接修改。该crontab文件是通过crontab命令创建的 在crontab文件中如何输入需要执行的命令和时间。该文件中每行都包括六个域，其中前五个域是指定命令被执行的时间，最后一个域是要被执行的命令。每个域之间使用空格或者制表符分隔。格式如下： 123minute hour day-of-month month-of-year day-of-week commands合法值 00-59 00-23 01-31 01-12 0-6 (0 is sunday) 除了数字还有几个个特殊的符号就是&quot;&quot;、&quot;/&quot;和&quot;-&quot;、&quot;,&quot;，代表所有的取值范围内的数字，&quot;/&quot;代表每的意思,&quot;/5&quot;表示每5个单位，&quot;-&quot;代表从某个数字到某个数字,&quot;,&quot;分开几个离散的数字。 语法crontab的语法规则格式： 代表意义 分钟 小时 日期 月份 周 命令 数字范围 0~59 0~23 1~31 1~12 0~7 需要执行的命令 周的数字为 0 或 7 时，都代表“星期天”的意思。 另外，还有一些辅助的字符，大概有下面这些： 特殊字符 代表意义 *(星号) 代表任何时刻都接受的意思。举例来说，0 12 * * * command 日、月、周都是*，就代表着不论何月、何日的礼拜几的12：00都执行后续命令的意思。 ,(逗号) 代表分隔时段的意思。举例来说，如果要执行的工作是3：00与6：00时，就会是：0 3,6 * * * command时间还是有五列，不过第二列是 3,6 ，代表3与6都适用 -(减号) 代表一段时间范围内，举例来说，8点到12点之间的每小时的20分都进行一项工作：20 8-12 * * * command仔细看到第二列变成8-12.代表 8,9,10,11,12 都适用的意思 /n(斜线) 那个n代表数字，即是每隔n单位间隔的意思，例如每五分钟进行一次，则：*/5 * * * * command用*与/5来搭配，也可以写成0-59/5，意思相同 用例 每天早上6点 10 6 * * * echo &quot;haha&quot; &gt;&gt; /tmp/test.txt 每两个小时 10 */2 * * * command 晚上11点到早上8点之间每两个小时和早上八点 10 23-7/2,8 * * * command 每个月的4号和每个礼拜的礼拜一到礼拜三的早上11点 10 11 4 * 1-3 command 1月1日早上4点 10 4 1 1 * command 每小时执行/etc/cron.hourly内的脚本 101 * * * * root run-parts /etc/cron.hourly 每天执行/etc/cron.daily内的脚本 10 2 * * * root run-parts /etc/cron.daily 每星期执行/etc/cron.weekly内的脚本 10 2 * * 0 root run-parts /etc/cron.weekly 每月去执行/etc/cron.monthly内的脚本 10 2 1 * * root run-parts /etc/cron.monthly 注意: “run-parts”这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是文件夹名。 每天的下午4点、5点、6点的5 min、15 min、25 min、35 min、45 min、55 min时执行命令。 15，15，25，35，45，55 16，17，18 * * * command 每周一，三，五的下午3：00系统进入维护状态，重新启动系统。 100 15 * * 1，3，5 shutdown -r +5 每小时的10分，40分执行用户目录下的innd/bbslin这个指令： 110，40 * * * * innd/bbslink 每小时的1分执行用户目录下的bin/account这个指令： 11 * * * * bin/account 每天早晨三点二十分执行用户目录下如下所示的两个指令（每个指令以;分隔）： 120 3 * * * （/bin/rm -f expire.ls logins.bad;bin/expire$#@62;expire.1st） 每分钟定时执行一次规则： 12每1分钟执行： */1 * * * *或者* * * * *每5分钟执行： */5 * * * * 每小时定时执行一次规则： 123每小时执行： 0 * * * *或者0 */1 * * *每天上午7点执行：0 7 * * *每天上午7点10分执行：10 7 * * * 每天定时执行一次规则： 1每天执行 0 0 * * * 每周定时执行一次规则： 1每周执行 0 0 * * 0 每月定时执行一次规则： 1每月执行 0 0 1 * * 每年定时执行一次规则： 1每年执行 0 0 1 1 * 其他例子 12345678910111213141516175 * * * * 指定每小时的第5分钟执行一次ls命令30 5 * * * ls 指定每天的 5:30 执行ls命令30 7 8 * * ls 指定每月8号的7：30分执行ls命令30 5 8 6 * ls 指定每年的6月8日5：30执行ls命令30 6 * * 0 ls 指定每星期日的6:30执行ls命令[注：0表示星期天，1表示星期1，以此类推，也可以用英文来表示，sun表示星期天，mon表示星期一等。]30 3 10,20 * * ls 每月10号及20号的3：30执行ls命令[注：“，”用来连接多个不连续的时段]25 8-11 * * * ls 每天8-11点的第25分钟执行ls命令[注：“-”用来连接连续的时段]*/15 * * * * ls 每15分钟执行一次ls命令 [即每个小时的第0 15 30 45 60分钟执行ls命令 ]30 6 */10 * * ls 每个月中，每隔10天6:30执行一次ls命令，即每月的1、11、21、31日是的6：30执行一次ls命令。 查找所有用户的crontab 1cut -d: -f1 /etc/passwd|xargs -i crontab -u &#123;&#125; -l ​]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcpdump常用命令]]></title>
    <url>%2Ftcpdump%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[介绍 tcpdump是一个用于截取网络分组，并输出分组内容的工具。tcpdump凭借强大的功能和灵活的截取策略，使其成为类UNIX系统下用于网络分析和问题排查的首选工具。 tcpdump提供了源代码，公开了接口，因此具备很强的可扩展性，对于网络维护和入侵者都是非常有用的工具。tcpdump存在于基本的Linux系统中，由于它需要将网络界面设置为混杂模式，普通用户不能正常执行，但具备root权限的用户可以直接执行它来获取网络上的信息。因此系统中存在网络分析工具主要不是对本机安全的威胁，而是对网络上的其他计算机的安全存在威胁。 常用选项 -A 以ASCII格式打印出所有分组，并将链路层的头最小化。 -c 在收到指定的数量的分组后，tcpdump就会停止。 -C 在将一个原始分组写入文件之前，检查文件当前的大小是否超过了参数file_size 中指定的大小。如果超过了指定大小，则关闭当前文件，然后在打开一个新的文件。参数 file_size 的单位是兆字节（是1,000,000字节，而不是1,048,576字节）。 -d 将匹配信息包的代码以人们能够理解的汇编格式给出。 -dd 将匹配信息包的代码以c语言程序段的格式给出。 -ddd 将匹配信息包的代码以十进制的形式给出。 -D 打印出系统中所有可以用tcpdump截包的网络接口。 -e 在输出行打印出数据链路层的头部信息。 -E 用spi@ipaddr algo:secret解密那些以addr作为地址，并且包含了安全参数索引值spi的IPsec ESP分组。 -f 将外部的Internet地址以数字的形式打印出来。 -F 从指定的文件中读取表达式，忽略命令行中给出的表达式。 -i 指定监听的网络接口。 -l 使标准输出变为缓冲行形式，可以把数据导出到文件。 -L 列出网络接口的已知数据链路。 -m 从文件module中导入SMI MIB模块定义。该参数可以被使用多次，以导入多个MIB模块。 -M 如果tcp报文中存在TCP-MD5选项，则需要用secret作为共享的验证码用于验证TCP-MD5选选项摘要（详情可参考RFC 2385）。 -b 在数据-链路层上选择协议，包括ip、arp、rarp、ipx都是这一层的。 -n 不把网络地址转换成名字。 -nn 不进行端口名称的转换。 -N 不输出主机名中的域名部分。例如，‘nic.ddn.mil‘只输出’nic‘。 -t 在输出的每一行不打印时间戳。 -O 不运行分组分组匹配（packet-matching）代码优化程序。 -P 不将网络接口设置成混杂模式。 -q 快速输出。只输出较少的协议信息。 -r 从指定的文件中读取包(这些包一般通过-w选项产生)。 -S 将tcp的序列号以绝对值形式输出，而不是相对值。 -s 从每个分组中读取最开始的snaplen个字节，而不是默认的68个字节。 -T 将监听到的包直接解释为指定的类型的报文，常见的类型有rpc远程过程调用）和snmp（简单网络管理协议；）。 -t 不在每一行中输出时间戳。 -tt 在每一行中输出非格式化的时间戳。 -ttt 输出本行和前面一行之间的时间差。 -tttt 在每一行中输出由date处理的默认格式的时间戳。 -u 输出未解码的NFS句柄。 -v 输出一个稍微详细的信息，例如在ip包中可以包括ttl和服务类型的信息。 -vv 输出详细的报文信息。 -w 直接将分组写入文件中，而不是不分析并打印出来。 表达式介绍表达式是一个正则表达式，tcpdump利用它作为过滤报文的条件，如果一个报文满足表 达式的条件，则这个报文将会被捕获。如果没有给出任何条件，则网络上所有的信息包 将会被截获。 在表达式中一般如下几种类型的关键字： 第一种是关于类型的关键字，主要包括host，net，port，例如 host 210.27.48.2， 指明 210.27.48.2是一台主机，net 202.0.0.0指明202.0.0.0是一个网络地址，port 23 指明端口号是23。如果没有指定类型，缺省的类型是host。 第二种是确定传输方向的关键字，主要包括src，dst，dst or src，dst and src， 这些关键字指明了传输的方向。举例说明，src 210.27.48.2 ，指明ip包中源地址是 210.27.48.2 ， dst net 202.0.0.0 指明目的网络地址是202.0.0.0。如果没有指明 方向关键字，则缺省是src or dst关键字。 第三种是协议的关键字，主要包括fddi，ip，arp，rarp，tcp，udp等类型。Fddi指明是在FDDI (分布式光纤数据接口网络)上的特定的网络协议，实际上它是”ether”的别名，fddi和ether 具有类似的源地址和目的地址，所以可以将fddi协议包当作ether的包进行处理和分析。 其他的几个关键字就是指明了监听的包的协议内容。如果没有指定任何协议，则tcpdump 将会 监听所有协议的信息包。 除了这三种类型的关键字之外，其他重要的关键字如下：gateway， broadcast，less， greater， 还有三种逻辑运算，取非运算是 ‘not ’ ‘! ‘， 与运算是’and’，’&amp;&amp;’;或运算是’or’ ，’||’； 这些关键字可以组合起来构成强大的组合条件来满足人们的需要。 举例 想要截获所有192.168.5.85 的主机收到的和发出的所有的分组： 1tcpdump host 192.168.5.85 如果想要获取主机192.168.5.95除了和主机192.168.5.6之外所有主机通信的ip包，使用命令 1tcpdump ip host 192.168.5.95 and ! 192.168.5.6 如果想要获取主机192.168.5.95接收或发出的ssh包，并且不转换主机名使用如下命令： 1tcpdump -nn -n tcp and src host 192.168.5.95 and port 22 获取主机192.168.228.246接收或发出的ssh包，并把mac地址也一同显示 1tcpdump -e -n -nn src host 192.168.228.246 and port 22 and tcp 抓取接口eth0的所有数据包 1tcpdump -i eth0 -c 100 抓取eth0上端口是22的数据包 1tcpdump -nn -c 50 -i eth0 port 22 抓取接口eth0上端口是999和主机192.168.50.200通信的数据包 1tcpdump -nn -c 50 -i eth0 port 999 and host 192.168.50.200 保存文件 1tcpdump -nn -c 50 -i eth0 port 999 and host 192.168.50.200 -w /home/gz/test.cap 监听eth0网卡HTTP 80端口的request和response 1tcpdump -i eth0 -A -s 0 &apos;tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)&apos; 监听eth0网卡HTTP 80端口的request(不包括response)，指定来源域名”example.com”，也可以指定IP”192.168.1.107” 1tcpdump -i eth0 -A -s 0 &apos;src example.com and tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)&apos; 监听本机发送至本机的HTTP 80端口的request和response 1tcpdump -i lo -A -s 0 &apos;tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)&apos; 监听eth0网卡HTTP 80端口的request和response，结果另存为cap文件 1tcpdump -i eth0 -A -s 0 &apos;tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)&apos; -w ./dump.cap]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7初始化配置(做标准化)]]></title>
    <url>%2FCentOS7%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE-%E5%81%9A%E6%A0%87%E5%87%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[安装标准化语言选择可选中文和英文 时区选择亚洲上海，CST时区 分区方式 挂载路径 分区格式 分区大小 备注 swap 内存2倍 boot 500M / 剩余所有空间 安装包的选择1）如果是一般的DVD镜像，建议选择最小化安装，然后手动选择安装以下软件包组： 类别 英文名称 中文名称 备注信息 基本系统（Base System） Base 基本 基本的系统组件 compatibility libraries 兼容程序库 库文件 Debugging Tools 调试工具 常用工具 开发（Development） Development Tools 开发工具 编译工具cmake，gcc等 语言支持 English 英语 Chinese 汉语 2）如果是minimal的镜像，基本没什么需要选择的，需要的安装包在装完系统后安装即可，系统会更加精简一些，大约需要安装292个软件包 用户管理员用户：root 普通用户：自定义一个 安全选项kdump：一般用不着关闭即可 以上是安装系统时可以进行的配置选项，安装完成还需要对系统做一些基础的初始化优化配置 初始化Centos系统（优化配置）修改网络地址修改IP地址为固定地址可以使用图形化界面配置 1nmtui # 这个是CentOS7版本下的setup命令，可以配置IP，主机名，DNS等信息 配置完成重启网卡，进行测试 123systemctl restart networkip addping www.baidu.com 至此，可以使用CRT等软件远程连接进行配置 创建常用工作目录生产环境下必须有个固定的目录存放一些安装软件和调试工具，否则每个管理员都随意存放软件，服务器的环境可以想而知 12mkdir -p /server/&#123;tools,scripts,backup&#125;cd /server/tools/ 设置DNS和主机名修改服务器主机名 1234vi /etc/hostname --------------------------------demohost-------------------------------- 设置hosts域名解析 1234vi /etc/hosts--------------------------------192.168.1.200 demohost-------------------------------- 设置DNS解析 12345vi /etc/resolv.conf --------------------------------nameserver 202.106.0.20nameserver 8.8.8.8-------------------------------- 手动修改网卡配置文件，删除UUID信息和MAC地址等方便克隆，禁用IPv6相关配置 123456789101112131415161718192021222324vim /etc/sysconfig/network-scripts/ifcfg-eth0------------------------------------[root@demohost tools]# vi /etc/sysconfig/network-scripts/ifcfg-eth0NAME=eth0DEVICE=eth0TYPE=EthernetONBOOT=yesBOOTPROTO=noneDEFROUTE=yesIPADDR=192.168.1.200PREFIX=24GATEWAY=192.168.1.1IPV4_FAILURE_FATAL=noDNS1=202.106.0.20DNS2=8.8.8.8IPV6INIT=noIPV6_AUTOCONF=noIPV6_DEFROUTE=noIPV6_PEERDNS=noIPV6_PEERROUTES=noIPV6_PRIVACY=noIPV6_FAILURE_FATAL=noARPCHECK=no # 禁用ARP检查-------------------------------------- 配置完成重启网卡，进行测试 123systemctl restart networkip addping www.baidu.com 安装常用工具安装工具(wget,rz,sz,tree,dos2unix,ifconfig,nslookup等) 12yum install gcc gcc-c++ cmake pcre pcre-devel zlib zlib-devel openssl openssl-devel vim wget telnet setuptool lrzsz dos2unix net-tools bind-utils tree screen iftop ntpdate tree lsof iftop iotop -yyum groupinstall &quot;Development tools&quot; -y 配置yum源（阿里云）配置阿里云base源 123mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bakwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo cat /etc/yum.repos.d/CentOS-Base.repo 配置阿里云epel源 1234mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repocat /etc/yum.repos.d/epel.repo 创建yum缓存进行测试 12yum clean allyum makecache fast 配置系统环境变量修改记录的历史命令数量 12sed -i s#HISTSIZE=1000#HISTSIZE=10000#g /etc/profilecat /etc/profile|grep HISTSIZE=10000 设置超时自动注销登陆 # 8h=28800s 123456echo &quot; &quot; &gt;&gt; /etc/profileecho &quot;# Auto-Logout for 4 hours by zhaoshuai on $(date +%F).&quot; &gt;&gt; /etc/profileecho &quot;export TMOUT=28800&quot; &gt;&gt; /etc/profiletail -4 /etc/profilesource /etc/profileecho $TMOUT 配置系统安全选项修改ssh配置加速远程连接只监听IPv4端口，关闭GSSAPI秘钥认证，关闭DNS解析 12345678sed -i s/&apos;#ListenAddress 0.0.0.0&apos;/&apos;ListenAddress 0.0.0.0&apos;/g /etc/ssh/sshd_configgrep ListenAddress /etc/ssh/sshd_configsed -i s/&apos;GSSAPIAuthentication yes&apos;/&apos;GSSAPIAuthentication no&apos;/g /etc/ssh/sshd_configgrep GSSAPIAuthentication /etc/ssh/sshd_configsed -i s/&apos;#UseDNS yes&apos;/&apos;UseDNS no&apos;/g /etc/ssh/sshd_configgrep UseDNS /etc/ssh/sshd_config 重启sshd服务 12/bin/systemctl restart sshd.service/bin/systemctl status sshd.service 关闭selinux立即关闭selinux（立即生效） 123getenforcesetenforce 0getenforce 返回信息如下 12# Enforcing/enabled # 执行，强制执行，开启时状态为1# Permissive/disabled # 许可的，自由的，关闭时状态为0 永久关闭selinux（重启也生效） 12sed -i s#SELINUX=enforcing#SELINUX=disabled#g /etc/selinux/configcat /etc/selinux/config |grep SELINUX=disabled 关闭防火墙内网一般不需要使用 123systemctl stop firewalldsystemctl disable firewalldsystemctl status firewalld 关闭其他不用的服务邮箱服务，CentOS7默认安装postfix，而不是sendmail 1234systemctl stop postfixsystemctl disable postfixsystemctl status postfixnetstat -anptl 修改内核参数修改文件句柄数12345678vim /etc/security/limits.conf -----------------------------------# 系统最大连接数* soft nofile 65535* hard nofile 65535* soft nproc 65535* hard nproc 65535----------------------------------- 修改完如下显示 1234567891011121314151617[root@demohost tools]# ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 15335max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 65535pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 65535virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited 配置时间同步安装ntp服务并配置开机自启动 CentOS7默认的时间同步服务是chrony，这里为了方便使用ntp服务 1234yum -y install ntpsystemctl enable ntpdsystemctl start ntpdsystemctl status ntpd 手动进行时间同步 12date/usr/sbin/ntpdate ntp1.aliyun.com 配置自动同步时间 123echo &quot;# made for sync time by zhaoshuai on $(date +%F)&quot;&gt;&gt; /var/spool/cron/rootecho &apos;*/5 * * * * /usr/sbin/ntpdate ntp1.aliyun.com &gt; /dev/null 2&gt;&amp;1&apos; &gt;&gt;/var/spool/cron/rootcrontab -l 注意：中国的时区应该为CST，为中部时区，如果是EST则为东部时区，UTC开启了夏时令也不合适，需要关闭安装CentOS系统时要去掉夏令时的选项，否则在夏令时的那一天会有时间的自动变换，如果某个服务在时间上有要求就会导致该服务承载的业务出现问题，所以要关闭夏令时 更新系统内核判断是否需要更新 一般来说建议更新到最新的内核版本，防止已知的系统漏洞问题 如果要安装指定版本的软件也不能随意升级内核版本 内核升级方法 升级前后需要查看内核版本 1cat /etc/redhat-release 更新内核 1yum update 内核升级完成需要重启系统才可以生效 1reboot 注意：内核升级期间一定不可以中断操作，否则重启会无法进入系统如果内核升级期间出错，需要能连到tty终端手动调整开机需要启动的内核然后登陆系统手动修改内核启动顺序，防止重启出错，最后再次进行内核升级尝试修复 拓展：yum update与upgrade的区别？ 123update会查询互联网上最新的内核软件包进行升级upgrade只查询当前yum源中比目前已安装软件版本高的那些总体来说update升级更加彻底 至此，CentOS7的系统初始化配置就完成了 CentOS初始化脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367#!/bin/bash# init centos7# 20160818# 检查是否为root用户，脚本必须在root权限下运行if [[ &quot;$(whoami)&quot; != &quot;root&quot; ]]; then echo &quot;please run this script as root !&quot; &gt;&amp;2 exit 1fiecho -e &quot;\033[31m the script only Support CentOS_7 x86_64 \033[0m&quot;echo -e &quot;\033[31m system initialization script, Please Seriously. press ctrl+C to cancel \033[0m&quot;# 检查是否为64位系统，这个脚本只支持64位脚本platform=`uname -i`if [ $platform != &quot;x86_64&quot; ];then echo &quot;this script is only for 64bit Operating System !&quot; exit 1fiif [ &quot;$1&quot; == &quot;&quot; ];then echo &quot;The host name is empty.&quot; exit 1else hostnamectl --static set-hostname $1 hostnamectl set-hostname $1ficat &lt;&lt; EOF+---------------------------------------+| your system is CentOS 7 x86_64 || start optimizing |+---------------------------------------+EOFsleep 1# 安装必要支持工具及软件工具yum_update()&#123;yum update -yyum install -y nmap unzip wget vim lsof xz net-tools iptables-services ntpdate ntp-doc psmisc&#125;# 设置时间同步 set timezone_time()&#123;timedatectl set-timezone Asia/Shanghai/usr/sbin/ntpdate 0.cn.pool.ntp.org &gt; /dev/null 2&gt;&amp;1/usr/sbin/hwclock --systohc/usr/sbin/hwclock -wcat &gt; /var/spool/cron/root &lt;&lt; EOF10 0 * * * /usr/sbin/ntpdate 0.cn.pool.ntp.org &gt; /dev/null 2&gt;&amp;1* * * * */1 /usr/sbin/hwclock -w &gt; /dev/null 2&gt;&amp;1EOFchmod 600 /var/spool/cron/root/sbin/service crond restartsleep 1&#125;# 修改文件打开数 set the file limitlimits_config()&#123;cat &gt; /etc/rc.d/rc.local &lt;&lt; EOF#!/bin/bashtouch /var/lock/subsys/localulimit -SHn 1024000EOFsed -i &quot;/^ulimit -SHn.*/d&quot; /etc/rc.d/rc.localecho &quot;ulimit -SHn 1024000&quot; &gt;&gt; /etc/rc.d/rc.localsed -i &quot;/^ulimit -s.*/d&quot; /etc/profilesed -i &quot;/^ulimit -c.*/d&quot; /etc/profilesed -i &quot;/^ulimit -SHn.*/d&quot; /etc/profile cat &gt;&gt; /etc/profile &lt;&lt; EOFulimit -c unlimitedulimit -s unlimitedulimit -SHn 1024000EOF source /etc/profileulimit -acat /etc/profile | grep ulimitif [ ! -f &quot;/etc/security/limits.conf.bak&quot; ]; then cp /etc/security/limits.conf /etc/security/limits.conf.bakficat &gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 1024000* hard nofile 1024000* soft nproc 1024000* hard nproc 1024000hive - nofile 1024000hive - nproc 1024000EOFif [ ! -f &quot;/etc/security/limits.d/20-nproc.conf.bak&quot; ]; then cp /etc/security/limits.d/20-nproc.conf /etc/security/limits.d/20-nproc.conf.bakficat &gt; /etc/security/limits.d/20-nproc.conf &lt;&lt; EOF* soft nproc 409600root soft nproc unlimitedEOFsleep 1&#125; # 优化内核参数 tune kernel parametres sysctl_config()&#123;if [ ! -f &quot;/etc/sysctl.conf.bak&quot; ]; then cp /etc/sysctl.conf /etc/sysctl.conf.bakfi#addcat &gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_retries1 = 3net.ipv4.tcp_retries2 = 5net.ipv4.tcp_fin_timeout = 10net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_max_tw_buckets = 60000net.ipv4.tcp_max_orphans = 32768net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_wmem = 4096 16384 13107200net.ipv4.tcp_rmem = 4096 87380 17476000net.ipv4.ip_local_port_range = 1024 65000net.ipv4.route.gc_timeout = 100net.core.somaxconn = 32768net.core.netdev_max_backlog = 32768net.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_tcp_timeout_established = 180vm.overcommit_memory = 1vm.swappiness = 1fs.file-max = 1024000EOF #reload sysctl/sbin/sysctl -psleep 1&#125;# 设置UTF-8 LANG=&quot;zh_CN.UTF-8&quot;LANG_config()&#123;echo &quot;LANG=\&quot;en_US.UTF-8\&quot;&quot;&gt;/etc/locale.confsource /etc/locale.conf&#125; #关闭SELINUX disable selinuxselinux_config()&#123;sed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/g&apos; /etc/selinux/configsetenforce 0sleep 1&#125;# iptables防护墙规则设置iptables_config()&#123;mkdir -p /opt/shcat &gt; /opt/sh/ipt.sh &lt;&lt; EOF#!/bin/bash/sbin/iptables -F/sbin/iptables -t raw -F/sbin/iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT/sbin/iptables -A INPUT -s 127.0.0.1 -j ACCEPT/sbin/iptables -A INPUT -m state --state UNTRACKED,ESTABLISHED,RELATED -j ACCEPT/sbin/iptables -A OUTPUT -j ACCEPT/sbin/iptables -A INPUT -s 192.168.10.152 -j ACCEPT/sbin/iptables -A INPUT -s 192.168.20.102 -j ACCEPT/sbin/iptables -A INPUT -p tcp --dport 80 -j ACCEPT/sbin/iptables -A INPUT -p tcp --dport 22 -j ACCEPT/sbin/iptables -A INPUT -s 192.168.10.0/255.255.255.0 -p tcp --dport 8080 -j ACCEPT/sbin/iptables -A INPUT -s 192.168.20.0/255.255.255.0 -p tcp --dport 8080 -j ACCEPT/sbin/iptables -t raw -A PREROUTING -s 192.168.10.0/255.255.255.0 -p tcp --dport 80 -j NOTRACK/sbin/iptables -t raw -A PREROUTING -s 192.168.20.0/255.255.255.0 -p tcp --dport 80 -j NOTRACK/sbin/iptables -t raw -A OUTPUT -s 192.168.10.0/255.255.255.0 -p tcp --sport 80 -j NOTRACK/sbin/iptables -t raw -A OUTPUT -s 192.168.20.0/255.255.255.0 -p tcp --sport 80 -j NOTRACK/sbin/iptables -A INPUT -s 192.168.10.0/255.255.255.0 -p icmp -j ACCEPT/sbin/iptables -A INPUT -s 192.168.20.0/255.255.255.0 -p icmp -j ACCEPT/sbin/iptables -A INPUT -j REJECT/sbin/iptables -A FORWARD -j REJECT/sbin/service iptables saveecho okEOFchmod +x /opt/sh/ipt.sh/opt/sh/ipt.sh/sbin/service iptables restart/sbin/iptables -nL/sbin/iptables -t raw -L -n#echo &quot;/opt/sh/ipt.sh&quot; &gt;&gt;/etc/rc.d/rc.local&#125;# SSH配置优化 set sshd_configsshd_config()&#123;if [ ! -f &quot;/etc/ssh/sshd_config.bak&quot; ]; then cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bakficat &gt;/etc/ssh/sshd_config&lt;&lt;EOFPort 22AddressFamily inetListenAddress 0.0.0.0Protocol 2HostKey /etc/ssh/ssh_host_rsa_keyHostKey /etc/ssh/ssh_host_ecdsa_keyHostKey /etc/ssh/ssh_host_ed25519_keySyslogFacility AUTHPRIVPermitRootLogin yesMaxAuthTries 6RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keysPasswordAuthentication yesChallengeResponseAuthentication noUsePAM yesUseDNS noX11Forwarding yesUsePrivilegeSeparation sandboxAcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGESAcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENTAcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGEAcceptEnv XMODIFIERSSubsystem sftp /usr/libexec/openssh/sftp-serverEOF/sbin/service sshd restart&#125;# 关闭ipv6 disable the ipv6ipv6_config()&#123;echo &quot;NETWORKING_IPV6=no&quot;&gt;/etc/sysconfig/networkecho 1 &gt; /proc/sys/net/ipv6/conf/all/disable_ipv6echo 1 &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6echo &quot;127.0.0.1 localhost localhost.localdomain&quot;&gt;/etc/hosts#sed -i &apos;s/IPV6INIT=yes/IPV6INIT=no/g&apos; /etc/sysconfig/network-scripts/ifcfg-enp0s8for line in $(ls -lh /etc/sysconfig/network-scripts/ifcfg-* | awk -F &apos;[ ]+&apos; &apos;&#123;print $9&#125;&apos;) doif [ -f $line ] then sed -i &apos;s/IPV6INIT=yes/IPV6INIT=no/g&apos; $line echo $ifidone&#125;# 设置历史命令记录格式 historyhistory_config()&#123;export HISTFILESIZE=10000000export HISTSIZE=1000000export PROMPT_COMMAND=&quot;history -a&quot;export HISTTIMEFORMAT=&quot;%Y-%m-%d_%H:%M:%S &quot;##export HISTTIMEFORMAT=&quot;&#123;\&quot;TIME\&quot;:\&quot;%F %T\&quot;,\&quot;HOSTNAME\&quot;:\&quot;\$HOSTNAME\&quot;,\&quot;LI\&quot;:\&quot;\$(who -u am i 2&gt;/dev/null| awk &apos;&#123;print \$NF&#125;&apos;|sed -e &apos;s/[()]//g&apos;)\&quot;,\&quot;LU\&quot;:\&quot;\$(who am i|awk &apos;&#123;print \$1&#125;&apos;)\&quot;,\&quot;NU\&quot;:\&quot;\$&#123;USER&#125;\&quot;,\&quot;CMD\&quot;:\&quot;&quot;cat &gt;&gt;/etc/bashrc&lt;&lt;EOFalias vi=&apos;vim&apos;HISTDIR=&apos;/var/log/command.log&apos;if [ ! -f \$HISTDIR ];thentouch \$HISTDIRchmod 666 \$HISTDIRfiexport HISTTIMEFORMAT=&quot;&#123;\&quot;TIME\&quot;:\&quot;%F %T\&quot;,\&quot;IP\&quot;:\&quot;\$(ip a | grep -E &apos;192.168|172&apos; | head -1 | awk &apos;&#123;print \$2&#125;&apos; | cut -d/ -f1)\&quot;,\&quot;LI\&quot;:\&quot;\$(who -u am i 2&gt;/dev/null| awk &apos;&#123;print \$NF&#125;&apos;|sed -e &apos;s/[()]//g&apos;)\&quot;,\&quot;LU\&quot;:\&quot;\$(who am i|awk &apos;&#123;print \$1&#125;&apos;)\&quot;,\&quot;NU\&quot;:\&quot;\$&#123;USER&#125;\&quot;,\&quot;CMD\&quot;:\&quot;&quot; export PROMPT_COMMAND=&apos;history 1|tail -1|sed &quot;s/^[ ]\+[0-9]\+ //&quot;|sed &quot;s/$/\&quot;&#125;/&quot;&gt;&gt; /var/log/command.log&apos;EOFsource /etc/bashrc&#125;# 服务优化设置service_config()&#123;/usr/bin/systemctl stop firewalld.service /usr/bin/systemctl disable firewalld.service/usr/bin/systemctl enable iptables.service/usr/bin/systemctl enable NetworkManager-wait-online.service/usr/bin/systemctl start NetworkManager-wait-online.service/usr/bin/systemctl stop postfix.service/usr/bin/systemctl disable postfix.servicechmod +x /etc/rc.localchmod +x /etc/rc.d/rc.local#ls -l /etc/rc.d/rc.local&#125;# 路由设置route_config()&#123;#localip=`ip a|grep &quot;inet &quot;|awk -F&quot; &quot; &apos;&#123;print $2&#125;&apos;|awk -F&quot;/&quot; &apos;&#123;print $1&#125;&apos;|egrep &quot;^192&quot; |head -n 1 |awk -F &apos;[.]&apos; &apos;&#123;print $3&#125;&apos;`#if [ &quot;$localip&quot; == &quot;10&quot; ];then# echo &quot;/usr/sbin/route add -net 192.168.20.0 netmask 255.255.255.0 gw 192.168.1.1&quot;&gt;/opt/sh/route.sh# echo &quot;route -n&quot;&gt;&gt;/opt/sh/route.sh#fi#if [ &quot;$localip&quot; == &quot;20&quot; ];then# echo &quot;/usr/sbin/route add -net 192.168.10.0 netmask 255.255.255.0 gw 192.168.2.1&quot;&gt;/opt/sh/route.sh# echo &quot;route -n&quot;&gt;&gt;/opt/sh/route.sh#fi#chmod +x /opt/sh/route.sh#/opt/sh/route.sh#echo &quot;/opt/sh/route.sh&quot; &gt;&gt;/etc/rc.localecho ok&#125;# VIM设置vim_config()&#123;cat &gt; /root/.vimrc &lt;&lt; EOFset history=1000EOF#autocmd InsertLeave * se cul#autocmd InsertLeave * se nocul#set nu#set bs=2#syntax on#set laststatus=2#set tabstop=4#set go=#set ruler#set showcmd#set cmdheight=1#hi CursorLine cterm=NONE ctermbg=blue ctermfg=white guibg=blue guifg=white#set hls#set cursorline#set ignorecase#set hlsearch#set incsearch#set helplang=cn&#125;# donedone_ok()&#123;cat &lt;&lt; EOF+-------------------------------------------------+| optimizer is done || it&apos;s recommond to restart this server ! || Please Reboot system |+-------------------------------------------------+EOF&#125;# mainmain()&#123; yum_update zone_time limits_config sysctl_config LANG_config selinux_config iptables_config sshd_config ipv6_config history_config service_config route_config vim_config done_ok&#125;main]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>初始化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7利用rsync搭建内网yum源镜像服务器]]></title>
    <url>%2FCentOS7%E5%88%A9%E7%94%A8rsync%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90%E9%95%9C%E5%83%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[rsync同步基本说明rsync的目的是实现本地主机和远程主机上的文件同步(包括本地推到远程，远程拉到本地两种同步方式)，也可以实现本地不同路径下文件的同步，但不能实现远程路径1到远程路径2之间的同步(scp可以实现)。 不考虑rsync的实现细节，就文件同步而言，涉及了源文件和目标文件的概念，还涉及了以哪边文件为同步基准。例如，想让目标主机上的文件和本地文件保持同步，则是以本地文件为同步基准，将本地文件作为源文件推送到目标主机上。反之，如果想让本地主机上的文件和目标主机上的文件保持同步，则目标主机上的文件为同步基准，实现方式是将目标主机上的文件作为源文件拉取到本地。当然，要保持本地的两个文件相互同步，rsync也一样能实现，这就像Linux中cp命令一样，以本地某文件作为源，另一文件作为目标文件，但请注意，虽然rsync和cp能达到相同的目的，但它们的实现方式是不一样的。 既然是文件同步，在同步过程中必然会涉及到源和目标两文件之间版本控制的问题，例如是否要删除源主机上没有但目标上多出来的文件，目标文件比源文件更新(newer than source)时是否仍要保持同步，遇到软链接时是拷贝软链接本身还是拷贝软链接所指向的文件，目标文件已存在时是否要先对其做个备份等等。 rsync同步过程中由两部分模式组成：决定哪些文件需要同步的检查模式以及文件同步时的同步模式。 检查模式是指按照指定规则来检查哪些文件需要被同步，例如哪些文件是明确被排除不传输的。默认情况下，rsync使用”quick check”算法快速检查源文件和目标文件的大小、mtime(修改时间)是否一致，如果不一致则需要传输。当然，也可以通过在rsync命令行中指定某些选项来改变quick check的检查模式，比如”–size-only”选项表示”quick check”将仅检查文件大小不同的文件作为待传输文件。rsync支持非常多的选项，其中检查模式的自定义性是非常有弹性的。 同步模式是指在文件确定要被同步后，在同步过程发生之前要做哪些额外工作。例如上文所说的是否要先删除源主机上没有但目标主机上有的文件，是否要先备份已存在的目标文件，是否要追踪链接文件等额外操作。rsync也提供非常多的选项使得同步模式变得更具弹性。 相对来说，为rsync手动指定同步模式的选项更常见一些，只有在有特殊需求时才指定检查模式，因为大多数检查模式选项都可能会影响rsync的性能。 安装步骤安装nginx1yum -y install nginx 配置nginx12345vim /usr/local/nginx/conf/nginx.conf #编辑配置文件，在http &#123;下面添加以下内容：autoindex on; #开启nginx目录浏览功能autoindex_exact_size off; #文件大小从KB开始显示autoindex_localtime on; #显示文件修改时间为服务器本地时间 重启nginx1systemctl restart nginx 此时可在浏览器输入服务器IP地址查看是否开启了目录浏览 同步镜像源安装rsync同步软件1yum -y install rsync xinetd 创建目录12mkdir -p /usr/local/nginx/html/centos #CentOS官方标准源mkdir -p /usr/local/nginx/html/epel #第三方epel源 确定源同步地址此处选择清华大学TUNA镜像源 创建同步源脚本123456789mkdir -p /home/crontab #创建目录vim /home/crontab/yum_rsync.sh #添加以下代码#!/bin/bash/usr/bin/rsync -avrt rsync://mirrors.tuna.tsinghua.edu.cn/centos/ --exclude-from=/usr/local/nginx/html/exclude_centos.list /usr/local/nginx/html/centos//usr/bin/rsync -avrt rsync://mirrors.tuna.tsinghua.edu.cn/epel/ --exclude-from=/usr/local/nginx/html/exclude_epel.list /usr/local/nginx/html/epel/ 给脚本添加执行权限1chmod +x /home/crontab/yum_rsync.sh 创建不需要同步的文件列表12345cd /usr/local/nginx/html/ #进入目录touch exclude_centos.list #创建文件touch exclude_epel.list #创建文件 把不需要同步的目录写到上面对应的文件中，每行一个目录，如： 123456vim exclude_epel.list4/4AS/4ES/4WS/ 添加定时脚本任务123456vim /etc/crontab #在最后一行添加以下代码0 1 * * * root /home/crontab/yum_rsync.sh #设置每天凌晨1点整开始执行脚本:wq! #保存退出service crond restart #重启 注意：等待脚本执行完毕，首次同步，耗费的时间比较长！ 创建repo配置文件CentOS7-centos配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243# CentOS-Base.repo## The mirror system uses the connecting IP address of the client and the# update status of each mirror to pick mirrors that are updated to and# geographically close to the client. You should use this for CentOS updates# unless you are manually picking other mirrors.## If the mirrorlist= does not work for you, as a fall back you can try the# remarked out baseurl= line instead.##[base]name=CentOS-$releasever - Basebaseurl=http://192.168.100.250/centos/$releasever/os/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osgpgcheck=1gpgkey=http://192.168.100.250/centos/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-$releasever - Updatesbaseurl=http://192.168.100.250/centos/$releasever/updates/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesgpgcheck=1gpgkey=http://192.168.100.250/centos/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-$releasever - Extrasbaseurl=http://192.168.100.250/centos/$releasever/extras/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasgpgcheck=1gpgkey=http://192.168.100.250/centos/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plusbaseurl=http://192.168.100.250/centos/$releasever/centosplus/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusgpgcheck=1enabled=0gpgkey=http://192.168.100.250/centos/RPM-GPG-KEY-CentOS-7 CentOS7-epel配置文件1234567891011121314151617181920212223242526[epel]name=Extra Packages for Enterprise Linux 7 - $basearchbaseurl=http://192.168.100.250/epel/7/$basearch#mirrorlist=http://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchfailovermethod=priorityenabled=1gpgcheck=1gpgkey=http://192.168.100.250/epel/RPM-GPG-KEY-EPEL-7[epel-debuginfo]name=Extra Packages for Enterprise Linux 7 - $basearch - Debugbaseurl=http://192.168.100.250/epel/7/$basearch/debug#mirrorlist=http://mirrors.fedoraproject.org/metalink?repo=epel-debug-7&amp;arch=$basearchfailovermethod=priorityenabled=0gpgkey=http://192.168.100.250/epel/RPM-GPG-KEY-EPEL-7gpgcheck=1[epel-source]name=Extra Packages for Enterprise Linux 7 - $basearch - Sourcebaseurl=http://192.168.100.250/epel/7/SRPMS#mirrorlist=http://mirrors.fedoraproject.org/metalink?repo=epel-source-7&amp;arch=$basearchfailovermethod=priorityenabled=0gpgkey=http://192.168.100.250/epel/RPM-GPG-KEY-EPEL-7gpgcheck=1 测试1234yum clean allyum makecacheyum repolistyum instll ntp]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>rsync</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装配置mysql5.5为utf-8编码]]></title>
    <url>%2FCentOS7%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEmysql5-5%E4%B8%BAutf-8%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[安装步骤安装1yum -y install mariadb mariadb-server 启动1systemctl start mariadb 开机启动1systemctl enable mariadb 配置安全策略1234567891011121314$ mysql_secure_installation首先设置密码，会提示先输入密码Enter current password for root (enter for none):&lt;–初次运行直接回车设置密码Set root password? [Y/n] &lt;– 是否设置root用户密码，输入y并回车或直接回车New password: &lt;– 设置root用户的密码Re-enter new password: &lt;– 再输入一次你设置的密码其他配置Remove anonymous users? [Y/n] &lt;– 是否删除匿名用户，回车Disallow root login remotely? [Y/n] &lt;–是否禁止root远程登录,回车,Remove test database and access to it? [Y/n] &lt;– 是否删除test数据库，回车Reload privilege tables now? [Y/n] &lt;– 是否重新加载权限表，回车 登陆1mysql -uroot -p 配置MariaDB的字符集文件/etc/my.cnf12345678$ vim /etc/my.cnf在[mysqld]标签下添加init_connect=&apos;SET collation_connection = utf8_unicode_ci&apos; init_connect=&apos;SET NAMES utf8&apos; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake 文件/etc/my.cnf.d/client.cnf1234$ vim /etc/my.cnf.d/client.cnf在[client]中添加default-character-set=utf8 文件/etc/my.cnf.d/mysql-clients.cnf1234$vim /etc/my.cnf.d/mysql-clients.cnf在[mysql]中添加default-character-set=utf8 全部配置完成，重启mariadb1systemctl restart mariadb 进入MariaDB查看字符集123456789101112131415161718192021222324mysql&gt; show variables like &quot;%character%&quot;;show variables like &quot;%collation%&quot;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)+----------------------+-----------------+| Variable_name | Value |+----------------------+-----------------+| collation_connection | utf8_unicode_ci || collation_database | utf8_unicode_ci || collation_server | utf8_unicode_ci |+----------------------+-----------------+3 rows in set (0.00 sec) 添加用户、配置权限创建用户1mysql&gt;create user username@localhost identified by &apos;password&apos;; 直接创建用户并授权的命令1mysql&gt;grant all on *.* to username@localhost indentified by &apos;password&apos;; 授予外网登陆权限1mysql&gt;grant all privileges on *.* to username@&apos;%&apos; identified by &apos;password&apos;; 授予权限并且可以授权1mysql&gt;grant all privileges on *.* to username@&apos;hostname&apos; identified by &apos;password&apos; with grant option; 只授予部分权限12把其中all privileges或者all改为select,insert,update,delete,create,drop,index,alter,grant,references,reload,shutdown,process,file；多个用逗号分隔]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>mysql</tag>
        <tag>mariadb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内部CA服务器和自签名证书]]></title>
    <url>%2F%E5%86%85%E9%83%A8CA%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E8%87%AA%E7%AD%BE%E5%90%8D%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[相关介绍在HTTPS的传输过程中，有一个非常关键的角色——数字证书，那什么是数字证书？又有什么作用呢？ 所谓数字证书，是一种用于电脑的身份识别机制。由数字证书颁发机构（CA）对使用私钥创建的签名请求文件做的签名（盖章），表示CA结构对证书持有者的认可。数字证书拥有以下几个优点： 使用数字证书能够提高用户的可信度 数字证书中的公钥，能够与服务端的私钥配对使用，实现数据传输过程中的加密和解密 在证认使用者身份期间，使用者的敏感个人数据并不会被传输至证书持有者的网络系统上 X.509证书包含三个文件：key，csr，crt。 key是服务器上的私钥文件，用于对发送给客户端数据的加密，以及对从客户端接收到数据的解密 csr是证书签名请求文件，用于提交给证书颁发机构（CA）对证书签名 crt是由证书颁发机构（CA）签名后的证书，或者是开发者自签名的证书，包含证书持有人的信息，持有人的公钥，以及签署者的签名等信息 备注：在密码学中，X.509是一个标准，规范了公开秘钥认证、证书吊销列表、授权凭证、凭证路径验证算法等。 创建CA openssl 的配置文件：/etc/pki/tls/openssl.cnf 重要参数配置路径 dir = /etc/pki/CA # Where everything is kept certs = /etc/pki/CA/certs # Where the issued certs are kept database = /etc/pki/CA/index.txt # database index file. new_certs_dir = /etc/pki/CA/newcerts # default place for new certs. certificate = /etc/pki/CA/cacert.pem # The CA certificate serial = /etc/pki/CA/serial # The current serial number private_key = /etc/pki/CA/private/cakey.pem # The private key 创建所需要的文件12touch /etc/pki/CA/index.txt 生成证书索引数据库文件echo 01 &gt; /etc/pki/CA/serial 指定第一个颁发证书的序列号,必须是两位十六进制数，99之后是9A 生成私钥123cd /etc/pki/CA/umask 066openssl genrsa -out /etc/pki/CA/private/cakey.pem 2048 生成自签名证书1234567openssl req -new -x509 –key /etc/pki/CA/private/cakey.pem -days 7300 -out /etc/pki/CA/cacert.pem-new: 生成新证书签署请求-x509: 专用于 CA 生成自签证书-key: 生成请求时用到的私钥文件-days n：证书的有效期限-out: 证书的保存路径提示输入国家，省，市，公司名称，部门名称，CA主机名（颁发者名称） 查看证书(Windows下查看生成的自签名证书,需要更改上述文件名后缀为.cer即可查看) 1openssl x509 -in /etc/pki/CA/cacert.pem -noout -text 颁发证书在需要证书的主机生成证书请求 如给nginx服务器生成私钥1234mkdir /root/keycd /root/keyumask 066openssl genrsa -out key/service.key 2048 生成证书申请文件1openssl req -new -key service.key -out service.csr 同样提示输入国家，省，市等信息。注意：国家，省，公司名称三项必须和CA一致。主机名称必须和网站域名相同，如www.centos73.com。或者使用泛域名，即*.centos73.com，匹配所有 将证书文件移动到CA服务器/etc/pki/CA/csr目录下1scp /app/service.csr 192.168.10.15:/etc/pki/CA/csr/ CA签署证书，并将证书颁发给请求者1openssl ca -in /etc/pki/CA/crl/service.csr -out /etc/pki/CA/certs/service.crt -days 365 生成certs/service.crt和newcerts/xx.pem文件，两个文件相同。 查看证书中的信息12345openssl x509 -in certs/service.crt -noout -text|issuer|subject|serial|datescat serialcat index.txt //V表示当前证书的状态正常openssl ca -status SERIAL 查看指定编号的证书状态cat index.txt.attr //yes表示subjects信息必须是唯一的，不能重复申请 吊销证书12345678910111213(1)在客户端获取要吊销的证书的serialopenssl x509 -in /etc/pki/CA/cacert.pem -noout -serial -subject(2)在CA上，根据客户提交的serial与subject信息，对比检验是否与index.txt文件中的信息一致，吊销证书：openssl ca -revoke /etc/pki/CA/newcerts/xx.pemcat index.txt //R表示证书已经失效(3)指定第一个吊销证书的编号注意：第一次更新证书吊销列表前，才需要执行echo 01 &gt; /etc/pki/CA/crlnumber(4)更新证书吊销列表：openssl ca -gencrl -out /etc/pki/CA/crl.pemlinux下查看crl文件：openssl crl -in /etc/pki/CA/crl.pem -noout -textWindows下查看吊销列表文件，需更改文件后缀为.crl 另外一种配置nginx自签名证书生成私钥1openssl genrsa -out server.key 2048 生成CSR1openssl req -new -key server.key -out server.csr 生成自签名证书1openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt Nginx 配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[root@nginx logs]# cat /usr/local/nginx/conf/nginx.conf#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # server &#123; listen 443 ssl; server_name nginx.iecas.com; ssl_certificate /etc/pki/CA/certs/server.crt; ssl_certificate_key /etc/pki/CA/private/server.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root html; index index.html index.htm; &#125; &#125;&#125;]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>CA</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04-hadoop3.1.0安装与配置]]></title>
    <url>%2FUbuntu16.04-hadoop3.1.0%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言 本文尝试源码安装部署hadoop3.1.0 系统为ubuntu16.04 需要具有一定基础，一些基本操作只是简略带过 我主要把各个部署方案的配置文件综合了起来，把一些常用的配置以及说明都写了上来 适合快速部署hadoop的朋友 服务器概览 序号 主机名 IP 配置 功能 备注 1 master 192.168.136.148 内存：4GCPU：2vCores 主节点namenoderesourcemanager 2 standby 192.168.136 内存：4GCPU：2vCores 备用主节点secondaryNameNode 默认在master上 3 slave1 192.168.136.149 内存：2GCPU：1vCores 从节点DataNodeNodeManager 4 slave2 192.168.136.150 内存：2GCPU：1vCores 从节点DataNodeNodeManager 部署配置hosts文件，设置静态IP与主机名1234567891011root@master:~# cat /etc/hosts127.0.0.1 localhost# The following lines are desirable for IPv6 capable hosts::1 localhost ip6-localhost ip6-loopbackff02::1 ip6-allnodesff02::2 ip6-allrouters192.168.136.148 master192.168.136.149 slave1192.168.136.150 slave2 配置时间同步关闭防火墙（centos关闭selinux）配置master免密登录其它节点JDK安装 源码安装或者apt安装 环境变量（apt安装不需要配置环境变量） 1234export JAVA_HOME=/opt/java/jdk1.8.0_172export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 安装hadoop下载hadoop，将其解压至/opt/目录下 12345678910[root@node1 opt]# cd /opt/ &amp; mkdir hadoop &amp;&amp; cd hadoop#解压hadoop-3.1.0.tar.gz[root@node1 hadoop]# tar -zxvf hadoop-3.1.0.tar.gz#修改环境变量[root@node1 hadoop]# vim /etc/profile# 在最后下添加export HADOOP_HOME=/opt/hadoop/hadoop-3.1.0export PATH=$PATH:$HADOOP_HOME/bin 1source /etc/profile 编辑配置文件12345修改配置文件&gt; 共需要配置/opt/hadoop/hadoop-3.1.0/etc/hadoop/下的六个个文件，分别是&gt;&gt; hadoop-env.sh、core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml、workers hadoop-env.shvim hadoop-env.sh export JAVA_HOME=/opt/java/jdk1.8.0_172/ export HDFS_NAMENODE_USER=&quot;root&quot; export HDFS_DATANODE_USER=&quot;root&quot; export HDFS_SECONDARYNAMENODE_USER=&quot;root&quot; export YARN_RESOURCEMANAGER_USER=&quot;root&quot; export YARN_NODEMANAGER_USER=&quot;root&quot;core-site.xml1234567891011121314151617&lt;configuration&gt; &lt;!-- 指定HDFS老大（namenode）的通信地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop流文件的缓冲区,单位为K --&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt; &lt;!-- 设置namenode的http通讯地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置secondarynamenode的http通讯地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;standby:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置dfs块大小，单位K，默认128Mb --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count &lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置namenode存放的路径 (最好指定多个路径，包括NFS，用逗号分隔，便于容灾)--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/data/name&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置hdfs副本数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置datanode存放的路径（可以指定多个路径，多个路径最好分别为不同的磁盘，提高效率） --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/data/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml123456789101112131415161718192021&lt;configuration&gt; &lt;!-- 通知框架MR使用YARN --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /opt/hadoop/hadoop-3.1.0/etc/hadoop, /opt/hadoop/hadoop-3.1.0/share/hadoop/common/*, /opt/hadoop/hadoop-3.1.0/share/hadoop/common/lib/*, /opt/hadoop/hadoop-3.1.0/share/hadoop/hdfs/*, /opt/hadoop/hadoop-3.1.0/share/hadoop/hdfs/lib/*, /opt/hadoop/hadoop-3.1.0/share/hadoop/mapreduce/*, /opt/hadoop/hadoop-3.1.0/share/hadoop/mapreduce/lib/*, /opt/hadoop/hadoop-3.1.0/share/hadoop/yarn/*, /opt/hadoop/hadoop-3.1.0/share/hadoop/yarn/lib/* &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml123456789101112131415161718192021222324252627282930313233&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置外网只需要替换外网ip为真实ip，否则默认为 localhost:8088 --&gt; &lt;!-- &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;外网ip:8088&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8025&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8040&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; masters新建一个masters的文件,这里指定的是secondary namenode 的主机 1234[root@node1 hadoop]# touch /opt/hadoop/hadoop-3.1.0/etc/hadoop/masters[root@node1 hadoop]# vim /opt/hadoop/hadoop-3.1.0/etc/hadoop/masters#添加standby workers在workers文件中添加slave节点 1234[root@node1 hadoop]# vim /opt/hadoop/hadoop-3.1.0/etc/hadoop/workers#添加slave1slave2 创建文件夹123[root@node1 hadoop]# mkdir -p /opt/hadoop/data/tmp[root@node1 hadoop]# mkdir -p /opt/hadoop/data/name[root@node1 hadoop]# mkdir -p /opt/hadoop/data/datanode 复制到其他主机12[root@node1 opt]# scp -r /opt/hadoop spark.node2:/opt/[root@node1 opt]# scp -r /opt/hadoop spark.node3:/opt/ 格式化 第一次启动得格式化1[root@node1 opt]# /opt/hadoop/hadoop-3.1.0/bin/hdfs namenode -format 启动12/opt/hadoop/hadoop-3.1.0/sbin/start-dfs.sh/opt/hadoop/hadoop-3.1.0/sbin/start-yarn.sh 查看1jps 关闭 【只在master上操作】12/opt/hadoop/hadoop-3.1.0/sbin/stop-dfs.sh/opt/hadoop/hadoop-3.1.0/sbin/stop-yarn.sh 删除（谨慎操作，只在master上操作）重置hadoop环境 [移除hadoop hdfs log文件] 12rm -rf /opt/hadoop/hadoop-3.1.0/logs/*rm -rf /opt/hadoop/data]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7-NFS服务的安装配置详解]]></title>
    <url>%2FCentOS7-NFS%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[NFS介绍在Linux下实现文件共享有多种方式，NFS就是其中之一。网络文件系统(NFS)协议是由Sun MicroSystem在20世纪80年代为了提供对共享文件的远程访问而设计和实现的。该协议采用Client/Server模型，通过使用Sun开发的远程过程调用协议(RPC Protocol)来实现运行在一台计算机上的程序来调用在另一台远程机器上运行的子程序，并且，它提供的外部数据表示(XDR)可以使得数据在不同平台上的计算机上进行交换。该协议可以在TCP协议或者是UDP协议上运行，而在此基础上，NFS在数据的传送过程中需要RPC命令得到确认，而且在需要的时候将会重传。 NFS原理NFS比较复杂，包括很多组件，通过特殊的协议进行交互。不同的组件在操作系统当中都使用不同的配置文件以及状态文件。下图说明了NFS的主要组件及配置文件。 NFS分为服务器和客户机两部分，每个主机都有自己的内核级服务：外部数据表示(XDR，eXternal Data Representation)、远程过程调用(RPC，Remote Procedure Call)、I/O监控程序和锁监控程序。每个主机还有自己的用户级服务。内核级服务和用户级服务都依赖于主机的功能：NFS客户机或者是NFS服务器。当然，还要依赖于每个主机使用的不同功能的配置文件（如果是服务器，则用的是/etc/exports配置文件，如果是客户机，则用的是/etc/fstab配置文件）。如果一台主机既是服务器又是客户机，那么它需要运行两个部分的服务。 在服务器端，portmap、 mountd、 nfsd三个监控程序将在后台运行。portmap监控程序用来注册基于rpc的服务。当一个RPC的监控程序启动的时候，它告诉portmap监控程序它在哪一个端口进行侦听，并且它在进行什么样的RPC服务。当一个客户机向服务器提出一个RPC请求，那么它就会和portmap监控程序取得联系以确定RPC消息应该发往的端口号。而Mountd监控程序的功能是来读取服务器端的/etc/exportfs文件并且创建一个将服务器的本地文件系统导出的主机和网络列表，因而客户机的挂接(mount)请求都被定位到mountd监控程序(daemon)。当验证了服务器确实具有挂接所请求的文件系统的权限以后，mountd为请求的挂接点返回一个文件句柄。而nfsd监控程序则被服务器用来处理客户机端发过来的请求，由于服务器需要同时处理多个客户机的请求，所以在缺省情况下，在Linux当中将会自动启动八个nfsd线程。当然，如果NFS服务器特别忙的时候，系统有可能根据实际情况启动三十个线程。 Centos7 NFS安装 关闭防火墙 关闭selinux 安装 1yum install -y rpcbind nfs-utils 编辑配置文件 12345678910vim /etc/exports/tmp *(rw,no_root_squash) #所有用户都可以以root权限访问tmp/home/public 192.168.100.0/24(rw) *(ro) #192.168.100.0的主机用户具有读写权限，其他用户只读/home/test 192.168.100.10(rw) #192.168.100.10的主机用户可读写 /home/linux *.centos.vbird(rw,all_squash,anonuid=45,anongid=45) # 如果要开放匿名，那么重点是 all_squash，并且要配合 anonuid 喔！no_root_squash表示外部用户进来可以以root的权限访问。/share *(rw,all_squash,anonuid=1000,anongid=1000,sync)/private 192.168.136.0/24(ro,root_squash,sync) 配置文件详细说明 123456789101112131415161718192021222324252627282930313233NFS服务的主要配置文件为 /etc/exports./etc/exports文件内容格式： &lt;输出目录&gt; 客户端（选项:访问权限,用户映射,其他] 输出目录是指NFS系统中所定义的共享给客户端使用的文件系统 客户端是定义网络中可以访问这个NFS共享目录的IP地址或网段或域名等 客户端常用的指定方式 指定ip地址的主机：192.168.100.1 指定一个子网：192.168.100.0/24 也可以写成:192.168.100.0/255.255.255.0 指定域名的主机：david.bsmart.cn 指定域中的所有主机：*.bsmart.cn 所有主机：* 选项用来设置输出目录的访问权限、用户映射等。 NFS主要有3类选项： 设置输出目录只读：ro 设置输出目录读写：rw 用户映射选项 all_squash：将远程访问的所有普通用户及所属组都映射为匿名用户或用户组（nfsnobody）； no_all_squash：与all_squash取反（默认设置）； root_squash：将root用户及所属组都映射为匿名用户或用户组（默认设置）； no_root_squash：与rootsquash取反； anonuid=xxx：将远程访问的所有用户都映射为匿名用户，并指定该用户为本地用户（UID=xxx）； anongid=xxx：将远程访问的所有用户组都映射为匿名用户组账户，并指定该匿名用户组账户为本地用户组账户（GID=xxx）； 其它选项 secure：限制客户端只能从小于1024的tcp/ip端口连接nfs服务器（默认设置）； insecure：允许客户端从大于1024的tcp/ip端口连接服务器； sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性； async：将数据先保存在内存缓冲区中，必要时才写入磁盘； wdelay：检查是否有相关的写操作，如果有则将这些写操作一起执行，这样可以提高效率（默认设置）； no_wdelay：若有写操作则立即执行，应与sync配合使用； subtree：若输��目录是一个子目录，则nfs服务器将检查其父目录的权限(默认设置)； no_subtree：即使输出目录是一个子目录，nfs服务器也不检查其父目录的权限，这样可以提高效率； 编辑共享文件夹权限 1234mkdir /sharechown nfsuser:nfsuser /sharemkdir /privatechmod 777 /private 启动NFS 1234systemctl enable nfssystemctl start nfssystemctl enable rpcbindsystemctl start rpcbind 查看命令 123showmount -e [server] 显示 NFS 服务器导出的所有共享。showmount -a [server] 列出客户端主机名或 IP 地址，以及使用“主机:目录”格式显示的安装目录。showmount -d [server] 显示 NFS 服务器上当前由某些 NFS 客户端安装的目录。 客户端挂载 LINUX 123安装nfs-utils(CentOS)或者nfs-common(Ubuntu)mount -t nfs 192.168.136.139:/share /mnt/share Windows 1234添加NFS服务mount \\192.168.136.139\share x:\或者直接地址栏访问\\192.168.136.139\share ​]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>nfs</tag>
        <tag>centos7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LINUX LVM 硬盘管理]]></title>
    <url>%2FLINUX%20LVM%20%E7%A1%AC%E7%9B%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[LVM简介LVM是 Logical Volume Manager(逻辑卷管理)的简写，它由Heinz Mauelshagen在Linux 2.4内核上实现。LVM将一个或多个硬盘的分区在逻辑上集合，相当于一个大硬盘来使用，当硬盘的空间不够使用的时候，可以继续将其它的硬盘的分区加入其中，这样可以实现磁盘空间的动态管理，相对于普通的磁盘分区有很大的灵活性。 与传统的磁盘与分区相比，LVM为计算机提供了更高层次的磁盘存储。它使系统管理员可以更方便的为应用与用户分配存储空间。在LVM管理下的存储卷可以按需要随时改变大小与移除(可能需对文件系统工具进行升级)。LVM也允许按用户组对存储卷进行管理，允许管理员用更直观的名称(如”sales’、 ‘development’)代替物理磁盘名(如’sda’、’sdb’)来标识存储卷。 LVM基本术语前面谈到，LVM是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。首先我们讨论以下几个LVM术语： 物理存储介质（The physical media）：这里指系统的存储设备：硬盘，如：/dev/hda1、/dev/sda等等，是存储系统最低层的存储单元。 物理卷（physical volume）：物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如RAID)，是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数。 卷组（Volume Group）：LVM卷组类似于非LVM系统中的物理硬盘，其由物理卷组成。可以在卷组上创建一个或多个“LVM分区”（逻辑卷），LVM卷组由一个或多个物理卷组成。 逻辑卷（logical volume）：LVM的逻辑卷类似于非LVM系统中的硬盘分区，在逻辑卷之上可以建立文件系统(比如/home或者/usr等)。 PE（physical extent）：每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可配置的，默认为4MB。 LE（logical extent）：逻辑卷也被划分为被称为LE(Logical Extents) 的可被寻址的基本单位。在同一个卷组中，LE的大小和PE是相同的，并且一一对应。 简单来说就是： PV:是物理的磁盘分区 VG:LVM中的物理的磁盘分区，也就是PV，必须加入VG，可以将VG理解为一个仓库或者是几个大的硬盘。 LV：也就是从VG中划分的逻辑分区 安装LVM首先确定系统中是否安装了lvm工具： 12[root@www root]# rpm –qa|grep lvmlvm-1.0.3-4 如果命令结果输入类似于上例，那么说明系统已经安装了LVM管理工具；如果命令没有输出则说明没有安装LVM管理工具，则需要从网络下载或者从光盘装LVM rpm工具包。 创建和管理LVM要创建一个LVM系统，一般需要经过以下步骤：1、 创建分区 使用分区工具（如：fdisk等）创建LVM分区，方法和创建其他一般分区的方式是一样的，区别仅仅是LVM的分区类型为8e。 fdisk /dev/sda 创建一个新分区 分区格式修改为8e partproobe 创建pv： pvcreate /dev/sda1 查看pv： pvs 创建vg： vgcreate VolumeGroupName /dev/sda1 查看vg： vgs 创建lv： lvcreate -L 2G(+增加多少G) -n LVName VolumeGroupName LV格式化及挂载： mkfs.xfs /dev/VolumeGroupNmae/LVName 挂载： mount 将挂载项写入 /etc/fstab 下 扩容当前分区 创建一个新分区 分区格式修改为8e partprobe 内核重读分区表 创建PV: pvcreate /dev/sda2 增大vg： vgextend VolumeGroupName /dev/sda2 增大LV： lvextend -L +1G /dev/VolumeGroupName/LVName 生效： resize2fs /dev/VolumeGroupName/LVName or xfs_growfs /dev/VolumeGroupName/LVName df -h 查看挂载情况]]></content>
      <categories>
        <category>存储</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>lvm</tag>
        <tag>硬盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nmap基础用法-常用命令总结]]></title>
    <url>%2FNmap%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Nmap介绍Nmap是一款开源免费的网络发现（Network Discovery）和安全审计（Security Auditing）工具。软件名字Nmap是Network Mapper的简称。Nmap最初是由Fyodor在1997年开始创建的。随后在开源社区众多的志愿者参与下，该工具逐渐成为最为流行安全必备工具之一。最新版的Nmap6.0在2012年5月21日发布，详情请参见：www.nmap.org。一般情况下，Nmap用于列举网络主机清单、管理服务升级调度、监控主机或服务运行状况。Nmap可以检测目标机是否在线、端口开放情况、侦测运行的服务类型及版本信息、侦测操作系统与设备类型等信息。Nmap的优点： 灵活。支持数十种不同的扫描方式，支持多种目标对象的扫描。 强大。Nmap可以用于扫描互联网上大规模的计算机。 可移植。支持主流操作系统：Windows/Linux/Unix/MacOS等等；源码开放，方便移植。 简单。提供默认的操作能覆盖大部分功能，基本端口扫描nmap targetip，全面的扫描nmap –A targetip。 自由。Nmap作为开源软件，在GPL License的范围内可以自由的使用。 文档丰富。Nmap官网提供了详细的文档描述。Nmap作者及其他安全专家编写了多部Nmap参考书籍。 社区支持。Nmap背后有强大的社区团队支持。 赞誉有加。获得很多的奖励，并在很多影视作品中出现（如黑客帝国2、Die Hard4等）。 流行。目前Nmap已经被成千上万的安全专家列为必备的工具之一。 使用方法使用主机名扫描12345678910111213141516[root@server1 ~]# nmap server2.tecmint.com Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 15:42 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.415 secondsYou have new mail in /var/spool/mail/root 使用IP地址扫描12345678910111213141516[root@server1 ~]# nmap 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-18 11:04 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind958/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.465 secondsYou have new mail in /var/spool/mail/root 扫描使用-v选项你可以看到下面的命令使用“ -v “选项后给出了远程机器更详细的信息。 123456789101112131415161718192021222324252627[root@server1 ~]# nmap -v server2.tecmint.com Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 15:43 ESTInitiating ARP Ping Scan against 192.168.0.101 [1 port] at 15:43The ARP Ping Scan took 0.01s to scan 1 total hosts.Initiating SYN Stealth Scan against server2.tecmint.com (192.168.0.101) [1680 ports] at 15:43Discovered open port 22/tcp on 192.168.0.101Discovered open port 80/tcp on 192.168.0.101Discovered open port 8888/tcp on 192.168.0.101Discovered open port 111/tcp on 192.168.0.101Discovered open port 3306/tcp on 192.168.0.101Discovered open port 957/tcp on 192.168.0.101The SYN Stealth Scan took 0.30s to scan 1680 total ports.Host server2.tecmint.com (192.168.0.101) appears to be up ... good.Interesting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.485 seconds Raw packets sent: 1681 (73.962KB) | Rcvd: 1681 (77.322KB) 扫描多台主机12345678910111213[root@server1 ~]# nmap 192.168.0.101 192.168.0.102 192.168.0.103 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:06 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems)Nmap finished: 3 IP addresses (1 host up) scanned in 0.580 seconds 扫描整个子网1234567891011121314151617181920212223[root@server1 ~]# nmap 192.168.0.* Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:11 ESTInteresting ports on server1.tecmint.com (192.168.0.100):Not shown: 1677 closed portsPORT STATE SERVICE22/tcp open ssh111/tcp open rpcbind851/tcp open unknown Interesting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 256 IP addresses (2 hosts up) scanned in 5.550 secondsYou have new mail in /var/spool/mail/root 扫描IP地址范围12345678910111213[root@server1 ~]# nmap 192.168.0.101-110 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:09 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 10 IP addresses (1 host up) scanned in 0.542 seconds 扫描多台服务器123456789101112131415[root@server1 ~]# nmap 192.168.0.101,102,103 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:09 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 3 IP addresses (1 host up) scanned in 0.552 secondsYou have new mail in /var/spool/mail/root 从文件中扫描服务器1234567891011121314151617181920212223242526272829303132[root@server1 ~]# nmap -iL nmaptest.txt Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-18 10:58 ESTInteresting ports on localhost.localdomain (127.0.0.1):Not shown: 1675 closed portsPORT STATE SERVICE22/tcp open ssh25/tcp open smtp111/tcp open rpcbind631/tcp open ipp857/tcp open unknown Interesting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind958/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Interesting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind958/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 3 IP addresses (3 hosts up) scanned in 2.047 seconds 排除一些主机后在进行扫描12345678910111213141516[root@server1 ~]# nmap 192.168.0.* --exclude 192.168.0.100 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:16 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 255 IP addresses (1 host up) scanned in 5.313 secondsYou have new mail in /var/spool/mail/root 扫描操作系统信息和路由追踪1234567891011121314151617181920212223242526272829[root@server1 ~]# nmap -A 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:25 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE VERSION22/tcp open ssh OpenSSH 4.3 (protocol 2.0)80/tcp open http Apache httpd 2.2.3 ((CentOS))111/tcp open rpcbind 2 (rpc #100000)957/tcp open status 1 (rpc #100024)3306/tcp open mysql MySQL (unauthorized)8888/tcp open http lighttpd 1.4.32MAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems)No exact OS matches for host (If you know what OS is running on it, see http://www.insecure.org/cgi-bin/nmap-submit.cgi).TCP/IP fingerprint:SInfo(V=4.11%P=i686-redhat-linux-gnu%D=11/11%Tm=52814B66%O=22%C=1%M=080027)TSeq(Class=TR%IPID=Z%TS=1000HZ)T1(Resp=Y%DF=Y%W=16A0%ACK=S++%Flags=AS%Ops=MNNTNW)T2(Resp=N)T3(Resp=Y%DF=Y%W=16A0%ACK=S++%Flags=AS%Ops=MNNTNW)T4(Resp=Y%DF=Y%W=0%ACK=O%Flags=R%Ops=)T5(Resp=Y%DF=Y%W=0%ACK=S++%Flags=AR%Ops=)T6(Resp=Y%DF=Y%W=0%ACK=O%Flags=R%Ops=)T7(Resp=Y%DF=Y%W=0%ACK=S++%Flags=AR%Ops=)PU(Resp=Y%DF=N%TOS=C0%IPLEN=164%RIPTL=148%RID=E%RIPCK=E%UCK=E%ULEN=134%DAT=E) Uptime 0.169 days (since Mon Nov 11 12:22:15 2013) Nmap finished: 1 IP address (1 host up) scanned in 22.271 seconds 启用Nmap的操作系统探测功能使用选项“-O”和“-osscan-guess”也帮助探测操作系统信息。 12345678910111213141516171819202122232425262728293031[root@server1 ~]# nmap -O server2.tecmint.com Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 17:40 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems)No exact OS matches for host (If you know what OS is running on it, see http://www.insecure.org/cgi-bin/nmap-submit.cgi).TCP/IP fingerprint:SInfo(V=4.11%P=i686-redhat-linux-gnu%D=11/11%Tm=52815CF4%O=22%C=1%M=080027)TSeq(Class=TR%IPID=Z%TS=1000HZ)T1(Resp=Y%DF=Y%W=16A0%ACK=S++%Flags=AS%Ops=MNNTNW)T2(Resp=N)T3(Resp=Y%DF=Y%W=16A0%ACK=S++%Flags=AS%Ops=MNNTNW)T4(Resp=Y%DF=Y%W=0%ACK=O%Flags=Option -O and -osscan-guess also helps to discover OSR%Ops=)T5(Resp=Y%DF=Y%W=0%ACK=S++%Flags=AR%Ops=)T6(Resp=Y%DF=Y%W=0%ACK=O%Flags=R%Ops=)T7(Resp=Y%DF=Y%W=0%ACK=S++%Flags=AR%Ops=)PU(Resp=Y%DF=N%TOS=C0%IPLEN=164%RIPTL=148%RID=E%RIPCK=E%UCK=E%ULEN=134%DAT=E) Uptime 0.221 days (since Mon Nov 11 12:22:16 2013) Nmap finished: 1 IP address (1 host up) scanned in 11.064 secondsYou have new mail in /var/spool/mail/root 扫描主机侦测防火墙下面的命令将扫描远程主机以探测该主机是否使用了包过滤器或防火墙。 12345678[root@server1 ~]# nmap -sA 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:27 ESTAll 1680 scanned ports on server2.tecmint.com (192.168.0.101) are UNfilteredMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.382 secondsYou have new mail in /var/spool/mail/root 扫描主机检测是否有防火墙保护123456789101112131415[root@server1 ~]# nmap -PN 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:30 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.399 seconds 找出网络中的在线主机使用“-sP”选项，我们可以简单的检测网络中有哪些在线主机，该选项会跳过端口扫描和其他一些检测。 1234567[root@server1 ~]# nmap -sP 192.168.0.* Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-18 11:01 ESTHost server1.tecmint.com (192.168.0.100) appears to be up.Host server2.tecmint.com (192.168.0.101) appears to be up.MAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems)Nmap finished: 256 IP addresses (2 hosts up) scanned in 5.109 seconds 执行快速扫描你可以使用“-F”选项执行一次快速扫描，仅扫描列在nmap-services文件中的端口而避开所有其它的端口。 1234567891011121314[root@server1 ~]# nmap -F 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 16:47 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1234 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.322 seconds 打印主机接口和路由123456789101112[root@server1 ~]# nmap --iflist Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 17:07 EST************************INTERFACES************************DEV (SHORT) IP/MASK TYPE UP MAClo (lo) 127.0.0.1/8 loopback upeth0 (eth0) 192.168.0.100/24 ethernet up 08:00:27:11:C7:89 **************************ROUTES**************************DST/MASK DEV GATEWAY192.168.0.0/0 eth0169.254.0.0/0 eth0 扫描特定的端口使用Nmap扫描远程机器的端口有各种选项，你可以使用“-p”选项指定你想要扫描的端口，默认情况下nmap只扫描TCP端口。 123456789[root@server1 ~]# nmap -p 80 server2.tecmint.com Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 17:12 ESTInteresting ports on server2.tecmint.com (192.168.0.101):PORT STATE SERVICE80/tcp open httpMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) sca 扫描TCP端口12345678910[root@server1 ~]# nmap -p T:8888,80 server2.tecmint.com Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 17:15 ESTInteresting ports on server2.tecmint.com (192.168.0.101):PORT STATE SERVICE80/tcp open http8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.157 seconds 扫描UDP端口12345678910[root@server1 ~]# nmap -sU 53 server2.tecmint.com Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 17:15 ESTInteresting ports on server2.tecmint.com (192.168.0.101):PORT STATE SERVICE53/udp open http8888/udp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.157 seconds 扫描多个端口12345678910[root@server1 ~]# nmap -p 80,443 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-18 10:56 ESTInteresting ports on server2.tecmint.com (192.168.0.101):PORT STATE SERVICE80/tcp open http443/tcp closed httpsMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.190 seconds 扫描指定范围内的端口1[root@server1 ~]# nmap -p 80-160 192.168.0.101 查找主机服务版本号123456789101112131415[root@server1 ~]# nmap -sV 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 17:48 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE VERSION22/tcp open ssh OpenSSH 4.3 (protocol 2.0)80/tcp open http Apache httpd 2.2.3 ((CentOS))111/tcp open rpcbind 2 (rpc #100000)957/tcp open status 1 (rpc #100024)3306/tcp open mysql MySQL (unauthorized)8888/tcp open http lighttpd 1.4.32MAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 12.624 seconds 使用TCP ACK (PA)和TCP Syn (PS)扫描远程主机有时候包过滤防火墙会阻断标准的ICMP ping请求，在这种情况下，我们可以使用TCP ACK和TCP Syn方法来扫描远程主机。 12345678910111213141516[root@server1 ~]# nmap -PS 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 17:51 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.360 secondsYou have new mail in /var/spool/mail/root 使用TCP ACK扫描远程主机上特定的端口1234567891011[root@server1 ~]# nmap -PA -p 22,80 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 18:02 ESTInteresting ports on server2.tecmint.com (192.168.0.101):PORT STATE SERVICE22/tcp open ssh80/tcp open httpMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.166 secondsYou have new mail in /var/spool/mail/root 使用TCP Syn扫描远程主机上特定的端口1234567891011[root@server1 ~]# nmap -PS -p 22,80 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 18:08 ESTInteresting ports on server2.tecmint.com (192.168.0.101):PORT STATE SERVICE22/tcp open ssh80/tcp open httpMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.165 secondsYou have new mail in /var/spool/mail/root 执行一次隐蔽的扫描12345678910111213141516[root@server1 ~]# nmap -sS 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 18:10 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.383 secondsYou have new mail in /var/spool/mail/root 使用TCP Syn扫描最常用的端口12345678910111213141516[root@server1 ~]# nmap -sT 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 18:12 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open ssh80/tcp open http111/tcp open rpcbind957/tcp open unknown3306/tcp open mysql8888/tcp open sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 0.406 secondsYou have new mail in /var/spool/mail/root 执行TCP空扫描以骗过防火墙12345678910111213141516[root@server1 ~]# nmap -sN 192.168.0.101 Starting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-11-11 19:01 ESTInteresting ports on server2.tecmint.com (192.168.0.101):Not shown: 1674 closed portsPORT STATE SERVICE22/tcp open|filtered ssh80/tcp open|filtered http111/tcp open|filtered rpcbind957/tcp open|filtered unknown3306/tcp open|filtered mysql8888/tcp open|filtered sun-answerbookMAC Address: 08:00:27:D9:8E:D7 (Cadmus Computer Systems) Nmap finished: 1 IP address (1 host up) scanned in 1.584 secondsYou have new mail in /var/spool/mail/root]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>nmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 安装ActiveMQ以及测试JAVA DEMO]]></title>
    <url>%2FUbuntu16.04%20%E5%AE%89%E8%A3%85ActiveMQ%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95JAVA%20DEMO%2F</url>
    <content type="text"><![CDATA[安装过程官网下载安装包1apache-activemq-5.15.4-bin.tar.gz 解压1tar xf apache-activemq-5.15.4-bin.tar.gz -C /opt/ 启动activemq12cd /opt/apache-activemq-5.15.4/./bin/activemq start 访问管理界面http://host:8161 用户名密码默认 admin/admin 测试新建JAVA项目 注意将安装目录下的jar包导入 生产者代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package activemq;import javax.jms.*;import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;public class Producer &#123; /** * 默认用户名 */ public static final String USERNAME = ActiveMQConnection.DEFAULT_USER; /** * 默认密码 */ public static final String PASSWORD = ActiveMQConnection.DEFAULT_PASSWORD; /** * 默认连接地址 */ public static final String BROKER_URL = &quot;tcp://192.168.136.140:61616&quot;; public static void main(String[] args) &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(USERNAME, PASSWORD, BROKER_URL); try &#123; //创建连接 Connection connection = connectionFactory.createConnection(); //开启连接 connection.start(); //创建会话，不需要事务 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //创建 Topic，用作消费者订阅消息 Topic myTestTopic = session.createTopic(&quot;activemq-topic-test1&quot;); //消息生产者 MessageProducer producer = session.createProducer(myTestTopic); for (int i = 1; i &lt;= 3; i++) &#123; TextMessage message = session.createTextMessage(&quot;发送消息 &quot; + i); producer.send(myTestTopic, message); &#125; //关闭资源 session.close(); connection.close(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 消费者代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package activemq;import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import javax.jms.*;public class Consumer &#123; /** * 默认用户名 */ public static final String USERNAME = ActiveMQConnection.DEFAULT_USER; /** * 默认密码 */ public static final String PASSWORD = ActiveMQConnection.DEFAULT_PASSWORD; /** * 默认连接地址 */ public static final String BROKER_URL = &quot;tcp://192.168.136.140:61616&quot;; public static void main(String[] args) &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(USERNAME, PASSWORD, BROKER_URL); try &#123; //创建连接 Connection connection = connectionFactory.createConnection(); //开启连接 connection.start(); //创建会话，不需要事务 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //创建 Topic Topic myTestTopic = session.createTopic(&quot;activemq-topic-test1&quot;); MessageConsumer messageConsumer = session.createConsumer(myTestTopic); messageConsumer.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; try &#123; System.out.println(&quot;消费者1 接收到消息：&quot; + ((TextMessage) message).getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); MessageConsumer messageConsumer2 = session.createConsumer(myTestTopic); messageConsumer2.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; try &#123; System.out.println(&quot;消费者2 接收到消息：&quot; + ((TextMessage) message).getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); MessageConsumer messageConsumer3 = session.createConsumer(myTestTopic); messageConsumer3.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; try &#123; System.out.println(&quot;消费者3 接收到消息：&quot; + ((TextMessage) message).getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); //让主线程休眠100秒，使消息消费者对象能继续存活一段时间从而能监听到消息 Thread.sleep(100 * 1000); //关闭资源 session.close(); connection.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>activemq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 安装erlangOTP20.3和RabbitMQ3.7.7]]></title>
    <url>%2FUbuntu16.04%20%E5%AE%89%E8%A3%85erlangOTP20.3%E5%92%8CRabbitMQ3.7.7%2F</url>
    <content type="text"><![CDATA[安装步骤安装JDK安装Erlang下载deb安装包https://www.erlang-solutions.com/resources/download.html 下载对应版本安装包 安装依赖环境123456789apt-get install libncurses5-devapt-get install libssl-devapt-get install m4apt-get install unixodbc unixodbc-devapt-get install freeglut3-devapt-get install xsltprocapt-get install fopapt-get install tk8.5apt-get install libwxbase3.0-0v5 libwxgtk3.0-0v5 libsctp1 libnotify4 安装erlang1dpkg -i esl-erlang_20.3-1~ubuntu~xenial_amd64.deb 安装RabbitMQ配置apt源1echo &quot;deb https://dl.bintray.com/rabbitmq/debian xenial main&quot; | sudo tee /etc/apt/sources.list.d/bintray.rabbitmq.list 导入gpg key1wget -O- https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.asc | sudo apt-key add - 安装12apt-get updateapt-get install rabbitmq-server 启动RabbitMQ12service rabbitmq-server startservice rabbitmq-server status 还有一个web管理界面，但guest用户只允许loaclhost访问，所以肯定没法用了。想要外网开启，需要做以下几步骤： 开启web访问的plugins 1rabbitmq-plugins enable rabbitmq_management 创建新用户 1rabbitmqctl add_user UserName（用户名） Password（密码） 分配权限 123456789rabbitmqctl set_user_tags UserName（上一步创建的用户名） monitoring（权限）权限有好几类：超级管理员(administrator)监控者(monitoring)策略制定者(policymaker)普通管理者(management)可以给一个用户分配多个权限：rabbitmqctl set_user_tags UserName monitoring policymaker 访问 1http://192.168.136.140:15672 ​]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>erlang</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vagrant 使用配置详解]]></title>
    <url>%2FVagrant%20%E4%BD%BF%E7%94%A8%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Vagrant介绍Vagrantfile，官方解释是这样的：The primary function of the Vagrantfile is to describe the type of machine required for a project, and how to configure and provision these machines。简单来说就是配置这个虚拟主机网络连接方式，端口转发，同步文件夹，以及怎么和puppet，chef结合的一个配置文件。执行完$ vagrant init后,在工作目录中,你会发现此文件。 安装官网下载安装包，下一步下一步 PS：需要下载virtualbox（版本最好都是最新版、这样应该不会有兼容性的问题） PPS：如果提示VT-X和hyper-V之类的问题，首先检查一下主板BIOS的VT-X虚拟化开启没有，然后将在windows控制面板的添加功能中卸载hyper-V的服务 使用 新建一个文件夹并进入 将下载好的BOX添加到本地镜像中 12vagrant box add &#123;本地BOX名称&#125; &#123;BOX路径&#125;vagrant box list #查看 启动 123vagrant init &#123;本地BOX名称&#125;# 会在本地生成Vagrantfile配置文件# 如果本地没有该BOX，则从默认仓库中拉取 启动虚拟机 1vagrant up 链接虚拟机 1vagrant ssh 配置文件详解集群文件例子12345678910111213141516171819202122232425Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :web do |web| web.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;web&quot;, &quot;--memory&quot;, &quot;512&quot;] #v.memory = 1024 #v.cpus = 2 #v.name = marchine1 end web.vm.box = &quot;CentOs7&quot; web.vm.hostname = &quot;web&quot; web.vm.network :private_network, ip: &quot;192.168.33.10&quot;# web.vm.network :public_network web.vm.network :forwarded_port, guest:80, host:8080 web.vm.synced_folder &quot;src/&quot;, &quot;/srv/website&quot; end config.vm.define :redis do |redis| redis.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;redis&quot;, &quot;--memory&quot;, &quot;512&quot;] end redis.vm.box = &quot;CentOs7&quot; redis.vm.hostname = &quot;redis&quot; redis.vm.network :private_network, ip: &quot;192.168.33.11&quot; endend 执行脚本内部脚本12345678910111213141516171819202122Vagrant.configure(&quot;2&quot;) do |config| config.vm.provision :shell, :inline =&gt; &quot;echo Hello, World&quot;end$script = &lt;&lt;SCRIPTecho I am provisioning...date &gt; /etc/vagrant_provisioned_atSCRIPTVagrant.configure(&quot;2&quot;) do |config| config.vm.provision :shell, :inline =&gt; $scriptend 外部脚本123456789101112131415161718Vagrant.configure(&quot;2&quot;) do |config| config.vm.provision :shell, :path =&gt; &quot;script.sh&quot; #脚本的路径相对于项目根，也可使用绝对路径end#脚本也可传递参数Vagrant.configure(&quot;2&quot;) do |config| config.vm.provision :shell do |s| s.inline = &quot;echo $1&quot; s.args = &quot;&apos;hello, world!&apos;&quot; endend Vagrant 常用命令清单 vagrant box add 添加box vagrant init 初始化 box vagrant up 启动虚拟机 vagrant ssh 登录虚拟机 vagrant box list 列出 Vagrant 当前 box 列表 vagrant box remove 删除相应的 box vagrant destroy 停止当前正在运行的虚拟机并销毁所有创建的资源 vagrant halt 关机 vagrant package 把当前的运行的虚拟机环境进行打包为 box 文件 vagrant plugin 安装卸载插件 vagrant reload 重新启动虚拟机，重新载入配置文件 vagrant resume 恢复被挂起的状态 vagrant status 获取当前虚拟机的状态 vagrant suspend 挂起当前的虚拟机 vagrant global-status 查看当前 vagrant 管理的所有 vm 信息]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ss-local+privoxy 代理]]></title>
    <url>%2Fss-local%2Bprivoxy%20%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[[TOC] 简介 ss-local 是 shadowsocks 的本地 socks5 服务器，因此，如果需要使用 ss-local 提供的 socks5 代理，就必须让应用程序使用 socks5 协议与之通信。但是很可惜，除了部分浏览器、软件直接支持 socks5 协议外，其它的都只支持 http 代理。因此，我们需要借助 privoxy 来将 http 代理协议转换为 socks5 代理协议，与后端的 ss-local 进行通信。 相关说明shadowsocks两种使用姿势： ss-local+privoxy:使用 privoxy 作为前端的 http 代理（支持 CONNECT），可选择全局、gfwlist 两种方式； ss-redir+iptables:支持代理所有 TCP、UDP 流量（本机 UDP 除外），可选择全局、绕过大陆地址两种方式 今天我们要说的是第一种姿势，即ss-local + privoxy；第二种姿势可以前往ss-redir 透明代理 这两种方式的主要区别在于：gfwlist、绕过大陆地址 gfwlist:gfwlist 就是一个包含了几乎所有被墙域名的列表，因此，使用这种模式只会让被墙的网站走代理；但是很多国外没被墙的域名还是走的直连，因此访问国外未墙网站的时候速度依旧很慢，甚至出现连接超时的情况； 绕过大陆地址:顾名思义，只有发往大陆地址的流量不会走代理，其它的不管有没有被墙，统统走代理上网；这样就不会出现 gfwlist 模式的国外未墙网站访问慢的问题了。我个人建议使用这种模式。 当然，对于 gfwlist 模式出现的问题也不是没有解决办法，我们只需把要走代理的网站添加至 privoxy 规则文件，一样可以走代理。 ss-local篇(客户端)安装1234# CentOS RHELyum -y install epel-releaseyum -y install python-pippip install shadowsocks 配置1234567891011121314151617181920212223vim /etc/shadowsocks.json&#123; "server": "1.2.3.4", "server_port": 8989, "method": "aes-128-cfb", "password": "123456", "fast_open": false, "local_address": "127.0.0.1", "local_port": 1080, "workers": 2&#125;## 配置说明：&#123; "server": "1.2.3.4", # 服务器IP "server_port": 8989, # 服务器Port "method": "aes-128-cfb", # 加密方式 "password": "123456", # 端口密码 "fast_open": false, # tcp_fastopen "local_address": "127.0.0.1", # 本地监听IP "local_port": 1080, # 本地监听Port "workers": 2 # worker进程数量&#125; 运行1sslocal -c /etc/shadowsocks.json -d start 停止1sslocal -c /etc/shadowsocks.json -d stop privoxy篇安装1yum -y install privoxy 全局模式全局模式是最简单最粗暴的，即：所有流量都走 ss-local，不区分什么国内国外。因此请你确定否需要这种模式，如果不需要，请跳过此段，直接到 -gfwlist模式 123456789101112131415161718192021222324### 新建 whitelist.action 白名单文件--- /etc/privoxy/whitelist.action ---&#123;&#123;alias&#125;&#125;# 代理(socks5)socks5 = +forward-override&#123;forward-socks5 127.0.0.1:1080 .&#125;# 直连direct = +forward-override&#123;forward .&#125;# 所有网站走代理&#123;socks5&#125;/# 以下网站走直连&#123;direct&#125;.ip.cn.chinaz.com--- /etc/privoxy/whitelist.action ---### 加载 whitelist.action 文件echo 'actionsfile whitelist.action' &gt;&gt; /etc/privoxy/config### 启动 privoxy.service 服务systemctl start privoxy.servicesystemctl -l status privoxy.service gfwlist 是由 AutoProxy 官方维护，由众多网民收集整理的一个中国大陆防火长城的屏蔽列表；因为这是 Firefox 浏览器直接使用的一种格式，因此，如果需要用在 privoxy 上就需要进行转换；这里我提供一个 shell 转换脚本，除了正则语法无法自动处理外，其它的基本 OK 123456789101112131415### 获取 gfwlist2privoxy 脚本curl -skL https://raw.github.com/zfl9/gfwlist2privoxy/master/gfwlist2privoxy -o gfwlist2privoxy### 生成 gfwlist.action 文件bash gfwlist2privoxy '127.0.0.1:1080'### 拷贝至 privoxy 配置目录cp -af gfwlist.action /etc/privoxy/### 加载 gfwlist.action 文件echo 'actionsfile gfwlist.action' &gt;&gt; /etc/privoxy/config### 启动 privoxy.service 服务systemctl start privoxy.servicesystemctl -l status privoxy.service 环境变量12345678910# privoxy 默认监听端口为 8118proxy="http://127.0.0.1:8118"export http_proxy=$proxyexport https_proxy=$proxyexport no_proxy="localhost, 127.0.0.1, ::1, ip.cn, chinaz.com"# no_proxy 环境变量是指不经过 privoxy 代理的地址或域名# 只能填写具体的 IP、域名后缀，多个条目之间使用 ',' 逗号隔开# 比如: export no_proxy="localhost, 192.168.1.1, ip.cn, chinaz.com"# 访问 localhost、192.168.1.1、ip.cn、*.ip.cn、chinaz.com、*.chinaz.com 将不使用代理 代理测试1234567891011# 访问各大网站，若均有网页源码输出则配置成功curl -sL www.baidu.comcurl -sL www.google.comcurl -sL www.google.com.hkcurl -sL www.google.co.jpcurl -sL www.youtube.comcurl -sL www.facebook.comcurl -sL www.wikipedia.org# 获取当前 IP 地址，应该显示本机 IPcurl -sL ip.chinaz.com/getip.aspx shell脚本因为每次都需要繁琐的设置 proxy 环境变量，因此这里提供一个简陋的 ss-privoxy 脚本，用于一键启用、关闭代理 新建ss-privoxy文件，chmod +x ss-privoxy、cp -af ss-privoxy /usr/local/bin/ 12345678910111213141516171819202122232425262728293031323334#!/bin/bashcase $1 instart) nohup sslocal -c /etc/ss-local.json &lt; /dev/null &amp;&gt;&gt; /var/log/ss-local.log &amp; systemctl start privoxy proxy="http://127.0.0.1:8118" export http_proxy=$proxy export https_proxy=$proxy export no_proxy="localhost, 127.0.0.1, ::1, ip.cn, chinaz.com" ;;stop) unset http_proxy https_proxy no_proxy systemctl stop privoxy pkill sslocal ;;reload) pkill sslocal nohup sslocal -c /etc/ss-local.json &lt; /dev/null &amp;&gt;&gt; /var/log/ss-local.log &amp; ;;set) proxy="http://127.0.0.1:8118" export http_proxy=$proxy export https_proxy=$proxy export no_proxy="localhost, 127.0.0.1, ::1, ip.cn, chinaz.com" ;;unset) unset http_proxy https_proxy no_proxy ;;*) echo "usage: source $0 start|stop|reload|set|unset" exit 1 ;;esac]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
        <tag>privoxy</tag>
        <tag>科学上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04 使用docker安装mongodb]]></title>
    <url>%2Fubuntu16.04%20%E4%BD%BF%E7%94%A8docker%E5%AE%89%E8%A3%85mongodb%2F</url>
    <content type="text"><![CDATA[安装步骤安装docker-ce12添加tuna镜像源apt-get install docker-ce 拉取镜像1docker pull mongo:3.6 启动1docker run -ti -d --name mongo -v /etc/mongod.conf:/etc/mongod.conf -v /data/db/:/data/db/ -p 27017:27017 mongo:3.4 --auth 其中配置文件可以从另一个容器中获取，/data/db 则是mongo的数据库位置 配置用户认证进入mongo shell 执行 12db.createUser(&#123; user: 'jsmith', pwd: 'some-initial-password', roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ] &#125;)db.createUser(&#123; user: 'root', pwd: '123', roles: [ &#123; role: "root", db: "admin" &#125; ] &#125;) 编辑配置文件12345678910111213141516171819202122232425# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# Where and how to store data.storage: #dbPath: /var/lib/mongodb dbPath: /data/db journal: enabled: true# engine:# mmapv1:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log# network interfacesnet: port: 27017 bindIp: 0.0.0.0]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>docker</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在域环境下配置组策略批量安装客户端常用软件]]></title>
    <url>%2F%E5%9C%A8%E5%9F%9F%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%85%8D%E7%BD%AE%E7%BB%84%E7%AD%96%E7%95%A5%E6%89%B9%E9%87%8F%E5%AE%89%E8%A3%85%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[部署方法：前期准备(一) 在主域控制器的F盘，新建一个目录，命名为“软件安装共享”，设置共享后，并确保user组用户具有读取，执行权限。（最好在“软件安装共享”的安全属性配置向下继承） (二) 打开主域控制器上的“Active Directory用户与计算机”右键“testx.com”属性，选择“组策略”“编辑”，进入“组策略编辑器” (三) 注意：将用户liyuan加入到Domain Administrators 安全组、Enterprise Administrators 安全组或 Group Policy Creator Owners 安全组。 以发布用户方式安装 依次展开“用户配置”“软件设置”“软件安装”。 新建一个“程序包”，在“文件名”后的文本框内输入UNC路径 （例如// 192.168.0.1）软件安装共享\ apache_2.2.4-win32-x86-no_ssl.msi 选择“apache_2.2.4-win32-x86-no_ssl.msi”软件后，点打开 配置部署方法为“已发布” 确定后，登陆客户端client-01进行测试。（测试失败可使用gpupdate/force命令刷新组策略后，重试。） 在客户端的“添加/删除程序”中“添加新程序”，找到“apache_2.2.4-win32-x86-no_ssl.msi”双击安装。 以指派用户方式安装 在部署软件时 选择“已指派” 选择“apache_2.2.4-win32-x86-no_ssl.msi”属性，选中“在登录时安装此应用程序” 登录客户端，自动安装软件，可卸载。 以指派计算机方式安装 在“AD用户与计算机”下，新建一个组织单元us，将computers下的client01客户端，移动到us里面。 右键“us”选择“属性”，点击“组策略”，新建一个策略。 点击“编辑”，展开“计算机设置”“软件设置”，在“软件安装”上右键，新建一个“程序包” 文件名后面的文本框最好用完整的计算机名，如\ ninglian-1.testx.com\软件安装共享\ apache_2.2.4-win32-x86-no_ssl.msi 默认部署类型“已指派” 在客户端上登录，显示如下界面，说明该软件通过组策略安装成功。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>AD</tag>
        <tag>域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给普通用户添加sudo权限]]></title>
    <url>%2F%E7%BB%99%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E6%B7%BB%E5%8A%A0sudo%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[12echo &quot;&#123;username&#125; ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/&#123;username&#125;sudo chmod 0440 /etc/sudoers.d/&#123;username&#125;]]></content>
      <tags>
        <tag>sudo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grafana+influxdb+telegraf监控系统]]></title>
    <url>%2Fgrafana%2Binfluxdb%2Btelegraf%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[基本原理 colletd/telegraf（收集数据） —&gt; influxdb（保存数据） —&gt; grafana（展示数据） 安装配置influxdb安装 cd /opt 下载官网的RPM包 yum localinstall influxdb-1.4.2.x86_64.rpm systemctl enable influxdb systemctl start influxdb telegraf cd /opt 下载官网的RPM包 yum localinstall ~ vim /etc/telegraf/telegraf.conf 编辑配置文件的[[outputs.influxdb]] 编辑配置文件的[[inputs.*]] systemctl enable telegraf systemctl start telegraf grafana cd /opt 下载官网的RPM包 yum localinstall ~ systemctl enable grafana-server systemctl start grafana-server 默认启动端口3000,账户密码均为admin]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>grafana</tag>
        <tag>influxdb</tag>
        <tag>telegraf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluentd+Elasticsearch+Kibana采集nginx日志]]></title>
    <url>%2FFluentd%2BElasticsearch%2BKibana%E9%87%87%E9%9B%86nginx%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[安装过程安装Fluentd12curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent2.sh | shsystemctl start td-agent 安装插件 12td-agent-gem install fluent-plugin-elasticsearchtd-agent-gem install fluent-plugin-typecast 安装Elasticsearch 下载软件包 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.tar.gz 创建组和用户 groupadd elsearch useradd elsearch -g elsearch -p elsearch chown -R elsearch:elsearch /usr/local/src/elsearch (可选)elasticsearch插件安装（github上面检查版本对应） ./bin/plugin install mobz/elasticsearch-head ./bin/plugin install lmenezes/elasticsearch-kopf/v2.1.1 编辑/etc/sysctl.conf 增加 vm.max_map_count=655360 查看是否生效 sysctl -p 编辑/etc/security/limits.conf，在文件尾添加以下内容 * soft nofile 65536 * hard /nofile 65536 编辑elasticsearch的配置文件 config/elasticsearch.yml,修改配置一下项 network.host: 0.0.0.0 切换用户elsearch 启动elasticsearch su elsearch -c bin/elasticsearch 如果要以守护进程方式运行加上参数-d 验证elasticsearch启动是否成功 curl localhost:9200 安装Kibana 下载解压到/opt/目录 wget https://artifacts.elastic.co/downloads/kibana/kibana-6.0.0-linux-x86_64.tar.gz 修改配置文件 config/kibana.yml server.port: 5601 server.host: “0.0.0.0” elasticsearch.url: “http://localhost:9200&quot; 运行kibana bin/kibana 配置fluentd 编辑配置文件 /etc/td-agent/td-agent.conf 123456789101112131415161718&lt;source&gt; @type tail path /var/log/nginx/access.log pos_file /var/log/nginx/access.log.pos tag nginx.access.log format /^(?&lt;ip&gt;\S+)\s-\s-\s\[(?&lt;time&gt;[^\]]*)\]\s"(?&lt;url&gt;[^\"]+)"\s(?&lt;status&gt;\d+)\s(?&lt;size&gt;\d+)\s"(?&lt;ref&gt;[^\"]+)"\s"(?&lt;agent&gt;[^\"]+)"\s"(?&lt;cookie_unb&gt;[^\"]+)"$/ time_format %d/%b/%Y:%H:%M:%S %z&lt;/source&gt;&lt;match nginx.access.log&gt; @type elasticsearch host 192.168.137.52 port 9200 logstash_format true flush_interval 10s #index_name fluentd #type_name fluentd&lt;/match&gt; 创建pos_file touch /var/log/nginx/access.log.pos 编辑权限 chmod 755 /var/log/nginx chmod a+rw /var/log/nginx/access.log.pos 重启fluentd systemctl restart fluentd 检查fluentd日志 tail -f /var/log/td-agent/td-agent.log 用kibana查看日志 参考资料： 正则表达式：http://fluentular.herokuapp.com/ http://blog.csdn.net/chenhaifeng2016/article/details/78683812 http://blog.csdn.net/qq_27252133/article/details/53520416]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>fluentd</tag>
        <tag>elasticsearch</tag>
        <tag>kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7.2-iSCSI安装配置]]></title>
    <url>%2FCentOS7.2-iSCSI%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[iSCSI简介 iSCSI是一种在Internet协议上，特别是以太网上进行数据块传输的标准，它是一种基于Ip Storage理论的新型存储技术，该技术是将存储行业广泛应用的SCSI接口与IP网络技术相结合，可以在IP网络上构建SAN存储区域网络，简单地说，iSCSI就是在IP网络上运行SCSI协议的一种网络存储技术。 iSCSI 主要是透过 TCP/IP 的技术，将储存设备端透过 iSCSI target (iSCSI 目标) 功能，做成可以提供磁盘的服务器端，再透过 iSCSI initiator (iSCSI 初始化用户) 功能，做成能够挂载使用 iSCSI target 的客户端，如此便能透过 iSCSI 协议来进行磁盘的应用了。 也就是说，iSCSI 这个架构主要将储存装置与使用的主机分为两个部分，分别是： iSCSI target：就是储存设备端，存放磁盘或 RAID 的设备，目前也能够将 Linux 主机仿真成 iSCSI target 了！目的在提供其他主机使用的『磁盘』； iSCSI initiator：就是能够使用 target 的客户端，通常是服务器。 也就是说，想要连接到 iSCSI target 的服务器，也必须要安装 iSCSI initiator 的相关功能后才能够使用 iSCSI target 提供的磁盘就是了。 服务器端配置安装软件1yum -y install targetcli 新建块存储12fdisk /dev/sdb新建/dev/sdb1 进入targetcli界面123456789101112&gt; /backstores/block create disk01 /dev/sdb1 #建立块存储disk01&gt; /iscsi create iqn.2018-14.com.iecas.store01:target01 #建立target对象 FQDN域名反写&gt; /iscsi/iqn.2018-14.com.iecas.store01:target01/tpg1/acls create iqn.2018-14.com.iecas.worker01:initiator01 #建立ACL允许该主机访问&gt; /iscsi/iqn.2018-14.com.iecas.store01:target01/tpg1/luns create /backstores/block/disk01 ## #创建LUN并且和存储设备相关联&gt; /iscsi/iqn.2018-14.com.iecas.store01:target01/portals delete 0.0.0.0 3260&gt; /iscsi/iqn.2018-14.com.iecas.store01:target01/portals create 192.168.136.139:3260 #配置target监听IP和端口（默认监听0.0.0.0:3260）（可选）设置用户名和密码&gt; /iscsi/iqn.2018-14.com.iecas.store01:target01/tpg1/acls/iqn.2018-14.com.iecas.worker01:initiator01 set userid=test&gt; /iscsi/iqn.2018-14.com.iecas.store01:target01/tpg1/acls/iqn.2018-14.com.iecas.worker01:initiator01 set password=test&gt; exit 防火墙设置12firewall=cmd --permanent --add-port=3260/tcpfirewall-cmd --reload 启动服务12systemctl start targetsystemctl enable target客户端配置 客户端设置安装1yum -y install iscsi-initiator-utils 配置访问主机标识名1echo &quot;InitiatorName=iqn.2018-14.com.iecas.worker01:initiator01&quot; &gt; /etc/iscsi/initiatorname.iscsi 编辑配置文件（未设置用户密码则不需要）1234567vim /etc/iscsi/iscsid.conf node.session.auth.authmethod = CHAP node.session.auth.username = test node.session.auth.password = test 启动123systemctl start iscsisystemctl enable iscsi 客户端操作发现目标123[root@localhost ~]# iscsiadm -m discovery -t sendtargets -p 192.168.136.139:3260192.168.136.139:3260,1 iqn.2018-14.com.iecas.store01:tartget01 登陆节点123456[root@localhost ~]# iscsiadm -m node –T iqn.2018-14.com.iecas.store01:tartget01 -p 192.168.136.139:3260 -lLogging in to [iface: default, target: iqn.2018-14.com.iecas.store01:tartget01, portal: 192.168.136.139,3260] (multiple)Login to [iface: default, target: iqn.2018-14.com.iecas.store01:tartget01, portal: 192.168.136.139,3260] successful. 系统启动时自动登陆1iscsiadm -m node –T iqn.2018-14.com.iecas.store01:tartget01 -p 192.168.136.139:3260 --op update -n node.startup -v automatic 查看磁盘信息1234567891011[root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 200M 0 part /boot└─sda3 8:3 0 19.8G 0 part ├─centos-root 253:0 0 15.8G 0 lvm / └─centos-swap 253:1 0 4G 0 lvm [SWAP]sdb 8:16 0 5G 0 disk sr0 11:0 1 4G 0 rom 登出节点123[root@localhost ~]# iscsiadm -m node –T iqn.2018-14.com.iecas.store01:tartget01 -p 192.168.136.139:3260 -uLogging out of session [sid: 6, target: iqn.2018-14.com.iecas.store01:tartget01, portal: 192.168.136.139,3260]Logout of [sid: 6, target: iqn.2018-14.com.iecas.store01:tartget01, portal: 192.168.136.139,3260] successful. 刷新节点1iscsiadm -m session –R 注销所有节点 1iscsiadm -m node --logoutall=all 其它 如果存储上的Target下新挂载了一个SAN资源，如何在服务器上连接并识别？ 1234567891011如果一个Target下新增了一个SAN资源，在服务器可以使用iscsiadm –m session –R命令刷新（rescan）已连接的iSCSI session以识别新的SAN资源[root@pe03 /]# iscsiadm -m session –R可以使用cat /proc/scsi/scsi或者fdisk -l来查看连接过来的卷，例如下面的scsi7 Channel: 00 Id: 00 Lun: 01即为新连接的SAN卷：[root@pe03 /]# cat /proc/scsi/scsi…………………………………Host: scsi7 Channel: 00 Id: 00 Lun: 00 Vendor: H3C Model: H3C ISCSI DISK Rev: v1.0 Type: Direct-Access ANSI SCSI revision: 04Host: scsi7 Channel: 00 Id: 00 Lun: 01 Vendor: H3C Model: H3C ISCSI DISK Rev: v1.0 Type: Direct-Access ANSI SCSI revision: 04 如何注销到target的连接？ 12345如果要注销到某一个特定的Target的连接，可以使用下列的命令：[root@pe03 /]# iscsiadm -m node -T iqn.2007-04.acme.com:h3c:200realm.rhel5 -p 200.200.10.200:3260 –u其中，iqn.2007-04.acme.com:h3c:200realm.rhel5是Target的名称，200.200.10.200是Target服务器的IP，实际使用时请根据实际情况修改。如果要注销到所有targets的连接，可以使用下列命令：[root@pe03 /]# iscsiadm -m node --logoutall=all 如何重启iscsi服务? 1停止对iSCSI磁盘的读写，卸载该磁盘上的文件系统，如果使用了LVM，则还需要去激活使用到该磁盘的VG，之后使用service iscsi restart重启iscsi服务 如何停止iscsi服务? 1停止对iSCSI磁盘的读写，卸载该磁盘上的文件系统，如果使用了LVM，则还需要去激活使用到该磁盘的VG，之后使用service iscsi stop停止iscsi服务 如何针对不同的target设置不同的CHAP认证？ 123456789101112131415如果有多个目标服务器启用了CHAP认证，并且它们的用户名和密码不一样，此时需要手动更改node文件。通过iscsiadm -m node -t sendtargets命令发现Target以后，在/var/lib/iscsi/nodes目录下会生成一个或多个以目的服务器上的Target名命名的文件夹，文件夹中有一个文件。此文件中是initiator登录target要使用到的配置参数，可以通过更改此参数文件来配置登录到每个Target的CHAP认证信息。可直接编辑该文件，在该文件中添加（或修改）此Target的CHAP认证用户名和密码：node.session.auth.authmethod = CHAPnode.session.auth.username = xxxxxx ――CHAP认证用户名node.session.auth.password = xxxxxx ――CHAP认证密码（至少12个字符）修改完成后重新登录Target即可也可以使用iscsiadm命令对登录某个target的CHAP认证参数进行修改[root@pe03 /]#iscsiadm -m node -T iqn.2000-03.com.h3c:.h3c-1.pe05-61 -p 200.200.10.101:3260 -o update --name=node.session.auth.authmethod --value=CHAP[root@pe03 /]#iscsiadm -m node -T iqn.2000-03.com.h3c:.h3c-1.pe05-61 -p 200.200.10.101:3260 -o update --name= node.session.auth.username --value=xxxxxxx[root@pe03 /]#iscsiadm -m node -T iqn.2000-03.com.h3c:.h3c-1.pe05-61 -p 200.200.10.101:3260 -o update --name= node.session.auth.password --value=xxxxxxx需要注意的是，发现Target的命令（iscsiadm -m node -t sendtargets）会自动按照/etc/iscsi/iscsi.conf文件中的参数配置刷新/var/lib/iscsi/nodes下initiator登录target要使用的参数文件，所以如果通过修改/var/lib/iscsi/nodes下的文件设置好CHAP认证后又对该存储服务器执行了发现target的操作，则需要再次修改该文件。 如何从操作系统中删除一个target的信息？ 12[root@pe03 /]# iscsiadm -m node -o delete -T iqn.2005-03.com.max -p 192.168.0.4:3260其中iqn.2005-03.com.max代表target的名称，192.168.0.4代表target的IP地址 如何查看就有哪些target记录在了Open-iSCSI数据库中? 1使用iscsiadm -m node命令]]></content>
      <categories>
        <category>存储</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>iSCSI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装Kafka和简单测试]]></title>
    <url>%2FCentOS7%E5%AE%89%E8%A3%85Kafka%E5%92%8C%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[初识Kafkakafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件kafka也可以支持每秒数十万的消息。 支持通过kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。 Kafka的目的是提供一个发布订阅解决方案，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群机来提供实时的消费。 在Kafka有几个比较重要的概念： broker 用于标识每一个Kafka服务，当然同一台服务器上可以开多个broker,只要他们的broker id不相同即可 Topic 消息主题，从逻辑上区分不同的消息类型 Partition 用于存放消息的队列，存放的消息都是有序的，同一主题可以分多个partition，如分多个partiton时，同样会以如partition1存放1,3,5消息,partition2存放2,4,6消息。 Produce 消息生产者，生产消息，可指定向哪个topic，topic哪个分区中生成消息。 Consumer 消息消费者，消费消息，同一消息只能被同一个consumer group中的consumer所消费。consumer是通过offset进行标识消息被消费的位置。当然consumer的个数取决于此topic所划分的partition，如同一group中的consumer个数大于partition的个数，多出的consumer将不会处理消息。 分布式搭建Kafka 序号 服务器名称 IP地址 用途 1 node01 192.168.136.139 2 node02 192.168.136.154 3 node03 192.168.136.155 安装JDK8以上版本（oracle jdk） 安装zookeeper 官网下载zookeeper 解压zookeeper 1tar xf zookeeper-3.4.12.tar.gz -C /opt/ 编辑zookeeper配置文件（集群配置文件相同） 1234567891011121314cd /opt/zookeeper-3.4.12/confcp zoo_sample.cfg zoo.cfgvim zoo.cfg[root@localhost conf]# cat zoo.cfg |grep -v &quot;^#&quot;tickTime=2000initLimit=10syncLimit=5dataDir=/var/lib/zookeeperdataLogDir=/var/log/zookeeperclientPort=2181server.1=node01:2888:3888server.2=node02:2888:3888server.3=node03:2888:3888 参数说明 tickTime这个时间是作为zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔,也就是说每个tickTime时间就会发送一个心跳。 initLimit这个配置项是用来配置zookeeper接受客户端（这里所说的客户端不是用户连接zookeeper服务器的客户端,而是zookeeper服务器集群中连接到leader的follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。 当已经超过10个心跳的时间（也就是tickTime）长度后 zookeeper 服务器还没有收到客户端的返回信息,那么表明这个客户端连接失败。总的时间长度就是 10*2000=20秒。 syncLimit这个配置项标识leader与follower之间发送消息,请求和应答时间长度,最长不能超过多少个tickTime的时间长度,总的时间长度就是5*2000=10秒。 dataDir顾名思义就是zookeeper保存数据的目录,默认情况下zookeeper将写数据的日志文件也保存在这个目录里； clientPort这个端口就是客户端连接Zookeeper服务器的端口,Zookeeper会监听这个端口接受客户端的访问请求； server.A=B:C:D中的A是一个数字,表示这个是第几号服务器,B是这个服务器的IP地址，C第一个端口用来集群成员的信息交换,表示这个服务器与集群中的leader服务器交换信息的端口，D是在leader挂掉时专门用来进行选举leader所用的端口。 创建serverID标识 除了修改zoo.cfg配置文件外,zookeeper集群模式下还要配置一个myid文件,这个文件需要放在dataDir目录下。 这个文件里面有一个数据就是A的值（该A就是zoo.cfg文件中server.A=B:C:D中的A）,在zoo.cfg文件中配置的dataDir路径中创建myid文件。 123node01: echo &quot;1&quot; &gt; /var/lib/zookeeper/myidnode02: echo &quot;2&quot; &gt; /var/lib/zookeeper/myidnode03: echo &quot;3&quot; &gt; /var/lib/zookeeper/myid scp到其它机器 12scp -r /opt/zookeeper-3.4.12 root@node02:/opt/scp -r /opt/zookeeper-3.4.12 root@node03:/opt/ 创建文件夹 12mkdir /var/lib/zookeepermkdir /var/log/zookeeper 启动 1[root@localhost conf]# /opt/zookeeper-3.4.12/bin/zkServer.sh start 查看状态 1[root@localhost conf]# /opt/zookeeper-3.4.12/bin/zkServer.sh status 安装kafka 官网下载kafka 解压kafka 1tar xf kafka_2.11-1.1.0.tgz -C /opt/ 修改配置文件（集群配置文件稍有不同） 1234567891011121314151617181920212223[root@localhost soft]# cd /opt/kafka_2.11-1.1.0/config/[root@localhost config]# cat server.properties |grep -v &quot;^#&quot;|grep -v &quot;^$&quot;broker.id=1 #node02:broker.id=2 #node03:broker.id=3listeners=PLAINTEXT://node01:9092 #node02:listeners=PLAINTEXT://node02:9092 #node03:listeners=PLAINTEXT://node03:9092num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/var/lib/kafkanum.partitions=2num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=node01:2181,node02:2181,node03:2181zookeeper.connection.timeout.ms=6000group.initial.rebalance.delay.ms=0 参数说明： broker.id broker唯一标识listeners kafka监听IP及安全方式log.dirs 日志存储num.partitions 创建topic时默认partition数量zookeeper.connect zookeeper服务器地址 启动kafka(所有集群启动) 12[root@node01 ~]# cd /opt/kafka_2.11-1.1.0/bin./kafka-server-start.sh -daemon ../config/server.properties 查看 1jps 使用kafka 创建topic 1./bin/kafka-topics.sh --zookeeper node01:2181,node02:2181,node03:2181 --create --topic my-test-topic --partitions 5 --replication-factor 1 创建生产者 1./bin/kafka-console-producer.sh --topic my-test-topic --broker-list node01:9092,node02:9092,node03:9092 创建消费者 1./bin/kafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --topic my-test-topic --from-beginning ​]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7-apache2.4.6优化]]></title>
    <url>%2FCentos7-apache2.4.6%20%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Apache配置文件优化 apache的配置文件优化 Apache禁止遍历目录 将Options Indexes FollowSymLinks中的Indexes 去掉，就可以禁止Apache 显示该目录结构。Indexes 的作用就是当该目录下没有index.html文件时，就显示目录结构。 apache 隐藏版本信息测试默认apache 的状态信息 1234567891011[root@1314it conf]# curl -Is localhostHTTP/1.1 200 OKDate: Tue, 16 Nov 2010 04:20:15 GMTServer: Apache/2.2.3(CentOS) DAV/2 PHP/5.1.6mod_perl/2.0.4Perl/v5.8.8X-Powered-By: PHP/5.1.6Connection: closeContent-Type: text/html;charset=GB2312 修改主配置文件httpd.conf，将下面两行添加到配置文件的末尾 12ServerSignature OffServerTokens ProductOnly 重启apache测试 1234567[root@1314it conf]# curl -Is localhostHTTP/1.1 200 OKDate: Tue, 16 Nov 2010 04:21:41 GMTServer: ApacheX-Powered-By: PHP/5.1.6Connection: closeContent-Type: text/html; charset=GB2312 关闭trace-method配置文件添加一行 1TraceEnable off Apache并发数调整12345678查看Apache的工作模式：apache -l如果出现prefork.c，则是工作在prefork模式下查看当前的连接数：ps aux|grep httpd|wc -lpgrep httpd|wc -l计算httpd占用内存的平均数：ps aux|grep -v grep|awk &apos;/httpd/&#123;sum+=$6;n++&#125;;END&#123;print sum/n&#125;&apos;使用ulimit -n 65535 增加文件打开数量 Apache工作模式及推荐配置123456789101112131415161718192021222324252627282930313233343536373839404142434445# prefork MPM# StartServers:启动时服务器的进程数# MinSpareServers:保有的备用进程的最小数目# MaxSpareServers:保有的备用进程的最大数目# MaxClients:服务器允许启动的最大进程数# MaxRequestsPerChild:一个服务进程允许的最大请求数&lt;IfModule prefork.c&gt; // 设置使用预生派(Prefork MPM)运行方式的参数，此方式是Redhat默认的方式StartServers 8 // 设置服务器启动时运行的进程数为8MinSpareServers 5 // 如果低于5个空闲子进程，就会创建新的子进程为客户提供服务MaxSpareServers 20 // 如果存在高于20个空闲子进程,就创建逐一删除的子进程来提高系统性能MaxClients 150 // 限制同一时间连接数不能超过150MaxRequestsPerChild 1000 // 限制每个子进程在结束请求之前能处理的连接请求为1000&lt;/IfModule&gt;# worker MPM# StartServers:启动时的服务进程数目# MaxClients:允许同时连接的最大用户数目# MinSpareThreads:保有的最小工作线程数目# MaxSpareThreads:允许保有的最大工作线程数目# ThreadsPerChild:每个服务进程中的工作线程常数# MaxRequestsPerChild:服务进程中允许的最大请求数目&lt;IfModule worker.c&gt; // 设置使用工作者模式(worker MPM)运行方式的参数StartServers 2MaxClients 150MinSpareThreads 25MaxSpareThreads 75ThreadsPerChild 25MaxRequestsPerChild 0&lt;/IfModule&gt;# perchild MPM# NumServers:服务进程数量# StartThreads:每个服务进程中的起始线程数量# MinSpareThreads:保有的最小线程数量# MaxSpareThreads:保有的最大线程数量# MaxThreadsPerChild:每个服务进程允许的最大线程数# MaxRequestsPerChild:每个服务进程允许连接的最大数量&lt;IfModule perchild.c&gt; // 设置使用独立子进程(Perchild MPM)运行方式的参数NumServers 5StartThreads 5MinSpareThreads 5MaxSpareThreads 10MaxThreadsPerChild 20MaxRequestsPerChild 0&lt;/IfModule&gt; 512M内存123456StartServers 5MinSpareServers 5MaxSpareServers 10ServerLimit 256MaxClients 256MaxRequestsPerChild 100 1G内存123456StartServers 10MinSpareServers 10MaxSpareServers 25ServerLimit 256MaxClients 256MaxRequestsPerChild 1000//或者设置为0，即不限制 用户敲下一个域名访问服务器的过程是如何进行的？ 首先客户的服务器会检查自己的缓存，如果有对应的ip， 则直接返回ip，客户使用ip去访问服务器，与服务器进行TCP三次握手， 三次握手建立完毕，发送HTTP数据请求到服务器，服务器进行响应。 如果没有，则请求会发往本地DNS服务器，本地的DNS服务器负责解析， 如果没有对应ip，发起迭代查询，直到查询到所需的ip地址， 然后再使用ip去访问服务器，进行tcp三次握手，三次握手建立完毕后， 发送http请求，服务器进行响应。]]></content>
      <categories>
        <category>web服务器</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>apache</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch安装配置]]></title>
    <url>%2Felasticsearch%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装步骤 安装jdk 编辑配置文件/etc/sysctl.conf ，添加如下 vm.max_map_count = 262144vm.swappiness = 1 从官网下载tar包 新建一个普通用户，切换到普通用户进行操作 将tar包解压至opt下 配置/opt/elasticsearch~/config/elasticsearch.yml文件 network.host: IP(0.0.0.0) http.port: 9200 使用普通用户启动elasticsearch ./bin/elasticsearch elasticsearch集群配置文件示例配置参数文件，在配置前，做个备份 1cp elasticsearch.yml elasticsearch.yml.bak 修改参数文件 12345678910111213141516171819202122232425262728293031集群名称：cluster.name: FVP-RAS节点名称：node.name:&quot;node1&quot;数据复制份数：index.number_of_replicas: 2数据文件：如果存放位置多个地方，用逗号分开每个目录path.data: /data1/elastic/data1file,/data2/elastic/data2file内存锁定：bootstrap.mlockall: true通讯地址：network.publish_host: 10.202.20.191恢复相关：能执行恢复最小节点数：gateway.recover_after_nodes:66个节点起来后，等10分钟才执行恢复gateway.recover_after_time:10m已有7个节点起来，马上执行恢复gateway.expected_nodes:7定义发现相关：定义发现主节点数：discovery.zen.minimum_master_nodes: 5注意：这里设定为N/2+1，8节点为8/2+1=5关闭自动发现节点：discovery.zen.ping.multicast.enabled: false定义发现的节点:discovery.zen.ping.unicast.hosts: [&quot;ras1.novalocal&quot;,&quot;ras2.novalocal&quot;,&quot;ras3.novalocal&quot;,&quot;ras4.novalocal&quot;,&quot;ras5.novalocal&quot;,&quot;ras6.novalocal&quot;,&quot;ras7.novalocal&quot;,&quot;ras8.novalocal&quot;]cd/opt/elasticsearch/binHeap内存大小：elasticsearch.in.sh 第四行添加ES_HEAP_SIZE=31232m 自己的elasticsearch集群配置文件123456789101112cluster.name: elastic-cluster-1node.name: elastic-node2node.master: truenode.data: truenetwork.host: 192.168.137.52http.port: 9200discovery.zen.ping.multicast.enabled: falsediscovery.zen.ping.unicast.hosts: [&quot;node1&quot;, &quot;node2&quot;,&quot;node3&quot;]discovery.zen.minimum_master_nodes: 2gateway.expected_nodes: 3gateway.recover_after_nodes: 2gateway.recover_after_time: 5m Trouble shooting123ERROR: bootstrap checks failed[1]:max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]:max virtual memory areas vm.max_map_count [65530] is too low,increase to at least [262144] for [1]编辑/etc/security/limits.conf 添加或修改如下内容 12* hard nofile 65536* soft nofile 65536 for [2]编辑/etc/sysctl.conf 直接在末尾添加 123vm.max_map_count=262144执行 sysctl -p /etc/sysctl.confsysctl -a |grep vm.max_map_count 看看是否修改成功]]></content>
      <categories>
        <category>服务部署</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible自动化运维详细教程及playbook详解]]></title>
    <url>%2Fansible%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B%E5%8F%8Aplaybook%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言 当下有许多的运维自动化工具( 配置管理 )，例如：Ansible、SaltStack、Puppet、Fabric 等。 Ansible 一种集成 IT 系统的配置管理、应用部署、执行特定任务的开源平台，是 AnsibleWorks 公司名下的项目，该公司由 Cobbler 及 Func 的作者于 2012 年创建成立。 Ansible 基于 Python 语言实现，由 Paramiko 和 PyYAML 两个关键模块构建。 Ansible 特点： 部署简单，只需在主控端部署 Ansible 环境，被控端无需做任何操作。 默认使用 SSH（Secure Shell）协议对设备进行管理。 主从集中化管理。 配置简单、功能强大、扩展性强。 支持 API 及自定义模块，可通过 Python 轻松扩展。 通过 Playbooks 来定制强大的配置、状态管理。 对云计算平台、大数据都有很好的支持。 提供一个功能强大、操作性强的 Web 管理界面和 REST API 接口 —- AWX 平台。 Ansible 与 SaltStack： 最大的区别是 Ansible 无需在被监控主机部署任何客户端代理，默认通过 SSH 通道进行远程命令执行或下发配置。 相同点是都具备功能强大、灵活的系统管理、状态配置，都使用 YAML 格式来描述配置，两者都提供丰富的模板及 API，对云计算平台、大数据都有很好的支持。 安装ansible1yum -y install ansible 配置ansible1234ls /etc/ansibleansible.cfg hosts roles# ansible.cfg 是 Ansible 工具的配置文件；hosts 用来配置被管理的机器；roles 是一个目录，playbook 将使用它 SSH秘钥认证12ssh-keygen -t rsassh-copy-id root@agent_host_ip 添加被管理主机12345vim /etc/ansible/hosts[Client]angent_host_ip_1angent_host_ip_2 测试ansible123456789101112shell &gt; ansible Client -m ping # 操作 Client 组 ( all 为操作 hosts 文件中所有主机 )，-m 指定执行 ping 模块，下面是返回结果192.168.12.129 | SUCCESS =&gt; &#123;&quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;# -i 指定 hosts 文件位置# -u username 指定 SSH 连接的用户名# -k 指定远程用户密码# -f 指定并发数# -s 如需要 root 权限执行时使用 ( 连接用户不是 root 时 )# -K -s 时，-K 输入 root 密码 hosts主机文件123456789101112131415161718192021222324252627282930313233343536373839404142shell &gt; vim /etc/ansible/hostswww.abc.com # 定义域名192.168.1.100 # 定义 IP192.168.1.150:37268 # 指定端口号[WebServer] # 定义分组192.168.1.10192.168.1.20192.168.1.30[DBServer] # 定义多个分组192.168.1.50192.168.1.60Monitor ansible_ssh_port=12378 ansible_ssh_host=192.168.1.200 # 定义别名# ansible_ssh_host 连接目标主机的地址# ansible_ssh_port 连接目标主机的端口，默认 22 时无需指定# ansible_ssh_user 连接目标主机默认用户# ansible_ssh_pass 连接目标主机默认用户密码# ansible_ssh_connection 目标主机连接类型，可以是 local 、ssh 或 paramiko# ansible_ssh_private_key_file 连接目标主机的 ssh 私钥# ansible_*_interpreter 指定采用非 Python 的其他脚本语言，如 Ruby 、Perl 或其他类似 ansible_python_interpreter 解释器[webservers] # 主机名支持正则描述www[01:50].example.com[dbservers]db-[a:f].example.com ansible常用模块123shell &gt; ansible-doc -l # 列出 Ansible 支持的模块shell &gt; ansible-doc ping # 查看该模块帮助信息 远程命令模块（command / script / shell）commandcommand 作为 Ansible 的默认模块，可以运行远程权限范围所有的 shell 命令，不支持管道符。 1shell &gt; ansible Client -m command -a &quot;free -m&quot; # 查看 Client 分组主机内存使用情况 scriptscript 的功能是在远程主机执行主控端存储的 shell 脚本文件，相当于 scp + shell 组合。 1shell &gt; ansible Client -m script -a &quot;/home/test.sh 12 34&quot; # 远程执行本地脚本 shellshell模块基本和command相同，但是shell支持管道符 1shell &gt; ansible Client -m shell -a &quot;/home/test.sh&quot; # 执行远程脚本 copy模块实现主控端向目标主机拷贝文件，类似于 scp 功能 1shell &gt; ansible Client -m copy -a &quot;src=/home/test.sh dest=/tmp/ owner=root group=root mode=0755&quot; # 向 Client 组中主机拷贝 test.sh 到 /tmp 下，属主、组为 root ，权限为 0755 file模块file模块可以帮助我们完成一些对文件的基本操作，比如，创建文件或目录、删除文件或目录、修改文件权限等 123456789101112131415161718192021222324252627282930313233343536在test70主机上创建一个名为testfile的文件，如果testfile文件已经存在，则会更新文件的时间戳，与touch命令的作用相同。ansible test70 -m file -a &quot;path=/testdir/testfile state=touch&quot;在test70主机上创建一个名为testdir的目录，如果testdir目录已经存在，则不进行任何操作。ansible test70 -m file -a &quot;path=/testdir/testdir state=directory&quot;在test70上为testfile文件创建软链接文件，软链接名为linkfile，执行下面命令的时候，testfile已经存在。ansible test70 -m file -a &quot;path=/testdir/linkfile state=link src=/testdir/testfile&quot;在test70上为testfile文件创建硬链接文件，硬链接名为hardfile，执行下面命令的时候，testfile已经存在。ansible test70 -m file -a &quot;path=/testdir/hardfile state=hard src=/testdir/testfile&quot;在创建链接文件时，如果源文件不存在，或者链接文件与其他文件同名时，强制覆盖同名文件或者创建链接文件，参考上述force参数的解释。ansible test70 -m file -a &quot;path=/testdir/linkfile state=link src=sourcefile force=yes&quot;删除远程机器上的指定文件或目录ansible test70 -m file -a &quot;path=/testdir/testdir state=absent&quot;在创建文件或目录的时候指定属主，或者修改远程主机上的文件或目录的属主。ansible test70 -m file -a &quot;path=/testdir/abc state=touch owner=zsy&quot;ansible test70 -m file -a &quot;path=/testdir/abc owner=zsy&quot;ansible test70 -m file -a &quot;path=/testdir/abc state=directory owner=zsy&quot;在创建文件或目录的时候指定属组，或者修改远程主机上的文件或目录的属组。ansible test70 -m file -a &quot;path=/testdir/abb state=touch group=zsy&quot;ansible test70 -m file -a &quot;path=/testdir/abb group=zsy&quot;ansible test70 -m file -a &quot;path=/testdir/abb state=directory group=zsy&quot;在创建文件或目录的时候指定权限，或者修改远程主机上的文件或目录的权限。ansible test70 -m file -a &quot;path=/testdir/abb state=touch mode=0644&quot;ansible test70 -m file -a &quot;path=/testdir/abb mode=0644&quot;ansible test70 -m file -a &quot;path=/testdir/binfile mode=4700&quot;ansible test70 -m file -a &quot;path=/testdir/abb state=directory mode=0644&quot;当操作远程主机中的目录时，同时递归的将目录中的文件的属主属组都设置为zsy。ansible test70 -m file -a &quot;path=/testdir/abd state=directory owner=zsy group=zsy recurse=yes&quot; unarchive作用： 将ansible主机上的压缩包在本地解压缩后传到远程主机上,默认设置为copy=yes, 将远程主机上的某个压缩包解压缩到指定路径下,需设置copy=no 12345#解压远程主机src路径下的包，解压至/opt/下，并将解压出来的文件属组和属主改为jiii[root@ljc project1]# ansible web -m unarchive -a &apos;src=/usr/share/nginx/html/WeCenter_3-2-1.zip copy=no dest=/opt/ group=jiii owner=jiii&apos;#解压本地管理机src路径下的包，解压至/opt/t下，并将解压出来的文件属组和属主改为jiii[root@ljc project1]# ansible web -m unarchive -a &apos;src=/root/project1/phpMyAdmin-4.8.4-all-languages.zip dest=/opt/t group=jiii owner=jiii&apos; stat模块获取远程文件状态信息，atime/ctime/mtime/md5/uid/gid 等信息 1shell &gt; ansible Client -m stat -a &quot;path=/etc/syctl.conf&quot; get_url实现在远程主机下载指定 URL 到本地，支持 sha256sum 文件校验 1shell &gt; ansible Client -m get_url -a &quot;url=http://www.baidu.com dest=/tmp/index.html mode=0440 force=yes&quot; yum软件包管理 1shell &gt; ansible Client -m yum -a &quot;name=curl state=latest&quot; corn远程主机 crontab 配置 1234shell &gt; ansible Client -m cron -a &quot;name=&apos;check dirs&apos; hour=&apos;5,2&apos; job=&apos;ls -alh &gt; /dev/null&apos;&quot;效果：* 5,2 * * * ls -alh &gt; /dev/null mount远程主机分区挂载 1shell &gt; ansible Client -m mount -a &quot;name=/mnt/data src=/dev/sd0 fstype=ext4 opts=ro state=present&quot; service远程主机系统服务管理 123shell &gt; ansible Client -m service -a &quot;name=nginx state=stoped&quot;shell &gt; ansible Client -m service -a &quot;name=nginx state=restarted&quot;shell &gt; ansible Client -m service -a &quot;name=nginx state=reloaded&quot; user远程主机用户管理 123shell &gt; ansible Client -m user -a &quot;name=wang comment=&apos;user wang&apos;&quot;shell &gt; ansible Client -m user -a &quot;name=wang state=absent remove=yes&quot; # 添加删除用户 ansible-playbook 详解命令详解1234567891011121314151617181920212223242526272829ansible-playbook playbook.yml [options]-u REMOTE_USER, --user=REMOTE_USER ＃ ssh 连接的用户名 -k, --ask-pass ＃ssh登录认证密码 -s, --sudo ＃sudo 到root用户，相当于Linux系统下的sudo命令-U SUDO_USER, --sudo-user=SUDO_USER ＃sudo 到对应的用户 -K, --ask-sudo-pass ＃用户的密码（—sudo时使用） -T TIMEOUT, --timeout=TIMEOUT ＃ ssh 连接超时，默认 10 秒 -C, --check ＃ 指定该参数后，执行 playbook 文件不会真正去执行，而是模拟执行一遍，然后输出本次执行会对远程主机造成的修改 -e EXTRA_VARS, --extra-vars=EXTRA_VARS ＃ 设置额外的变量如：key=value 形式 或者 YAML or JSON，以空格分隔变量，或用多个-e -f FORKS, --forks=FORKS ＃ 进程并发处理，默认 5 -i INVENTORY, --inventory-file=INVENTORY ＃ 指定 hosts 文件路径，默认 default=/etc/ansible/hosts -l SUBSET, --limit=SUBSET ＃ 指定一个 pattern，对- hosts:匹配到的主机再过滤一次 --list-hosts ＃ 只打印有哪些主机会执行这个 playbook 文件，不是实际执行该 playbook --list-tasks ＃ 列出该 playbook 中会被执行的 task --private-key=PRIVATE_KEY_FILE ＃ 私钥路径 --step ＃ 同一时间只执行一个 task，每个 task 执行前都会提示确认一遍 --syntax-check ＃ 只检测 playbook 文件语法是否有问题，不会执行该 playbook -t TAGS, --tags=TAGS ＃当 play 和 task 的 tag 为该参数指定的值时才执行，多个 tag 以逗号分隔 --skip-tags=SKIP_TAGS ＃ 当 play 和 task 的 tag 不匹配该参数指定的值时，才执行 -v, --verbose ＃输出更详细的执行过程信息，-vvv可得到所有执行过程信息。 YAML语法 YAML的语法和其他高阶语言类似并且可以简单表达清单、散列表、标量等数据结构。（列表用横杆表示，键值对用冒号分割，键值对里又可以嵌套另外的键值对） YAML文件扩展名通常为.yaml或者.yml。下面为示例 一定要对齐，只能使用空格 12345678910111213name: tomage: 21gender: malespourse: name: lily gender: femalechildren: - name: susan age: 2 gender: feamle - name: sunny age: 10 gender: male Playbook目录结构123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@localhost project]# tree.├── group_vars &lt;- 所有主机的公共变量存放位置│ └── all &lt;- all对应公共变量│ │__ group_name &lt;- 必须对应主机组名称├──inventory│ └── hosts &lt;- 需要管理的主机的列表信息├── roles &lt;- roles 存放模块, 当前有 etcd, initial, loop 三个模块│ ├── etcd│ │ ├── files &lt;- 需要直接复制到 client 的文件存放位置│ │ │ └── etcd-proxy.service &lt;--即每个主机配置一样│ │ ├── handlers &lt;- 用于服务管理用的控制文件│ │ │ └── main.yml│ │ ├── tasks &lt;- ansible 任务文件│ │ │ ├── config.yml│ │ │ ├── main.yml│ │ │ ├── package.yml│ │ │ └── service.yml│ │ └── templates &lt;- 需要复制到 client 中的模板文件, 会配合变量进行配置变换│ │ │ └── etcd-proxy.conf &lt;-- 即每个主机配置可能不一样│ │ │___vars│ │ └──main.yml &lt;-- roles变量文件│ ├── initial│ │ ├── files│ │ │ ├── hosts│ │ │ ├── resolv.conf│ │ │ └── updatedb.conf│ │ ├── handlers│ │ ├── tasks│ │ │ ├── main.yml│ │ │ ├── mlocate.yml│ │ │ ├── package.yml│ │ │ ├── sysctl.yml│ │ │ └── yumrepo.yml│ │ └── templates│ │ ├── centos7.repo│ │ └── docker.repo│ └── loop│ ├── files│ ├── handlers│ ├── tasks│ │ ├── main.yml│ │ └── t1.yml│ └── templates└── site.yml &lt;- 主控制入口文件 核心组件 tasks：任务 variables：变量 templates：模板 handlers：处理器 roles：角色 标签处理意义：通过tags和任务对象进行捆绑，控制部分或者指定的task执行 打标签 123对一个对象打一个标签对一个对象打多个标签打标签的对象包括：单个task任务、include对象、roles对象等 标签使用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152-t : 执行指定的tag标签任务--skip-tags : 执行--skip-tags之外的标签任务# 示例devops@devops-virtual-machine:/etc/ansible$ cat f14.yml---- hosts : 192.168.56.11 remote_user : root tasks : - name: create file 1 shell: touch /tmp/file1.txt tags: - cfile1 - cfile3 - name: create file 2 shell : touch /tmp/file2.txt tags: - cfile2# 执行剧本# 执行剧本，只执行tags为cfile1的任务devops@devops-virtual-machine:/etc/ansible$ ansible-playbook ./f14.yml -t cfile1PLAY [192.168.56.11] ************************************************************************************TASK [Gathering Facts] **********************************************************************************ok: [192.168.56.11]TASK [create file 1] ************************************************************************************ [WARNING]: Consider using file module with state=touch rather than running touchchanged: [192.168.56.11]PLAY RECAP **********************************************************************************************192.168.56.11 : ok=2 changed=1 unreachable=0 failed=0# 执行剧本不包含tags为cfile1的任务devops@devops-virtual-machine:/etc/ansible$ ansible-playbook ./f14.yml --skip-tags cfile1PLAY [192.168.56.11] ************************************************************************************TASK [Gathering Facts] **********************************************************************************ok: [192.168.56.11]TASK [create file 2] ************************************************************************************ [WARNING]: Consider using file module with state=touch rather than running touchchanged: [192.168.56.11]PLAY RECAP **********************************************************************************************192.168.56.11 : ok=2 changed=1 unreachable=0 failed=0 playbook简单示例第一个示例12345678910111213vim /root/first.yml- hosts: all remote_user: root vars: httpd_port=80 tasks: - name: install httpd yum: name=httpd state=present - name: install php yum: name=php state=present - name: start httpd service: name=httpd state=started enabled=true hosts 定义单个主机或组，vars定义变量，remote_user定义执行命令的远程用户，tasks定义执行哪些命令，handlers定义调用哪些处理器 vars(变量)： 变量命名： 字母数字下划线组成，只能以字母开头 变量种类： facts（内置变量） 由远程主机发回的主机属性信息，这些信息被保存在ansible变量当中 例如：ansible 192.168.238.170 -m setup 来获取远程主机上的属性信息，这些属性信息保存在facts中 通过命令行传递 通过命令行传递：ansible-playbook test.yml –extra-vars “host=www user=tom“（如果剧本中已有此处定义的变量则会被覆盖） 通过roles传递 主机变量 在/etc/ansible/hosts中定义 123&gt; [web1]&gt; 192.168.1.1 name=haha&gt; 组变量 123&gt; [group_name:vars]&gt; foo=bar&gt; hosts : /etc/abible/hosts 中指定的远程主机，并用指定的属性进行连接 123456&gt; ansible_ssh_port 连接远程主机使用的端口&gt;&gt; ansible_ssh_user 连接远程主机使用的用户&gt;&gt; ansible_ssh_pass 连接远程主机使用的密码&gt; 123456&gt; cat /etc/ansible/hosts&gt;&gt; [web1]&gt; web1.hostname ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass=123&gt; web2.hostname&gt; 第二个示例123456789101112131415161718192021222324vim /root/second.yml- hosts: web1 remote_user: root vars: username: bob password: 123 tasks: - name: add user user: name=&#123;&#123; username &#125;&#125; state=present when: ansible_os_family == &quot;Debian&quot; - name: set password shell: echo &#123;&#123; password &#125;&#125; |passwd --stdin &#123;&#123; username &#125;&#125; - name: install httpd php yum: name=&#123;&#123; item &#125;&#125; state=present with_items: - httpd - php - name: add two users user: name=&#123;&#123; item &#125;&#125; state=present groups=&#123;&#123; item.groups &#125;&#125; with_items: - &#123; name: &apos;user1&apos;, groups: &apos;group1&apos;&#125; - &#123; name: &apos;user2&apos;, groups: &apos;group2&apos;&#125; - 在playbook中调用变量的方式为{{ variable }} - when语句用来条件测试 - ansible_os_family 是facts中内置的属性信息 ansible_os_family的信息可以使用ansible all -m setup | grep ansible_os_family 查看 - 在task中调用内置的item变量；在某task后面使用with_items语句来定义元素列表 第三个示例123456789101112131415161718192021vim /root/third.yml- hosts: web1 remote_user: root vars: httpd_port=80 tasks: - name: install httpd yum: name=httpd state=present - name: install php yum: name=php state=present - name: copy config file copy: src=/root/httpd.conf dest=/etc/httpd/conf/httpd.conf notify: restart httpd - name: start httpd service: name=httpd state=started enabled=true handlers: - name: restart httpd service: name=httpd state=restarted 上面的意思是copy中复制过去的文件跟远程主机上的文件不同，就通过notify调用handlers，即重启httpd服务。 handler是重启服务是最通用的用法 第四个示例123vim /etc/ansible/hosts[web1]192.168.1.1 http_port=80 1234vim /root/httpd.conf……Listen &#123;&#123; http_port &#125;&#125;…… 12345678910111213141516171819vim /root/fourth.yml- hosts: web1 remote_user: root vars: httpd_port=80 tasks: - name: install httpd yum: name=httpd state=present - name: copy config file template: src=/root/httpd.conf dest=/etc/httpd/conf/httpd.conf notify: restart httpd - name: start httpd service: name=httpd state=started enabled=true handlers: - name: restart httpd service: name=httpd state=restarted > templates：用于生成文本文件（配置文件） > > 模板文件中可使用jinja2表达式，表达式要定义在{{ }}，也可以简单地仅执行变量替换 第五个示例roles：roles用于实现“代码复用”，roles以特定的层次型格式组织起来的playbook元素（variables, tasks, templates,handlers）；可被playbook以role的名字直接进行调用 roles的文件结构： files/：此角色中用到的所有文件均放置于此目录中 templates/： Jinja2模板文件存放位置 tasks/：任务列表文件；可以有多个，但至少有一个叫做main.yml的文件 handlers/：处理器列表文件；可以有多个，但至少有一个叫做main.yml的文件 vars/：变量字典文件；可以有多个，但至少有一个叫做main.yml的文件 meta/：此角色的特殊设定及依赖关系 123mkdir /root/rolescd /root/rolesmkdir -p web1/&#123;files, templayes, tasks, handlers, vars, meta&#125; 1234vim web1/vars/main.ymluser: tomgroup: tomhttp_port: 8080 12345678910111213vim web1/tasks/main.yml- name: install httpd yum: name=httpd state=present- name: copy config file template: src=httpd.conf dest=/etc/httpd/conf/httpd.conf notify: restart httpd tags: conf- name: start httpd service: name=httpd state=started enabled=true 这里的template指的是相对路径--&gt;web1/templatestags可以在运行时指定标签任务 12345vim web1/handlers/main.ymlhandlers:- name: restart httpd service: name=httpd state=restarted 12345vim web1/templates/httpd.conf……Listen &#123;&#123; http_port &#125;&#125;…… 定义一个调用roles文件 1234567891011vim /root/web1.yml- hosts: web1 remote_user: root roles: - web1 - &#123; role:web2, http_port:8080 &#125; hosts：web1 指在/etc/ansible/hosts中定义的组，上面有定义roles: web1 指的是当前目录下的web1目录，也可通过role传递变量， 也可调用多个role这样只需更改hosts的主机就可以实现不同主机的代码重用了 运行 123ansible-playbook web1.yml指定运行任务：ansible-playbook -t conf web1.yml 使用ansible-playbook安装zabbix定义hosts123456shell &gt; vim /etc/ansible/hosts[mini]129.139.153.78:16283155.139.190.94:12573 定义入口文件install_zabbix_agent.yml12345678shell &gt; vim /etc/ansible/install_zabbix_agent.yml---- hosts: mini roles: - install_zabbix_agent## 可以看到将要安装的主机组为 mini 组，角色为 install_zabbix_agent 定义角色 install_zabbix_agent12345678910111213141516shell &gt; tree /etc/ansible/roles/install_zabbix_agent/├── files│ └── zabbix-2.4.5.tar.gz├── tasks│ └── main.yml├── templates│ ├── zabbix_agentd│ └── zabbix_agentd.conf└── vars └── main.yml## 建立 files 目录，存放编译安装过的 zabbix_agent 目录的压缩文件，用于拷贝到远程主机## 建立 tasks 目录，用于编写将要执行的任务## 建立 templates 目录，用于存放可变的模板文件## 建立 vars 目录，用于存放变量信息 建立tasks主文件1234567891011121314151617181920212223shell &gt; cat /etc/ansible/roles/install_zabbix_agent/tasks/main.yml--- - name: Install Software yum: name=&#123;&#123; item &#125;&#125; state=latest with_items: - libcurl-devel - name: Create Zabbix User user: name=&#123;&#123; zabbix_user &#125;&#125; state=present createhome=no shell=/sbin/nologin - name: Copy Zabbix.tar.gz copy: src=zabbix-&#123;&#123; zabbix_version &#125;&#125;.tar.gz dest=&#123;&#123; zabbix_dir &#125;&#125;/src/zabbix-&#123;&#123; zabbix_version &#125;&#125;.tar.gz owner=root group=root - name: Uncompression Zabbix.tar.gz shell: tar zxf &#123;&#123; zabbix_dir &#125;&#125;/src/zabbix-&#123;&#123; zabbix_version &#125;&#125;.tar.gz -C &#123;&#123; zabbix_dir &#125;&#125;/ - name: Copy Zabbix Start Script template: src=zabbix_agentd dest=/etc/init.d/zabbix_agentd owner=root group=root mode=0755 - name: Copy Zabbix Config File template: src=zabbix_agentd.conf dest=&#123;&#123; zabbix_dir &#125;&#125;/zabbix/etc/zabbix_agentd.conf owner=&#123;&#123; zabbix_user &#125;&#125; group=&#123;&#123; zabbix_user &#125;&#125; mode=0644 - name: Modify Zabbix Dir Permisson file: path=&#123;&#123; zabbix_dir &#125;&#125;/zabbix owner=&#123;&#123; zabbix_user &#125;&#125; group=&#123;&#123; zabbix_user &#125;&#125; mode=0755 recurse=yes - name: Start Zabbix Service shell: /etc/init.d/zabbix_agentd start - name: Add Boot Start Zabbix Service shell: chkconfig --level 35 zabbix_agentd on 建立主变量文件 变量使用说明 1234567shell &gt; cat /etc/ansible/roles/install_zabbix_agent/vars/main.ymlzabbix_dir: /usr/localzabbix_version: 2.4.5zabbix_user: zabbixzabbix_port: 10050zabbix_server_ip: 131.142.101.120 建立模板文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119shell &gt; cat /etc/ansible/roles/install_zabbix_agent/templates/zabbix_agentd#!/bin/bash## chkconfig: - 90 10# description: Starts and stops Zabbix Agent using chkconfig# Tested on Fedora Core 2 - 5# Should work on all Fedora Core versions## @name: zabbix_agentd# @author: Alexander Hagenah &lt;hagenah@topconcepts.com&gt;# @created: 18.04.2006## Modified for Zabbix 2.0.0# May 2012, Zabbix SIA## Source function library.. /etc/init.d/functions# Variables# Edit these to match your system settings # Zabbix-Directory BASEDIR=&#123;&#123; zabbix_dir &#125;&#125;/zabbix # Binary File BINARY_NAME=zabbix_agentd # Full Binary File Call FULLPATH=$BASEDIR/sbin/$BINARY_NAME # PID file PIDFILE=/tmp/$BINARY_NAME.pid # Establish args ERROR=0 STOPPING=0## No need to edit the things below## application checking statusif [ -f $PIDFILE ] &amp;&amp; [ -s $PIDFILE ] then PID=`cat $PIDFILE` if [ &quot;x$PID&quot; != &quot;x&quot; ] &amp;&amp; kill -0 $PID 2&gt;/dev/null &amp;&amp; [ $BINARY_NAME == `ps -e | grep $PID | awk &apos;&#123;print $4&#125;&apos;` ] then STATUS=&quot;$BINARY_NAME (pid `pidof $APP`) running..&quot; RUNNING=1 else rm -f $PIDFILE STATUS=&quot;$BINARY_NAME (pid file existed ($PID) and now removed) not running..&quot; RUNNING=0 fielse if [ `ps -e | grep $BINARY_NAME | head -1 | awk &apos;&#123; print $1 &#125;&apos;` ] then STATUS=&quot;$BINARY_NAME (pid `pidof $APP`, but no pid file) running..&quot; else STATUS=&quot;$BINARY_NAME (no pid file) not running&quot; fi RUNNING=0fi# functionsstart() &#123; if [ $RUNNING -eq 1 ] then echo &quot;$0 $ARG: $BINARY_NAME (pid $PID) already running&quot; else action $&quot;Starting $BINARY_NAME: &quot; $FULLPATH touch /var/lock/subsys/$BINARY_NAME fi&#125;stop() &#123; echo -n $&quot;Shutting down $BINARY_NAME: &quot; killproc $BINARY_NAME RETVAL=$? echo [ $RETVAL -eq 0 ] &amp;&amp; rm -f /var/lock/subsys/$BINARY_NAME RUNNING=0&#125;# logiccase &quot;$1&quot; in start) start ;; stop) stop ;; status) status $BINARY_NAME ;; restart) stop sleep 10 start ;; help|*) echo $&quot;Usage: $0 &#123;start|stop|status|restart|help&#125;&quot; cat &lt;&lt;EOF start - start $BINARY_NAME stop - stop $BINARY_NAME status - show current status of $BINARY_NAME restart - restart $BINARY_NAME if running by sending a SIGHUP or start if not running help - this screenEOF exit 1 ;;esacexit 0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297shell &gt; cat /etc/ansible/roles/install_zabbix_agent/templates/zabbix_agentd.conf# This is a config file for the Zabbix agent daemon (Unix)# To get more information about Zabbix, visit http://www.zabbix.com############ GENERAL PARAMETERS #################### Option: PidFile# Name of PID file.## Mandatory: no# Default:# PidFile=/tmp/zabbix_agentd.pid### Option: LogFile# Name of log file.# If not set, syslog is used.## Mandatory: no# Default:# LogFile=LogFile=/tmp/zabbix_agentd.log### Option: LogFileSize# Maximum size of log file in MB.# 0 - disable automatic log rotation.## Mandatory: no# Range: 0-1024# Default:# LogFileSize=1### Option: DebugLevel# Specifies debug level# 0 - basic information about starting and stopping of Zabbix processes# 1 - critical information# 2 - error information# 3 - warnings# 4 - for debugging (produces lots of information)## Mandatory: no# Range: 0-4# Default:# DebugLevel=3### Option: SourceIP# Source IP address for outgoing connections.## Mandatory: no# Default:# SourceIP=### Option: EnableRemoteCommands# Whether remote commands from Zabbix server are allowed.# 0 - not allowed# 1 - allowed## Mandatory: no# Default:# EnableRemoteCommands=0### Option: LogRemoteCommands# Enable logging of executed shell commands as warnings.# 0 - disabled# 1 - enabled## Mandatory: no# Default:# LogRemoteCommands=0##### Passive checks related### Option: Server# List of comma delimited IP addresses (or hostnames) of Zabbix servers.# Incoming connections will be accepted only from the hosts listed here.# If IPv6 support is enabled then &apos;127.0.0.1&apos;, &apos;::127.0.0.1&apos;, &apos;::ffff:127.0.0.1&apos; are treated equally.## Mandatory: no# Default:# Server=Server=&#123;&#123; zabbix_server_ip &#125;&#125;### Option: ListenPort# Agent will listen on this port for connections from the server.## Mandatory: no# Range: 1024-32767# Default:# ListenPort=10050ListenPort=&#123;&#123; zabbix_port &#125;&#125;### Option: ListenIP# List of comma delimited IP addresses that the agent should listen on.# First IP address is sent to Zabbix server if connecting to it to retrieve list of active checks.## Mandatory: no# Default:# ListenIP=0.0.0.0### Option: StartAgents# Number of pre-forked instances of zabbix_agentd that process passive checks.# If set to 0, disables passive checks and the agent will not listen on any TCP port.## Mandatory: no# Range: 0-100# Default:# StartAgents=3##### Active checks related### Option: ServerActive# List of comma delimited IP:port (or hostname:port) pairs of Zabbix servers for active checks.# If port is not specified, default port is used.# IPv6 addresses must be enclosed in square brackets if port for that host is specified.# If port is not specified, square brackets for IPv6 addresses are optional.# If this parameter is not specified, active checks are disabled.# Example: ServerActive=127.0.0.1:20051,zabbix.domain,[::1]:30051,::1,[12fc::1]## Mandatory: no# Default:# ServerActive=#ServerActive=127.0.0.1:10051### Option: Hostname# Unique, case sensitive hostname.# Required for active checks and must match hostname as configured on the server.# Value is acquired from HostnameItem if undefined.## Mandatory: no# Default:# Hostname=Hostname=&#123;&#123; ansible_all_ipv4_addresses[1] &#125;&#125;### Option: HostnameItem# Item used for generating Hostname if it is undefined. Ignored if Hostname is defined.# Does not support UserParameters or aliases.## Mandatory: no# Default:# HostnameItem=system.hostname### Option: HostMetadata# Optional parameter that defines host metadata.# Host metadata is used at host auto-registration process.# An agent will issue an error and not start if the value is over limit of 255 characters.# If not defined, value will be acquired from HostMetadataItem.## Mandatory: no# Range: 0-255 characters# Default:# HostMetadata=### Option: HostMetadataItem# Optional parameter that defines an item used for getting host metadata.# Host metadata is used at host auto-registration process.# During an auto-registration request an agent will log a warning message if# the value returned by specified item is over limit of 255 characters.# This option is only used when HostMetadata is not defined.## Mandatory: no# Default:# HostMetadataItem=### Option: RefreshActiveChecks# How often list of active checks is refreshed, in seconds.## Mandatory: no# Range: 60-3600# Default:# RefreshActiveChecks=120### Option: BufferSend# Do not keep data longer than N seconds in buffer.## Mandatory: no# Range: 1-3600# Default:# BufferSend=5### Option: BufferSize# Maximum number of values in a memory buffer. The agent will send# all collected data to Zabbix Server or Proxy if the buffer is full.## Mandatory: no# Range: 2-65535# Default:# BufferSize=100### Option: MaxLinesPerSecond# Maximum number of new lines the agent will send per second to Zabbix Server# or Proxy processing &apos;log&apos; and &apos;logrt&apos; active checks.# The provided value will be overridden by the parameter &apos;maxlines&apos;,# provided in &apos;log&apos; or &apos;logrt&apos; item keys.## Mandatory: no# Range: 1-1000# Default:# MaxLinesPerSecond=100############ ADVANCED PARAMETERS #################### Option: Alias# Sets an alias for an item key. It can be used to substitute long and complex item key with a smaller and simpler one.# Multiple Alias parameters may be present. Multiple parameters with the same Alias key are not allowed.# Different Alias keys may reference the same item key.# For example, to retrieve the ID of user &apos;zabbix&apos;:# Alias=zabbix.userid:vfs.file.regexp[/etc/passwd,^zabbix:.:([0-9]+),,,,\1]# Now shorthand key zabbix.userid may be used to retrieve data.# Aliases can be used in HostMetadataItem but not in HostnameItem parameters.## Mandatory: no# Range:# Default:### Option: Timeout# Spend no more than Timeout seconds on processing## Mandatory: no# Range: 1-30# Default:Timeout=20### Option: AllowRoot# Allow the agent to run as &apos;root&apos;. If disabled and the agent is started by &apos;root&apos;, the agent# will try to switch to the user specified by the User configuration option instead.# Has no effect if started under a regular user.# 0 - do not allow# 1 - allow## Mandatory: no# Default:# AllowRoot=0### Option: User# Drop privileges to a specific, existing user on the system.# Only has effect if run as &apos;root&apos; and AllowRoot is disabled.## Mandatory: no# Default:# User=zabbix### Option: Include# You may include individual files or all files in a directory in the configuration file.# Installing Zabbix will create include directory in /usr/local/etc, unless modified during the compile time.## Mandatory: no# Default:# Include=# Include=/usr/local/etc/zabbix_agentd.userparams.conf# Include=/usr/local/etc/zabbix_agentd.conf.d/# Include=/usr/local/etc/zabbix_agentd.conf.d/*.conf####### USER-DEFINED MONITORED PARAMETERS ########## Option: UnsafeUserParameters# Allow all characters to be passed in arguments to user-defined parameters.# 0 - do not allow# 1 - allow## Mandatory: no# Range: 0-1# Default:UnsafeUserParameters=1### Option: UserParameter# User-defined parameter to monitor. There can be several user-defined parameters.# Format: UserParameter=&lt;key&gt;,&lt;shell command&gt;# See &apos;zabbix_agentd&apos; directory for examples.## Mandatory: no# Default:# UserParameter=####### LOADABLE MODULES ########## Option: LoadModulePath# Full path to location of agent modules.# Default depends on compilation options.## Mandatory: no# Default:# LoadModulePath=$&#123;libdir&#125;/modules### Option: LoadModule# Module to load at agent startup. Modules are used to extend functionality of the agent.# Format: LoadModule=&lt;module.so&gt;# The modules must be located in directory specified by LoadModulePath.# It is allowed to include multiple LoadModule parameters.## Mandatory: no# Default:# LoadModule= 安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950shell &gt; ansible-playbook /etc/ansible/install_zabbix_agent.ymlPLAY [mini] *******************************************************************GATHERING FACTS ***************************************************************ok: [129.139.153.78]ok: [155.139.190.94]TASK: [install_zabbix_agent | Install Software] *******************************changed: [155.139.190.94] =&gt; (item=libcurl-devel)changed: [129.139.153.78] =&gt; (item=libcurl-devel)TASK: [install_zabbix_agent | Create Zabbix User] *****************************changed: [129.139.153.78]changed: [155.139.190.94]TASK: [install_zabbix_agent | Copy Zabbix.tar.gz] *****************************changed: [129.139.153.78]changed: [155.139.190.94]TASK: [install_zabbix_agent | Uncompression Zabbix.tar.gz] ********************changed: [129.139.153.78]changed: [155.139.190.94]TASK: [install_zabbix_agent | Copy Zabbix Start Script] ***********************changed: [155.139.190.94]changed: [129.139.153.78]TASK: [install_zabbix_agent | Copy Zabbix Config File] ************************changed: [129.139.153.78]changed: [155.139.190.94]TASK: [install_zabbix_agent | Modify Zabbix Dir Permisson] ********************changed: [155.139.190.94]changed: [129.139.153.78]TASK: [install_zabbix_agent | Start Zabbix Service] ***************************changed: [129.139.153.78]changed: [155.139.190.94]TASK: [install_zabbix_agent | Add Boot Start Zabbix Service] ******************changed: [129.139.153.78]changed: [155.139.190.94]PLAY RECAP ********************************************************************155.139.190.94 : ok=10 changed=9 unreachable=0 failed=0129.139.153.78 : ok=10 changed=9 unreachable=0 failed=0## 关注一下，启动脚本跟配置文件中变量的引用。## 完成安装，可以去客户机检查效果了 ！]]></content>
      <categories>
        <category>运维自动化</category>
      </categories>
      <tags>
        <tag>ansible</tag>
        <tag>playbook</tag>
        <tag>自动化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7和Ubuntu16.04 破解Linux文件数、进程数限制]]></title>
    <url>%2FCentOS7%E5%92%8CUbuntu16.04%20%E7%A0%B4%E8%A7%A3Linux%E6%96%87%E4%BB%B6%E6%95%B0%E3%80%81%E8%BF%9B%E7%A8%8B%E6%95%B0%E9%99%90%E5%88%B6%2F</url>
    <content type="text"><![CDATA[修改limits.conf1234567vim /etc/security/limits.conf添加如下内容*(root) soft nofile 65536* hard nofile 65536* soft noproc 65536* hard noproc 65536 执行命令12ulimit -n 65536#sysctl -p #如不指定文件则默认/etc/sysctl.conf 查看是否生效123sysctl -a |grep &apos;&apos;或者ulimit -n ​]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>ubuntu</tag>
        <tag>进程数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 常用命令]]></title>
    <url>%2Fhexo-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Hexo 命令 关于Hexo常用命令的记录 hexo generate 生成静态网页 -d –deploy: 文件生成后立即部署网站 -w –watch: 监视文件变动 hexo publish 发布草稿 hexo server -p： 指定端口 -s：使用静态文件 -l：启动日志记录 hexo deploy -g：部署之前预先生成静态文件 hexo render 渲染单个文件 hexo migrate 插件支持 hexo clean 清除缓存文件（db.json）和已生成的静态文件(public) hexo list 列出网站资料 hexo version]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>haha</tag>
      </tags>
  </entry>
</search>
