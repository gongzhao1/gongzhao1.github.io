<!DOCTYPE html>




<html class="theme-next muse" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="kubernetes,">





  <link rel="alternate" href="/atom.xml" title="GZ" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="官方文档：https://kubernetes.io/ 中文文档: http://docs.kubernetes.org.cn/ GitHub: https://github.com/kubernetes/kubernetes  [TOC]">
<meta name="keywords" content="kubernetes">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes技巧及常用组件了解">
<meta property="og:url" content="https://gongzhao1.coding.me/Kubernetes技巧及常用组件了解/index.html">
<meta property="og:site_name" content="GZ">
<meta property="og:description" content="官方文档：https://kubernetes.io/ 中文文档: http://docs.kubernetes.org.cn/ GitHub: https://github.com/kubernetes/kubernetes  [TOC]">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes01.png">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes02.png">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes03.png">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes04.png">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes05.png">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes06.png">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes07.png">
<meta property="og:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes08.png">
<meta property="og:updated_time" content="2020-04-08T14:20:25.838Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kubernetes技巧及常用组件了解">
<meta name="twitter:description" content="官方文档：https://kubernetes.io/ 中文文档: http://docs.kubernetes.org.cn/ GitHub: https://github.com/kubernetes/kubernetes  [TOC]">
<meta name="twitter:image" content="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes01.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: false,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://gongzhao1.coding.me/Kubernetes技巧及常用组件了解/">





  <title>Kubernetes技巧及常用组件了解 | GZ</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">GZ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">痛苦是财富，这话是扯淡。少年，痛苦就是痛苦，对痛苦的思考才是财富</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://gongzhao1.coding.me/Kubernetes技巧及常用组件了解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="弓昭">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/panda.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Kubernetes技巧及常用组件了解</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-04T11:06:55+08:00">
                2019-08-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index">
                    <span itemprop="name">kubernetes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次阅读
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>官方文档：<a href="https://kubernetes.io/" target="_blank" rel="noopener">https://kubernetes.io/</a></p>
<p>中文文档: <a href="http://docs.kubernetes.org.cn/" target="_blank" rel="noopener">http://docs.kubernetes.org.cn/</a></p>
<p>GitHub: <a href="https://github.com/kubernetes/kubernetes" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes</a></p>
</blockquote>
<p>[TOC]</p>
<a id="more"></a>

<h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><h2 id="集群外部"><a href="#集群外部" class="headerlink" title="集群外部:"></a>集群外部:</h2><p>1.使用kubectl proxy</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl proxy --port=8080</span><br></pre></td></tr></table></figure>

<p>2.使用curl http方式访问</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8080/api/</span><br></pre></td></tr></table></figure>

<p>3.使用curl https方式访问,认证方式为token</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">APISERVER=$(kubectl config view | grep server | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)</span><br><span class="line"></span><br><span class="line">TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d &apos; &apos;) | grep -E &apos;^token&apos; | cut -f2 -d&apos;:&apos; | tr -d &apos;\t&apos;)</span><br><span class="line"></span><br><span class="line">curl  --header &quot;Authorization: Bearer $TOKEN&quot; --insecure $APISERVER/api/v1/nodes</span><br></pre></td></tr></table></figure>

<p>4.使用kubectl https方式访问,认证方式为证书,双向认证</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#cd /etcd/kubernetes/ssl</span><br><span class="line"></span><br><span class="line">#openssl genrsa -out pwm.key 2048</span><br><span class="line"></span><br><span class="line">#openssl req -new -key pwm.key -out pwm.csr -subj &quot;/CN=pwm&quot;</span><br><span class="line"></span><br><span class="line">#openssl x509 -req -days 365 -in pwm.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out pwm.crt</span><br><span class="line"></span><br><span class="line">#创建用户pwm,已经想要的role和rolebongding</span><br><span class="line"></span><br><span class="line"># kubectl --server=https://192.168.61.100:6443 \</span><br><span class="line"></span><br><span class="line">--certificate-authority=ca.pem  \</span><br><span class="line"></span><br><span class="line">--client-certificate=pwm.crt \</span><br><span class="line"></span><br><span class="line">--client-key=pwm.key \</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl get nodes</span><br></pre></td></tr></table></figure>

<h2 id="集群内部"><a href="#集群内部" class="headerlink" title="集群内部:"></a>集群内部:</h2><p>1.在使用了service account的pod内部</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -k -v -H &quot;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; https://kubernetes.default/api/v1/namespaces/</span><br><span class="line"></span><br><span class="line">curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.cem -v -H &quot;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; https://kubernetes.default/api/v1/namespaces/</span><br></pre></td></tr></table></figure>

<h1 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h1><p>ConfigMap是用来存储配置文件的kubernetes资源对象，所有的配置内容都存储在etcd中。</p>
<p>官方示例：<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/</a></p>
<h2 id="创建ConfigMap"><a href="#创建ConfigMap" class="headerlink" title="创建ConfigMap"></a>创建ConfigMap</h2><p>创建ConfigMap的方式有4种：</p>
<ul>
<li>通过直接在命令行中指定configmap参数创建，即<code>--from-literal</code></li>
<li>通过指定文件创建，即将一个配置文件创建为一个ConfigMap<code>--from-file=&lt;文件&gt;</code></li>
<li>通过指定目录创建，即将一个目录下的所有配置文件创建为一个ConfigMap，<code>--from-file=&lt;目录&gt;</code></li>
<li>事先写好标准的configmap的yaml文件，然后kubectl create -f 创建</li>
</ul>
<h3 id="通过命令行参数-from-literal创建"><a href="#通过命令行参数-from-literal创建" class="headerlink" title="通过命令行参数--from-literal创建"></a>通过命令行参数<code>--from-literal</code>创建</h3><p>创建命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap test-config1 --from-literal=db.host=10.5.10.116 --from-listeral=db.port=&apos;3306&apos;</span><br></pre></td></tr></table></figure>

<h3 id="指定文件创建"><a href="#指定文件创建" class="headerlink" title="指定文件创建"></a>指定文件创建</h3><p>配置文件app.properties的内容： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">port=3306</span><br><span class="line">socket=......</span><br></pre></td></tr></table></figure>

<p> 创建命令（可以有多个<code>--from-file</code>）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap test-config2 --from-file=./app.properties1</span><br></pre></td></tr></table></figure>

<p>假如不想configmap中的key为默认的文件名，还可以在创建时指定key名字：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap game-config-3 --from-file=&lt;my-key-name&gt;=&lt;path-to-file&gt;</span><br></pre></td></tr></table></figure>

<h3 id="指定目录创建"><a href="#指定目录创建" class="headerlink" title="指定目录创建"></a>指定目录创建</h3><p>configs 目录下的config-1和config-2内容如下所示： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">config-1:</span><br><span class="line">aaaaa</span><br><span class="line">bbbbb</span><br><span class="line">ccccc</span><br><span class="line"></span><br><span class="line">config-2:</span><br><span class="line">ddddd</span><br><span class="line">eeeee</span><br></pre></td></tr></table></figure>

<p>创建命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap test-config3 --from-file=./configs1</span><br></pre></td></tr></table></figure>

<p>可以看到指定目录创建时configmap内容中的各个文件会创建一个key/value对，key是文件名，value是文件内容。</p>
<p>那假如目录中还包含子目录呢？继续做实验：<br>在上一步的configs目录下创建子目录subconfigs，并在subconfigs下面创建两个配置文件，指定目录configs创建名为test-config4的configmap:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap test-config4 --from-file=./configs1</span><br></pre></td></tr></table></figure>

<p><strong>结果说明指定目录时只会识别其中的文件，忽略子目录</strong></p>
<h3 id="通过事先写好configmap的标准yaml文件创建"><a href="#通过事先写好configmap的标准yaml文件创建" class="headerlink" title="通过事先写好configmap的标准yaml文件创建"></a>通过事先写好configmap的标准yaml文件创建</h3><p>yaml文件如图所示： </p>
<p><strong>注意其中一个key的value有多行内容时的写法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: test-cfg</span><br><span class="line">  namespace: default</span><br><span class="line">data:</span><br><span class="line">  cache_host: memcached-gcxt</span><br><span class="line">  cache_port: &quot;11211&quot;</span><br><span class="line">  cache_prefix: gcxt</span><br><span class="line">  my.cnf: |</span><br><span class="line">    [mysqld]</span><br><span class="line">    log-bin = mysql-bin</span><br><span class="line">  app.properties: |</span><br><span class="line">    property.1 = value-1</span><br><span class="line">    property.2 = value-2</span><br><span class="line">    property.3 = value-3</span><br></pre></td></tr></table></figure>

<p>创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f test-cfg.yml</span><br></pre></td></tr></table></figure>

<h2 id="使用ConfigMap"><a href="#使用ConfigMap" class="headerlink" title="使用ConfigMap"></a>使用ConfigMap</h2><p>使用ConfigMap有三种方式:</p>
<ul>
<li>第一种是通过环境变量的方式，直接传递给pod</li>
<li>第二种是通过在pod的命令行下运行的方式(启动命令中)</li>
<li>第三种是作为volume的方式挂载到pod内</li>
</ul>
<h3 id="通过环境变量使用"><a href="#通过环境变量使用" class="headerlink" title="通过环境变量使用"></a>通过环境变量使用</h3><p>使用<code>valueFrom.configMapKeyRef</code> name、key指定要用的key:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: dapi-test-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-container</span><br><span class="line">      image: k8s.gcr.io/busybox</span><br><span class="line">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</span><br><span class="line">      env:</span><br><span class="line">        - name: SPECIAL_LEVEL_KEY</span><br><span class="line">          valueFrom:</span><br><span class="line">            configMapKeyRef:</span><br><span class="line">              name: special-config</span><br><span class="line">              key: special.how</span><br><span class="line">        - name: LOG_LEVEL</span><br><span class="line">          valueFrom:</span><br><span class="line">            configMapKeyRef:</span><br><span class="line">              name: env-config</span><br><span class="line">              key: log_level</span><br><span class="line">  restartPolicy: Never</span><br></pre></td></tr></table></figure>

<p>还可以通过<code>envFrom.configMapRef</code>、name使得configmap中的所有key/value对都自动变成环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: dapi-test-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-container</span><br><span class="line">      image: k8s.gcr.io/busybox</span><br><span class="line">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</span><br><span class="line">      envFrom:</span><br><span class="line">      - configMapRef:</span><br><span class="line">          name: special-config</span><br><span class="line">  restartPolicy: Never</span><br></pre></td></tr></table></figure>

<h3 id="在启动命令中引用"><a href="#在启动命令中引用" class="headerlink" title="在启动命令中引用"></a>在启动命令中引用</h3><p>在命令行下引用时，需要先设置为环境变量，之后可以通过$(VAR_NAME)设置容器启动命令的启动参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: dapi-test-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-container</span><br><span class="line">      image: k8s.gcr.io/busybox</span><br><span class="line">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ]</span><br><span class="line">      env:</span><br><span class="line">        - name: SPECIAL_LEVEL_KEY</span><br><span class="line">          valueFrom:</span><br><span class="line">            configMapKeyRef:</span><br><span class="line">              name: special-config</span><br><span class="line">              key: SPECIAL_LEVEL</span><br><span class="line">        - name: SPECIAL_TYPE_KEY</span><br><span class="line">          valueFrom:</span><br><span class="line">            configMapKeyRef:</span><br><span class="line">              name: special-config</span><br><span class="line">              key: SPECIAL_TYPE</span><br><span class="line">  restartPolicy: Never</span><br></pre></td></tr></table></figure>

<h3 id="作为volume挂载使用"><a href="#作为volume挂载使用" class="headerlink" title="作为volume挂载使用"></a>作为volume挂载使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configmap</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-configmap</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx-configmap</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:     </span><br><span class="line">        - name: config-volume4</span><br><span class="line">          mountPath: /tmp/config4   </span><br><span class="line">      volumes:</span><br><span class="line">      - name: config-volume4</span><br><span class="line">        configMap:</span><br><span class="line">          name: test-config</span><br></pre></td></tr></table></figure>

<p>这样会在在<code>/tmp/config4</code>文件夹下以每一个key为文件名value为值创建了多个文件</p>
<blockquote>
<p>注意：如果原有<code>/tmp/config4</code>目录下有文件或者文件夹，将会全部覆盖掉，只留有<code>test-config</code>中的key文件</p>
</blockquote>
<p><strong>假如不想以key名作为配置文件名可以引入items 字段，在其中逐个指定要用相对路径path替换的key：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line"> - name: config-volume4</span><br><span class="line">   configMap:</span><br><span class="line">     name: test-config4</span><br><span class="line">     items:</span><br><span class="line">     - key: my.cnf</span><br><span class="line">       path: path/to/mysql-key #最终生成/tmp/config4/path/to/mysql-key文件</span><br><span class="line">     - key: cache_host</span><br><span class="line">       path: cache-host</span><br></pre></td></tr></table></figure>

<p><strong>备注：</strong></p>
<ol>
<li>删除configmap后原pod不受影响；然后再删除pod后，重启的pod的events会报找不到cofigmap的volume；</li>
<li>pod起来后再通过kubectl edit configmap …修改configmap，过一会pod内部的配置也会刷新。</li>
<li>在容器内部修改挂进去的配置文件后，过一会内容会再次被刷新为原始configmap内容</li>
</ol>
<h2 id="深度解析mountPath-subPath-key-path的关系和作用"><a href="#深度解析mountPath-subPath-key-path的关系和作用" class="headerlink" title="深度解析mountPath,subPath,key,path的关系和作用"></a>深度解析mountPath,subPath,key,path的关系和作用</h2><p>结论：</p>
<p>kubernetes key (pod.spec.volums[0].configMap.items[0].key)用于指定configMap中的哪些条目可用于挂载</p>
<p>kubernetes path (pod.spec.volums[0].configMap.items[0].path)用于将key重命名</p>
<p>kubernetes suPath (pod.spec.containers[0].volumeMounts.subPath)决定容器中有无挂载（按名字从key，有path时以path为主，中比对是否存在要的条目）</p>
<p>kubernetes mountPath (pod.spec.containers[0].volumeMounts.mountPath)决定容器中挂载的结果文件名</p>
<blockquote>
<p>无subPath时：</p>
<p>mountPath为文件夹，其下挂载ConfigMap中的文件，key为文件名，value为文件内容，并且会覆盖掉原有mountPath中的内容。如果此时指定了<code>vloumes.configMap.items[0].path</code>，则path条目用于将key重命名</p>
<p>有subPath时：</p>
<p>subPath匹配为true时，mountPath为文件名</p>
<p>subPath匹配为false时，mountPath为文件夹名</p>
</blockquote>
<h3 id="mountPath结合subPath作用"><a href="#mountPath结合subPath作用" class="headerlink" title="mountPath结合subPath作用"></a>mountPath结合subPath作用</h3><p>有subPath时且subPath推荐筛选结果为true，mountPath指定到文件名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# cat pod-configmap-testvolume.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    purpose: test-configmap-volume</span><br><span class="line">  name: testvolume</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-configmap-volume</span><br><span class="line">      image: tomcat:8</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ]</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: /etc/config/app.properties #此处配合suPath使用时，app.properties为文件名，即pod容器中只生成了/etc/config目录，目录之下为文件，只有一个名为app.properties的文件（subPath筛选只挂载app.properties文件）</span><br><span class="line">          subPath: app.properties</span><br><span class="line">  volumes:</span><br><span class="line">    - name: config-volume</span><br><span class="line">      configMap:</span><br><span class="line">         name: test-cfg</span><br><span class="line">         items:</span><br><span class="line">           - key: cache_host</span><br><span class="line">             path: path/to/special-key-cache</span><br><span class="line">           - key: app.properties</span><br><span class="line">             path: app.properties</span><br></pre></td></tr></table></figure>

<p>进入容器查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bash</span><br><span class="line">root@testvolume:/usr/local/tomcat# cd /etc/config</span><br><span class="line">root@testvolume:/etc/config# ls -l</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root 63 Apr 12 01:59 app.properties</span><br><span class="line">root@testvolume:/etc/config# cat app.properties </span><br><span class="line">property.1 = value-1</span><br><span class="line">property.2 = value-2</span><br><span class="line">property.3 = value-3</span><br></pre></td></tr></table></figure>

<h3 id="有subPath但筛选结果为false"><a href="#有subPath但筛选结果为false" class="headerlink" title="有subPath但筛选结果为false,"></a>有subPath但筛选结果为false,</h3><p>容器中生成一个空目录/etc/config/app.properties，无文件</p>
<p>subPath筛选范围优先级为pod.spec.volums[0].configMap.items[0].path&gt;pod.spec.volums[0].configMap.items[0].key&gt;configMap.key，本例中为path,即在path指定的条目【“cache_host”,”app-properties “注意中间是横杠不是点】找是否有subPath项“app.properties”注意中间为点，查找结果为false,所以无文件挂载。容器将“/etc/config/app.properties”当成一个待创建的路径。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# vi pod-configmap-testvolume.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    purpose: test-configmap-volume</span><br><span class="line">  name: testvolume</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-configmap-volume</span><br><span class="line">      image: tomcat:8</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ]</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: /etc/config/app.properties</span><br><span class="line">          subPath: app.properties</span><br><span class="line">  volumes:</span><br><span class="line">    - name: config-volume</span><br><span class="line">      configMap:</span><br><span class="line">         name: test-cfg</span><br><span class="line">         items:</span><br><span class="line">           - key: cache_host</span><br><span class="line">             path: path/to/special-key-cache</span><br><span class="line">           - key: app.properties</span><br><span class="line">             path: app-properties #此处path相当于更改文件名mv app.properties app-properties</span><br></pre></td></tr></table></figure>

<p>进入容器查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bash</span><br><span class="line">root@testvolume:/usr/local/tomcat# cd /etc/config</span><br><span class="line">root@testvolume:/etc/config# ls -l</span><br><span class="line">total 0</span><br><span class="line">drwxrwxrwx 2 root root 6 Apr 12 02:11 app.properties</span><br><span class="line">root@testvolume:/etc/config# cat app.properties</span><br><span class="line">cat: app.properties: Is a directory</span><br><span class="line">root@testvolume:/etc/config# cd app.properties</span><br><span class="line">root@testvolume:/etc/config/app.properties# ls #此目录下为空</span><br></pre></td></tr></table></figure>

<h3 id="无-subPath-path相当于重命名"><a href="#无-subPath-path相当于重命名" class="headerlink" title="无 subPath,path相当于重命名"></a>无 subPath,path相当于重命名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# cat pod-configmap-testvolume.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    purpose: test-configmap-volume</span><br><span class="line">  name: testvolume</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-configmap-volume</span><br><span class="line">      image: tomcat:8</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ]</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: /etc/config/app.properties ##此处app.properties为目录</span><br><span class="line">          #subPath: app.properties</span><br><span class="line">  volumes:</span><br><span class="line">    - name: config-volume</span><br><span class="line">      configMap:</span><br><span class="line">         name: test-cfg</span><br><span class="line">         items:</span><br><span class="line">           - key: cache_host</span><br><span class="line">             path: path/to/special-key-cache</span><br><span class="line">           - key: app.properties</span><br><span class="line">             path: app-properties #此处path相当于重命名</span><br></pre></td></tr></table></figure>

<p>进入容器内查看</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bash</span><br><span class="line">root@testvolume:/usr/local/tomcat# cd /etc/config</span><br><span class="line">root@testvolume:/etc/config# ls -l</span><br><span class="line">total 0</span><br><span class="line">drwxrwxrwx 3 root root 93 Apr 12 02:20 app.properties</span><br><span class="line">root@testvolume:/etc/config# cd app.properties</span><br><span class="line">root@testvolume:/etc/config/app.properties# ls -l</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 21 Apr 12 02:20 app-properties -&gt; ..data/app-properties</span><br><span class="line">lrwxrwxrwx 1 root root 11 Apr 12 02:20 path -&gt; ..data/path</span><br><span class="line">root@testvolume:/etc/config/app.properties# cat app-properties </span><br><span class="line">property.1 = value-1</span><br><span class="line">property.2 = value-2</span><br><span class="line">property.3 = value-3</span><br><span class="line">root@testvolume:/etc/config/app.properties# cat path</span><br><span class="line">cat: path: Is a directory</span><br><span class="line">root@testvolume:/etc/config/app.properties# cd path</span><br><span class="line">root@testvolume:/etc/config/app.properties/path# cd to</span><br><span class="line">root@testvolume:/etc/config/app.properties/path/to# ls -l</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root 9 Apr 12 02:20 special-key-cache</span><br><span class="line">root@testvolume:/etc/config/app.properties/path/to# cat special-key-cache </span><br><span class="line">mysql-k8s</span><br></pre></td></tr></table></figure>

<h3 id="有subPath且筛选结果为true-mouthPath指定文件名，可以和subPath不一样"><a href="#有subPath且筛选结果为true-mouthPath指定文件名，可以和subPath不一样" class="headerlink" title="有subPath且筛选结果为true,mouthPath指定文件名，可以和subPath不一样"></a>有subPath且筛选结果为true,mouthPath指定文件名，可以和subPath不一样</h3><p><strong>subPath决定有无，mountPath决定文件名</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# vi pod-configmap-testvolume.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    purpose: test-configmap-volume</span><br><span class="line">  name: testvolume</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-configmap-volume</span><br><span class="line">      image: tomcat:8</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      #command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(MY_CACHE_HOST)&quot; ]</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: /etc/config/z.txt #subPath决定有无，mountPath决定文件名为z.txt</span><br><span class="line">          subPath: app-properties</span><br><span class="line">  volumes:</span><br><span class="line">    - name: config-volume</span><br><span class="line">      configMap:</span><br><span class="line">         name: test-cfg</span><br><span class="line">         items:</span><br><span class="line">           - key: cache_host</span><br><span class="line">             path: path/to/special-key-cache</span><br><span class="line">           - key: app.properties</span><br><span class="line">             path: app-properties</span><br></pre></td></tr></table></figure>

<p>进入容器查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-objs]# kubectl exec -it testvolume /bin/bash</span><br><span class="line">root@testvolume:/usr/local/tomcat# cd /etc/config</span><br><span class="line">root@testvolume:/etc/config# ls</span><br><span class="line">z.txt</span><br><span class="line">root@testvolume:/etc/config# pwd</span><br><span class="line">/etc/config</span><br><span class="line">root@testvolume:/etc/config# cat z.txt </span><br><span class="line">property.1 = value-1</span><br><span class="line">property.2 = value-2</span><br><span class="line">property.3 = value-3</span><br></pre></td></tr></table></figure>

<h3 id="configmap的热更新研究"><a href="#configmap的热更新研究" class="headerlink" title="configmap的热更新研究"></a>configmap的热更新研究</h3><p>更新 ConfigMap 后：</p>
<ul>
<li>使用该 ConfigMap 挂载的 Env 不会同步更新</li>
<li>使用该 ConfigMap 挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新</li>
</ul>
<p>ENV 是在容器启动的时候注入的，启动之后 kubernetes 就不会再改变环境变量的值，且同一个 namespace 中的 pod 的环境变量是不断累加的，参考 Kubernetes中的服务发现与docker容器间的环境变量传递源码探究。为了更新容器中使用 ConfigMap 挂载的配置，可以通过滚动更新 pod 的方式来强制重新挂载 ConfigMap，也可以在更新了 ConfigMap 后，先将副本数设置为 0，然后再扩容。</p>
<h1 id="ServiceAccount"><a href="#ServiceAccount" class="headerlink" title="ServiceAccount"></a>ServiceAccount</h1><p>Service account是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的。它与User account不同</p>
<ul>
<li>User account是为人设计的，而service account则是为Pod中的进程调用Kubernetes API而设计；</li>
<li>User account是跨namespace的，而service account则是仅局限它所在的namespace；</li>
<li>每个namespace都会自动创建一个default service account</li>
<li>Token controller检测service account的创建，并为它们创建<a href="https://www.kubernetes.org.cn/secret" target="_blank" rel="noopener">secret</a></li>
<li>开启ServiceAccount Admission Controller后<ul>
<li>每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout）</li>
<li>验证Pod引用的service account已经存在，否则拒绝创建</li>
<li>如果Pod没有指定ImagePullSecrets，则把service account的ImagePullSecrets加到Pod中</li>
<li>每个container启动后都会挂载该service account的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">ca.crt</span><br><span class="line">namespace</span><br><span class="line">token</span><br></pre></td></tr></table></figure>

<h2 id="创建Service-Account"><a href="#创建Service-Account" class="headerlink" title="创建Service Account"></a>创建Service Account</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create serviceaccount jenkins</span><br><span class="line">serviceaccount &quot;jenkins&quot; created</span><br><span class="line">$ kubectl get serviceaccounts jenkins -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2017-05-27T14:32:25Z</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;45559&quot;</span><br><span class="line">  selfLink: /api/v1/namespaces/default/serviceaccounts/jenkins</span><br><span class="line">  uid: 4d66eb4c-42e9-11e7-9860-ee7d8982865f</span><br><span class="line">secrets:</span><br><span class="line">- name: jenkins-token-l9v7v</span><br></pre></td></tr></table></figure>

<p>自动创建的secret：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kubectl get secret jenkins-token-l9v7v -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  ca.crt: (APISERVER CA BASE64 ENCODED)</span><br><span class="line">  namespace: ZGVmYXVsdA==</span><br><span class="line">  token: (BEARER TOKEN BASE64 ENCODED)</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/service-account.name: jenkins</span><br><span class="line">    kubernetes.io/service-account.uid: 4d66eb4c-42e9-11e7-9860-ee7d8982865f</span><br><span class="line">  creationTimestamp: 2017-05-27T14:32:25Z</span><br><span class="line">  name: jenkins-token-l9v7v</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;45558&quot;</span><br><span class="line">  selfLink: /api/v1/namespaces/default/secrets/jenkins-token-l9v7v</span><br><span class="line">  uid: 4d697992-42e9-11e7-9860-ee7d8982865f</span><br><span class="line">type: kubernetes.io/service-account-token</span><br></pre></td></tr></table></figure>

<p><strong>添加ImagePullSecrets</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2015-08-07T22:02:39Z</span><br><span class="line">  name: default</span><br><span class="line">  namespace: default</span><br><span class="line">  selfLink: /api/v1/namespaces/default/serviceaccounts/default</span><br><span class="line">  uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6</span><br><span class="line">secrets:</span><br><span class="line">- name: default-token-uudge</span><br><span class="line">imagePullSecrets:</span><br><span class="line">- name: myregistrykey</span><br></pre></td></tr></table></figure>

<h2 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h2><p>Service Account为服务提供了一种方便的认证机制，但它不关心授权的问题。可以配合<a href="https://kubernetes.io/docs/admin/authorization/#a-quick-note-on-service-accounts" target="_blank" rel="noopener">RBAC</a>来为Service Account鉴权：</p>
<ul>
<li>配置–authorization-mode=RBAC和–runtime-config=rbac.authorization.k8s.io/v1alpha1</li>
<li>配置–authorization-rbac-super-user=admin</li>
<li>定义Role、ClusterRole、RoleBinding或ClusterRoleBinding</li>
</ul>
<p>比如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># This role allows to read pods in the namespace &quot;default&quot;</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1alpha1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: pod-reader</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;] # The API group &quot;&quot; indicates the core API Group.</span><br><span class="line">    resources: [&quot;pods&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line">    nonResourceURLs: []</span><br><span class="line">---</span><br><span class="line"># This role binding allows &quot;default&quot; to read pods in the namespace &quot;default&quot;</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1alpha1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-pods</span><br><span class="line">  namespace: default</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount # May be &quot;User&quot;, &quot;Group&quot; or &quot;ServiceAccount&quot;</span><br><span class="line">    name: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: pod-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>

<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#定义namespace：test</span><br><span class="line">cat &gt;&gt; test.yaml &lt;&lt; EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">    name: test</span><br><span class="line">     labels:</span><br><span class="line">         name: test</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#创建namespace：test</span><br><span class="line">kubectl create -f ./test.yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#查看命名空间test的sa</span><br><span class="line">kubectl get sa -n test</span><br><span class="line">NAME      SECRETS   AGE</span><br><span class="line">default   1         3h</span><br><span class="line">##说明：</span><br><span class="line">（1）如果kubernetes开启了ServiceAccount（–admission_control=…,ServiceAccount,… ）那么会在每个namespace下面都会创建一个默认的default的sa。如上命令查看的default ！</span><br><span class="line">（2）ServiceAccount默认是开启的。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#查看命名空间test生成的default</span><br><span class="line">kubectl get sa default -o yaml -n test</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">    creationTimestamp: 2018-05-31T06:21:10Z</span><br><span class="line">    name: default</span><br><span class="line">    namespace: test</span><br><span class="line">    resourceVersion: &quot;45560&quot;</span><br><span class="line">    selfLink: /api/v1/namespaces/test/serviceaccounts/default</span><br><span class="line">    uid: cf57c735-649a-11e8-adc5-000c290a7d06</span><br><span class="line">secrets:</span><br><span class="line">- name: default-token-ccf9m</span><br><span class="line">##说明：</span><br><span class="line">（1）当用户再该namespace下创建pod的时候都会默认使用这个sa；</span><br><span class="line">（2）每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout）；</span><br><span class="line">（3）每个container启动后都会挂载对应的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#创建deploy</span><br><span class="line">cat &gt;&gt; nginx_deploy.yaml &lt;&lt; EOF</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">    name: nginx-test</span><br><span class="line">    namespace: test</span><br><span class="line">spec:</span><br><span class="line">    replicas: 2</span><br><span class="line">    template:</span><br><span class="line">        metadata:</span><br><span class="line">            labels:</span><br><span class="line">                app: nginx</span><br><span class="line">        spec:</span><br><span class="line">            containers:</span><br><span class="line">            - name: nginx</span><br><span class="line">                image: nginx:1.7.9</span><br><span class="line">                ports:</span><br><span class="line">                - containerPort: 80</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#查看生成的Pods</span><br><span class="line">kubectl get po -n test</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-test-75675f5897-7l5bc   1/1       Running   0          1h</span><br><span class="line">nginx-test-75675f5897-b7pcn   1/1       Running   0          1h</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#查看其中一个Pod的详细信息，如：nginx-test-75675f5897-7l5bc</span><br><span class="line">kubectl describe po nginx-test-75675f5897-7l5bc -n test</span><br><span class="line">##其中default-token-ccf9m，请留意！</span><br><span class="line">Environment:    &lt;none&gt;</span><br><span class="line">Mounts:</span><br><span class="line">  /var/run/secrets/kubernetes.io/serviceaccount from default-token-ccf9m (ro)</span><br><span class="line">Conditions:</span><br><span class="line">Type           Status</span><br><span class="line">Initialized    True</span><br><span class="line">Ready          True</span><br><span class="line">PodScheduled   True</span><br><span class="line">Volumes:</span><br><span class="line">default-token-ccf9m:</span><br><span class="line">Type:        Secret (a volume populated by a Secret)</span><br><span class="line">SecretName:  default-token-ccf9m</span><br><span class="line">##说明：</span><br><span class="line">（1）每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout）；</span><br><span class="line">（2）每个container启动后都会挂载对应的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#进入其中一个Pod的容器内，如：nginx-test-75675f5897-7l5bc</span><br><span class="line">kubectl exec -it nginx-test-75675f5897-7l5bc  /bin/bash --namespace=test</span><br><span class="line">##在容器内执行：</span><br><span class="line">ls -l  /var/run/secrets/kubernetes.io/serviceaccount/</span><br><span class="line">lrwxrwxrwx 1 root root 13 May 31 08:15 ca.crt -&gt; ..data/ca.crt</span><br><span class="line">lrwxrwxrwx 1 root root 16 May 31 08:15 namespace -&gt; ..data/namespace</span><br><span class="line">lrwxrwxrwx 1 root root 12 May 31 08:15 token -&gt; ..data/token</span><br><span class="line">##说明：</span><br><span class="line">可以看到已将ca.crt 、namespace和token放到容器内了，那么这个容器就</span><br><span class="line">可以通过https的请求访问apiserver了。</span><br></pre></td></tr></table></figure>

<p>手工创建ServiceAccount</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#编辑heapster_test.yaml文件</span><br><span class="line">cat &gt;&gt; heapster_test.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">    name: heapster</span><br><span class="line">    namespace: test</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#创建Service Account：heapster</span><br><span class="line">kubectl create -f heapster_test.yaml</span><br><span class="line">serviceaccount &quot;heapster&quot; created</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#查看Service Account：heapster</span><br><span class="line">kubectl get sa -o yaml -n test</span><br><span class="line">##主要内容如下：</span><br><span class="line">    secrets:</span><br><span class="line">    - name: heapster-token-7xrlg</span><br></pre></td></tr></table></figure>

<h1 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>　运行在Pod中的应用是向客户端提供服务的守护进程，比如，nginx、tomcat、etcd等等，它们都是受控于控制器的资源对象，存在生命周期，我们知道Pod资源对象在自愿或非自愿终端后，只能被重构的Pod对象所替代，属于不可再生类组件。而在动态和弹性的管理模式下，Service为该类Pod对象提供了一个固定、统一的访问接口和负载均衡能力。</p>
<p>　　其实，就是说Pod存在生命周期，有销毁，有重建，无法提供一个固定的访问接口给客户端。并且为了同类的Pod都能够实现工作负载的价值，由此Service资源出现了，可以为一类Pod资源对象提供一个固定的访问接口和负载均衡，类似于阿里云的负载均衡或者是LVS的功能。</p>
<p>　　但是要知道的是，Service和Pod对象的IP地址，一个是虚拟地址，一个是Pod IP地址，都仅仅在集群内部可以进行访问，无法接入集群外部流量。而为了解决该类问题的办法可以是在单一的节点上做端口暴露（hostPort）以及让Pod资源共享工作节点的网络名称空间（hostNetwork）以外，还可以使用NodePort或者是LoadBalancer类型的Service资源，或者是有7层负载均衡能力的Ingress资源。</p>
<p>　　Service是Kubernetes的核心资源类型之一，Service资源基于标签选择器将一组Pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求到组内的Pod对象，如下图所示，它向客户端隐藏了真是的，处理用户请求的Pod资源，使得从客户端上看，就像是由Service直接处理并响应一样，是不是很像负载均衡器呢！</p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes01.png" alt="image"></p>
<p>　　Service对象的IP地址也称为Cluster IP，它位于为Kubernetes集群配置指定专用的IP地址范围之内，是一种虚拟的IP地址，它在Service对象创建之后保持不变，并且能够被同一集群中的Pod资源所访问。Service端口用于接受客户端请求，并将请求转发至后端的Pod应用的相应端口，这样的代理机制，也称为端口代理，它是基于TCP/IP 协议栈的传输层。</p>
<h3 id="Service的实现模型"><a href="#Service的实现模型" class="headerlink" title="Service的实现模型"></a>Service的实现模型</h3><p>　　在 Kubernetes 集群中，每个 Node 运行一个 <code>kube-proxy</code> 进程。<code>kube-proxy</code> 负责为 <code>Service</code> 实现了一种 VIP（虚拟 IP）的形式，而不是 <code>ExternalName</code> 的形式。 在 Kubernetes v1.0 版本，代理完全在 userspace。在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。 从 Kubernetes v1.2 起，默认就是 iptables 代理。在Kubernetes v1.8.0-beta.0中，添加了ipvs代理。在 Kubernetes v1.0 版本，<code>Service</code> 是 “4层”（TCP/UDP over IP）概念。 在 Kubernetes v1.1 版本，新增了 <code>Ingress</code> API（beta 版），用来表示 “7层”（HTTP）服务。</p>
<p>kube-proxy 这个组件始终监视着apiserver中有关service的变动信息，获取任何一个与service资源相关的变动状态，通过watch监视，一旦有service资源相关的变动和创建，kube-proxy都要转换为当前节点上的能够实现资源调度规则（例如：iptables、ipvs）</p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes02.png" alt="image"></p>
<h4 id="userspace代理模式"><a href="#userspace代理模式" class="headerlink" title="userspace代理模式"></a>userspace代理模式</h4><p>这种模式，当客户端Pod请求内核空间的service iptables后，把请求转到给用户空间监听的kube-proxy 的端口，由kube-proxy来处理后，再由kube-proxy将请求转给内核空间的 service ip，再由service iptalbes根据请求转给各节点中的的service pod。</p>
<p>　　由此可见这个模式有很大的问题，由客户端请求先进入内核空间的，又进去用户空间访问kube-proxy，由kube-proxy封装完成后再进去内核空间的iptables，再根据iptables的规则分发给各节点的用户空间的pod。这样流量从用户空间进出内核带来的性能损耗是不可接受的。在Kubernetes 1.1版本之前，userspace是默认的代理模型。</p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes03.png" alt="image"></p>
<h4 id="iptables代理模式"><a href="#iptables代理模式" class="headerlink" title="iptables代理模式"></a>iptables代理模式</h4><p>　　客户端IP请求时，直接请求本地内核service ip，根据iptables的规则直接将请求转发到到各pod上，因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。iptables代理模式由Kubernetes 1.1版本引入，自1.2版本开始成为默认类型。</p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes04.png" alt="image"></p>
<h4 id="ipvs代理模式"><a href="#ipvs代理模式" class="headerlink" title="ipvs代理模式"></a>ipvs代理模式</h4><p>Kubernetes自1.9-alpha版本引入了ipvs代理模式，自1.11版本开始成为默认设置。客户端IP请求时到达内核空间时，根据ipvs的规则直接分发到各pod上。kube-proxy会监视Kubernetes <code>Service</code>对象和<code>Endpoints</code>，调用<code>netlink</code>接口以相应地创建ipvs规则并定期与Kubernetes <code>Service</code>对象和<code>Endpoints</code>对象同步ipvs规则，以确保ipvs状态与期望一致。访问服务时，流量将被重定向到其中一个后端Pod。</p>
<p>与iptables类似，ipvs基于netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如：</p>
<ul>
<li><code>rr</code>：轮询调度</li>
<li><code>lc</code>：最小连接数</li>
<li><code>dh</code>：目标哈希</li>
<li><code>sh</code>：源哈希</li>
<li><code>sed</code>：最短期望延迟</li>
<li><code>nq</code>：不排队调度</li>
</ul>
<p><strong>注意： ipvs模式假定在运行kube-proxy之前在节点上都已经安装了IPVS内核模块。当kube-proxy以ipvs代理模式启动时，kube-proxy将验证节点上是否安装了IPVS模块，如果未安装，则kube-proxy将回退到iptables代理模式。</strong></p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes05.png" alt="image"></p>
<p> 如果某个服务后端pod发生变化，标签选择器适应的pod有多一个，适应的信息会立即反映到apiserver上,而kube-proxy一定可以watch到etc中的信息变化，而将它立即转为ipvs或者iptables中的规则，这一切都是动态和实时的，删除一个pod也是同样的原理。如图：</p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes06.png" alt="image"></p>
<h2 id="Service的定义"><a href="#Service的定义" class="headerlink" title="Service的定义"></a>Service的定义</h2><h3 id="Service字段含义"><a href="#Service字段含义" class="headerlink" title="Service字段含义"></a>Service字段含义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl explain svc</span><br><span class="line">KIND:     Service</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">     Service is a named abstraction of software service (for example, mysql)</span><br><span class="line">     consisting of local port (for example 3306) that the proxy listens on, and</span><br><span class="line">     the selector that determines which pods will answer requests sent through</span><br><span class="line">     the proxy.</span><br><span class="line"></span><br><span class="line">FIELDS:</span><br><span class="line">   apiVersion    &lt;string&gt;</span><br><span class="line">     APIVersion defines the versioned schema of this representation of an</span><br><span class="line">     object. Servers should convert recognized schemas to the latest internal</span><br><span class="line">     value, and may reject unrecognized values. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/api-conventions.md#resources</span><br><span class="line"></span><br><span class="line">   kind    &lt;string&gt;</span><br><span class="line">     Kind is a string value representing the REST resource this object</span><br><span class="line">     represents. Servers may infer this from the endpoint the client submits</span><br><span class="line">     requests to. Cannot be updated. In CamelCase. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds</span><br><span class="line"></span><br><span class="line">   metadata    &lt;Object&gt;</span><br><span class="line">     Standard object&apos;s metadata. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata</span><br><span class="line"></span><br><span class="line">   spec    &lt;Object&gt;</span><br><span class="line">     Spec defines the behavior of a service.</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status</span><br><span class="line"></span><br><span class="line">   status    &lt;Object&gt;</span><br><span class="line">     Most recently observed status of the service. Populated by the system.</span><br><span class="line">     Read-only. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status</span><br></pre></td></tr></table></figure>

<p>其中重要的4个字段：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion:</span><br><span class="line">kind:</span><br><span class="line">metadata:</span><br><span class="line">spec:</span><br><span class="line">　　clusterIP: 可以自定义，也可以动态分配</span><br><span class="line">　　ports:（与后端容器端口关联）</span><br><span class="line">　　selector:（关联到哪些pod资源上）</span><br><span class="line">　　type：服务类型</span><br></pre></td></tr></table></figure>

<h3 id="service的类型"><a href="#service的类型" class="headerlink" title="service的类型"></a>service的类型</h3><p>对一些应用（如 Frontend）的某些部分，可能希望通过外部（Kubernetes 集群外部）IP 地址暴露 Service。</p>
<p>Kubernetes <code>ServiceTypes</code> 允许指定一个需要的类型的 Service，默认是 <code>ClusterIP</code> 类型。</p>
<p><code>Type</code> 的取值以及行为如下：</p>
<ul>
<li><strong>ClusterIP：</strong>通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 <code>ServiceType</code>。</li>
<li><strong>NodePort：</strong>通过每个 Node 上的 IP 和静态端口（<code>NodePort</code>）暴露服务。<code>NodePort</code> 服务会路由到 <code>ClusterIP</code> 服务，这个 <code>ClusterIP</code> 服务会自动创建。通过请求 <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>，可以从集群的外部访问一个 <code>NodePort</code> 服务。</li>
<li><strong>LoadBalancer：</strong>使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 <code>NodePort</code> 服务和 <code>ClusterIP</code> 服务。</li>
<li><strong>ExternalName：</strong>通过返回 <code>CNAME</code> 和它的值，可以将服务映射到 <code>externalName</code> 字段的内容（例如， <code>foo.bar.example.com</code>）。 没有任何类型代理被创建，这只有 Kubernetes 1.7 或更高版本的 <code>kube-dns</code> 才支持。</li>
</ul>
<h4 id="ClusterIP的service类型演示"><a href="#ClusterIP的service类型演示" class="headerlink" title="ClusterIP的service类型演示"></a>ClusterIP的service类型演示</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master mainfests]# cat redis-svc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: redis</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:　　#标签选择器，必须指定pod资源本身的标签</span><br><span class="line">    app: redis</span><br><span class="line">    role: logstor</span><br><span class="line">  type: ClusterIP　　#指定服务类型为ClusterIP</span><br><span class="line">  ports: 　　#指定端口</span><br><span class="line">  - port: 6379　　#暴露给服务的端口</span><br><span class="line">  - targetPort: 6379　　#容器的端口</span><br><span class="line">[root@k8s-master mainfests]# kubectl apply -f redis-svc.yaml </span><br><span class="line">service/redis created</span><br><span class="line">[root@k8s-master mainfests]# kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    36d</span><br><span class="line">redis        ClusterIP   10.107.238.182   &lt;none&gt;        6379/TCP   1m</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# kubectl describe svc redis</span><br><span class="line">Name:              redis</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;redis&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;ports&quot;:[&#123;&quot;port&quot;:6379,&quot;targetPort&quot;:6379&#125;...</span><br><span class="line">Selector:          app=redis,role=logstor</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.107.238.182　　#service ip</span><br><span class="line">Port:              &lt;unset&gt;  6379/TCP</span><br><span class="line">TargetPort:        6379/TCP</span><br><span class="line">Endpoints:         10.244.1.16:6379　　#此处的ip+端口就是pod的ip+端口</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# kubectl get pod redis-5b5d6fbbbd-v82pw -o wide</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">redis-5b5d6fbbbd-v82pw   1/1       Running   0          20d       10.244.1.16   k8s-node01</span><br></pre></td></tr></table></figure>

<p>从上演示可以总结出：service不会直接到pod，service是直接到endpoint资源，就是地址加端口，再由endpoint再关联到pod。</p>
<p>service只要创建完，就会在dns中添加一个资源记录进行解析，添加完成即可进行解析。资源记录的格式为：SVC_NAME.NS_NAME.DOMAIN.LTD.<br>默认的集群service 的A记录：svc.cluster.local.<br>redis服务创建的A记录：redis.default.svc.cluster.local.</p>
<h4 id="NodePort的service类型演示"><a href="#NodePort的service类型演示" class="headerlink" title="NodePort的service类型演示"></a>NodePort的service类型演示</h4><p>　　NodePort即节点Port，通常在部署Kubernetes集群系统时会预留一个端口范围用于NodePort，其范围默认为：30000~32767之间的端口。定义NodePort类型的Service资源时，需要使用.spec.type进行明确指定。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master mainfests]# kubectl get pods --show-labels |grep myapp-deploy</span><br><span class="line">myapp-deploy-69b47bc96d-4hxxw   1/1       Running   0          12m       app=myapp,pod-template-hash=2560367528,release=canary</span><br><span class="line">myapp-deploy-69b47bc96d-95bc4   1/1       Running   0          12m       app=myapp,pod-template-hash=2560367528,release=canary</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt   1/1       Running   0          12m       app=myapp,pod-template-hash=2560367528,release=canary</span><br><span class="line">myapp-deploy-69b47bc96d-pjv74   1/1       Running   0          12m       app=myapp,pod-template-hash=2560367528,release=canary</span><br><span class="line">myapp-deploy-69b47bc96d-rf7bs   1/1       Running   0          12m       app=myapp,pod-template-hash=2560367528,release=canary</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# cat myapp-svc.yaml #为myapp创建service</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: myapp</span><br><span class="line">    release: canary</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports: </span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">    nodePort: 30080</span><br><span class="line">[root@k8s-master mainfests]# kubectl apply -f myapp-svc.yaml </span><br><span class="line">service/myapp created</span><br><span class="line">[root@k8s-master mainfests]# kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        36d</span><br><span class="line">myapp        NodePort    10.101.245.119   &lt;none&gt;        80:30080/TCP   5s</span><br><span class="line">redis        ClusterIP   10.107.238.182   &lt;none&gt;        6379/TCP       28m</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# while true;do curl http://192.168.56.11:30080/hostname.html;sleep 1;done</span><br><span class="line">myapp-deploy-69b47bc96d-95bc4</span><br><span class="line">myapp-deploy-69b47bc96d-4hxxw</span><br><span class="line">myapp-deploy-69b47bc96d-pjv74</span><br><span class="line">myapp-deploy-69b47bc96d-rf7bs</span><br><span class="line">myapp-deploy-69b47bc96d-95bc4</span><br><span class="line">myapp-deploy-69b47bc96d-rf7bs</span><br><span class="line">myapp-deploy-69b47bc96d-95bc4</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# while true;do curl http://192.168.56.11:30080/;sleep 1;done</span><br><span class="line">  Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">  Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">  Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">  Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">  Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">  Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br></pre></td></tr></table></figure>

<p>从以上例子，可以看到通过NodePort方式已经实现了从集群外部端口进行访问，访问链接如下：<a href="http://192.168.56.11:30080/。实践中并不鼓励用户自定义使用节点的端口，因为容易和其他现存的Service冲突，建议留给系统自动配置。" target="_blank" rel="noopener">http://192.168.56.11:30080/。实践中并不鼓励用户自定义使用节点的端口，因为容易和其他现存的Service冲突，建议留给系统自动配置。</a></p>
<h4 id="Pod的会话保持"><a href="#Pod的会话保持" class="headerlink" title="Pod的会话保持"></a>Pod的会话保持</h4><p>　　Service资源还支持Session affinity（粘性会话）机制，可以将来自同一个客户端的请求始终转发至同一个后端的Pod对象，这意味着它会影响调度算法的流量分发功用，进而降低其负载均衡的效果。因此，当客户端访问Pod中的应用程序时，如果有基于客户端身份保存某些私有信息，并基于这些私有信息追踪用户的活动等一类的需求时，那么应该启用session affinity机制。</p>
<p>　　Service affinity的效果仅仅在一段时间内生效，默认值为10800秒，超出时长，客户端再次访问会重新调度。该机制仅能基于客户端IP地址识别客户端身份，它会将经由同一个NAT服务器进行原地址转换的所有客户端识别为同一个客户端，由此可知，其调度的效果并不理想。Service 资源 通过. spec. sessionAffinity 和. spec. sessionAffinityConfig 两个字段配置粘性会话。 spec. sessionAffinity 字段用于定义要使用的粘性会话的类型，它仅支持使用“ None” 和“ ClientIP” 两种属性值。如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master mainfests]# kubectl explain svc.spec.sessionAffinity</span><br><span class="line">KIND:     Service</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">FIELD:    sessionAffinity &lt;string&gt;</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">     Supports &quot;ClientIP&quot; and &quot;None&quot;. Used to maintain session affinity. Enable</span><br><span class="line">     client IP based session affinity. Must be ClientIP or None. Defaults to</span><br><span class="line">     None. More info:</span><br><span class="line">     https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies</span><br></pre></td></tr></table></figure>

<p>sessionAffinity支持ClientIP和None 两种方式，默认是None（随机调度） ClientIP是来自于同一个客户端的请求调度到同一个pod中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master mainfests]# vim myapp-svc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: myapp</span><br><span class="line">    release: canary</span><br><span class="line">  sessionAffinity: ClientIP</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports: </span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">    nodePort: 30080</span><br><span class="line">[root@k8s-master mainfests]# kubectl apply -f myapp-svc.yaml </span><br><span class="line">service/myapp configured</span><br><span class="line">[root@k8s-master mainfests]# kubectl describe svc myapp</span><br><span class="line">Name:                     myapp</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   &lt;none&gt;</span><br><span class="line">Annotations:              kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;myapp&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;ports&quot;:[&#123;&quot;nodePort&quot;:30080,&quot;port&quot;:80,&quot;ta...</span><br><span class="line">Selector:                 app=myapp,release=canary</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.101.245.119</span><br><span class="line">Port:                     &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:               80/TCP</span><br><span class="line">NodePort:                 &lt;unset&gt;  30080/TCP</span><br><span class="line">Endpoints:                10.244.1.18:80,10.244.1.19:80,10.244.2.15:80 + 2 more...</span><br><span class="line">Session Affinity:         ClientIP</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br><span class="line">[root@k8s-master mainfests]# while true;do curl http://192.168.56.11:30080/hostname.html;sleep 1;done</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt</span><br></pre></td></tr></table></figure>

<p>也可以使用打补丁的方式进行修改yaml内的内容，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch svc myapp -p &apos;&#123;&quot;spec&quot;:&#123;&quot;sessionAffinity&quot;:&quot;ClusterIP&quot;&#125;&#125;&apos;  #session保持，同一ip访问同一个pod</span><br><span class="line"></span><br><span class="line">kubectl patch svc myapp -p &apos;&#123;&quot;spec&quot;:&#123;&quot;sessionAffinity&quot;:&quot;None&quot;&#125;&#125;&apos;    #取消session</span><br></pre></td></tr></table></figure>

<h2 id="Headless-Service"><a href="#Headless-Service" class="headerlink" title="Headless Service"></a>Headless Service</h2><p>有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（<code>spec.clusterIP</code>）的值为 <code>&quot;None&quot;</code> 来创建 <code>Headless</code> Service。</p>
<p>这个选项允许开发人员自由寻找他们自己的方式，从而降低与 Kubernetes 系统的耦合性。 应用仍然可以使用一种自注册的模式和适配器，对其它需要发现机制的系统能够很容易地基于这个 API 来构建。</p>
<p>对这类 <code>Service</code> 并不会分配 Cluster IP，kube-proxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 <code>Service</code> 是否定义了 selector。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">（1）编写headless service配置清单</span><br><span class="line">[root@k8s-master mainfests]# cp myapp-svc.yaml myapp-svc-headless.yaml </span><br><span class="line">[root@k8s-master mainfests]# vim myapp-svc-headless.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp-headless</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: myapp</span><br><span class="line">    release: canary</span><br><span class="line">  clusterIP: &quot;None&quot;　　#headless的clusterIP值为None</span><br><span class="line">  ports: </span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">（2）创建headless service </span><br><span class="line">[root@k8s-master mainfests]# kubectl apply -f myapp-svc-headless.yaml </span><br><span class="line">service/myapp-headless created</span><br><span class="line">[root@k8s-master mainfests]# kubectl get svc</span><br><span class="line">NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        36d</span><br><span class="line">myapp            NodePort    10.101.245.119   &lt;none&gt;        80:30080/TCP   1h</span><br><span class="line">myapp-headless   ClusterIP   None             &lt;none&gt;        80/TCP         5s</span><br><span class="line">redis            ClusterIP   10.107.238.182   &lt;none&gt;        6379/TCP       2h</span><br><span class="line"></span><br><span class="line">（3）使用coredns进行解析验证</span><br><span class="line">[root@k8s-master mainfests]# dig -t A myapp-headless.default.svc.cluster.local. @10.96.0.10</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A myapp-headless.default.svc.cluster.local. @10.96.0.10</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; Got answer:</span><br><span class="line">;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 62028</span><br><span class="line">;; flags: qr aa rd ra; QUERY: 1, ANSWER: 5, AUTHORITY: 0, ADDITIONAL: 1</span><br><span class="line"></span><br><span class="line">;; OPT PSEUDOSECTION:</span><br><span class="line">; EDNS: version: 0, flags:; udp: 4096</span><br><span class="line">;; QUESTION SECTION:</span><br><span class="line">;myapp-headless.default.svc.cluster.local. IN A</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">myapp-headless.default.svc.cluster.local. 5 IN A 10.244.1.18</span><br><span class="line">myapp-headless.default.svc.cluster.local. 5 IN A 10.244.1.19</span><br><span class="line">myapp-headless.default.svc.cluster.local. 5 IN A 10.244.2.15</span><br><span class="line">myapp-headless.default.svc.cluster.local. 5 IN A 10.244.2.16</span><br><span class="line">myapp-headless.default.svc.cluster.local. 5 IN A 10.244.2.17</span><br><span class="line"></span><br><span class="line">;; Query time: 4 msec</span><br><span class="line">;; SERVER: 10.96.0.10#53(10.96.0.10)</span><br><span class="line">;; WHEN: Thu Sep 27 04:27:15 EDT 2018</span><br><span class="line">;; MSG SIZE  rcvd: 349</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# kubectl get svc -n kube-system</span><br><span class="line">NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP   36d</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# kubectl get pods -o wide -l app=myapp</span><br><span class="line">NAME                            READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">myapp-deploy-69b47bc96d-4hxxw   1/1       Running   0          1h        10.244.1.18   k8s-node01</span><br><span class="line">myapp-deploy-69b47bc96d-95bc4   1/1       Running   0          1h        10.244.2.16   k8s-node02</span><br><span class="line">myapp-deploy-69b47bc96d-hwbzt   1/1       Running   0          1h        10.244.1.19   k8s-node01</span><br><span class="line">myapp-deploy-69b47bc96d-pjv74   1/1       Running   0          1h        10.244.2.15   k8s-node02</span><br><span class="line">myapp-deploy-69b47bc96d-rf7bs   1/1       Running   0          1h        10.244.2.17   k8s-node02</span><br><span class="line"></span><br><span class="line">（4）对比含有ClusterIP的service解析</span><br><span class="line">[root@k8s-master mainfests]# dig -t A myapp.default.svc.cluster.local. @10.96.0.10</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A myapp.default.svc.cluster.local. @10.96.0.10</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; Got answer:</span><br><span class="line">;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50445</span><br><span class="line">;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1</span><br><span class="line"></span><br><span class="line">;; OPT PSEUDOSECTION:</span><br><span class="line">; EDNS: version: 0, flags:; udp: 4096</span><br><span class="line">;; QUESTION SECTION:</span><br><span class="line">;myapp.default.svc.cluster.local. IN    A</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">myapp.default.svc.cluster.local. 5 IN    A    10.101.245.119</span><br><span class="line"></span><br><span class="line">;; Query time: 1 msec</span><br><span class="line">;; SERVER: 10.96.0.10#53(10.96.0.10)</span><br><span class="line">;; WHEN: Thu Sep 27 04:31:16 EDT 2018</span><br><span class="line">;; MSG SIZE  rcvd: 107</span><br><span class="line"></span><br><span class="line">[root@k8s-master mainfests]# kubectl get svc</span><br><span class="line">NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        36d</span><br><span class="line">myapp            NodePort    10.101.245.119   &lt;none&gt;        80:30080/TCP   1h</span><br><span class="line">myapp-headless   ClusterIP   None             &lt;none&gt;        80/TCP         11m</span><br><span class="line">redis            ClusterIP   10.107.238.182   &lt;none&gt;        6379/TCP       2h</span><br></pre></td></tr></table></figure>

<p>从以上的演示可以看到对比普通的service和headless service，headless service做dns解析是直接解析到pod的，而servcie是解析到ClusterIP的，那么headless有什么用呢？？？这将在statefulset中应用到，这里暂时仅仅做了解什么是headless service和创建方法。</p>
<h1 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h1><h2 id="相关组件关系"><a href="#相关组件关系" class="headerlink" title="相关组件关系"></a>相关组件关系</h2><p>通俗的讲:</p>
<ul>
<li>Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务</li>
<li>Ingress 是反向代理规则，用来规定 HTTP/S 请求应该被转发到哪个 Service 上，比如根据请求中不同的 Host 和 url 路径让请求落到不同的 Service 上</li>
<li>Ingress Controller 就是一个反向代理程序，它负责解析 Ingress 的反向代理规则，如果 Ingress 有增删改的变动，所有的 Ingress Controller 都会及时更新自己相应的转发规则，当 Ingress Controller 收到请求后就会根据这些规则将请求转发到对应的 Service。</li>
</ul>
<p>Kubernetes 并没有自带 Ingress Controller，它只是一种标准，具体实现有多种，需要自己单独安装，常用的是 Nginx Ingress Controller 和 Traefik Ingress Controller。 所以 Ingress 是一种转发规则的抽象，Ingress Controller 的实现需要根据这些 Ingress 规则来将请求转发到对应的 Service，我画了个图方便大家理解：</p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes07.png" alt="image"></p>
<p>从图中可以看出，Ingress Controller 收到请求，匹配 Ingress 转发规则，匹配到了就转发到后端 Service，而 Service 可能代表的后端 Pod 有多个，选出一个转发到那个 Pod，最终由那个 Pod 处理请求。</p>
<p>有同学可能会问，既然 Ingress Controller 要接受外面的请求，而 Ingress Controller 是部署在集群中的，怎么让 Ingress Controller 本身能够被外面访问到呢，有几种方式：</p>
<ul>
<li>Ingress Controller 用 Deployment 方式部署，给它添加一个 Service，类型为 LoadBalancer，这样会自动生成一个 IP 地址，通过这个 IP 就能访问到了，并且一般这个 IP 是高可用的（前提是集群支持 LoadBalancer，通常云服务提供商才支持，自建集群一般没有）</li>
<li>使用集群内部的某个或某些节点作为边缘节点，给 node 添加 label 来标识，Ingress Controller 用 DaemonSet 方式部署，使用 nodeSelector 绑定到边缘节点，保证每个边缘节点启动一个 Ingress Controller 实例，用 hostPort 直接在这些边缘节点宿主机暴露端口，然后我们可以访问边缘节点中 Ingress Controller 暴露的端口，这样外部就可以访问到 Ingress Controller 了</li>
<li>Ingress Controller 用 Deployment 方式部署，给它添加一个 Service，类型为 NodePort，部署完成后查看会给出一个端口，通过 <code>kubectl get svc</code> 我们可以查看到这个端口，这个端口在集群的每个节点都可以访问，通过访问集群节点的这个端口就可以访问 Ingress Controller 了。但是集群节点这么多，而且端口又不是 80和443，太不爽了，一般我们会在前面自己搭个负载均衡器，比如用 Nginx，将请求转发到集群各个节点的那个端口上，这样我们访问 Nginx 就相当于访问到 Ingress Controller 了</li>
</ul>
<p>一般比较推荐的是前面两种方式。</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>向外网暴露集群内服务，以使客户端能够访问，有以下几种方法，本文重点描述Ingress。</p>
<p><strong>LoadBalancer</strong></p>
<p>LoadBalancer一般由云服务供应商提供或者用户自定义，运行在集群之外。在创建service时为其配置LoadBalancer相关参数，当从外网访问集群内servcie时，用户直接连接到LoadBalancer服务器，LoadBalancer服务器再将流量转发到集群内service。Loadbalancer配置及使用方法与各云服务供应商有关，本文不详细描述。</p>
<p><strong>NodePort</strong></p>
<p>这种方式要求集群中部分节点有被外网访问的能力。Kubernetes为每个NodePort类型的服务在集群中的每个节点上分配至少一个主机网络端口号。客户通过能被外网访问的节点IP加上节点端口的方式访问服务。大多数情况下不会通过这种方式向集群外暴露服务，原因有四。</p>
<ul>
<li>其一：大多情况下，为了安全起见，集群中的节点位于完全内网环境中，不应该有被外网直接访问的能力。一般外网访问集群中的节点都是通过边界服务器如网关、跳板等，而这种边界服务器需要通过各种方式进行安全加固。</li>
<li>其二：如果集群内节点可以从外网直接访问的话，则会将集群内节点地址、服务名称、端口号等信息直接暴露在外，非常不安全。</li>
<li>其三：服务端口号一般由系统自动分配，并非固定，而服务名称也可能发生变更，此时外部客户端需要跟踪变更并修改，属于重试耦合。</li>
<li>其四：这种方式，每个服务至少向外网暴露一个端口号，当服务很多时不易于管理。</li>
</ul>
<p><strong>Ingress</strong></p>
<p>Ingress不是某种产品、组件的名称，它应该是kubernetes向集群外暴露服务的一种思路、技术，用户完全可以根据这种思路提供自己的Ingress实现，当然kubernetes提供了默认Ingress实现还有其它第三方实现，一般无需自己开发。它的思路是这样的，首先在集群内运行一个服务或者pod也可以是容器，不管是什么它至少应该有一个外网可以访问的IP，至少向外网开放一个端口号，让它充当反向代理服务器。当外网想要访问集群内service时，只需访问这个反向代理服务器并指定相关参数，代理服务器根据请求参数并结合内部规则，将请求转发到service。这种思路与LoadBalancer的不同之处是它就位于集群内，而LoadBalancer位于集群外。与NodePort的不同之处是集群只向外暴露一个服务或者pod等，而NodePort是暴露全部service。</p>
<p>Kubernetes用nginx实现反向代理服务器，称为Ingress Controller，是pod类型资源。同时提供了Ingress类型对象，通过创建Ingress对象配置nginx反向代理服务器的转发规则。Nginx反向代理服务器收到来自外网的请求后，用请求的URL地址、请求头字段区别不同service，然后转发请求。</p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>GitHub：<a href="https://github.com/kubernetes/ingress-nginx/tree/nginx-0.20.0/deploy" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/tree/nginx-0.20.0/deploy</a></p>
<p>mandatory.yml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: default-http-backend</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: default-http-backend</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: default-http-backend</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: default-http-backend</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      containers:</span><br><span class="line">        - name: default-http-backend</span><br><span class="line">          # Any image is permissible as long as:</span><br><span class="line">          # 1. It serves a 404 page at /</span><br><span class="line">          # 2. It serves 200 on a /healthz endpoint</span><br><span class="line">          image: k8s.gcr.io/defaultbackend-amd64:1.5</span><br><span class="line">          livenessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 8080</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 30</span><br><span class="line">            timeoutSeconds: 5</span><br><span class="line">          ports:</span><br><span class="line">            - containerPort: 8080</span><br><span class="line">          resources:</span><br><span class="line">            limits:</span><br><span class="line">              cpu: 10m</span><br><span class="line">              memory: 20Mi</span><br><span class="line">            requests:</span><br><span class="line">              cpu: 10m</span><br><span class="line">              memory: 20Mi</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: default-http-backend</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: default-http-backend</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app.kubernetes.io/name: default-http-backend</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-serviceaccount</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - endpoints</span><br><span class="line">      - nodes</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses/status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    resourceNames:</span><br><span class="line">      # Defaults to &quot;&lt;election-id&gt;-&lt;ingress-class&gt;&quot;</span><br><span class="line">      # Here: &quot;&lt;ingress-controller-leader&gt;-&lt;nginx&gt;&quot;</span><br><span class="line">      # This has to be adapted if you change either parameter</span><br><span class="line">      # when launching the nginx-ingress-controller.</span><br><span class="line">      - &quot;ingress-controller-leader-nginx&quot;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role-nisa-binding</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole-nisa-binding</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<h3 id="更改暴露方式"><a href="#更改暴露方式" class="headerlink" title="更改暴露方式"></a>更改暴露方式</h3><p>此刻问题来了通过yaml创建的deploy以及server来看好像并没有把nginx端口映射到宿主机上,那么我访问宿主机ip就不会有任何返回,这里可以通过hostport+DaemonSet来解决这个问题<br>修改yaml文件<br>1.修改nginx 部署方式为DaemonSet<br>2.注释replicas: 1<br>3.增加 hostNetwork: true 在spec: 段内增加<br>4.增加hostPort 在Ports段内增加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line"> # replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: siriuszg/nginx-ingress-controller:0.20.0</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">              hostPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">              hostPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<h2 id="创建一个tomcat并用ingress7层代理转发"><a href="#创建一个tomcat并用ingress7层代理转发" class="headerlink" title="创建一个tomcat并用ingress7层代理转发"></a>创建一个tomcat并用ingress7层代理转发</h2><p>创建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">cat tomcat-ingress.yaml </span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat</span><br><span class="line">    release: canary</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata: </span><br><span class="line">  name: tomcat-deploy</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector: </span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat</span><br><span class="line">      release: canary</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat</span><br><span class="line">        release: canary</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat</span><br><span class="line">        image: tomcat:7-alpine</span><br><span class="line">        ports:</span><br><span class="line">        - name: httpd</span><br><span class="line">          containerPort: 8080</span><br></pre></td></tr></table></figure>

<p>查看</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod | grep tomcat</span><br><span class="line">tomcat-deploy-64b488b68-wk45q   1/1     Running   0          29m</span><br><span class="line">kubectl get svc | grep tomcat</span><br><span class="line">tomcat        ClusterIP   10.0.0.183   &lt;none&gt;        8080/TCP       29m</span><br></pre></td></tr></table></figure>

<p>创建ingress</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat ingress-tomcat.yaml </span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-tomcat</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations: </span><br><span class="line">    kubernets.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: www.aa.com  #用来解析的域名地址</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: </span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat   #集群服务的名字</span><br><span class="line">          servicePort: 8080        #集群服务开放的端口</span><br></pre></td></tr></table></figure>

<p>访问测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> curl -H &quot;host:www.aa.com&quot; http://10.167.130.206:80  #IP地址为运行ingress-nginx-controller的主机地址,因为只有运行了这个容器才会监听宿主的80端口。</span><br><span class="line"></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;html lang=&quot;en&quot;&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">        &lt;title&gt;Apache Tomcat/7.0.91&lt;/title&gt;</span><br></pre></td></tr></table></figure>

<p>可用命令查看ingress列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">kubectl get ingress</span><br><span class="line">NAME             HOSTS        ADDRESS   PORTS   AGE</span><br><span class="line">ingress-tomcat   www.aa.com             80      34m</span><br><span class="line"></span><br><span class="line">kubectl describe ingress ingress-tomcat</span><br><span class="line">Name:             ingress-tomcat</span><br><span class="line">Namespace:        default</span><br><span class="line">Address:          </span><br><span class="line">Default backend:  default-http-backend:80 (&lt;none&gt;)</span><br><span class="line">Rules:</span><br><span class="line"> Host        Path  Backends</span><br><span class="line"> ----        ----  --------</span><br><span class="line"> www.aa.com  </span><br><span class="line">                tomcat:8080 (&lt;none&gt;)</span><br></pre></td></tr></table></figure>

<h2 id="用ingress来代理4层请求"><a href="#用ingress来代理4层请求" class="headerlink" title="用ingress来代理4层请求"></a>用ingress来代理4层请求</h2><p>创建mysql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">cat mysql.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql</span><br><span class="line"> namespace: default</span><br><span class="line">spec:</span><br><span class="line"> type: ClusterIP</span><br><span class="line"> selector:</span><br><span class="line">   app: mysql</span><br><span class="line">   release: canary</span><br><span class="line"> ports:</span><br><span class="line"> - name: mysql</span><br><span class="line">   port: 3306</span><br><span class="line">   targetPort: 3306</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet  #每个node都运行一个pod，我就两个node正好用来测试负载效果</span><br><span class="line">metadata: </span><br><span class="line"> name: mysql-daemonset</span><br><span class="line">spec:</span><br><span class="line"># replicas: 1</span><br><span class="line"> selector: </span><br><span class="line">   matchLabels:</span><br><span class="line">     app: mysql</span><br><span class="line">     release: canary</span><br><span class="line"> template:</span><br><span class="line">   metadata:</span><br><span class="line">     labels:</span><br><span class="line">       app: mysql</span><br><span class="line">       release: canary</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - name: mysql</span><br><span class="line">       image: mysql</span><br><span class="line">       env:</span><br><span class="line">         - name: MYSQL_ROOT_PASSWORD  #mysql镜像必须的变量,不写这个变量mysql跑不起来</span><br><span class="line">           value: &quot;mysql&quot;</span><br><span class="line">       ports:</span><br><span class="line">       - name: mysql</span><br><span class="line">         containerPort: 3306</span><br><span class="line">         </span><br><span class="line">         </span><br><span class="line">kubectl apply -f  mysql.yaml #部署mysql pod</span><br><span class="line">kubectl get pod</span><br><span class="line">mysql-daemonset-2xdr7           1/1     Running   0          63m</span><br><span class="line">mysql-daemonset-stvhf           1/1     Running   0          63m</span><br></pre></td></tr></table></figure>

<p>修改configmap文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat configmap.yaml </span><br><span class="line"></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">data:</span><br><span class="line"> 3306: &quot;default/mysql:3306&quot; #我们的mysql是在默认命名空间里创建的这个自行查看更改</span><br></pre></td></tr></table></figure>

<blockquote>
<p>添加或增减configmap直接在这个configmap文件中新增或去除即可,用kubectl apply重新应用,尽量不要直接kubectl delete -f configmap.yaml 因为这样会把整个tcp-services都删掉,删掉后node节点检测不到数据就不会对规则更新,这个和7层代理不太一样,7层可以一个服务创建一个name,4层在创建ingress服务时候就指定tcp-services和udp-services两个文件了,定义位置可以看ingress.yaml的281-282行</p>
</blockquote>
<p>另外有一点,因为创建完ingress时候Node节点就是监听80和443的,在配置这个mysql时排错过程中发现,3306端口并不会默认监听,只有ingress可以正常连接到mysql集群时,node才会去监听3306端口,有错误可以按照这个思路排错,配置过程中也遇到很多问题,排错思路,容器&gt;容器ip&gt;集群ip&gt;ingress。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">断开连接几次试试，应该是轮训算法，分别在两个pod的数据库里写了a和b用来测试负载效果</span><br><span class="line"></span><br><span class="line">mysql&gt; show databases; </span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| a                  |</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">mysql&gt; show databases; </span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| b                  |</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>

<h2 id="其它示例"><a href="#其它示例" class="headerlink" title="其它示例"></a>其它示例</h2><h3 id="Single-Service-Ingress"><a href="#Single-Service-Ingress" class="headerlink" title="Single Service Ingress"></a>Single Service Ingress</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: test-ingress</span><br><span class="line">spec:</span><br><span class="line">  backend:</span><br><span class="line">    serviceName: testsvc</span><br><span class="line">    servicePort: 80</span><br></pre></td></tr></table></figure>

<p>创建对象：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get ing</span><br><span class="line">NAME                RULE          BACKEND        ADDRESS</span><br><span class="line">test-ingress        -             testsvc:80     107.178.254.228</span><br></pre></td></tr></table></figure>

<p>以上配置中没有具体的rule，所以诸如http(s)://107.178.254.228/xxx之类的请求都转发到testsvc的80端口。</p>
<h3 id="其于URL转发"><a href="#其于URL转发" class="headerlink" title="其于URL转发"></a>其于URL转发</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">foo.bar.com -&gt; 178.91.123.132 -&gt; / foo    s1:80</span><br><span class="line">                                 / bar    s2:80</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: test</span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io/rewrite-target: /</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.bar.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /foo</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: s1</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /bar</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: s2</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure>

<p>创建对象：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get ing</span><br><span class="line">NAME      RULE          BACKEND   ADDRESS</span><br><span class="line">test      -</span><br><span class="line">          foo.bar.com</span><br><span class="line">          /foo          s1:80</span><br><span class="line">          /bar          s2:80</span><br></pre></td></tr></table></figure>

<h3 id="基于名称的虚拟主机"><a href="#基于名称的虚拟主机" class="headerlink" title="基于名称的虚拟主机"></a>基于名称的虚拟主机</h3><p>实现如下目标：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foo.bar.com --|                 |-&gt; foo.bar.com s1:80</span><br><span class="line">              | 178.91.123.132  |</span><br><span class="line">bar.foo.com --|                 |-&gt; bar.foo.com s2:8</span><br></pre></td></tr></table></figure>

<p>这种方式的核心逻辑是用http请求中的host字段区分不同服务，而不是URL。如host: foo.bar.com的请求被转发到s1服务80端口，如host: bar.foo.com的请求被转发到s2服务80端口。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: test</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.bar.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: s1</span><br><span class="line">          servicePort: 80</span><br><span class="line">  - host: bar.foo.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: s2</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure>

<h3 id="TLS"><a href="#TLS" class="headerlink" title="TLS"></a>TLS</h3><p>利用Secret类型对象为Ingress Controller提供私钥及证书，对通信链路加密。 </p>
<p>Secret配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  tls.crt: base64 encoded cert</span><br><span class="line">  tls.key: base64 encoded key</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: testsecret</span><br><span class="line">  namespace: default</span><br><span class="line">type: Secret</span><br></pre></td></tr></table></figure>

<p>在Ingress对象中引用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: no-rules-map</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - foo.bar.com</span><br><span class="line">    secretName: testsecret</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.bar.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: s1</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure>

<h1 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>在具有以下特点时使用StatefulSets：</p>
<ul>
<li>稳定性，唯一的网络标识符。</li>
<li>稳定性，持久化存储。</li>
<li>有序的部署和扩展。</li>
<li>有序的删除和终止。</li>
<li>有序的自动滚动更新。</li>
</ul>
<p>Pod调度运行时，如果应用不需要任何稳定的标示、有序的部署、删除和扩展，则应该使用一组无状态副本的控制器来部署应用，例如 <a href="http://docs.kubernetes.org.cn/317.html" target="_blank" rel="noopener">Deployment</a> 或 <a href="http://docs.kubernetes.org.cn/314.html" target="_blank" rel="noopener">ReplicaSet</a>更适合无状态服务需求。</p>
<p>RC、Deployment、DaemonSet都是面向无状态的服务，它们所管理的Pod的IP、名字，启停顺序等都是随机的，而StatefulSet是什么？顾名思义，有状态的集合，管理所有有状态的服务，比如MySQL、MongoDB集群等。<br>StatefulSet本质上是Deployment的一种变体，在v1.9版本中已成为<strong>GA</strong>版本，它为了解决有状态服务的问题，它所管理的Pod拥有固定的Pod名称，启停顺序，在StatefulSet中，Pod名字称为网络标识(hostname)，还必须要用到共享存储。<br>在Deployment中，与之对应的服务是service，而在StatefulSet中与之对应的headless service，headless service，即无头服务，与service的区别就是它没有Cluster IP，解析它的名称时将返回该Headless Service对应的全部Pod的Endpoint列表。<br>除此之外，StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod副本创建了一个DNS域名，这个域名的格式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$(podname).(headless server name)   </span><br><span class="line">FQDN： $(podname).(headless server name).namespace.svc.cluster.local</span><br></pre></td></tr></table></figure>

<h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    name: web</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx # has to match .spec.template.metadata.labels</span><br><span class="line">  serviceName: &quot;nginx&quot;  #声明它属于哪个Headless Service.</span><br><span class="line">  replicas: 3 # by default is 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx # has to match .spec.selector.matchLabels</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: k8s.gcr.io/nginx-slim:0.8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: www</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:   #可看作pvc的模板</span><br><span class="line">  - metadata:</span><br><span class="line">      name: www</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">      storageClassName: &quot;gluster-heketi&quot;  #存储类名，改为集群中已存在的</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 1Gi</span><br></pre></td></tr></table></figure>

<p>通过该配置文件，可看出StatefulSet的三个组成部分：</p>
<ul>
<li>Headless Service：名为nginx，用来定义Pod网络标识( DNS domain)。</li>
<li>StatefulSet：定义具体应用，名为Nginx，有三个Pod副本，并为每个Pod定义了一个域名。</li>
<li>volumeClaimTemplates： 存储卷申请模板，创建PVC，指定pvc名称大小，将自动创建pvc，且pvc必须由存储类供应。</li>
</ul>
<p><strong>为什么需要 headless service 无头服务？</strong><br>在用Deployment时，每一个Pod名称是没有顺序的，是随机字符串，因此是Pod名称是无序的，但是在statefulset中要求必须是有序 ，每一个pod不能被随意取代，pod重建后pod名称还是一样的。而pod IP是变化的，所以是以Pod名称来识别。pod名称是pod唯一性的标识符，必须持久稳定有效。这时候要用到无头服务，它可以给每个Pod一个唯一的名称 。<br><strong>为什么需要volumeClaimTemplate？</strong><br>对于有状态的副本集都会用到持久存储，对于分布式系统来讲，它的最大特点是数据是不一样的，所以各个节点不能使用同一存储卷，每个节点有自已的专用存储，但是如果在Deployment中的Pod template里定义的存储卷，是所有副本集共用一个存储卷，数据是相同的，因为是基于模板来的 ，而statefulset中每个Pod都要自已的专有存储卷，所以statefulset的存储卷就不能再用Pod模板来创建了，于是statefulSet使用volumeClaimTemplate，称为卷申请模板，它会为每个Pod生成不同的pvc，并绑定pv， 从而实现各pod有专用存储。这就是为什么要用volumeClaimTemplate的原因。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nginx.yaml </span><br><span class="line">service &quot;nginx&quot; created</span><br><span class="line">statefulset &quot;web&quot; created</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#第一个是创建web-0</span><br><span class="line">$ kubectl get pod</span><br><span class="line">web-0                     1/1       ContainerCreating   0          51s</span><br><span class="line"></span><br><span class="line">#待web-0 running且ready时，创建web-1</span><br><span class="line">$ kubectl get pod</span><br><span class="line">web-0                     1/1       Running             0          51s</span><br><span class="line">web-1                     0/1       ContainerCreating   0          42s</span><br><span class="line"></span><br><span class="line">#待web-1 running且ready时，创建web-2</span><br><span class="line">$ kubectl get pod</span><br><span class="line">web-0                     1/1       Running             0          1m</span><br><span class="line">web-1                     1/1       Running             0          45s</span><br><span class="line">web-2                     1/1       ContainerCreating   0          36s</span><br><span class="line"></span><br><span class="line">#最后三个Pod全部running且ready</span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">web-0                     1/1       Running   0          4m</span><br><span class="line">web-1                     1/1       Running   0          3m</span><br><span class="line">web-2                     1/1       Running   0          1m</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME              STATUS    VOLUME                                  CAPACITY   ACCESS MODES   STORAGECLASS     AGE</span><br><span class="line">www-web-0         Bound     pvc-ecf003f3-828d-11e8-8815-000c29774d39   2G        RWO          gluster-heketi   7m</span><br><span class="line">www-web-1         Bound     pvc-0615e33e-828e-11e8-8815-000c29774d39   2G        RWO          gluster-heketi   6m</span><br><span class="line">www-web-2         Bound     pvc-43a97acf-828e-11e8-8815-000c29774d39   2G        RWO          gluster-heketi   4m</span><br></pre></td></tr></table></figure>

<p>如果集群中没有StorageClass的动态供应PVC的机制，也可以提前手动创建多个PV、PVC，手动创建的PVC名称必须符合之后创建的StatefulSet命名规则：<strong>(volumeClaimTemplates.name)-(pod_name)</strong></p>
<p>Statefulset名称为web 三个Pod副本: web-0，web-1,web-2，volumeClaimTemplates名称为：www，那么自动创建出来的PVC名称为www-web-[0-2]，为每个Pod创建一个PVC。</p>
<p><strong>规律总结</strong></p>
<ul>
<li>匹配Pod name(网络标识)的模式为：<code>$(statefulset名称)</code>-<code>$(序号)</code>，比如上面的示例：web-0，web-1，web-2。</li>
<li>StatefulSet为每个Pod副本创建了一个DNS域名，这个域名的格式为：<strong>$(podname).(headless server name)</strong>，也就意味着服务间是通过Pod域名来通信而非Pod IP，因为当Pod所在Node发生故障时，Pod会被飘移到其它Node上，Pod IP会发生变化，但是Pod域名不会有变化。</li>
<li>StatefulSet使用Headless服务来控制Pod的域名，这个域名的FQDN为：<code>$(service name)</code>.<code>$(namespace)</code>.svc.cluster.local，其中，“cluster.local”指的是集群的域名。</li>
<li>根据volumeClaimTemplates，为每个Pod创建一个pvc，pvc的命名规则匹配模式：<strong>(volumeClaimTemplates.name)-(pod_name)</strong>，比如上面的volumeMounts.name=www， Pod name=web-[0-2]，因此创建出来的PVC是www-web-0、www-web-1、www-web-2。</li>
<li>删除Pod不会删除其pvc，手动删除pvc将自动释放pv。<br>关于Cluster Domain、headless service名称、StatefulSet 名称如何影响StatefulSet的Pod的DNS域名的示例：</li>
</ul>
<table>
<thead>
<tr>
<th>Cluster Domain</th>
<th>Service (ns/name)</th>
<th>StatefulSet (ns/name)</th>
<th>StatefulSet Domain</th>
<th>Pod DNS</th>
<th>Pod Hostname</th>
</tr>
</thead>
<tbody><tr>
<td>cluster.local</td>
<td>default/nginx</td>
<td>default/web</td>
<td>nginx.default.svc.cluster.local</td>
<td>web-{0..N-1}.nginx.default.svc.cluster.local</td>
<td>web-{0..N-1}</td>
</tr>
<tr>
<td>cluster.local</td>
<td>foo/nginx</td>
<td>foo/web</td>
<td>nginx.foo.svc.cluster.local</td>
<td>web-{0..N-1}.nginx.foo.svc.cluster.local</td>
<td>web-{0..N-1}</td>
</tr>
<tr>
<td>kube.local</td>
<td>foo/nginx</td>
<td>foo/web</td>
<td>nginx.foo.svc.kube.local</td>
<td>web-{0..N-1}.nginx.foo.svc.kube.local</td>
<td>web-{0..N-1}</td>
</tr>
</tbody></table>
<p><strong>Statefulset的启停顺序：</strong></p>
<ul>
<li>有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到N-1）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。</li>
<li>有序删除：当Pod被删除时，它们被终止的顺序是从N-1到0。</li>
<li>有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态。　</li>
</ul>
<p><strong>Statefulset Pod管理策略：</strong><br>在v1.7以后，通过允许修改Pod排序策略，同时通过.spec.podManagementPolicy字段确保其身份的唯一性。</p>
<ul>
<li>OrderedReady：上述的启停顺序，默认设置。</li>
<li>Parallel：告诉StatefulSet控制器并行启动或终止所有Pod，并且在启动或终止另一个Pod之前不等待前一个Pod变为Running and Ready或完全终止。</li>
</ul>
<p><strong>StatefulSet使用场景：</strong></p>
<ul>
<li>稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现。</li>
<li>稳定的网络标识符，即Pod重新调度后其PodName和HostName不变。</li>
<li>有序部署，有序扩展，基于init containers来实现。</li>
<li>有序收缩。</li>
</ul>
<p><strong>更新策略</strong></p>
<p>在Kubernetes 1.7及更高版本中，通过.spec.updateStrategy字段允许配置或禁用Pod、labels、source request/limits、annotations自动滚动更新功能。<br><strong>OnDelete：</strong>通过.spec.updateStrategy.type 字段设置为OnDelete，StatefulSet控制器不会自动更新StatefulSet中的Pod。用户必须手动删除Pod，以使控制器创建新的Pod。<br><strong>RollingUpdate：</strong>通过.spec.updateStrategy.type 字段设置为RollingUpdate，实现了Pod的自动滚动更新，如果.spec.updateStrategy未指定，则此为默认策略。<br>StatefulSet控制器将删除并重新创建StatefulSet中的每个Pod。它将以Pod终止（从最大序数到最小序数）的顺序进行，一次更新每个Pod。在更新下一个Pod之前，必须等待这个Pod Running and Ready。<br><strong>Partitions：</strong>通过指定 .spec.updateStrategy.rollingUpdate.partition 来对 RollingUpdate 更新策略进行分区，如果指定了分区，则当 StatefulSet 的 .spec.template 更新时，具有大于或等于分区序数的所有 Pod 将被更新。<br>具有小于分区的序数的所有 Pod 将不会被更新，即使删除它们也将被重新创建。如果 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 大于其 .spec.replicas，则其 .spec.template 的更新将不会传播到 Pod。在大多数情况下，不需要使用分区。</p>
<h1 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h1><p>官方网站：<a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Kubenetes以插件的形式提供DNS服务，一般是运行在kube-system名称空间下的service，拥有固定IP地址。插件运行起来后，配置各个节点上的kubelet，告诉它集群中DNS服务的IP地址，kebelet在启动容器时再将DNS服务器的地址告诉容器，容器再使用此DNS服务器进行域名解析。</p>
<p><strong>能通过DNS名称得到什么？</strong></p>
<p>集群中的service在创建时会被分配DNS名称，包含DNS服务自己。默认情况下客户pod的DNS搜索列表包含pod本身的namespace与集群默认域名，以下示例说明。</p>
<p>假设有一个名为foo的服务，们于bar名称空间。运行在bar名称空间中的其它pod直接以foo做为关键字查询DNS记录，对于bar名称空间中的pod需要使用关键字foo.bar查询foo的DNS记录。</p>
<p>以下小节详细介绍kubernetes DNS支持的记录类型及层次布局。</p>
<h2 id="SERVICE"><a href="#SERVICE" class="headerlink" title="SERVICE"></a>SERVICE</h2><h3 id="A-records"><a href="#A-records" class="headerlink" title="A records"></a>A records</h3><p>普通服务（非无头服务）的名称被指派一条DNS A类记录，如位于my-namespace名称空间下的my-svc服务，为其指派的A类DNS记录为”my-svc.my-namespace.svc.cluster.local”，这条记录会被解析成服务的集群虚拟IP地址。</p>
<p>如果my-svn为无头服务，同样为其分配”my-svc.my-namespace.svc.cluster.local”的Ａ类记录。与普通服务不同，如果无头服务包含标签选择器，则此Ａ类记录会被解析成所有标签选择器选中pod的pod网络地址，用户可以通过某种算法如循环使用返回的条目集合。</p>
<h3 id="SRV-records"><a href="#SRV-records" class="headerlink" title="SRV records"></a>SRV records</h3><p>当普通或者是无头服务包含命名端口时，创建此类SRV条目，例如:</p>
<p>“_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local”，有多个命名端口则创建多条记录。对于普通服务，此条记录被解析成my-port-name所对应的端口号与一条CNAME记录：”my-svc.my-namespace.svc.cluster.local”。对于包含标签选择器的无头服务，其解析结果为每个pod中的my-port-name对应的端口号及每个pod的CNAME记录：<br>pod-name.my-svc.my-namespace.svc.cluster.local。</p>
<h2 id="Pods"><a href="#Pods" class="headerlink" title="Pods"></a>Pods</h2><p>本节提及之pod应该是指由用户直接创建，而非由ReplicaSet等副本控制器创建。</p>
<h3 id="A-records-1"><a href="#A-records-1" class="headerlink" title="A records"></a>A records</h3><p>如果功能被开启，pod以如下格式被分配A类记录：”pod-ip-address.my-namespace.pod.cluster.local”。例如pod的ip为1.2.3.4，名称空间为default，则在DNS中的Ａ类记录为”1-2-3-4.default.pod.cluster.local，当查询时此此条记录被解析成pod名称。</p>
<h3 id="Pod’s-hostname-and-subdomain-fields"><a href="#Pod’s-hostname-and-subdomain-fields" class="headerlink" title="Pod’s hostname and subdomain fields"></a>Pod’s hostname and subdomain fields</h3><p>默认情况下，pod的hostname与pod名称相同。同时pod Spec有一个可选字段hostname，其值优先于pod名称被设置成hostname。另外，pod Spec还包含subdomain可选字段，可以为pod设置子域。假如为pod设置hostname为foo，subdomain设置为bar，其位于my-namespace名称空间下，则其有如下的全限定域名：”foo.bar.my-namespace.svc.cluster.local”。此条记录被解析成pod的IP地址。</p>
<p>大多数情况下，用户不直接创建pod，而是创建各种类型本控制器。用户直接创建pod的一种常见场景是创建包含选择器的无头服务，然后直接创建pod，让无头服务中的选择器选中自己创建的pod。如果打算为自己创建的pod创建A类记录，则必需在pod Spec中设置hostname字段。示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: default-subdomain</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    name: busybox</span><br><span class="line">  clusterIP: None</span><br><span class="line">  ports:</span><br><span class="line">  - name: foo # Actually, no port is needed.</span><br><span class="line">    port: 1234</span><br><span class="line">    targetPort: 1234</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox1</span><br><span class="line">  labels:</span><br><span class="line">    name: busybox</span><br><span class="line">spec:</span><br><span class="line">  hostname: busybox-1</span><br><span class="line">  subdomain: default-subdomain</span><br><span class="line">  containers:</span><br><span class="line">  - image: busybox</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;3600&quot;</span><br><span class="line">    name: busybox</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox2</span><br><span class="line">  labels:</span><br><span class="line">    name: busybox</span><br><span class="line">spec:</span><br><span class="line">  hostname: busybox-2</span><br><span class="line">  subdomain: default-subdomain</span><br><span class="line">  containers:</span><br><span class="line">  - image: busybox</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;3600&quot;</span><br><span class="line">    name: busybox</span><br></pre></td></tr></table></figure>

<p>上例的结果就是既会为无头服务default-subdomain创建A类解析条目”default-subdomain.my-namespace.svc.cluster.local”，也会单独为每个pod创建诸如”busybox-1.default-subdomain.my-namespace.svc.cluster.local”、”busybox-2.default-subdomain.my-namespace.svc.cluster.local”，分别被解析成pod的IP地址。前文讲过，如果没有为pod Spec指定hostname字段，则不创建后两条记录。</p>
<p>上述记录的生成过程大概是先选中pod，根据pod生成endpoint对象，根据生成的endpoint对象生成以上记录。如果无头服务没有标签选择器，则可以手动为其创建endpoint，如果打算为手动创建的endpoint单独添加记录，则必需在其Spec中设置hostname字段，其作用与在pod中设置相同。</p>
<h3 id="Pod’s-DNS-Policy"><a href="#Pod’s-DNS-Policy" class="headerlink" title="Pod’s DNS Policy"></a>Pod’s DNS Policy</h3><p>以上介绍的是kubernetes如何为service、pod创建DNS记录。那么如何定义pod内部解析域名时的规则呢？可以设置pod Spec中的dnsPolicy字段，有如下几种取值：</p>
<ul>
<li>“Default“:从节点继承DNS相关配置，对节点依赖性强。</li>
<li>“ClusterFirst“:如果DNS查询与配置好的默认集群域名前缀不匹配，则将查询请求转发到从节点继承而来，作为查询的上游服务器。</li>
<li>“ClusterFirstWithHostNet“:如果pod工作在主机网络，就将dnsPolicy设置成“ClusterFirstWithHostNet”，这样效率更高。</li>
<li>“None“:1.9版本引入的新特性(Beta in v1.10)。完全忽略kubernetes系统提供的DNS，以pod Spec中dnsConfig配置取而代之。</li>
</ul>
<p>如果dnsPolicy字段未设置，默认策略是”ClusterFirst”。</p>
<p>以下示例使用”ClusterFirstWithHostNet”，因为pod工作在主机网络：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: busybox</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: busybox</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  dnsPolicy: ClusterFirstWithHostNet</span><br></pre></td></tr></table></figure>

<h3 id="Pod’s-DNS-Config"><a href="#Pod’s-DNS-Config" class="headerlink" title="Pod’s DNS Config"></a>Pod’s DNS Config</h3><p>DNS Config从1.9版本引入，1.10版本可用，新增加特性的目的是为增强用户对pod之DNS控制。首先在apiServer与kubelet中设置特性开关，如”–feature-gates=CustomPodDNS=true,…”，而后在pod Spec中将dnsPolicy设置成None，并新添加dnsConfig字段。</p>
<p>dnsConfig字段：</p>
<ul>
<li>nameservers:DNS服务器IP地址，最多三个。如果dnsPolicy为None则此字段至少包含一个IP地址，为其它值时可选。此字段之地址会与其它方式生成的地址合并去重。</li>
<li>searches：查询域名，可选。与其它策略生成的域名合并去重。</li>
<li>options:对象选项列给，每个对象必需有name属性，value属性可选。</li>
</ul>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: dns-example</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test</span><br><span class="line">      image: nginx</span><br><span class="line">  dnsPolicy: &quot;None&quot;</span><br><span class="line">  dnsConfig:</span><br><span class="line">    nameservers:</span><br><span class="line">      - 1.2.3.4</span><br><span class="line">    searches:</span><br><span class="line">      - ns1.svc.cluster.local</span><br><span class="line">      - my.dns.search.suffix</span><br><span class="line">    options:</span><br><span class="line">      - name: ndots</span><br><span class="line">        value: &quot;2&quot;</span><br><span class="line">      - name: edns0</span><br></pre></td></tr></table></figure>

<p>创建pod后，其/etc/resolv.conf内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver 1.2.3.4</span><br><span class="line">search ns1.svc.cluster.local my.dns.search.suffix</span><br><span class="line">options ndots:2 edns0</span><br></pre></td></tr></table></figure>

<h1 id="调度之节点亲和性"><a href="#调度之节点亲和性" class="headerlink" title="调度之节点亲和性"></a>调度之节点亲和性</h1><p>Affinity 翻译成中文是“亲和性”，它对应的是 Anti-Affinity，我们翻译成“互斥”。这两个词比较形象，可以把 pod 选择 node 的过程类比成磁铁的吸引和互斥，不同的是除了简单的正负极之外，pod 和 node 的吸引和互斥是可以灵活配置的。</p>
<p>Affinity的优点：</p>
<ul>
<li>匹配有更多的逻辑组合，不只是字符串的完全相等</li>
<li>调度分成软策略(soft)和硬策略(hard)，在软策略下，如果没有满足调度条件的节点，pod会忽略这条规则，继续完成调度。</li>
</ul>
<p>目前主要的node affinity：</p>
<ul>
<li>requiredDuringSchedulingIgnoredDuringExecution<br>表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。</li>
<li>requiredDuringSchedulingRequiredDuringExecution<br>表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中RequiredDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点。</li>
<li>preferredDuringSchedulingIgnoredDuringExecution<br>表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。</li>
<li>preferredDuringSchedulingRequiredDuringExecution<br>表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。</li>
</ul>
<blockquote>
<p>软策略和硬策略的区分是有用处的，硬策略适用于 pod 必须运行在某种节点，否则会出现问题的情况，比如集群中节点的架构不同，而运行的服务必须依赖某种架构提供的功能；软策略不同，它适用于满不满足条件都能工作，但是满足条件更好的情况，比如服务最好运行在某个区域，减少网络传输等。这种区分是用户的具体需求决定的，并没有绝对的技术依赖。</p>
</blockquote>
<p>下面是一个官方的示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-node-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: kubernetes.io/e2e-az-name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - e2e-az1</span><br><span class="line">            - e2e-az2</span><br><span class="line">      preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">      - weight: 1</span><br><span class="line">        preference:</span><br><span class="line">          matchExpressions:</span><br><span class="line">          - key: another-node-label-key</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - another-node-label-value</span><br><span class="line">  containers:</span><br><span class="line">  - name: with-node-affinity</span><br><span class="line">    image: gcr.io/google_containers/pause:2.0</span><br></pre></td></tr></table></figure>

<p>这个 pod 同时定义了 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 两种 nodeAffinity。第一个要求 pod 运行在特定 AZ 的节点上，第二个希望节点最好有对应的 another-node-label-key:another-node-label-value 标签。</p>
<p>这里的匹配逻辑是label在某个列表中，可选的操作符有：</p>
<ul>
<li>In: label的值在某个列表中</li>
<li>NotIn：label的值不在某个列表中</li>
<li>Exists：某个label存在</li>
<li>DoesNotExist：某个label不存在</li>
<li>Gt：label的值大于某个值（字符串比较）</li>
<li>Lt：label的值小于某个值（字符串比较）</li>
</ul>
<p>如果nodeAffinity中nodeSelector有多个选项，节点满足任何一个条件即可；如果matchExpressions有多个选项，则节点必须同时满足这些选项才能运行pod 。</p>
<h1 id="滚动升级"><a href="#滚动升级" class="headerlink" title="滚动升级"></a>滚动升级</h1><p><strong>服务升级</strong></p>
<p>修改其中的image</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl set image deployment/demoservice  demoservice=lib/demoservicelib:1.1.0 --namespace=demospace</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit deployment demoservice -n demospace</span><br></pre></td></tr></table></figure>

<p><strong>查看deployments版本</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout history deployments demoservice -n demospace</span><br></pre></td></tr></table></figure>

<p>查看deployments指定版本信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout history deployments demoservice -n demospace --revision=2</span><br></pre></td></tr></table></figure>

<p><strong>回滚</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment/demoservice --namespace=demospace</span><br></pre></td></tr></table></figure>

<p>回滚到指定版本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment/demoservice --to-revision=2 --namespace=demospace</span><br></pre></td></tr></table></figure>

<p><strong>查看历史</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe deployment/demoservice --namespace=demospace</span><br></pre></td></tr></table></figure>

<h1 id="设置配额"><a href="#设置配额" class="headerlink" title="设置配额"></a>设置配额</h1><h2 id="配置Namespace资源限制"><a href="#配置Namespace资源限制" class="headerlink" title="配置Namespace资源限制"></a>配置Namespace资源限制</h2><p>中文文档：<a href="http://docs.kubernetes.org.cn/746.html" target="_blank" rel="noopener">http://docs.kubernetes.org.cn/746.html</a></p>
<h2 id="配置容器资源限制"><a href="#配置容器资源限制" class="headerlink" title="配置容器资源限制"></a>配置容器资源限制</h2><p>对于一个pod来说，资源最基础的2个的指标就是：CPU和内存。<br>Kubernetes提供了个采用requests和limits 两种类型参数对资源进行预分配和使用限制。<br>limit 会限制pod的资源利用：</p>
<ul>
<li>当pod 内存超过limit时，会被oom。</li>
<li>当cpu超过limit时，不会被kill，但是会限制不超过limit值。</li>
</ul>
<h3 id="测试内存限制"><a href="#测试内存限制" class="headerlink" title="测试内存限制"></a>测试内存限制</h3><p>部署一个压测容器，压测时会分配250M内存，但实际pod的内存limit为100Mi</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: memory-demo</span><br><span class="line">  namespace: example</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: memory-demo-2-ctr</span><br><span class="line">    image: polinux/stress</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;50Mi&quot;</span><br><span class="line">      limits:</span><br><span class="line">        memory: &quot;100Mi&quot;</span><br><span class="line">    command: [&quot;stress&quot;]</span><br><span class="line">    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;250M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]</span><br></pre></td></tr></table></figure>

<p>部署后查看pod状态，可以看到pod被OOM，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  kubectl -n example get po</span><br><span class="line">NAME             READY     STATUS        RESTARTS   AGE</span><br><span class="line">memory-demo      0/1       OOMKilled     1          11s</span><br></pre></td></tr></table></figure>

<h3 id="测试CPU限制"><a href="#测试CPU限制" class="headerlink" title="测试CPU限制"></a>测试CPU限制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: cpu-demo</span><br><span class="line">  namespace: example</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: cpu-demo-ctr</span><br><span class="line">    image: vish/stress</span><br><span class="line">    resources:</span><br><span class="line">      limits:</span><br><span class="line">        cpu: &quot;1&quot;</span><br><span class="line">      requests:</span><br><span class="line">        cpu: &quot;0.5&quot;</span><br><span class="line">    args:</span><br><span class="line">    - -cpus</span><br><span class="line">    - &quot;2&quot;</span><br></pre></td></tr></table></figure>

<p>查看容器信息，可以看到pod 虽然不会被kill掉，但是实际使用cpu被限制只有1000m。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> kubectl -n example top po cpu-demo</span><br><span class="line">NAME       CPU(cores)   MEMORY(bytes)</span><br><span class="line">cpu-demo   1000m        0Mi</span><br></pre></td></tr></table></figure>

<h3 id="容器服务质量（QoS）"><a href="#容器服务质量（QoS）" class="headerlink" title="容器服务质量（QoS）"></a>容器服务质量（QoS）</h3><p>Kubernetes 提供服务质量管理，根据容器的资源配置，将pod 分为Guaranteed, Burstable, BestEffort 3个级别。当资源紧张时根据分级决定调度和驱逐策略，这三个分级分别代表：</p>
<ul>
<li>Guaranteed： pod中所有容器都设置了limit和request， 并且相等（设置limit后假如没有设置request会自动设置为limit值）</li>
<li>Burstable： pod中有容器未设置limit， 或者limit和request不相等。这种类型的pod在调度节点时， 可能出现节点超频的情况。</li>
<li>BestEffort： pod中没有任何容器设置request和limit。</li>
</ul>
<p>计算qos代码：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/helper/qos/qos.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/helper/qos/qos.go</a></p>
<h4 id="不同QoS对容器影响"><a href="#不同QoS对容器影响" class="headerlink" title="不同QoS对容器影响"></a>不同QoS对容器影响</h4><p><strong>oom：</strong></p>
<p>Kubernetes会根据QoS设置oom的评分调整参数<code>oom_score_adj</code>，oom_killer 根据 内存使用情况算出oom_score， 并且和<code>oom_score_adj</code>综合评价，进程的评分越高，当发生oom时越优先被kill。</p>
<table>
<thead>
<tr>
<th>QoS</th>
<th>oom_score_adj</th>
</tr>
</thead>
<tbody><tr>
<td>Guaranteed</td>
<td>-998</td>
</tr>
<tr>
<td>BestEffort</td>
<td>1000</td>
</tr>
<tr>
<td>Burstable</td>
<td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td>
</tr>
</tbody></table>
<p>当节点内存不足时，QoS为Guaranteed 的pod 最后被kill。 而BestEffort 级别的pod优先被kill。 其次是Burstable，根据计算公式 oom_score_adj 值范围2到999，设置的request越大，oom_score_adj越低，oom时保护程度越高。</p>
<p><strong>实践</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">节点信息：</span><br><span class="line"># kubectl describe no cn-beijing.i-2zeavb11mttnqnnicwj9 | grep -A 3 Capacity</span><br><span class="line">Capacity:</span><br><span class="line"> cpu:     4</span><br><span class="line"> memory:  8010196Ki</span><br><span class="line"> pods:    110</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: memory-demo-qos-1</span><br><span class="line">  namespace: example</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: memory-demo-qos-1</span><br><span class="line">    image: polinux/stress</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;200Mi&quot;</span><br><span class="line">    command: [&quot;stress&quot;]</span><br><span class="line">    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;50M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: memory-demo-qos-2</span><br><span class="line">  namespace: example</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: memory-demo-qos-2</span><br><span class="line">    image: polinux/stress</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;400Mi&quot;</span><br><span class="line">    command: [&quot;stress&quot;]</span><br><span class="line">    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;50M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: memory-demo-qos-3</span><br><span class="line">  namespace: example</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: memory-demo-qos-3</span><br><span class="line">    image: polinux/stress</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;200Mi&quot;</span><br><span class="line">        cpu: &quot;2&quot;</span><br><span class="line">      limits:</span><br><span class="line">        memory: &quot;200Mi&quot;</span><br><span class="line">        cpu: &quot;2&quot;</span><br><span class="line">    command: [&quot;stress&quot;]</span><br><span class="line">    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;50M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]</span><br></pre></td></tr></table></figure>

<p>单个节点可分配内存为8010196Ki， 大约7822.45Mi。<br>根据Burstable 的计算方式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">request 200Mi: (1000 - 1000*200/7822.45) 约为975</span><br><span class="line"></span><br><span class="line">request 400Mi: (1000 - 1000*400/7822.45) 约为950</span><br></pre></td></tr></table></figure>

<p>我们分别查看这3个pod的oom参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// request 200Mi</span><br><span class="line">  kubectl -n example exec  memory-demo-qos-1 cat /proc/1/oom_score_adj</span><br><span class="line">975</span><br><span class="line"></span><br><span class="line">// request 400Miß</span><br><span class="line">  kubectl -n example exec  memory-demo-qos-2 cat /proc/1/oom_score_adj</span><br><span class="line">949</span><br><span class="line"></span><br><span class="line">// Guaranteed</span><br><span class="line">  kubectl -n example exec  memory-demo-qos-3 cat /proc/1/oom_score_adj</span><br><span class="line">-998</span><br></pre></td></tr></table></figure>

<p>设置oom 规则代码：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/qos/policy.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/qos/policy.go</a></p>
<p><strong>pod 驱逐：</strong></p>
<p>当节点的内存和cpu资源不足，开始驱逐节点上的pod时。QoS同样会影响驱逐的优先级。顺序如下：</p>
<ol>
<li>kubelet 优先驱逐 BestEffort的pod 和 实际占用资源大于requests的Burstable pod。</li>
</ol>
<ul>
<li>接下来驱逐实际占用资源小于request的Burstable pod。</li>
<li>QoS为Guaranteed的pod最后驱逐， kubelet 会保证Guaranteed的pod 不会因为其他pod的资源消耗而被驱逐。</li>
<li>当QoS相同时，kubelet 根据<a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/" target="_blank" rel="noopener">Priority</a>计算驱逐的优先级</li>
</ul>
<h2 id="ResourceQuota"><a href="#ResourceQuota" class="headerlink" title="ResourceQuota"></a>ResourceQuota</h2><p>Kubernetes提供ResourceQuota对象，用于配置限制namespace内的每种类型的k8s对象数量和资源（cpu，内存）。</p>
<ul>
<li>一个namespace中可以创建一个或多个ResourceQuota</li>
<li>如果namespace中配置了ResourceQuota， 部署时必须设置request和limit， 否则会拒绝创建请求。</li>
<li>可以通过这是limitRange配置每个pod默认的requests和limits避免上述问题</li>
<li>1.10以后支持扩展资源 详见：<a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: mem-cpu-demo</span><br><span class="line">  namespace: example</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    requests.cpu: &quot;3&quot;</span><br><span class="line">    requests.memory: 1Gi</span><br><span class="line">    limits.cpu: &quot;5&quot;</span><br><span class="line">    limits.memory: 2Gi</span><br><span class="line">    pods: &quot;5&quot;</span><br></pre></td></tr></table></figure>

<h2 id="LimitRange"><a href="#LimitRange" class="headerlink" title="LimitRange"></a>LimitRange</h2><p>LimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值，以及大小范围。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: LimitRange</span><br><span class="line">metadata:</span><br><span class="line">  name: mem-limit-range</span><br><span class="line">  namespace: example</span><br><span class="line">spec:</span><br><span class="line">  limits:</span><br><span class="line">  - default:  # default limit</span><br><span class="line">      memory: 512Mi</span><br><span class="line">      cpu: 2</span><br><span class="line">    defaultRequest:  # default request</span><br><span class="line">      memory: 256Mi</span><br><span class="line">      cpu: 0.5</span><br><span class="line">    max:  # max limit</span><br><span class="line">      memory: 800Mi</span><br><span class="line">      cpu: 3</span><br><span class="line">    min:  # min request</span><br><span class="line">      memory: 100Mi</span><br><span class="line">      cpu: 0.3</span><br><span class="line">    maxLimitRequestRatio:  # max value for limit / request</span><br><span class="line">      memory: 2</span><br><span class="line">      cpu: 2</span><br><span class="line">    type: Container # limit type, support: Container / Pod / PersistentVolumeClaim</span><br></pre></td></tr></table></figure>

<p>limitRange支持的参数如下：</p>
<ul>
<li>default 代表默认的limit</li>
<li>defaultRequest 代表默认的request</li>
<li>max 代表limit的最大值</li>
<li>min 代表request的最小值</li>
<li>maxLimitRequestRatio 代表 limit / request的最大值。由于节点是根据pod request 调度资源，可以做到节点超卖，maxLimitRequestRatio 代表pod最大超卖比例。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>Kubernetes 提供request 和 limit 两种方式设置容器资源。</li>
<li>为了提高资源利用率，k8s调度时根据pod 的request值计算调度策略，从而实现节点资源超卖。</li>
<li>k8s根据limit限制pod使用资源，当内存超过limit时会触发oom。 且限制pod的cpu 不允许超过limit。</li>
<li>根据pod的 request和limit，k8s会为pod 计算服务质量，并分为Guaranteed, Burstable, BestEffort 这3级。当节点资源不足时，发生驱逐或者oom时， Guaranteed 级别的pod 优先保护， Burstable 节点次之（request越大，使用资源量越少 保护级别越高）， BestEffort 最先被驱逐。</li>
<li>Kubernetes提供了RequestQuota和LimitRange 用于设置namespace 内pod 的资源范围 和 规模总量。 RequestQuota 用于设置各种类型对象的数量， cpu和内存的总量。 LimitRange 用于设置pod或者容器 request和limit 的默认值，最大最小值， 以及超卖比例（limit / request）。</li>
<li>对于一些重要的线上应用，我们应该合理设置limit和request，limit和request 设置一致，资源不足时k8s会优先保证这些pod正常运行。</li>
<li>为了提高资源利用率。 对一些非核心，并且资源不长期占用的应用，可以适当减少pod的request，这样pod在调度时可以被分配到资源不是十分充裕的节点，提高使用率。但是当节点的资源不足时，也会优先被驱逐或被oom kill。</li>
</ul>
<h1 id="PV-amp-PVC"><a href="#PV-amp-PVC" class="headerlink" title="PV &amp; PVC"></a>PV &amp; PVC</h1><p>本质上，Kubernetes Volume 是一个目录，这一点与 Docker Volume 类似。当 Volume 被 mount 到 Pod，Pod 中的所有容器都可以访问这个 Volume。Kubernetes Volume 也支持多种 backend 类型，包括 emptyDir、hostPath、GCE Persistent Disk、AWS Elastic Block Store、NFS、Ceph 等，完整列表可参考 <a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes</a></p>
<h2 id="emptyDir"><a href="#emptyDir" class="headerlink" title="emptyDir"></a>emptyDir</h2><p>emptyDir 是最基础的 Volume 类型。正如其名字所示，一个 emptyDir Volume 是 Host 上的一个空目录。</p>
<p>emptyDir Volume 对于容器来说是持久的，对于 Pod 则不是。当 Pod 从节点删除时，Volume 的内容也会被删除。但如果只是容器被销毁而 Pod 还在，则 Volume 不受影响。</p>
<p>也就是说：emptyDir Volume 的生命周期与 Pod 一致。</p>
<p>Pod 中的所有容器都可以共享 Volume，它们可以指定各自的 mount 路径。下面通过例子来实践 emptyDir，配置文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: producer-consumer</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: producer</span><br><span class="line">    image: busybox</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: shared-volume</span><br><span class="line">      mountPath: /producer_dir</span><br><span class="line">    args:</span><br><span class="line">    - /bin/sh</span><br><span class="line">    - -c</span><br><span class="line">    - echo &quot;hello world&quot; &gt; /producer_dir/hello; sleep 30000</span><br><span class="line">  - name: consumer</span><br><span class="line">    image: busybox</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: shared-volume</span><br><span class="line">      mountPath: /consumer_dir</span><br><span class="line">    args:</span><br><span class="line">    - /bin/sh</span><br><span class="line">    - -c</span><br><span class="line">    - cat /consumer_dir/hello; sleep 30000</span><br><span class="line">  volumes:</span><br><span class="line">  - name: shared-volume</span><br><span class="line">    emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>这里我们模拟了一个 producer-consumer 场景。Pod 有两个容器 producer和 consumer，它们共享一个 Volume。producer 负责往 Volume 中写数据，consumer 则是从 Volume 读取数据。</p>
<p>文件最底部 volumes 定义了一个 emptyDir 类型的 Volume shared-volume。</p>
<p>producer 容器将 shared-volume mount 到 /producer_dir 目录。</p>
<p>producer 通过 echo 将数据写到文件 hello 里。</p>
<p>consumer 容器将 shared-volume mount 到 /consumer_dir 目录。</p>
<p>consumer 通过 cat 从文件 hello 读数据。</p>
<p>执行如下命令创建 Pod：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f emptydir.yaml </span><br><span class="line">pod/producer-consumer created</span><br><span class="line">[root@master ~]# kubectl get pods</span><br><span class="line">NAME                READY   STATUS    RESTARTS   AGE</span><br><span class="line">producer-consumer   2/2     Running   0          8s</span><br><span class="line">[root@master ~]# kubectl logs producer-consumer consumer</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure>

<p><code>kubectl logs</code> 显示容器 consumer 成功读到了 producer 写入的数据，验证了两个容器共享 emptyDir Volume。</p>
<p>emptyDir 是 Host 上创建的临时目录，其优点是能够方便地为 Pod 中的容器提供共享存储，不需要额外的配置。但它不具备持久性，如果 Pod 不存在了，emptyDir 也就没有了。根据这个特性，emptyDir 特别适合 Pod 中的容器需要临时共享存储空间的场景，比如前面的生产者消费者用例。</p>
<h2 id="hostPath"><a href="#hostPath" class="headerlink" title="hostPath"></a>hostPath</h2><p>hostPath Volume 的作用是将 Docker Host 文件系统中已经存在的目录 mount 给 Pod 的容器。大部分应用都不会使用 hostPath Volume，因为这实际上增加了 Pod 与节点的耦合，限制了 Pod 的使用。不过那些需要访问 Kubernetes 或 Docker 内部数据（配置文件和二进制库）的应用则需要使用 hostPath。</p>
<p>下面的例子，我们把主机上的目录<code>/data/pod/v1</code>挂载到 Pod 上容器的<code>/usr/share/nginx/html/</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-vol-hostPath</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: mytest</span><br><span class="line">    image: wangzan18/mytest:v1</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html</span><br><span class="line">      mountPath: /usr/share/nginx/html/</span><br><span class="line">  volumes:</span><br><span class="line">  - name: html</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /data/pod/v1</span><br><span class="line">      type: DirectoryOrCreate</span><br></pre></td></tr></table></figure>

<p>如果 Pod 被销毁了，hostPath 对应的目录也还会被保留，从这点看，hostPath 的持久性比 emptyDir 强。不过一旦 Host 崩溃，hostPath 也就没法访问了。</p>
<h2 id="PV-amp-PVC介绍"><a href="#PV-amp-PVC介绍" class="headerlink" title="PV&amp;PVC介绍"></a>PV&amp;PVC介绍</h2><p>PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需<br>关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。<br>pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。</p>
<h2 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h2><p>pv和pvc遵循以下生命周期：</p>
<ul>
<li>供应准备。通过集群外的存储系统或者云平台来提供存储持久化支持。<br>- 静态提供：管理员手动创建多个PV，供PVC使用。<br>- 动态提供：动态创建PVC特定的PV，并绑定。</li>
<li>绑定。用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。</li>
<li>使用。用户可在pod中像volume一样使用pvc。</li>
<li>释放。用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。</li>
<li>回收(Reclaiming)。pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。<br>- 保留策略：允许人工处理保留的数据。<br>- 删除策略：将删除pv和外部关联的存储资源，需要插件支持。<br>- 回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。</li>
</ul>
<blockquote>
<p>目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。</p>
</blockquote>
<h2 id="Provisioning"><a href="#Provisioning" class="headerlink" title="Provisioning"></a>Provisioning</h2><p>两种方式提供的PV资源供给：</p>
<ul>
<li><p>static</p>
<p>通过集群管理者创建多个PV，为集群“使用者”提供存储能力而隐藏真实存储的细节。并且存在于kubenretes api中，可被直接使用。</p>
</li>
<li><p>dynamic</p>
<p>动态卷供给是kubernetes独有的功能，这一功能允许按需创建存储建。在此之前，集群管理员需要事先在集群外由存储提供者或者云提供商创建存储卷，成功之后再创建PersistentVolume对象，才能够在kubernetes中使用。动态卷供给能让集群管理员不必进行预先创建存储卷，而是随着用户需求进行创建。在1.5版本提高了动态卷的弹性和可用性。在此前1.4版本中加入了一个 新的 API 对象 StorageClass，可以定义多个 StorageClass 对象，并可以分别指定存储插件、设置参数，用于提供不同的存储卷。这样的设计让集群管理员能够在同一个集群内，定义和提供不同类型的、不同参数的卷（相同或者不同的存储系统）。这样的设计还确保了最终用户在无需了解太多的情况下，有能力选择不同的存储选项。</p>
</li>
</ul>
<h2 id="PV类型"><a href="#PV类型" class="headerlink" title="PV类型"></a>PV类型</h2><p>pv支持以下类型:</p>
<ul>
<li>GCEPersistentDisk</li>
<li>AWSElasticBlockStore</li>
<li>NFS</li>
<li>iSCSI</li>
<li>RBD (Ceph Block Device)</li>
<li>Glusterfs</li>
<li>AzureFile</li>
<li>AzureDisk</li>
<li>CephFS</li>
<li>cinder</li>
<li>FC</li>
<li>FlexVolume</li>
<li>Flocker</li>
<li>PhotonPersistentDisk</li>
<li>Quobyte</li>
<li>VsphereVolume</li>
<li>HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)</li>
</ul>
<h2 id="PV属性"><a href="#PV属性" class="headerlink" title="PV属性:"></a>PV属性:</h2><ul>
<li><p>访问模式,与pv的语义相同。在请求资源时使用特定模式。</p>
<p><code>accessModes</code> 指定访问模式为 ReadWriteOnce，支持的访问模式有：</p>
<p><code>ReadWriteOnce</code> – PV 能以 read-write 模式 mount 到单个节点。</p>
<p><code>ReadOnlyMany</code> – PV 能以 read-only 模式 mount 到多个节点。</p>
<p><code>ReadWriteMany</code> – PV 能以 read-write 模式 mount 到多个节点。</p>
</li>
<li><p>资源,申请的存储资源数额。</p>
</li>
</ul>
<h2 id="PV卷阶段状态："><a href="#PV卷阶段状态：" class="headerlink" title="PV卷阶段状态："></a>PV卷阶段状态：</h2><ul>
<li>Available – 资源尚未被claim使用</li>
<li>Bound – 卷已经被绑定到claim了</li>
<li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li>
<li>Failed – 卷自动回收失败</li>
</ul>
<h2 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h2><p>创建pv</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: ebs-pv</span><br><span class="line">  labels:</span><br><span class="line">    type: amazonEBS</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 5Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  awsElasticBlockStore:</span><br><span class="line">    volumeID: vol-079c492115a7be6e1</span><br><span class="line">    fsType: ext4</span><br></pre></td></tr></table></figure>

<p>创建pvc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pvc</span><br><span class="line">  labels:</span><br><span class="line">    type: amazonEBS</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br></pre></td></tr></table></figure>

<p>创建deployment</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-with-pvc</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        service: nginx</span><br><span class="line">        app: test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx-with-pvc</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /test-ebs</span><br><span class="line">          name: my-pvc</span><br><span class="line">      volumes:</span><br><span class="line">      - name: my-pvc</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: nginx-pvc</span><br></pre></td></tr></table></figure>

<h2 id="回收策略"><a href="#回收策略" class="headerlink" title="回收策略"></a>回收策略</h2><p><code>PersistentVolumes</code> 可以有多种回收策略，包括 “Retain”、”Recycle” 和 “Delete”。对于动态配置的 <code>PersistentVolumes</code>来说，默认回收策略为 “Delete”。这表示当用户删除对应的 <code>PersistentVolumeClaim</code> 时，动态配置的 volume 将被自动删除。如果 volume 包含重要数据时，这种自动行为可能是不合适的。那种情况下，更适合使用 “Retain” 策略。使用 “Retain” 时，如果用户删除 <code>PersistentVolumeClaim</code>，对应的 <code>PersistentVolume</code> 不会被删除。相反，它将变为 <code>Released</code> 状态，表示所有的数据可以被手动恢复。</p>
<p><strong>示例：</strong></p>
<p>pvc.yml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-test</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: ceph-rbd </span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure>

<p>deployment.yml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-rbd</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx</span><br><span class="line">          image: nginx</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          ports:</span><br><span class="line">            - containerPort: 80</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: ceph-rbd-volume</span><br><span class="line">              mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">      volumes:</span><br><span class="line">      - name: ceph-rbd-volume</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: pvc-test</span><br></pre></td></tr></table></figure>

<p>新建pvc、deployment、写入数据并删除pvc操作过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@lab1 test]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 root root 533 Oct 24 17:54 nginx.yaml</span><br><span class="line">-rw-r--r-- 1 root root 187 Oct 24 17:55 pvc.yaml</span><br><span class="line">[root@lab1 test]# kubectl apply -f pvc.yaml </span><br><span class="line">persistentvolumeclaim/pvc-test created</span><br><span class="line">[root@lab1 test]# kubectl get pvc </span><br><span class="line">NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-test           Bound    pvc-069c4486-d773-11e8-bd12-000c2931d938   1Gi        RWO            ceph-rbd       7s</span><br><span class="line">[root@lab1 test]# kubectl apply -f nginx.yaml </span><br><span class="line">deployment.extensions/nginx-rbd created</span><br><span class="line">[root@lab1 test]# kubectl get pod |grep nginx-rbd</span><br><span class="line">nginx-rbd-7c6449886-thv25           1/1     Running   0          33s</span><br><span class="line">[root@lab1 test]# kubectl exec -it nginx-rbd-7c6449886-thv25 -- /bin/bash -c &apos;echo ygqygq2 &gt; /usr/share/nginx/html/ygqygq2.html&apos;        </span><br><span class="line">[root@lab1 test]# kubectl exec -it nginx-rbd-7c6449886-thv25 -- cat /usr/share/nginx/html/ygqygq2.html</span><br><span class="line">ygqygq2</span><br><span class="line">[root@lab1 test]# kubectl delete -f nginx.yaml </span><br><span class="line">deployment.extensions &quot;nginx-rbd&quot; deleted</span><br><span class="line">[root@lab1 test]# kubectl get pvc pvc-test     </span><br><span class="line">NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-test   Bound    pvc-069c4486-d773-11e8-bd12-000c2931d938   1Gi        RWO            ceph-rbd       4m10s</span><br><span class="line">[root@lab1 test]# kubectl delete pvc pvc-test  # 删除PVC</span><br><span class="line">persistentvolumeclaim &quot;pvc-test&quot; deleted</span><br><span class="line">[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS   REASON   AGE</span><br><span class="line">pvc-069c4486-d773-11e8-bd12-000c2931d938   1Gi        RWO            Retain           Released   default/pvc-test   ceph-rbd                4m33s</span><br><span class="line">[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938 -o yaml &gt; /tmp/pvc-069c4486-d773-11e8-bd12-000c2931d938.yaml  # 保留备用</span><br></pre></td></tr></table></figure>

<p>从上面可以看到，pvc删除后，pv变成Released状态。</p>
<p>再次创建同名PVC，查看是否分配原来PV操作过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@lab1 test]# kubectl apply -f pvc.yaml </span><br><span class="line">persistentvolumeclaim/pvc-test created</span><br><span class="line">[root@lab1 test]# kubectl get pvc  # 查看新建的PVC              </span><br><span class="line">NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-test           Bound    pvc-f2df48ea-d773-11e8-b6c8-000c29ea3e30   1Gi        RWO            ceph-rbd       19s</span><br><span class="line">[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938  # 查看原来的PV</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS   REASON   AGE</span><br><span class="line">pvc-069c4486-d773-11e8-bd12-000c2931d938   1Gi        RWO            Retain           Released   default/pvc-test   ceph-rbd                7m18s</span><br></pre></td></tr></table></figure>

<p>从上面可以看到，PVC分配的是新的PV，因为PV状态不是Available。</p>
<p>那怎么才能让PV状态变成Available呢？我们来查看之前的PV：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[root@lab1 test]# cat /tmp/pvc-069c4486-d773-11e8-bd12-000c2931d938.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    pv.kubernetes.io/provisioned-by: ceph.com/rbd</span><br><span class="line">    rbdProvisionerIdentity: ceph.com/rbd</span><br><span class="line">  creationTimestamp: 2018-10-24T09:56:06Z</span><br><span class="line">  finalizers:</span><br><span class="line">  - kubernetes.io/pv-protection</span><br><span class="line">  name: pvc-069c4486-d773-11e8-bd12-000c2931d938</span><br><span class="line">  resourceVersion: &quot;11752758&quot;</span><br><span class="line">  selfLink: /api/v1/persistentvolumes/pvc-069c4486-d773-11e8-bd12-000c2931d938</span><br><span class="line">  uid: 06b57ef7-d773-11e8-bd12-000c2931d938</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi</span><br><span class="line">  claimRef:</span><br><span class="line">    apiVersion: v1</span><br><span class="line">    kind: PersistentVolumeClaim</span><br><span class="line">    name: pvc-test</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;11751559&quot;</span><br><span class="line">    uid: 069c4486-d773-11e8-bd12-000c2931d938</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  rbd:</span><br><span class="line">    fsType: ext4</span><br><span class="line">    image: kubernetes-dynamic-pvc-06a25bd3-d773-11e8-8c3e-0a580af400d5</span><br><span class="line">    keyring: /etc/ceph/keyring</span><br><span class="line">    monitors:</span><br><span class="line">    - 192.168.105.92:6789</span><br><span class="line">    - 192.168.105.93:6789</span><br><span class="line">    - 192.168.105.94:6789</span><br><span class="line">    pool: kube</span><br><span class="line">    secretRef:</span><br><span class="line">      name: ceph-secret</span><br><span class="line">      namespace: kube-system</span><br><span class="line">    user: kube</span><br><span class="line">  storageClassName: ceph-rbd</span><br><span class="line">status:</span><br><span class="line">  phase: Released</span><br></pre></td></tr></table></figure>

<p>从上面可以看到，spec.claimRef这段，仍保留之前的PVC信息。</p>
<p>我们大胆删除spec.claimRef这段。再次查看PV：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit pv pvc-069c4486-d773-11e8-bd12-000c2931d938</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@lab1 test]# kubectl get pv pvc-069c4486-d773-11e8-bd12-000c2931d938 </span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">pvc-069c4486-d773-11e8-bd12-000c2931d938   1Gi        RWO            Retain           Available           ceph-rbd                10m</span><br></pre></td></tr></table></figure>

<p>从上面可以看到，之前的PV <code>pvc-069c4486-d773-11e8-bd12-000c2931d938</code>已经变为Available。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>当前版本Kubernetes PVC存储大小是唯一能被设置或请求的资源，因我们没有修改PVC的大小，在PV的Available状态下，有PVC请求分配相同大小时，PV会被分配出去并绑定成功。<br>在PV变成Available过程中，最关键的是PV的<code>spec.claimRef</code>字段，该字段记录着原来PVC的绑定信息，删除绑定信息，即可重新释放PV从而达到Available。</p>
<h1 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h1><p>在kubeconfig配置文件中设置一个环境项。 如果指定了一个已存在的名字，将合并新字段并覆盖旧字段。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl config set-context NAME [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace]</span><br></pre></td></tr></table></figure>

<p><strong>示例</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 设置gce环境项中的user字段，不影响其他字段。</span><br><span class="line">$ kubectl config set-context gce --user=cluster-admin --namespace=test --cluster=test</span><br></pre></td></tr></table></figure>

<p><strong>选项</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--cluster=&quot;&quot;: 设置kuebconfig配置文件中环境选项中的集群。</span><br><span class="line">--namespace=&quot;&quot;: 设置kuebconfig配置文件中环境选项中的命名空间。</span><br><span class="line">--user=&quot;&quot;: 设置kuebconfig配置文件中环境选项中的用户。</span><br></pre></td></tr></table></figure>

<h1 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h1><h2 id="存储类介绍"><a href="#存储类介绍" class="headerlink" title="存储类介绍"></a>存储类介绍</h2><p>Kubernetes集群管理员通过提供不同的存储类，可以满足用户不同的服务质量级别、备份策略和任意策略要求的存储需求。动态存储卷供应使用StorageClass进行实现，其允许存储卷按需被创建。如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。</p>
<p>基于StorageClass的动态存储供应整体过程如下图所示：</p>
<p><img src="https://gongzhao-1256784911.cos.ap-shanghai.myqcloud.com/hexo/images/kubernetes/kubernetes08.png" alt="img"></p>
<p>1）集群管理员预先创建存储类（StorageClass）；</p>
<p>2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)；</p>
<p>3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)；</p>
<p>4）系统读取存储类的信息；</p>
<p>5）系统基于存储类的信息，在后台自动创建PVC需要的PV；</p>
<p>6）用户创建一个使用PVC的Pod；</p>
<p>7）Pod中的应用通过PVC进行数据的持久化；</p>
<p>8）而PVC使用PV进行数据的最终持久化处理。</p>
<h3 id="定义存储类"><a href="#定义存储类" class="headerlink" title="定义存储类"></a>定义存储类</h3><p>每一个存储类都包含provisioner、parameters和reclaimPolicy这三个参数域，当一个属于某个类的PersistentVolume需要被动态提供时，将会使用上述的参数域。</p>
<p>存储类对象的名称非常重要，用户通过名称类请求特定的存储类。管理员创建存储类对象时，会设置类的名称和其它的参数，存储类的对象一旦被创建，将不能被更新。管理员能够为PVC指定一个默认的存储类。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: standard</span><br><span class="line"># 指定存储类的供应者</span><br><span class="line">provisioner: kubernetes.io/aws-ebs</span><br><span class="line">parameters:</span><br><span class="line">  type: gp2</span><br><span class="line"># 指定回收策略</span><br><span class="line">reclaimPolicy: Retain</span><br><span class="line">mountOptions:</span><br><span class="line">  - debug</span><br></pre></td></tr></table></figure>

<h3 id="供应者"><a href="#供应者" class="headerlink" title="供应者"></a>供应者</h3><p>存储类有一个供应者的参数域，此参数域决定PV使用什么存储卷插件。参数必需进行设置：</p>
<table>
<thead>
<tr>
<th><strong>存储卷</strong></th>
<th><strong>内置供应者</strong></th>
<th><strong>配置例子</strong></th>
</tr>
</thead>
<tbody><tr>
<td>AWSElasticBlockStore</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#aws" target="_blank" rel="noopener">AWS</a></td>
</tr>
<tr>
<td>AzureFile</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-file" target="_blank" rel="noopener">Azure File</a></td>
</tr>
<tr>
<td>AzureDisk</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-disk" target="_blank" rel="noopener">Azure Disk</a></td>
</tr>
<tr>
<td>CephFS</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>Cinder</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#openstack-cinder" target="_blank" rel="noopener">OpenStack Cinder</a></td>
</tr>
<tr>
<td>FC</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>FlexVolume</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>Flocker</td>
<td>✓</td>
<td>–</td>
</tr>
<tr>
<td>GCEPersistentDisk</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#gce" target="_blank" rel="noopener">GCE</a></td>
</tr>
<tr>
<td>Glusterfs</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs" target="_blank" rel="noopener">Glusterfs</a></td>
</tr>
<tr>
<td>iSCSI</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>PhotonPersistentDisk</td>
<td>✓</td>
<td>–</td>
</tr>
<tr>
<td>Quobyte</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#quobyte" target="_blank" rel="noopener">Quobyte</a></td>
</tr>
<tr>
<td>NFS</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>RBD</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd" target="_blank" rel="noopener">Ceph RBD</a></td>
</tr>
<tr>
<td>VsphereVolume</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#vsphere" target="_blank" rel="noopener">vSphere</a></td>
</tr>
<tr>
<td>PortworxVolume</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#portworx-volume" target="_blank" rel="noopener">Portworx Volume</a></td>
</tr>
<tr>
<td>ScaleIO</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#scaleio" target="_blank" rel="noopener">ScaleIO</a></td>
</tr>
<tr>
<td>StorageOS</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#storageos" target="_blank" rel="noopener">StorageOS</a></td>
</tr>
<tr>
<td>Local</td>
<td>–</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#local" target="_blank" rel="noopener">Local</a></td>
</tr>
</tbody></table>
<p>Kubernetes的存储类并不局限于表中的“interneal”供应者，“interneal”供应者的名称带有“kubernetes.io”前缀；也可以允许和指定外部的供应者，外部供应者通过独立的程序进行实现。外部供应者的作者对代码在何处生存、如何供应、如何运行、使用什么卷插件（包括Flex）等有充分的判断权，kubernetes-incubator/external-storage仓库中存在编写外部提供者的类库。例如，NFS不是内部的供应者，但也是可以使用。在kubernetes-incubator/external-storage仓库中以列表的形式展示了一些外部的供应者，一些第三方供应商也提供了他们自己的外部供应者。</p>
<h3 id="提供者的参数"><a href="#提供者的参数" class="headerlink" title="提供者的参数"></a>提供者的参数</h3><p>存储类存在很多描述存储卷的参数，依赖不同的提供者可能有不同的参数。例如，对于type参数，它的值可能为io1。当一个参数被省略，则使用默认的值。</p>
<h3 id="回收策略-1"><a href="#回收策略-1" class="headerlink" title="回收策略"></a>回收策略</h3><p>通过存储类创建的持久化存储卷通过reclaimPolicy参数来指定，它的值可以是Delete或者Retain，默认为Delete。对于通过手工创建的，并使用存储类进行管理的持久化存储卷，将使用任何在创建时指定的存储卷。</p>
<h3 id="挂接选项"><a href="#挂接选项" class="headerlink" title="挂接选项"></a>挂接选项</h3><p>通过存储类动态创建的持久化存储卷，会存在一个通过mountOptions参数指定的挂接选择。如果存储卷插件不支持指定的挂接选项，这提供存储供应就会失败，在存储类或者PV中都不会对挂接选项进行验证，因此需要在设置时进行确认。</p>
<h2 id="使用存储类"><a href="#使用存储类" class="headerlink" title="使用存储类"></a>使用存储类</h2><p>动态存储卷供应基于StorageClass的API对象的来实现，集群管理员能够按需定义StorageClass对象，每一个StorageClass对象能够指定一个存储卷插件（即供应者）。集群管理员能够在一个集群中定义各种存储卷供应，用户不需要了解存储的细节和复杂性，就能够选择符合自己要求的存储。</p>
<h3 id="启用动态供应"><a href="#启用动态供应" class="headerlink" title="启用动态供应"></a>启用动态供应</h3><p>为了启用动态供应，集群管理员需要预先为用户创建一个或者多个存储类对象。存储类对象定义了使用哪个供应者，以及供应者相关的参数。下面是存储类的一个示例，它创建一个名称为slow的存储类，使用gce供应者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: slow</span><br><span class="line">provisioner: kubernetes.io/gce-pd</span><br><span class="line">parameters:</span><br><span class="line">  type: pd-standard</span><br></pre></td></tr></table></figure>

<p>下面创建了一个名为“fast”的存储类，其提供类似固态磁盘的存储卷磁盘：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: fast</span><br><span class="line">provisioner: kubernetes.io/gce-pd</span><br><span class="line">parameters:</span><br><span class="line">  type: pd-ssd</span><br></pre></td></tr></table></figure>

<h3 id="使用动态供应"><a href="#使用动态供应" class="headerlink" title="使用动态供应"></a>使用动态供应</h3><p>用户通过在PersistentVolumeClaim中包含一个存储类，来请求动态供应存储。在Kubernetes v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储；在v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。</p>
<p>下面是请求fast存储类的持久化存储卷声明的YAML配置文件示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: claim1</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line"># 指定所使用的存储类，此存储类将会自动创建符合要求的PV</span><br><span class="line"> storageClassName: fast</span><br><span class="line"> resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 30Gi</span><br></pre></td></tr></table></figure>

<p>此声明将使用类似于固态存储磁盘，当持久化存储卷声明被删除后，存储卷也将会被销毁。</p>
<h3 id="默认行为"><a href="#默认行为" class="headerlink" title="默认行为"></a>默认行为</h3><p>如果Kubernetes的集群中没有指定存储类，集群管理员可以通过执行下面的设置，启用默认的存储类：</p>
<ul>
<li>标记一个默认的StorageClass对象；</li>
<li>确定API server中DefaultStorage接入控制器已被启用</li>
</ul>
<p>管理员能够通过添加storageclass.kubernetes.io/is-default-class注释，标记一个特定的StorageClass作为默认的存储类。在集群中，如果存在一个默认的StorageClass，系统将能够在不指定storageClassName 的情况下创建一个PersistentVolume，DefaultStorageClass接入控制器会自动将storageClassName指向默认的存储类。注意：在一个集群中，最多只能有一个默认的存储类，如果没有默认的存储类，那么如果在PersistentVolumeClaim中没有显示指定storageClassName，则将无法创建PersistentVolume。</p>
<h2 id="NFS存储类示例"><a href="#NFS存储类示例" class="headerlink" title="NFS存储类示例"></a>NFS存储类示例</h2><h3 id="部署nfs-provisioner"><a href="#部署nfs-provisioner" class="headerlink" title="部署nfs-provisioner"></a>部署nfs-provisioner</h3><p>为nfs-provisioner实例选择存储状态和数据的存储卷，并将存储卷挂接到容器的/export</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> volumeMounts:</span><br><span class="line">    - name: export-volume</span><br><span class="line">      mountPath: /export</span><br><span class="line">volumes:</span><br><span class="line">  - name: export-volume</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /tmp/nfs-provisioner</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>为StorageClass选择一个供应者名称，并在deploy/kubernetes/deployment.yaml进行设置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">args:</span><br><span class="line">  - &quot;-provisioner=example.com/nfs&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>完整的deployment.yaml文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-provisioner</span><br><span class="line">  labels:</span><br><span class="line">    app: nfs-provisioner</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">    - name: nfs</span><br><span class="line">      port: 2049</span><br><span class="line">    - name: mountd</span><br><span class="line">      port: 20048</span><br><span class="line">    - name: rpcbind</span><br><span class="line">      port: 111</span><br><span class="line">    - name: rpcbind-udp</span><br><span class="line">      port: 111</span><br><span class="line">      protocol: UDP</span><br><span class="line">  selector:</span><br><span class="line">    app: nfs-provisioner</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-provisioner</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: nfs-provisioner</span><br><span class="line">          image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.8</span><br><span class="line">          ports:</span><br><span class="line">            - name: nfs</span><br><span class="line">              containerPort: 2049</span><br><span class="line">            - name: mountd</span><br><span class="line">              containerPort: 20048</span><br><span class="line">            - name: rpcbind</span><br><span class="line">              containerPort: 111</span><br><span class="line">            - name: rpcbind-udp</span><br><span class="line">              containerPort: 111</span><br><span class="line">              protocol: UDP</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              add:</span><br><span class="line">                - DAC_READ_SEARCH</span><br><span class="line">                - SYS_RESOURCE</span><br><span class="line">          args:</span><br><span class="line">            # 定义提供者的名称，存储类通过此名称指定提供者</span><br><span class="line">            - &quot;-provisioner=nfs-provisioner&quot;</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_IP</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: status.podIP</span><br><span class="line">            - name: SERVICE_NAME</span><br><span class="line">              value: nfs-provisioner</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          imagePullPolicy: &quot;IfNotPresent&quot;</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: export-volume</span><br><span class="line">              mountPath: /export</span><br><span class="line">      volumes:</span><br><span class="line">        - name: export-volume</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /srv</span><br></pre></td></tr></table></figure>

<p>在设置好deploy/kubernetes/deployment.yaml文件后，通过kubectl create命令在Kubernetes集群中部署nfs-provisioner。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f &#123;path&#125;/deployment.yaml</span><br></pre></td></tr></table></figure>

<h3 id="创建StorageClass"><a href="#创建StorageClass" class="headerlink" title="创建StorageClass"></a>创建StorageClass</h3><p>下面是example-nfs的StorageClass配置文件，此配置文件定义了一个名称为nfs-storageclass的存储类，此存储类的提供者为nfs-provisioner。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-storageclass</span><br><span class="line">  provisioner: nfs-provisioner</span><br></pre></td></tr></table></figure>

<p>通过kubectl create -f命令使用上面的配置文件创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f deploy/kubernetes/class.yaml</span><br></pre></td></tr></table></figure>

<p>storageclass “example-nfs” created</p>
<p>在存储类被正确创建后，就可以创建PersistenetVolumeClaim来请求StorageClass，而StorageClass将会为PersistenetVolumeClaim自动创建一个可用PersistentVolume。</p>
<h3 id="创建PersistenetVolumeClaim"><a href="#创建PersistenetVolumeClaim" class="headerlink" title="创建PersistenetVolumeClaim"></a>创建PersistenetVolumeClaim</h3><p>PersistenetVolumeClaim是对PersistenetVolume的声明，即PersistenetVolume为存储的提供者，而PersistenetVolumeClaim为存储的消费者。下面是PersistentVolumeClaim的YAML配置文件，此配置文件通过<em>spec.storageClassName</em>字段指定所使用的存储储类。</p>
<p>在此配置文件中，使用<em>nfs-storageclass</em>存储类为PersistenetVolumeClaim创建PersistenetVolume，所要求的PersistenetVolume存储空间大小为1Mi，可以被多个容器进行读取和写入操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  storageClassName： nfs-storageclass</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Mi</span><br></pre></td></tr></table></figure>

<p>通过kubectl create命令创建上述的持久化存储卷声明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f &#123;path&#125;/claim.yaml</span><br></pre></td></tr></table></figure>

<h3 id="创建使用PersistenVolumeClaim的部署"><a href="#创建使用PersistenVolumeClaim的部署" class="headerlink" title="创建使用PersistenVolumeClaim的部署"></a>创建使用PersistenVolumeClaim的部署</h3><p> 在这里定义名为busybox-deployment的部署YAML配置文件，使用的镜像为busybox。基于busybox镜像的容器需要对<strong>/mnt</strong>目录下的数据进行持久化，在YAML文件指定使用名称为nfs的PersistenVolumeClaim对容器的数据进行持久化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># This mounts the nfs volume claim into /mnt and continuously</span><br><span class="line"># overwrites /mnt/index.html with the time and hostname of the pod. </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:  </span><br><span class="line">  name: busybox-deployment</span><br><span class="line">spec:  </span><br><span class="line">  replicas: 2  </span><br><span class="line">  selector:    </span><br><span class="line">    name: busybox-deployment</span><br><span class="line">  template:    </span><br><span class="line">    metadata:      </span><br><span class="line">      labels:        </span><br><span class="line">        name: busybox-deployment    </span><br><span class="line">    spec:      </span><br><span class="line">      containers:      </span><br><span class="line">      - image: busybox        </span><br><span class="line">        command:          </span><br><span class="line">        - sh          </span><br><span class="line">        - -c          </span><br><span class="line">        - &apos;while true; do date &gt; /mnt/index.html; hostname &gt;&gt; /mnt/index.html; sleep $(($RANDOM % 5 + 5)); done&apos;        </span><br><span class="line">        imagePullPolicy: IfNotPresent        </span><br><span class="line">        name: busybox        </span><br><span class="line">        volumeMounts:          </span><br><span class="line">        # name must match the volume name below          </span><br><span class="line">        - name: nfs            </span><br><span class="line">          mountPath: &quot;/mnt&quot;     </span><br><span class="line">     # </span><br><span class="line">     volumes:      </span><br><span class="line">     - name: nfs        </span><br><span class="line">       persistentVolumeClaim:          </span><br><span class="line">         claimName: nfs-pvc</span><br></pre></td></tr></table></figure>

<p>通过kubectl create创建busy-deployment部署：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f &#123;path&#125;/nfs-busybox-deployment.yaml</span><br></pre></td></tr></table></figure>

<h1 id="liveness和readiness探针"><a href="#liveness和readiness探针" class="headerlink" title="liveness和readiness探针"></a>liveness和readiness探针</h1><p>官方文档：<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></p>
<p>当你使用kuberentes的时候，有没有遇到过Pod在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过kubernetes是如何检测pod是否还存活？虽然容器已经启动，但是kubernetes如何知道容器的进程是否准备好对外提供服务了呢？</p>
<p>Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去（谁的程序还没几个bug呢）。</p>
<p>Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。</p>
<h2 id="livenessProbe"><a href="#livenessProbe" class="headerlink" title="livenessProbe"></a>livenessProbe</h2><h3 id="定义-liveness命令"><a href="#定义-liveness命令" class="headerlink" title="定义 liveness命令"></a>定义 liveness命令</h3><p>许多长时间运行的应用程序最终会转换到broken状态，除非重新启动，否则无法恢复。Kubernetes提供了liveness probe来检测和补救这种情况。</p>
<p>在本次实验中，你将基于 <code>gcr.io/google_containers/busybox</code>镜像创建运行一个容器的Pod。以下是Pod的配置文件<code>exec-liveness.yaml</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    test: liveness</span><br><span class="line">  name: liveness-exec</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    args:</span><br><span class="line">    - /bin/sh</span><br><span class="line">    - -c</span><br><span class="line">    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600</span><br><span class="line">    image: gcr.io/google_containers/busybox</span><br><span class="line">    livenessProbe:</span><br><span class="line">      exec:</span><br><span class="line">        command:</span><br><span class="line">        - cat</span><br><span class="line">        - /tmp/healthy</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>该配置文件给<a href="https://www.kubernetes.org.cn/tags/pod" target="_blank" rel="noopener">Pod</a>配置了一个容器。<code>periodSeconds</code> 规定kubelet要每隔5秒执行一次liveness probe。 <code>initialDelaySeconds</code> 告诉kubelet在第一次执行probe之前要的等待5秒钟。探针检测命令是在容器中执行 <code>cat /tmp/healthy</code> 命令。如果命令执行成功，将返回0，kubelet就会认为该容器是活着的并且很健康。如果返回非0值，kubelet就会杀掉这个容器并重启它。</p>
<p>容器启动时，执行该命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/sh -c &quot;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&quot;</span><br></pre></td></tr></table></figure>

<p>在容器生命的最初30秒内有一个 <code>/tmp/healthy</code> 文件，在这30秒内 <code>cat /tmp/healthy</code>命令会返回一个成功的返回码。30秒后， <code>cat /tmp/healthy</code> 将返回失败的返回码。</p>
<p>创建Pod：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/exec-liveness.yaml</span><br></pre></td></tr></table></figure>

<p>在30秒内，查看Pod的event：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod liveness-exec</span><br></pre></td></tr></table></figure>

<p>结果显示没有失败的liveness probe：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FirstSeen    LastSeen    Count   From            SubobjectPath           Type        Reason      Message</span><br><span class="line">--------- --------    -----   ----            -------------           --------    ------      -------</span><br><span class="line">24s       24s     1   &#123;default-scheduler &#125;                    Normal      Scheduled   Successfully assigned liveness-exec to worker0</span><br><span class="line">23s       23s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Pulling     pulling image &quot;gcr.io/google_containers/busybox&quot;</span><br><span class="line">23s       23s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Pulled      Successfully pulled image &quot;gcr.io/google_containers/busybox&quot;</span><br><span class="line">23s       23s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]</span><br><span class="line">23s       23s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Started     Started container with docker id 86849c15382e</span><br></pre></td></tr></table></figure>

<p>启动35秒后，再次查看pod的event：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod liveness-exec</span><br></pre></td></tr></table></figure>

<p>在最下面有一条信息显示liveness probe失败，容器被删掉并重新创建。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message</span><br><span class="line">--------- --------    -----   ----            -------------           --------    ------      -------</span><br><span class="line">37s       37s     1   &#123;default-scheduler &#125;                    Normal      Scheduled   Successfully assigned liveness-exec to worker0</span><br><span class="line">36s       36s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Pulling     pulling image &quot;gcr.io/google_containers/busybox&quot;</span><br><span class="line">36s       36s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Pulled      Successfully pulled image &quot;gcr.io/google_containers/busybox&quot;</span><br><span class="line">36s       36s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]</span><br><span class="line">36s       36s     1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Normal      Started     Started container with docker id 86849c15382e</span><br><span class="line">2s        2s      1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Warning     Unhealthy   Liveness probe failed: cat: can&apos;t open &apos;/tmp/healthy&apos;: No such file or directory</span><br></pre></td></tr></table></figure>

<p>再等30秒，确认容器已经重启：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod liveness-exec</span><br></pre></td></tr></table></figure>

<p>从输出结果来<code>RESTARTS</code>值加1了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME            READY     STATUS    RESTARTS   AGE</span><br><span class="line">liveness-exec   1/1       Running   1          1m</span><br></pre></td></tr></table></figure>

<h3 id="定义一个liveness-HTTP请求"><a href="#定义一个liveness-HTTP请求" class="headerlink" title="定义一个liveness HTTP请求"></a>定义一个liveness HTTP请求</h3><p>我们还可以使用HTTP GET请求作为liveness probe。下面是一个基于<code>gcr.io/google_containers/liveness</code>镜像运行了一个容器的Pod的例子<code>http-liveness.yaml</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    test: liveness</span><br><span class="line">  name: liveness-http</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    args:</span><br><span class="line">    - /server</span><br><span class="line">    image: gcr.io/google_containers/liveness</span><br><span class="line">    livenessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: /healthz</span><br><span class="line">        port: 8080</span><br><span class="line">        httpHeaders:</span><br><span class="line">          - name: X-Custom-Header</span><br><span class="line">            value: Awesome</span><br><span class="line">      initialDelaySeconds: 3</span><br><span class="line">      periodSeconds: 3</span><br></pre></td></tr></table></figure>

<p>该配置文件只定义了一个容器，<code>livenessProbe</code> 指定kubelete需要每隔3秒执行一次liveness probe。<code>initialDelaySeconds</code> 指定kubelet在该执行第一次探测之前需要等待3秒钟。该探针将向容器中的server的8080端口发送一个HTTP GET请求。如果server的<code>/healthz</code>路径的handler返回一个成功的返回码，kubelet就会认定该容器是活着的并且很健康。如果返回失败的返回码，kubelet将杀掉该容器并重启它。</p>
<p>任何大于200小于400的返回码都会认定是成功的返回码。其他返回码都会被认为是失败的返回码。</p>
<p>查看该server的源码：<a href="http://k8s.io/docs/user-guide/liveness/image/server.go" target="_blank" rel="noopener">server.go</a>.</p>
<p>最开始的10秒该容器是活着的， <code>/healthz</code> handler返回200的状态码。这之后将返回500的返回码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">http.HandleFunc(&quot;/healthz&quot;, func(w http.ResponseWriter, r *http.Request) &#123;</span><br><span class="line">    duration := time.Now().Sub(started)</span><br><span class="line">    if duration.Seconds() &gt; 10 &#123;</span><br><span class="line">        w.WriteHeader(500)</span><br><span class="line">        w.Write([]byte(fmt.Sprintf(&quot;error: %v&quot;, duration.Seconds())))</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        w.WriteHeader(200)</span><br><span class="line">        w.Write([]byte(&quot;ok&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>容器启动3秒后，kubelet开始执行健康检查。第一次健康监测会成功，但是10秒后，健康检查将失败，kubelet将杀掉和重启容器。</p>
<p>创建一个Pod来测试一下HTTP liveness检测：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/http-liveness.yaml</span><br></pre></td></tr></table></figure>

<p>After 10 seconds, view Pod events to verify that liveness probes have failed and the Container has been restarted:</p>
<p>10秒后，查看Pod的event，确认liveness probe失败并重启了容器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod liveness-http</span><br></pre></td></tr></table></figure>

<h3 id="定义TCP-liveness探针"><a href="#定义TCP-liveness探针" class="headerlink" title="定义TCP liveness探针"></a>定义TCP liveness探针</h3><p>第三种liveness probe使用TCP Socket。 使用此配置，kubelet将尝试在指定端口上打开容器的套接字。 如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: goproxy</span><br><span class="line">  labels:</span><br><span class="line">    app: goproxy</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: goproxy</span><br><span class="line">    image: k8s.gcr.io/goproxy:0.1</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    readinessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 10</span><br><span class="line">    livenessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      periodSeconds: 20</span><br></pre></td></tr></table></figure>

<p>如您所见，TCP检查的配置与HTTP检查非常相似。 此示例同时使用了readiness和liveness probe。 容器启动后5秒钟，kubelet将发送第一个readiness probe。 这将尝试连接到端口8080上的goproxy容器。如果探测成功，则该pod将被标记为就绪。Kubelet将每隔10秒钟执行一次该检查。</p>
<p>除了readiness probe之外，该配置还包括liveness probe。 容器启动15秒后，kubelet将运行第一个liveness probe。 就像readiness probe一样，这将尝试连接到goproxy容器上的8080端口。如果liveness probe失败，容器将重新启动。</p>
<p><strong>使用命名的端口</strong></p>
<p>可以使用命名的ContainerPort作为HTTP或TCP liveness检查：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ports:</span><br><span class="line">- name: liveness-port</span><br><span class="line">  containerPort: 8080</span><br><span class="line">  hostPort: 8080</span><br><span class="line"></span><br><span class="line">livenessProbe:</span><br><span class="line">  httpGet:</span><br><span class="line">  path: /healthz</span><br><span class="line">  port: liveness-port</span><br></pre></td></tr></table></figure>

<h2 id="定义readiness探针"><a href="#定义readiness探针" class="headerlink" title="定义readiness探针"></a>定义readiness探针</h2><p>有时，应用程序暂时无法对外部流量提供服务。 例如，应用程序可能需要在启动期间加载大量数据或配置文件。 在这种情况下，你不想杀死应用程序，但你也不想发送请求。 Kubernetes提供了readiness probe来检测和减轻这些情况。 Pod中的容器可以报告自己还没有准备，不能处理Kubernetes服务发送过来的流量。</p>
<p>Readiness probe的配置跟liveness probe很像。唯一的不同是使用 <code>readinessProbe</code>而不是<code>livenessProbe</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">readinessProbe:</span><br><span class="line">  exec:</span><br><span class="line">    command:</span><br><span class="line">    - cat</span><br><span class="line">    - /tmp/healthy</span><br><span class="line">  initialDelaySeconds: 5</span><br><span class="line">  periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>Readiness probe的HTTP和TCP的探测器配置跟liveness probe一样。</p>
<p>Readiness和livenss probe可以并行用于同一容器。 使用两者可以确保流量无法到达未准备好的容器，并且容器在失败时重新启动。</p>
<h2 id="配置Probe"><a href="#配置Probe" class="headerlink" title="配置Probe"></a>配置Probe</h2><p>Probe中有很多精确和详细的配置，通过它们你能准确的控制liveness和readiness检查：</p>
<ul>
<li><code>initialDelaySeconds</code>：容器启动后第一次执行探测是需要等待多少秒。</li>
<li><code>periodSeconds</code>：执行探测的频率。默认是10秒，最小1秒。</li>
<li><code>timeoutSeconds</code>：探测超时时间。默认1秒，最小1秒。</li>
<li><code>successThreshold</code>：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。</li>
<li><code>failureThreshold</code>：探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。</li>
</ul>
<p>HTTP probe中可以给 <code>httpGet</code>设置其他配置项：</p>
<ul>
<li><code>host</code>：连接的主机名，默认连接到pod的IP。你可能想在http header中设置”Host”而不是使用IP。</li>
<li><code>scheme</code>：连接使用的schema，默认HTTP。</li>
<li><code>path</code>: 访问的HTTP server的path。</li>
<li><code>httpHeaders</code>：自定义请求的header。HTTP运行重复的header。</li>
<li><code>port</code>：访问的容器的端口名字或者端口号。端口号必须介于1和65525之间。</li>
</ul>
<p>对于HTTP探测器，kubelet向指定的路径和端口发送HTTP请求以执行检查。 Kubelet将probe发送到容器的IP地址，除非地址被<code>httpGet</code>中的可选<code>host</code>字段覆盖。 在大多数情况下，你不想设置主机字段。 有一种情况下你可以设置它。 假设容器在127.0.0.1上侦听，并且Pod的<code>hostNetwork</code>字段为true。 然后，在<code>httpGet</code>下的<code>host</code>应该设置为127.0.0.1。 如果你的pod依赖于虚拟主机，这可能是更常见的情况，你不应该是用<code>host</code>，而是应该在<code>httpHeaders</code>中设置<code>Host</code>头。</p>
<h1 id="初始化容器"><a href="#初始化容器" class="headerlink" title="初始化容器"></a>初始化容器</h1><h2 id="理解初始容器"><a href="#理解初始容器" class="headerlink" title="理解初始容器"></a>理解初始容器</h2><p>一个pod里可以运行多个容器,它也可以运行一个或者多个初始容器,初始容器先于应用容器运行,除了以下两点外,初始容器和普通容器没有什么两样:</p>
<ul>
<li>它们总是<code>run to completion</code></li>
<li>一个初始容器必须成功运行另一个才能运行</li>
</ul>
<p>如果pod中的一个初始容器运行失败,则kubernetes会尝试重启pod直到初始容器成功运行,如果pod的重启策略设置为<code>从不(never)</code>,则不会重启.</p>
<p>创建容器时,在podspec里添加<code>initContainers</code>字段,则指定容器即为初始容器,它们的返回状态作为数组保存在<code>.status.initContainerStatuses</code>里(与普通容器状态存储字段<code>.status.containerStatuses</code>类似)</p>
<h3 id="初始容器和普通容器的不同"><a href="#初始容器和普通容器的不同" class="headerlink" title="初始容器和普通容器的不同:"></a>初始容器和普通容器的不同:</h3><p>初始容器支持所有普通容器的特征,包括资源配额限制和存储卷以及安全设置.但是对资源申请和限制处理初始容器略有不同,下面会介绍.此外,初始容器不支持可用性探针(readiness probe),因为它在<code>ready</code>之前必须<code>run to completion</code></p>
<p>如果在一个pod里指定了多个初始容器,则它们会<code>依次</code>启动起来(pod内的普通容器并行启动),并且只有上一个成功下一个才能启动.当所有的初始容器都启动了,kubernetes才开始启普通应用容器.</p>
<h2 id="初始容器能做什么"><a href="#初始容器能做什么" class="headerlink" title="初始容器能做什么"></a>初始容器能做什么</h2><p>由于初始容器和普通应用容器是分开的镜像,因此他在做一些初始化工作很有优势:</p>
<ul>
<li>它们可以包含并且运行一些出于安全考虑不适合和应用放在一块的小工具.</li>
<li>它们可以一些小工具和自定义代码来做些初始化工作,这样就不需要在普通应用容器里使用<code>sed</code>,<code>awk</code>,<code>python</code>或者<code>dig</code>来做初始化工作了</li>
<li>应用构建者和发布者可以独立工作,而不必再联合起来处理同一个pod</li>
<li>它们使用linux <code>namespaces</code>因此它们和普通应用pod拥有不同的文件系统视图.因此他们可以被赋予普通应用容器获取不到的<code>secrets</code></li>
<li>它们在应用容器启动前运行,因此它们可以阻止或者延缓普通应用容器的初始化直到需要的条件满足</li>
</ul>
<p>示例:</p>
<ul>
<li>通过执行shell命令来等待一个服务创建完成,命令如下:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in &#123;1..100&#125;; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1</span><br></pre></td></tr></table></figure>

<ul>
<li>通过<code>downward API</code>把当前pod注册到远程服务器,命令如下:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &apos;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&apos;</span><br></pre></td></tr></table></figure>

<ul>
<li>在容器启动之前等待一定时间:例如<code>sleep 60</code></li>
<li>克隆一个git仓库到存储目录</li>
<li>通过模板工具动态把一些值写入到主应用程序的配置文件里.</li>
</ul>
<p>更多详细示例请查看pod应用环境<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/" target="_blank" rel="noopener">布置指南</a></p>
<h2 id="初始容器使用"><a href="#初始容器使用" class="headerlink" title="初始容器使用"></a>初始容器使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp-container</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo The app is running! &amp;&amp; sleep 3600&apos;]</span><br><span class="line">  initContainers:</span><br><span class="line">  - name: init-myservice</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&apos;sh&apos;, &apos;-c&apos;, &apos;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&apos;]</span><br><span class="line">  - name: init-mydb</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&apos;sh&apos;, &apos;-c&apos;, &apos;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&apos;]</span><br></pre></td></tr></table></figure>

<p>以上pod定义包含两个初始容器,第一个等待<code>myservice</code>服务可用,第二个等待<code>mydb</code>服务可用,这两个pod执行完成,应用容器开始执行.</p>
<p>下面是<code>myservice</code>和<code>mydb</code>两个服务的yaml文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: myservice</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 9376</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: mydb</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 9377</span><br></pre></td></tr></table></figure>

<p>上面定义的pod可以通过以下使用初始化和调试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f myapp.yaml</span><br><span class="line">pod/myapp-pod created</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get -f myapp.yaml</span><br><span class="line"></span><br><span class="line">NAME        READY     STATUS     RESTARTS   AGE</span><br><span class="line">myapp-pod   0/1       Init:0/2   0          6m</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Name:          myapp-pod</span><br><span class="line">Namespace:     default</span><br><span class="line">[...]</span><br><span class="line">Labels:        app=myapp</span><br><span class="line">Status:        Pending</span><br><span class="line">[...]</span><br><span class="line">Init Containers:</span><br><span class="line">  init-myservice:</span><br><span class="line">[...]</span><br><span class="line">    State:         Running</span><br><span class="line">[...]</span><br><span class="line">  init-mydb:</span><br><span class="line">[...]</span><br><span class="line">    State:         Waiting</span><br><span class="line">      Reason:      PodInitializing</span><br><span class="line">    Ready:         False</span><br><span class="line">[...]</span><br><span class="line">Containers:</span><br><span class="line">  myapp-container:</span><br><span class="line">[...]</span><br><span class="line">    State:         Waiting</span><br><span class="line">      Reason:      PodInitializing</span><br><span class="line">    Ready:         False</span><br><span class="line">[...]</span><br><span class="line">Events:</span><br><span class="line">  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message</span><br><span class="line">  ---------    --------    -----    ----                      -------------                           --------      ------        -------</span><br><span class="line">  16s          16s         1        &#123;default-scheduler &#125;                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201</span><br><span class="line">  16s          16s         1        &#123;kubelet 172.17.4.201&#125;    spec.initContainers&#123;init-myservice&#125;     Normal        Pulling       pulling image &quot;busybox&quot;</span><br><span class="line">  13s          13s         1        &#123;kubelet 172.17.4.201&#125;    spec.initContainers&#123;init-myservice&#125;     Normal        Pulled        Successfully pulled image &quot;busybox&quot;</span><br><span class="line">  13s          13s         1        &#123;kubelet 172.17.4.201&#125;    spec.initContainers&#123;init-myservice&#125;     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]</span><br><span class="line">  13s          13s         1        &#123;kubelet 172.17.4.201&#125;    spec.initContainers&#123;init-myservice&#125;     Normal        Started       Started container with docker id 5ced34a04634</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs myapp-pod -c init-myservice # Inspect the first init container</span><br><span class="line">kubectl logs myapp-pod -c init-mydb      # Inspect the second init container</span><br></pre></td></tr></table></figure>

<p>当我们启动<code>mydb</code>和<code>myservice</code>两个服务后,我们可以看到初始容器完成并且<code>myapp-pod</code> pod被创建.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f services.yaml</span><br><span class="line"></span><br><span class="line">service/myservice created</span><br><span class="line">service/mydb created</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get -f myapp.yaml</span><br><span class="line">NAME        READY     STATUS    RESTARTS   AGE</span><br><span class="line">myapp-pod   1/1       Running   0          9m</span><br></pre></td></tr></table></figure>

<p>这些示例非常简单但是应该能为你创建自己的初始容器提供一些灵感</p>
<h2 id="行为细节"><a href="#行为细节" class="headerlink" title="行为细节"></a>行为细节</h2><ul>
<li>在启动pod的过程中,在存储卷和网络创建以后,初始容器依次创建.上一个容器必须返回成功下一个才能启动,如果由于运行时错误或者其它异常退出,它会依照<code>restartPolicy</code>来重试,然而,如果<code>restartPolicy</code>设置为<code>Always</code>,初始容器实际上使用的是<code>OnFailure</code>策略</li>
<li>如果pod重启了,则所有的初始容器要重新执行</li>
<li>对初始容器的<code>spec</code>的更改仅限于<code>镜像(image)</code>字段的修改,更改了初始容器的镜像字段相当于重启pod</li>
<li>由于初始容器可以被重启,重试和重新执行,因此它里面的代码应当是幂等的,尤其是写入文件到<code>EmptyDirs</code>的代码应当注意文件可能已经存在</li>
<li>容器中的所有初始容器和普通容器名称必须惟一.</li>
</ul>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>基于初始容器的执行顺序,以下关于资源的规则适用:</p>
<ul>
<li><p>对于特定资源,所有初始容器申请的最高的生效</p>
</li>
<li><p>对于pod,相同资源申请取以下两者较高的一个:</p>
<p>1) 所有普通应用容器申请的资源总和<br>2) 初始容器申请的生效的资源(上面说到,初始容器申请资源取所有初始容器申请最大的一个)</p>
</li>
<li><p>调度基于生效的初始请求,这就意味着初始容器可以申请预留资源,即便在pod以后的整个生命周期都用不到</p>
</li>
</ul>
<h2 id="pod重启原因"><a href="#pod重启原因" class="headerlink" title="pod重启原因"></a>pod重启原因</h2><p>一个pod基于以下列出的原因,会重启,重新执行初始容器:</p>
<ul>
<li>用户更新初始容器的<code>PodSpec</code>导致镜像发生改变.普通应用容器改变只会使应用容器重启</li>
<li>由于<code>restartPolicy</code>被设置为<code>Always</code>,导致所有容器均被中止,强制重启,由于垃圾回收初始容器的初始状态记录丢失</li>
</ul>
<h4 id="定义pod-postStart或preStop"><a href="#定义pod-postStart或preStop" class="headerlink" title="定义pod postStart或preStop"></a>定义pod postStart或preStop</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master01 manifests]# cat poststart-pod.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: poststart-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: buxybox-httpd</span><br><span class="line">    image: busybox</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;mkdir&quot;, &quot;-p&quot;,&quot; /data/web/html&quot;]</span><br><span class="line">    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 3600&quot;]</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    
	
	<div>
	  
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">--------------------本文结束，感谢您的阅读--------------------</div>
    
</div>
	  
	</div>
	
	<div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/Kubernetes技巧及常用组件了解/">Kubernetes技巧及常用组件了解</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 弓昭 的个人博客">弓昭</a></p>
  <p><span>发布时间:</span>2019年08月04日 - 11:06</p>
  <p><span>最后更新:</span>2020年04月08日 - 22:20</p>
  <p><span>原始链接:</span><a href="/Kubernetes技巧及常用组件了解/" title="Kubernetes技巧及常用组件了解">https://gongzhao1.coding.me/Kubernetes技巧及常用组件了解/</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://gongzhao1.coding.me/Kubernetes技巧及常用组件了解/" aria-label="复制成功！"></i></span>
  </p>
  <p><span>联系邮箱:</span>gongzhao1@foxmail.com</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
	});
    });  
</script>

      
	</div>
	


    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat.jpg" alt="弓昭 WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/ali.jpg" alt="弓昭 Alipay">
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/kubernetes/" rel="tag"><i class="fa fa-tag"></i>kubernetes</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/kubernetes之helm详解/" rel="next" title="kubernetes之helm详解">
                <i class="fa fa-chevron-left"></i> kubernetes之helm详解
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Kubernetes部署nginx服务器/" rel="prev" title="Kubernetes部署nginx服务器">
                Kubernetes部署nginx服务器 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC80MTcyNy8xODI3Mw=="></div>
    
  </div>



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/uploads/panda.jpg" alt="弓昭">
          <p class="site-author-name" itemprop="name">弓昭</p>
           
              <p class="site-description motion-element" itemprop="description">弓昭的个人主页，主要涉及网络、运维、前端、Python等等知识</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">85</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">74</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/gongzhao1" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://blog.csdn.net/zhao12795969" target="_blank" title="csdn">
                  
                    <i class="fa fa-fw fa-crosshairs"></i>
                  
                    
                      csdn
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.jianshu.com/u/1dbca13c6043" target="_blank" title="简书">
                  
                    <i class="fa fa-fw fa-heartbeat"></i>
                  
                    
                      简书
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://ccieh3c.com/" title="网络之路" target="_blank">网络之路</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://jm.taobao.org/" title="阿里云中间件" target="_blank">阿里云中间件</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#API"><span class="nav-number">1.</span> <span class="nav-text">API</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#集群外部"><span class="nav-number">1.1.</span> <span class="nav-text">集群外部:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集群内部"><span class="nav-number">1.2.</span> <span class="nav-text">集群内部:</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ConfigMap"><span class="nav-number">2.</span> <span class="nav-text">ConfigMap</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#创建ConfigMap"><span class="nav-number">2.1.</span> <span class="nav-text">创建ConfigMap</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#通过命令行参数-from-literal创建"><span class="nav-number">2.1.1.</span> <span class="nav-text">通过命令行参数--from-literal创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指定文件创建"><span class="nav-number">2.1.2.</span> <span class="nav-text">指定文件创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指定目录创建"><span class="nav-number">2.1.3.</span> <span class="nav-text">指定目录创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#通过事先写好configmap的标准yaml文件创建"><span class="nav-number">2.1.4.</span> <span class="nav-text">通过事先写好configmap的标准yaml文件创建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用ConfigMap"><span class="nav-number">2.2.</span> <span class="nav-text">使用ConfigMap</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#通过环境变量使用"><span class="nav-number">2.2.1.</span> <span class="nav-text">通过环境变量使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在启动命令中引用"><span class="nav-number">2.2.2.</span> <span class="nav-text">在启动命令中引用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#作为volume挂载使用"><span class="nav-number">2.2.3.</span> <span class="nav-text">作为volume挂载使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度解析mountPath-subPath-key-path的关系和作用"><span class="nav-number">2.3.</span> <span class="nav-text">深度解析mountPath,subPath,key,path的关系和作用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mountPath结合subPath作用"><span class="nav-number">2.3.1.</span> <span class="nav-text">mountPath结合subPath作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有subPath但筛选结果为false"><span class="nav-number">2.3.2.</span> <span class="nav-text">有subPath但筛选结果为false,</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#无-subPath-path相当于重命名"><span class="nav-number">2.3.3.</span> <span class="nav-text">无 subPath,path相当于重命名</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有subPath且筛选结果为true-mouthPath指定文件名，可以和subPath不一样"><span class="nav-number">2.3.4.</span> <span class="nav-text">有subPath且筛选结果为true,mouthPath指定文件名，可以和subPath不一样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#configmap的热更新研究"><span class="nav-number">2.3.5.</span> <span class="nav-text">configmap的热更新研究</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ServiceAccount"><span class="nav-number">3.</span> <span class="nav-text">ServiceAccount</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#创建Service-Account"><span class="nav-number">3.1.</span> <span class="nav-text">创建Service Account</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#授权"><span class="nav-number">3.2.</span> <span class="nav-text">授权</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#示例"><span class="nav-number">3.3.</span> <span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Service"><span class="nav-number">4.</span> <span class="nav-text">Service</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概念"><span class="nav-number">4.1.</span> <span class="nav-text">概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Service的实现模型"><span class="nav-number">4.1.1.</span> <span class="nav-text">Service的实现模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#userspace代理模式"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">userspace代理模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#iptables代理模式"><span class="nav-number">4.1.1.2.</span> <span class="nav-text">iptables代理模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ipvs代理模式"><span class="nav-number">4.1.1.3.</span> <span class="nav-text">ipvs代理模式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Service的定义"><span class="nav-number">4.2.</span> <span class="nav-text">Service的定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Service字段含义"><span class="nav-number">4.2.1.</span> <span class="nav-text">Service字段含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#service的类型"><span class="nav-number">4.2.2.</span> <span class="nav-text">service的类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ClusterIP的service类型演示"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">ClusterIP的service类型演示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NodePort的service类型演示"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">NodePort的service类型演示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pod的会话保持"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">Pod的会话保持</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Headless-Service"><span class="nav-number">4.3.</span> <span class="nav-text">Headless Service</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ingress"><span class="nav-number">5.</span> <span class="nav-text">Ingress</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#相关组件关系"><span class="nav-number">5.1.</span> <span class="nav-text">相关组件关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#概述"><span class="nav-number">5.2.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署"><span class="nav-number">5.3.</span> <span class="nav-text">部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#更改暴露方式"><span class="nav-number">5.3.1.</span> <span class="nav-text">更改暴露方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建一个tomcat并用ingress7层代理转发"><span class="nav-number">5.4.</span> <span class="nav-text">创建一个tomcat并用ingress7层代理转发</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#用ingress来代理4层请求"><span class="nav-number">5.5.</span> <span class="nav-text">用ingress来代理4层请求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其它示例"><span class="nav-number">5.6.</span> <span class="nav-text">其它示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Single-Service-Ingress"><span class="nav-number">5.6.1.</span> <span class="nav-text">Single Service Ingress</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其于URL转发"><span class="nav-number">5.6.2.</span> <span class="nav-text">其于URL转发</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于名称的虚拟主机"><span class="nav-number">5.6.3.</span> <span class="nav-text">基于名称的虚拟主机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TLS"><span class="nav-number">5.6.4.</span> <span class="nav-text">TLS</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#StatefulSet"><span class="nav-number">6.</span> <span class="nav-text">StatefulSet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概述-1"><span class="nav-number">6.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#示例-1"><span class="nav-number">6.2.</span> <span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DNS"><span class="nav-number">7.</span> <span class="nav-text">DNS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍"><span class="nav-number">7.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SERVICE"><span class="nav-number">7.2.</span> <span class="nav-text">SERVICE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-records"><span class="nav-number">7.2.1.</span> <span class="nav-text">A records</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SRV-records"><span class="nav-number">7.2.2.</span> <span class="nav-text">SRV records</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pods"><span class="nav-number">7.3.</span> <span class="nav-text">Pods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-records-1"><span class="nav-number">7.3.1.</span> <span class="nav-text">A records</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pod’s-hostname-and-subdomain-fields"><span class="nav-number">7.3.2.</span> <span class="nav-text">Pod’s hostname and subdomain fields</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pod’s-DNS-Policy"><span class="nav-number">7.3.3.</span> <span class="nav-text">Pod’s DNS Policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pod’s-DNS-Config"><span class="nav-number">7.3.4.</span> <span class="nav-text">Pod’s DNS Config</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#调度之节点亲和性"><span class="nav-number">8.</span> <span class="nav-text">调度之节点亲和性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#滚动升级"><span class="nav-number">9.</span> <span class="nav-text">滚动升级</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#设置配额"><span class="nav-number">10.</span> <span class="nav-text">设置配额</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#配置Namespace资源限制"><span class="nav-number">10.1.</span> <span class="nav-text">配置Namespace资源限制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#配置容器资源限制"><span class="nav-number">10.2.</span> <span class="nav-text">配置容器资源限制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#测试内存限制"><span class="nav-number">10.2.1.</span> <span class="nav-text">测试内存限制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试CPU限制"><span class="nav-number">10.2.2.</span> <span class="nav-text">测试CPU限制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#容器服务质量（QoS）"><span class="nav-number">10.2.3.</span> <span class="nav-text">容器服务质量（QoS）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#不同QoS对容器影响"><span class="nav-number">10.2.3.1.</span> <span class="nav-text">不同QoS对容器影响</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResourceQuota"><span class="nav-number">10.3.</span> <span class="nav-text">ResourceQuota</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LimitRange"><span class="nav-number">10.4.</span> <span class="nav-text">LimitRange</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">10.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PV-amp-PVC"><span class="nav-number">11.</span> <span class="nav-text">PV &amp; PVC</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#emptyDir"><span class="nav-number">11.1.</span> <span class="nav-text">emptyDir</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hostPath"><span class="nav-number">11.2.</span> <span class="nav-text">hostPath</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PV-amp-PVC介绍"><span class="nav-number">11.3.</span> <span class="nav-text">PV&amp;PVC介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#生命周期"><span class="nav-number">11.4.</span> <span class="nav-text">生命周期</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Provisioning"><span class="nav-number">11.5.</span> <span class="nav-text">Provisioning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PV类型"><span class="nav-number">11.6.</span> <span class="nav-text">PV类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PV属性"><span class="nav-number">11.7.</span> <span class="nav-text">PV属性:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PV卷阶段状态："><span class="nav-number">11.8.</span> <span class="nav-text">PV卷阶段状态：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#示例-2"><span class="nav-number">11.9.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#回收策略"><span class="nav-number">11.10.</span> <span class="nav-text">回收策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#小结"><span class="nav-number">11.10.1.</span> <span class="nav-text">小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Context"><span class="nav-number">12.</span> <span class="nav-text">Context</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#StorageClass"><span class="nav-number">13.</span> <span class="nav-text">StorageClass</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#存储类介绍"><span class="nav-number">13.1.</span> <span class="nav-text">存储类介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#定义存储类"><span class="nav-number">13.1.1.</span> <span class="nav-text">定义存储类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#供应者"><span class="nav-number">13.1.2.</span> <span class="nav-text">供应者</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提供者的参数"><span class="nav-number">13.1.3.</span> <span class="nav-text">提供者的参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回收策略-1"><span class="nav-number">13.1.4.</span> <span class="nav-text">回收策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#挂接选项"><span class="nav-number">13.1.5.</span> <span class="nav-text">挂接选项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用存储类"><span class="nav-number">13.2.</span> <span class="nav-text">使用存储类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#启用动态供应"><span class="nav-number">13.2.1.</span> <span class="nav-text">启用动态供应</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用动态供应"><span class="nav-number">13.2.2.</span> <span class="nav-text">使用动态供应</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#默认行为"><span class="nav-number">13.2.3.</span> <span class="nav-text">默认行为</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NFS存储类示例"><span class="nav-number">13.3.</span> <span class="nav-text">NFS存储类示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#部署nfs-provisioner"><span class="nav-number">13.3.1.</span> <span class="nav-text">部署nfs-provisioner</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建StorageClass"><span class="nav-number">13.3.2.</span> <span class="nav-text">创建StorageClass</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建PersistenetVolumeClaim"><span class="nav-number">13.3.3.</span> <span class="nav-text">创建PersistenetVolumeClaim</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建使用PersistenVolumeClaim的部署"><span class="nav-number">13.3.4.</span> <span class="nav-text">创建使用PersistenVolumeClaim的部署</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#liveness和readiness探针"><span class="nav-number">14.</span> <span class="nav-text">liveness和readiness探针</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#livenessProbe"><span class="nav-number">14.1.</span> <span class="nav-text">livenessProbe</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#定义-liveness命令"><span class="nav-number">14.1.1.</span> <span class="nav-text">定义 liveness命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义一个liveness-HTTP请求"><span class="nav-number">14.1.2.</span> <span class="nav-text">定义一个liveness HTTP请求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义TCP-liveness探针"><span class="nav-number">14.1.3.</span> <span class="nav-text">定义TCP liveness探针</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#定义readiness探针"><span class="nav-number">14.2.</span> <span class="nav-text">定义readiness探针</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#配置Probe"><span class="nav-number">14.3.</span> <span class="nav-text">配置Probe</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#初始化容器"><span class="nav-number">15.</span> <span class="nav-text">初始化容器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#理解初始容器"><span class="nav-number">15.1.</span> <span class="nav-text">理解初始容器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始容器和普通容器的不同"><span class="nav-number">15.1.1.</span> <span class="nav-text">初始容器和普通容器的不同:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#初始容器能做什么"><span class="nav-number">15.2.</span> <span class="nav-text">初始容器能做什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#初始容器使用"><span class="nav-number">15.3.</span> <span class="nav-text">初始容器使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#行为细节"><span class="nav-number">15.4.</span> <span class="nav-text">行为细节</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#资源"><span class="nav-number">15.5.</span> <span class="nav-text">资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pod重启原因"><span class="nav-number">15.6.</span> <span class="nav-text">pod重启原因</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义pod-postStart或preStop"><span class="nav-number">15.6.0.1.</span> <span class="nav-text">定义pod postStart或preStop</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy;  2018 - 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">弓昭</span>
</div>


<div class="powered-by">
	<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
	  本站访客数:<span id="busuanzi_value_site_uv"></span>
	</span>
</div>


<div class="theme-info">

  <span class="post-count">博客全站共258.5k字</span>
</div>



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>


  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

</body>
</html>
